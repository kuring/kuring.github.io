[{"content":"背景 我的个人博客从 2013 年开始搭建，一直在使用 Github Pages 来承载，先后使用过 jekyll、farbox、hexo 等来构建静态博客，折腾过不少次。\n本次我决定对博客做如下的改动：\n使用 hugo 来构建静态博客。hexo 来管理博客总有些麻烦，涉及到很多依赖包的版本管理问题。我之前曾经尝试迁移到 hugo 来管理博客，但发现有些复杂，得需要全面了解 hugo 才能保证兼容性，比如 hexo 和 hugo 中对于博客文章的 url 链接并不相同，如果博客文章之间有超链接的情况，就会导致死链的出现。AI 时代到来后，Cursor 是我一直在深度使用的编程工具，心血来潮决定使用 Cursor 来完成博客的迁移。在整体迁移的过程中，我并未对 hugo 系统有较深的了解，甚至官方的文档都没怎么阅读，仅靠跟 Cursor 的交互就完成了所有的需求。 放弃域名 kuring.me。博客我一直写的不太多，也没啥流量。再加上 AI 的兴起，我的很多笔记类的博客文章反而更没有必要了。个人的一些感悟也有微信公账号可以承载。我决定弃用之前一直在使用的域名 kuring.me，但放弃域名我并不打算放弃自己的博客。 Github Pages 的访问速度较慢，决定同时寻找其他解决方案来加快博客在国内的访问速度。 迁移工作涉及：\n所有已发布文章 草稿文章 静态资源文件（图片、参考文档等） 保持原有 URL 结构不变（SEO 友好） 配置 GitHub Actions 自动部署到 GitHub Pages 配置 Cloudflare Pages 部署（提升中国大陆访问速度） 迁移步骤 1. 需求分析和计划制定 首先，我向 Cursor 描述了完整的迁移需求：\n1 2 3 4 5 6 7 8 9 我想将我的博客从 hexo 迁移到 hugo，请帮我完成整体的迁移。 hexo 博客系统的信息： 1. 本地目录：~/github/kuring/kuring.github.io hugo 博客系统为全新创建： 1. 本地目录：~/github/kuring/hugo 2. 我期望将该项目同步到 github 的 kuring/hugo 的 main 分支上 3. 当 kuring/hugo 的 main 分支提交后自动构建博客，并使用 github actions发布到 kuring/kuring.github.io 的 gh-pages 分支。 Cursor 在 Plan Mode 下分析了需求，并提出了几个关键问题：\n主题选择（我选择了 Stack 主题） URL 格式保持策略（我选择保持原格式） 2. 项目初始化 Cursor 自动执行了以下步骤：\n1 2 3 4 5 # 初始化 Hugo 项目 hugo new site . --force # 安装 Stack 主题 git submodule add git@github.com:CaiJimmy/hugo-theme-stack.git themes/hugo-theme-stack 3. 配置文件迁移 Cursor 分析了 Hexo 的 _config.yml，并创建了对应的 hugo.yaml：\n站点基本信息（标题、语言等） URL 格式配置（保持 /post/:title/ 格式） Giscus 评论系统配置 分页、标签、分类等设置 4. 内容迁移 这是最复杂的部分，Cursor 处理了：\n4.1 文章迁移 将所有 markdown 文件从 source/_posts 复制到 content/post 保持原有的目录结构（k8s/、ai/、knowledge-share/ 等） 处理 front matter 格式转换： url: → slug:（Hugo 使用 slug 字段） tags: value → tags: [value]（转换为列表格式） 统一 Tags: 为 tags: 4.2 草稿迁移 原先博客中有很多草稿类型的文档，这些文档并不会真正发布，但需要一起迁移到 hugo 中。\n复制 source/_drafts 到 content/draft 为所有草稿文件添加 draft: true 标记 4.3 静态资源迁移 source/images/ → static/images/ source/ref/ → static/ref/ source/draw.io/ → static/draw.io/ source/CNAME → static/CNAME 5. 格式修复 迁移过程中发现了一些格式问题，Cursor 自动创建了 Python 脚本来批量修复：\n1 2 3 4 5 6 # 修复 tags 格式的脚本示例 def fix_front_matter(content): # 将 url: 转换为 slug: # 将 tags: value 转换为 tags: [value] # 统一 Tags: 为 tags: ... 总共修复了 177 篇发布文章和 4 篇草稿的格式问题。\n6. GitHub Actions 配置 Cursor 创建了 .github/workflows/deploy.yml，实现了：\n当 main 分支有 push 时自动触发 使用 Hugo 构建静态站点 跨仓库部署到 kuring/kuring.github.io 的 gh-pages 分支 使用 Personal Access Token 实现跨仓库权限 7. Cloudflare Pages 配置 由于 GitHub Pages 在中国大陆访问速度较慢，我决定同时配置 Cloudflare Pages 部署。\n在配置完成 Cloudflare Pages 后发现，在中国大陆完全无法访问，但在公司网络正好可以访问，而且访问速度还比较快。\n这需要：\n7.1 多平台部署兼容性检查 首先，需要确保博客配置支持多平台部署：\nbaseurl 配置：设置为 /，让 Hugo 根据实际访问域名自动生成 URL Giscus 评论系统：使用 GitHub 仓库存储评论，两个平台共享同一套评论 硬编码 URL 修复：将代码示例中的绝对 URL 改为相对路径 7.2 Cloudflare Pages 配置步骤 连接 GitHub 仓库：\n在 Cloudflare Dashboard 中创建 Pages 项目 连接 kuring/hugo 仓库 构建设置：\nFramework preset: Hugo Build command: git submodule update --init --recursive \u0026amp;\u0026amp; hugo --minify Build output directory: public 设置 HUGO_VERSION 环境变量为具体版本号（如 0.153.3，不要使用 latest，可以使用 hugo version 命令查看本地使用 hugo 版本） 自动部署：\nProduction branch: main 启用 Preview deployments（每个 PR 自动创建预览） 7.3 多平台部署的优势 GitHub Pages：kuring.github.io - 作为主要部署平台 Cloudflare Pages：kuring.pages.dev - 提供更快的中国大陆访问速度 共享评论系统：两个平台使用同一套 Giscus 评论，评论数据统一管理 URL 兼容性：使用相对路径和正确的 baseurl 配置，确保两个平台都能正常工作 8. URL 兼容性保证 这是迁移中最关键的部分。Cursor 确保了：\n使用 permalinks.post: /post/:slug/ 保持 URL 格式 对于有 url: 字段的文章，转换为 slug: 字段 所有文章的 URL 与 Hexo 版本完全一致 配置 baseurl: / 支持多平台部署（GitHub Pages 和 Cloudflare Pages） 结语 使用 Cursor 较为完美的帮我完成了迁移工作。仅需要在本地执行 git push 命令即可完成博客的发布操作。\n如果在没有 AI 辅助的情况下，要完成博客系统的迁移我至少要花上一天的时间，而使用 cursor 前后差不多用了一个小时的时间就完成了，可以说效率大幅度的提升。\n美中不足的是博客系统的访问，需要尝试 kuring.github.io 或者 kuring.pages.dev 两个域名，但因为博客与我而言并非高频访问场景，且用户量极少，该问题暂且可以忍受。\n","date":"2025-12-28T00:00:00Z","permalink":"/post/hexo-to-hugo-migration-with-cursor/","title":"使用 Cursor 完成 Hexo 到 Hugo 博客迁移实践"},{"content":"1 MVC 结构 分为了 Model（模型层）、View（视图层）、Controller（控制器层）。\n1.1 Model（模型层） 职责：表示应用程序的数据结构和业务逻辑。 包含内容：\n数据实体类（如 User, Product） 数据访问逻辑（如数据库 CRUD） 业务逻辑方法（如计算、状态切换） 1.2 View（视图层） 职责：负责数据的展示和用户界面。 包含内容：\n页面模板（HTML / XML / JSX 等） UI 渲染逻辑 与用户交互（输入、按钮等） 1.3 Controller（控制器层） 职责：接收用户请求，调用模型处理数据，并选择视图进行展示。 包含内容：\n路由逻辑 请求分发 控制流程协调（调用 Model → 返回给 View） 2 MVC 的交互流程（经典模式） 一个典型的用户请求处理流程如下：\n用户交互： 用户在 View 上进行操作（例如，点击提交按钮）。 请求路由： View 将用户的输入/事件（例如，表单数据）传递给对应的 Controller（通常由框架的路由机制完成）。 Controller 处理： Controller 解析用户输入。 Controller 调用一个或多个 Model 对象的方法： 可能更新 Model 的状态（例如，将表单数据保存到数据库）。 可能从 Model 获取数据。 Model 更新与通知： Model 执行业务逻辑，更新自身状态（可能涉及数据库操作等）。 Model 通知所有注册的观察者（通常是依赖它的 View）状态已改变（这一步是观察者模式，在被动视图变体中可能由 Controller 显式通知）。 View 更新： View 接收到 Model 状态改变的通知（或由 Controller 明确告知）。 View 从 Model 查询最新的数据。 View 使用新数据重新渲染用户界面。 用户看到更新： 用户看到更新后的 View。 3 MVC 的变体 3.1 更精细划分 很多项目是前后端分离的，后端负责暴露接口给前端调用。为了更好地解耦和扩展，很多项目在经典 MVC 基础上再进一步划分层次，例如：\n1 2 3 4 5 6 7 8 9 ┌────────────────────┐ │ Controller │ ← 接收请求，协调流程 ├────────────────────┤ │ Service Layer │ ← 业务逻辑（可选扩展层） ├────────────────────┤ │ Model Layer │ ← 实体数据结构 / ORM / 数据访问 ├────────────────────┤ │ Repository DAO │ ← 数据库交互（封装 SQL / ORM） └────────────────────┘ ","date":"2025-06-08T00:00:00Z","permalink":"/post/mvc-%E6%9E%B6%E6%9E%84/","title":"MVC 架构"},{"content":"1 核心概念 1.1 战略设计 从业务视角出发，建立业务领域模型，划分领域边界，建立通用语言的限界上下文，限界上下文可以作为微服务设计的参考边界。 战略设计主要流程包括：建立统一语言、领域分解、领域建模。\n1.2 战术设计 从技术视角出发，侧重于领域模型的技术实现，完成软件开发和落地。\n1.2.1 核心概念 1.2.2 实体 具有唯一标识符 ID 和生命周期的对象。状态会随时间变化，但标识符不会变化。实体通常会封装跟自身状态相关的核心业务逻辑和不变规则。\n1.2.3 值对象 描述事物某些特征或者属性的对象。 具有如下特点：\n无标识。 不可变。一旦创建后属性值不能再改变。 通常用来描述实体的属性，比如 Customer 实体有一个 Address 值对象。 可组合。值对象可以包含其他的值对象。Address 可能包含 Street, City, ZipCode 等更小的值对象。 例子： Money (金额 + 货币), Address (街道、城市、邮编), Color (RGB值), DateRange (开始日期 + 结束日期), ProductInfo (产品ID、名称、单价 - 用于订单项中)。\n1.2.4 聚合 \u0026amp; 聚合根 聚合将一组强关联的实体和值对象组合在一起，形成一个一致性边界和一个事务边界。每个聚合有一个聚合根。\n聚合根：是聚合的唯一入口点，通常是一个实体。\n例子： Order (聚合根) 包含：\n自身属性 (订单号、下单时间、状态、总金额) OrderItem (实体) 列表 - 每个订单项有自己的 ProductId, ProductName, UnitPrice, Quantity, LineTotal (值对象？) ShippingAddress (值对象) BillingAddress (值对象) 要修改某个 OrderItem 的数量，必须调用 Order 聚合根的 ChangeItemQuantity(OrderItemId, NewQuantity) 方法。Order 会检查新数量是否有效，重新计算该订单项金额和订单总金额。 1.2.5 领域服务 当某个操作或业务逻辑不适合放在实体或值对象内部时，将其封装在领域服务中。它代表了一个无状态的操作或过程。\n场景：\n操作涉及多个聚合/实体的协调。(例如：TransferService 处理银行转账，需要操作 源账户 和 目标账户 两个聚合根)。 操作本身是一个无状态的计算或转换。(例如：复杂的 RiskAssessmentService 计算贷款风险)。 需要调用外部系统或基础设施（但核心逻辑仍在领域层）。(例如：NotificationService 封装发送通知的规则，实际发送动作可能由基础设施层实现)。 关键特征：\n无状态： 服务本身不持有业务状态。 领域概念： 服务执行的操作本身是领域专家关心的核心业务概念（如“转账”、“风险评估”）。 接口定义在领域层： 具体实现在领域层或基础设施层（如果需要访问外部资源）。 与“应用服务”的区别：\n应用服务： 位于应用层，负责协调领域对象、领域服务、仓储、事务、权限、外部调用等，完成一个用户用例（Use Case）。它更偏重流程协调和技术层面。 领域服务： 位于领域层，封装了核心的、无法放入实体/值对象的领域逻辑。它只关心业务规则。 1.2.6 Repository （仓储或资源库） 只用于聚合根！ 提供类似集合（Collection）的接口，负责聚合的持久化（保存）和检索（查询）。\n关键特征：\n聚合根入口： 只负责聚合根的持久化和加载。加载时，会重建整个聚合（包含内部实体和值对象）。保存时，保存整个聚合的变更。 领域层接口： 仓储的接口定义在领域层，因为它表达的是领域模型需要什么样的持久化能力（如 IOrderRepository 定义 Add(Order order), GetById(OrderId id), Save(Order order) 等方法）。 基础设施层实现： 仓储的具体实现在基础设施层（如 SqlOrderRepository, MongoOrderRepository）。它知道如何操作数据库、缓存、文件系统等。 解耦： 领域层只依赖于仓储接口，完全不知道底层存储细节（SQL, NoSQL, File）。这符合依赖倒置原则（DIP）。 查询分离： 仓储通常只提供基于聚合根ID的简单查询。复杂的查询（跨聚合、报表）建议使用单独的查询层（如CQRS模式中的Query Side），避免污染领域模型和仓储。 1.2.7 工厂 负责封装复杂对象（尤其是聚合）创建逻辑的对象或方法。\n适用场景：\n对象的创建过程很复杂，涉及多个步骤、规则校验、依赖组合，不适合放在构造函数中（构造函数应尽量简单）。 需要解耦创建逻辑和使用逻辑。 需要根据条件创建不同的实现（结合抽象工厂模式）。 形式：\n独立工厂类： OrderFactory.createOrder(customer, items, ...) 聚合根上的工厂方法： Order.createDraft(customer, ...) (静态方法) 作用： 将复杂的构造逻辑集中管理，保持客户端代码和领域对象（尤其是聚合根）的简洁。\n1.2.8 领域事件 表示在领域中发生的、对业务有重要意义的事件。\n关键特征：\n过去时： 事件名通常是过去时态 (如 OrderPlaced, PaymentReceived, InventoryLow)。 包含信息： 包含事件发生时相关的数据 (如 OrderPlaced 事件包含 OrderId, CustomerId, OrderItems, TotalAmount, Timestamp)。 由聚合根发布： 通常由聚合根在其状态发生重要变更后发布。发布事件是聚合根业务操作的一部分。 轻量级通知： 事件本身不包含“如何处理”的逻辑，它只是一个通知。 作用：\n解耦限界上下文： 最重要的作用！ 一个上下文内的聚合根发布事件，其他上下文可以订阅这些事件并触发本地操作，实现松耦合的集成。这是实现最终一致性的基础。(例如：订单上下文 发布 OrderConfirmed 事件，库存上下文 订阅该事件并扣减库存，物流上下文 订阅该事件并安排发货)。 驱动内部流程： 同一个聚合或限界上下文内，事件可以触发后续步骤。(例如：Order 支付成功后发布 PaymentReceived 事件，触发自身状态变更为 已支付 并发布 OrderPaid 事件)。 审计追踪： 记录系统中发生的重要业务事实。 CQRS 读模型更新： 更新用于查询的读模型。 实现： 通常需要基础设施支持（事件总线、消息队列）来实现可靠的事件发布和订阅。\n2 分层架构 严格分层架构：某层只能与直接位于的下层发生耦合。 松散分层架构：允许上层与任意下层发生耦合。\n在领域驱动设计（DDD）中采用的是松散分层架构，层间关系不那么严格。每层都可能使用它下面所有层的服务，而不仅仅是下一层的服务。\n2.1 应用层 负责编排、转发、校验等，该层应该尽可能的做薄，它知道“做什么”（流程步骤），但不知道“怎么做”（具体业务规则怎么做在领域层）。\n职责：\n协调者： 编排领域对象（实体、值对象、聚合根）、领域服务、资源库等，完成一个具体的用户用例 (Use Case) 或系统任务。它代表一个业务场景的完整流程。 事务管理： 通常负责定义和管理事务边界（确保一个用例内的操作要么全成功，要么全失败）。 安全认证： 执行权限检查（用户是否有权执行此操作？）。 基础验证： 执行简单的、不依赖领域上下文的验证（如 ID 是否存在）。 发布领域事件： 接收领域层产生的事件，并负责将其发布到事件总线/消息队列（通常委托给基础设施层）。 返回结果： 将执行结果（通常是 DTO）返回给用户界面层，或处理异步事件。 包含的内容：\n应用服务： 这是这一层的核心组件。每个应用服务方法通常对应一个用户用例或一个原子性系统任务。方法名通常是动词，描述操作（如 PlaceOrderService.execute(PlaceOrderCommand)）。 命令处理器 / 查询处理器 (CQRS)： 如果采用 CQRS 模式，应用层会包含处理 Command 和 Query 的处理器。 DTO (输入/输出)： Command, Query, Response 等对象，用于层间数据传输。 2.2 领域层 领域层是绝对的核心，包含了实体、值对象、聚合与聚合根、领域服务、领域事件、仓储接口、工厂接口/实现。\n2.3 基础设施层 职责： 为上层提供具体的技术实现细节和与外部世界的交互能力。是系统的“工具箱”和“适配器”。\n包含的内容：\n仓储实现： 提供 IOrderRepository 等的具体实现（如 SqlOrderRepository, MongoOrderRepository），负责与数据库（SQL/NoSQL）、文件系统、缓存等持久化机制交互。 外部服务客户端实现： 调用第三方 API、支付网关、短信服务、邮件服务的具体实现。 消息通信： 实现消息队列（RabbitMQ, Kafka）的生产者/消费者，事件总线 (IEventBus 的实现)。 文件 I/O： 读写文件、操作存储（S3, Azure Blob）的具体代码。 网络通信： HTTP 客户端、RPC 客户端的封装。 配置管理： 读取配置文件、环境变量。 日志记录实现： ILogger 接口的具体实现（如 Log4Net, Serilog）。 身份认证/授权实现： 与 Auth0、OAuth2 服务器等集成的具体代码。 框架集成： ORM (Entity Framework, Hibernate), Web 框架 (ASP.NET Core, Spring Boot) 的特定配置和粘合代码。 2.4 各层的数据对象 每一层都有特定的数据：\nVO（View Object）：视图对象。 DTO（Data Transfer Object）：数据传输对象，主要用于远程调用等需要大量传输对象的地方。比如我们一张表有100个字段，那么对应的PO就有100个属性。但是我们界面上只要显示10个字段，客户端用WEB service来获取数据，没有必要把整个PO对象传递到客户端，这时我们就可以用只有这10个属性的DTO来传递结果到客户端，这样也不会暴露服务端表结构。 DO（Domain Object）：领域对象，就是从现实世界中抽象出来的有形或无形的业务实体。 PO（Persistent Object）：持久化对象，它跟持久层（通常是关系型数据库）的数据结构形成一一对应的映射关系，如果持久层是关系型数据库，那么，数据表中的每个字段（或若干个）就对应PO的一个（或若干个）属性。 3 DDD 架构的好处 3.1 传统的贫血模型 在 MVC 模型中，后端很容易分裂出 Repository层（数据访问）、Service层（业务逻辑）、Controller层（暴露接口）。很容易分裂出贫血模型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 ////////// Controller+VO(View Object) ////////// public class UserController { private UserService userService; //通过构造函数或者IOC框架注入 public UserVo getUserById(Long userId) { UserBo userBo = userService.getUserById(userId); UserVo userVo = [...convert userBo to userVo...]; return userVo; } } public class UserVo {//省略其他属性、get/set/construct方法 private Long id; private String name; private String cellphone; } ////////// Service+BO(Business Object) ////////// public class UserService { private UserRepository userRepository; //通过构造函数或者IOC框架注入 public UserBo getUserById(Long userId) { UserEntity userEntity = userRepository.getUserById(userId); UserBo userBo = [...convert userEntity to userBo...]; return userBo; } } public class UserBo {//省略其他属性、get/set/construct方法 private Long id; private String name; private String cellphone; } ////////// Repository+Entity ////////// public class UserRepository { public UserEntity getUserById(Long userId) { //... } } public class UserEntity {//省略其他属性、get/set/construct方法 private Long id; private String name; private String cellphone; } 在上述示例中，UserBo 只包含数据，不包含任何业务逻辑，业务逻辑集中在UserService中。Service层的数据和业务逻辑，被分割为BO和Service两个类中。像UserBo这样，只包含数据，不包含业务逻辑的类，就叫作贫血模型（Anemic Domain Model）。\n这种贫血模型将数据与操作分离，破坏了面向对象的封装特性，是一种典型的面向过程的编程风格。\n几乎所有的 Web 类项目都是按照贫血模型来开发的。\n3.2 基于 DDD 的充血模型 在充血模型中数据和对应的业务逻辑被封装到同一个类中。在充血模型中，Service层包含Service类和Domain类两部分。Domain就相当于贫血模型中的BO。不过，Domain与BO的区别在于它是基于充血模型开发的，既包含数据，也包含业务逻辑。而Service类变得非常单薄。\n4 项目实战 下面是一个订单管理系统的代码示例。\n目录结构如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 order-ddd/ ├── application/ # 应用层 │ └── OrderApplicationService.java ├── domain/ # 领域层 │ ├── model/ # 实体、值对象、聚合根 │ │ ├── Order.java │ │ ├── OrderItem.java │ │ └── Customer.java │ ├── repository/ # 仓储接口 │ │ └── OrderRepository.java │ └── service/ # 领域服务 │ └── OrderDomainService.java ├── infrastructure/ # 基础设施层 │ ├── persistence/ # 数据库持久化实现 │ │ └── OrderRepositoryImpl.java │ └── event/ # 事件发布等 │ └── OrderEventPublisher.java └── interfaces/ # 用户接口层（如 REST API） └── OrderController.java 4.1 领域层 4.1.1 领域模型层 Order.java 聚合根\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package com.example.order.domain.model; import java.util.List; import java.util.UUID; public class Order { private String orderId; private String customerId; private List\u0026lt;OrderItem\u0026gt; items; private OrderStatus status; public Order(String customerId, List\u0026lt;OrderItem\u0026gt; items) { this.orderId = UUID.randomUUID().toString(); this.customerId = customerId; this.items = items; this.status = OrderStatus.CREATED; } public void confirm() { if (status != OrderStatus.CREATED) { throw new IllegalStateException(\u0026#34;Only created orders can be confirmed.\u0026#34;); } this.status = OrderStatus.CONFIRMED; } // Getters } OrderItem.java - 值对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package com.example.order.domain.model; public class OrderItem { private String productId; private int quantity; private double price; public OrderItem(String productId, int quantity, double price) { this.productId = productId; this.quantity = quantity; this.price = price; } public double getTotalPrice() { return quantity * price; } // Getters } Customer.java - 实体\n1 2 3 4 5 6 7 8 9 10 11 12 13 package com.example.order.domain.model; public class Customer { private String id; private String name; public Customer(String id, String name) { this.id = id; this.name = name; } // Getters } OrderStatus.java - 枚举\n1 2 3 4 5 6 7 package com.example.order.domain.model; public enum OrderStatus { CREATED, CONFIRMED, CANCELLED } 4.1.2 领域仓储层 OrderRepository.java - 领域层仓储接口\n1 2 3 4 5 6 7 8 9 package com.example.order.domain.repository; import com.example.order.domain.model.Order; public interface OrderRepository { void save(Order order); Order findById(String orderId); Order update(Order order); } 4.1.3 领域服务层 OrderDomainService.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 package com.example.order.domain.service; import com.example.order.domain.model.Order; import com.example.order.domain.model.OrderItem; import com.example.order.domain.model.OrderStatus; import com.example.order.domain.repository.OrderRepository; import java.util.List; public class OrderDomainService { public boolean validateOrderItems(List\u0026lt;OrderItem\u0026gt; items) { for (OrderItem item : items) { if (item.getQuantity() \u0026lt;= 0 || item.getPrice() \u0026lt;= 0) { return false; } } return true; } public void validateOrderStatusForConfirm(Order order) { if (order.getStatus() != OrderStatus.CREATED) { throw new IllegalStateException(\u0026#34;Order can only be confirmed if it is in CREATED status.\u0026#34;); } } public boolean isOrderValidForCreate(String customerId, List\u0026lt;OrderItem\u0026gt; items) { return validateOrderItems(items); } public void confirmOrder(Order order) { validateOrderStatusForConfirm(order); order.confirm(); } } 4.2 应用层 OrderApplicationService.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 package com.example.order.application; import com.example.order.domain.model.Customer; import com.example.order.domain.model.Order; import com.example.order.domain.repository.OrderRepository; import com.example.order.infrastructure.event.OrderEventPublisher; import java.util.List; public class OrderApplicationService { private final OrderRepository orderRepository; private final OrderEventPublisher eventPublisher; public OrderApplicationService(OrderRepository orderRepository, OrderEventPublisher eventPublisher) { this.orderRepository = orderRepository; this.eventPublisher = eventPublisher; } public String createOrder(String customerId, List\u0026lt;OrderItem\u0026gt; items) { Order order = new Order(customerId, items); orderRepository.save(order); eventPublisher.publishOrderCreatedEvent(order.getOrderId()); return order.getOrderId(); } public void confirmOrder(String orderId) { Order order = orderRepository.findById(orderId); order.confirm(); orderRepository.update(order); eventPublisher.publishOrderConfirmedEvent(orderId); } } 4.3 基础设施层（infrastructure） OrderRepositoryImpl.java - 模拟数据库操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package com.example.order.infrastructure.persistence; import com.example.order.domain.model.Order; import com.example.order.domain.repository.OrderRepository; import java.util.HashMap; import java.util.Map; public class OrderRepositoryImpl implements OrderRepository { private Map\u0026lt;String, Order\u0026gt; db = new HashMap\u0026lt;\u0026gt;(); @Override public void save(Order order) { db.put(order.getOrderId(), order); } @Override public Order update(Order order) { return db.put(order.getOrderId(), order); } @Override public Order findById(String orderId) { return db.get(orderId); } } OrderEventPublisher.java - 发布事件\n1 2 3 4 5 6 7 8 9 10 11 package com.example.order.infrastructure.event; public class OrderEventPublisher { public void publishOrderCreatedEvent(String orderId) { System.out.println(\u0026#34;Order Created Event Published: \u0026#34; + orderId); } public void publishOrderConfirmedEvent(String orderId) { System.out.println(\u0026#34;Order Confirmed Event Published: \u0026#34; + orderId); } } 4.4 接口层 OrderController.java - 简单的控制层\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package com.example.order.interfaces; import com.example.order.application.OrderApplicationService; import com.example.order.domain.model.OrderItem; import java.util.List; public class OrderController { private final OrderApplicationService orderService; public OrderController(OrderApplicationService orderService) { this.orderService = orderService; } public String createOrder(String customerId, List\u0026lt;OrderItem\u0026gt; items) { return orderService.createOrder(customerId, items); } public void confirmOrder(String orderId) { orderService.confirmOrder(orderId); } } 5 参考资料 领域驱动设计：从理论到实践，一文带你掌握DDD！ 极客时间 - 设计模式之美 - 业务开发常用的基于贫血模型的MVC架构违背OOP吗？ awesome-ddd ","date":"2025-06-08T00:00:00Z","permalink":"/post/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1-ddd/","title":"领域驱动设计 DDD"},{"content":"1 特性 1.1 封装 封装是将对象的属性（数据）和方法（操作）包装在一起，并隐藏对象的内部实现细节，只暴露必要的接口给外部使用。\n1.2 继承 继承是指一个类（子类）可以继承另一个类（父类）的属性和方法，从而实现代码复用和功能扩展。\n1.3 多态 多态指相同的接口或方法调用，可以表现出不同的行为，具体行为取决于对象的实际类型。\n2 SOLID 原则 2.1 开闭原则 OCP 定义：软件实体（类、模块、函数）应当对扩展开放，对修改关闭。\n简单理解：不要修改已有代码，而是增加代码来扩展行为。\n2.2 单一职责原则 SRP 定义：一个类只有一个引起他变化的原因。\n简单理解：一个类或者模块应该仅做一件事，并且将这件事情做好。\n典型代码实例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 // 职责1: 只负责生成报告核心内容 public class ReportGenerator { private String data; public ReportGenerator(String data) { this.data = data; } public String generate() { // ... 核心生成逻辑 ... return reportContent; } } // 职责2: 只负责报告格式转换 public interface ReportFormatter { String format(String content); } public class HtmlFormatter implements ReportFormatter { @Override public String format(String content) { // ... HTML格式化逻辑 ... return formattedContent; } } public class PdfFormatter implements ReportFormatter { @Override public String format(String content) { // ... PDF格式化逻辑 (可能调用外部库) ... return formattedContent; } } // 职责3: 只负责报告持久化 public interface ReportRepository { void save(String content, String filename); } public class FileSystemRepository implements ReportRepository { @Override public void save(String content, String filename) { // ... 文件I/O操作 ... } } // 职责4: 只负责报告发送 public interface ReportSender { void send(String content, String recipient); } public class EmailReportSender implements ReportSender { @Override public void send(String content, String recipient) { // ... 邮件发送逻辑 ... } } // 高层模块/客户端代码 (组合各个职责) public class ReportService { private ReportGenerator generator; private ReportFormatter formatter; private ReportRepository repository; private ReportSender sender; // 通过构造器或Setter注入依赖 public ReportService(ReportGenerator generator, ReportFormatter formatter, ReportRepository repository, ReportSender sender) { this.generator = generator; this.formatter = formatter; this.repository = repository; this.sender = sender; } public void createAndSendReport(String filename, String recipient, String formatType) { // 1. 生成内容 String content = generator.generate(); // 2. 格式化 (根据formatType选择Formatter, 这里简化) String formattedContent = formatter.format(content); // 3. 保存 repository.save(formattedContent, filename); // 4. 发送 sender.send(formattedContent, recipient); } } 2.3 依赖倒置原则 DIP 定义：\n高层模块不应该依赖于低层模块，两者都应该依赖于抽象。 抽象不应该依赖于细节，细节应该依赖于抽象。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 // 步骤1: 定义抽象接口 (高层和低层都将依赖它) public interface DataRepository { void save(String data); } // 步骤2: 低层模块 - 具体实现1: MySQL public class MySQLDatabase implements DataRepository { @Override public void save(String data) { // ... 具体的 MySQL 保存逻辑 ... System.out.println(\u0026#34;Saving data to MySQL: \u0026#34; + data); } } // 步骤2: 低层模块 - 具体实现2: PostgreSQL (新增实现很容易) public class PostgreSQLDatabase implements DataRepository { @Override public void save(String data) { // ... 具体的 PostgreSQL 保存逻辑 ... System.out.println(\u0026#34;Saving data to PostgreSQL: \u0026#34; + data); } } // 步骤2: 低层模块 - 具体实现3: 文件系统 (新增实现很容易) public class FileSystemRepository implements DataRepository { @Override public void save(String data) { // ... 具体的文件保存逻辑 ... System.out.println(\u0026#34;Saving data to file: \u0026#34; + data); } } // 步骤3: 修改高层模块 - 只依赖抽象接口 DataRepository public class ReportService { private DataRepository repository; // 依赖抽象！ // 步骤5: 依赖注入 (通过构造函数) public ReportService(DataRepository repository) { this.repository = repository; // 接收注入的具体实现 } public void generateReport(String reportData) { // ... 生成报告的复杂业务逻辑 (核心策略) 不变 ... repository.save(reportData); // 通过接口调用 } } // 客户端使用 (负责组装对象，决定具体实现) public class Client { public static void main(String[] args) { // 选择并创建具体的低层实现 DataRepository mySqlRepo = new MySQLDatabase(); // DataRepository pgRepo = new PostgreSQLDatabase(); // DataRepository fsRepo = new FileSystemRepository(); // 将依赖注入高层模块 ReportService reportService = new ReportService(mySqlRepo); // 注入 MySQL // ReportService reportService = new ReportService(pgRepo); // 注入 PostgreSQL // ReportService reportService = new ReportService(fsRepo); // 注入文件系统 reportService.generateReport(\u0026#34;Important Report Data\u0026#34;); // 输出取决于注入的具体实现: // 注入 MySQL: Saving data to MySQL: Important Report Data // 注入 PostgreSQL: Saving data to PostgreSQL: Important Report Data // 注入 FileSystem: Saving data to file: Important Report Data } } 2.4 里氏替换原则 定义：子类型必须能够替换掉他们的父类型，程序的行为仍保持正确。\n通俗理解：子类不能违背父类的语义和契约。\n2.5 接口隔离原则 定义：客户端不应该被迫依赖它不使用的方法。\n通俗理解：一个接口只包含调用者真正需要的方法。\n","date":"2025-06-08T00:00:00Z","permalink":"/post/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","title":"面向对象"},{"content":"CQRS（Command Query Responsibility Segregation，命令查询职责分离）是一种在复杂业务系统中常用的架构模式，其核心思想是将“读操作”（Query）和“写操作”（Command）进行职责分离，从而提升系统的可维护性、扩展性和性能。\n命令（Command）：负责处理数据的更新操作（如创建、修改、删除），通常涉及业务逻辑和事务管理。 查询（Query）：仅用于读取数据，专注于高效的数据检索和展示，不包含业务逻辑。 1 具体示例 命令模型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // 1. 定义命令类 public class CreateOrderCommand { private String orderId; private String productId; private int quantity; // 构造函数、Getter/Setter } // 2. 命令处理服务 @Service public class OrderCommandService { @Autowired private OrderRepository orderRepository; @Autowired private InventoryRepository inventoryRepository; @Autowired private EventPublisher eventPublisher; public void handleCreateOrder(CreateOrderCommand command) { // 1. 创建订单 Order order = new Order(command.getOrderId(), command.getProductId(), command.getQuantity()); order.setStatus(\u0026#34;Pending\u0026#34;); orderRepository.save(order); // 2. 扣减库存 Inventory inventory = inventoryRepository.findByProductId(command.getProductId()); inventory.deduct(command.getQuantity()); inventoryRepository.save(inventory); // 3. 发布事件 eventPublisher.publish(new OrderCreatedEvent(command.getOrderId(), command.getProductId(), command.getQuantity())); } } 事件模型（Event Model）\n1 2 3 4 5 6 7 8 // 领域事件：订单创建事件 public class OrderCreatedEvent { private String orderId; private String productId; private int quantity; // 构造函数、Getter/Setter } 查询模型（Query Model）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // 1. 查询服务接口 public interface OrderQueryService { OrderView getOrderStatus(String orderId); } // 2. 查询视图类（订单状态缓存） public class OrderView { private String orderId; private String status; private LocalDateTime createdAt; // Getter/Setter } // 3. 查询服务实现 @Service public class OrderQueryServiceImpl implements OrderQueryService { @Autowired private RedisTemplate\u0026lt;String, OrderView\u0026gt; redisTemplate; @Override public OrderView getOrderStatus(String orderId) { return redisTemplate.opsForValue().get(\u0026#34;order:\u0026#34; + orderId); } // 订阅事件并更新缓存 @EventListener public void onOrderCreated(OrderCreatedEvent event) { OrderView orderView = new OrderView(); orderView.setOrderId(event.getOrderId()); orderView.setStatus(\u0026#34;Pending\u0026#34;); orderView.setCreatedAt(LocalDateTime.now()); redisTemplate.opsForValue().set(\u0026#34;order:\u0026#34; + event.getOrderId(), orderView); } } 事件总线（Event Bus）\n1 2 3 4 5 6 7 8 9 // 简化的事件发布接口 public interface EventPublisher { void publish(Event event); } // 简化的事件监听接口 public interface EventListener { void onEvent(Event event); } 1.1.1 工作流程 用户下单 客户端调用命令服务 OrderCommandService.handleCreateOrder()。 命令服务创建订单并扣减库存，发布 OrderCreatedEvent。 更新查询视图 查询服务 OrderQueryServiceImpl 监听到 OrderCreatedEvent。 将订单状态缓存到 Redis，生成 OrderView。 查询订单状态 客户端调用查询服务 OrderQueryService.getOrderStatus()。 查询服务从 Redis 快速返回订单状态。 ","date":"2025-06-08T00:00:00Z","permalink":"/post/%E5%91%BD%E4%BB%A4%E6%9F%A5%E8%AF%A2%E8%81%8C%E8%B4%A3%E5%88%86%E7%A6%BB-cqrs/","title":"命令查询职责分离 CQRS"},{"content":"通过 pyenv 命令可以管理本地的 python 版本和 python 环境（各类安装包）。\n安装 python 版本 mac 下目前已经无法通过 brew 命令安装 python2，可以使用 pyenv 工具来安装 python 2。\n1 2 brew install pyenv pyenv install 2.7.18 下载下来的 python 版本位于 ~/.pyenv/versions 目录下。\n创建 virtualenv 环境 需要先安装插件 pyenv-virtualenv：\n1 brew install pyenv-virtualenv 并在 ~/.zshrc 文件中新增加如下内容，并新打开终端窗口，确保插件被加载：\n1 2 3 4 5 # 添加 pyenv 和 pyenv-virtualenv 初始化 export PATH=\u0026#34;$HOME/.pyenv/bin:$PATH\u0026#34; eval \u0026#34;$(pyenv init --path)\u0026#34; eval \u0026#34;$(pyenv init -)\u0026#34; eval \u0026#34;$(pyenv virtualenv-init -)\u0026#34; 创建新的 python 环境 python2，并激活该环境：\n1 2 pyenv virtualenv 2.7.18 python2 pyenv activate python2 创建的环境会存在于 ~/.pyenv/versions 目录下。\n","date":"2025-03-05T00:00:00Z","permalink":"/post/pyenv-%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"pyenv 的使用"},{"content":" NLP：自然语言处理 Natural Language Processing 词向量：Word Vector，一种寻找词和词之间相似性的技术，词汇在各个维度上的特征用数值向量进行表示，利用这些维度上的特征相似程度，从而判断出词和词之间的相似程度。通常又叫做 ”词嵌入“（Word Embedding）。 词嵌入：Word Embedding。将词映射到向量空间的过程。 张量：概念源自物理学和数学中的张量分析，可以理解为多维数组的通用说法。可以是一维、或者 N 维的数组。 泛化：Generalization，模型在未见过数据上的表现能力。 过拟合：Overfitting，模型在训练数据上表现良好，但在新数据上表现不佳。 欠拟合：Underfitting，模型在训练数据上和新数据上都表现不佳。 One-Hot 编码：一种用于处理分类数据的编码方法，基本思想是将每个类别的特征表示为一个二进制向量，向量的长度等于类别的数量。对于每个样本，该样本属于的类别的位置上的值为1，其余位置的值为0。这种编码方式确保了每个样本的表示是稀疏的，即除了一个位置为1外，其他位置都是0。 ","date":"2025-02-06T00:00:00Z","permalink":"/post/ai-%E6%9C%AF%E8%AF%AD%E8%A1%A8%E4%B8%8D%E6%96%AD%E6%9B%B4%E6%96%B0%E4%B8%AD/","title":"AI 术语表（不断更新中）"},{"content":" 本文为对《GPT 图解 - 大模型是怎样构建的》一书的学习笔记，所有的例子和代码均来源于本书。\n基本介绍 Bag-of-Words 模型为早期的语言模型，诞生于 1954 年。\n核心思想：将文本中的词看作一个个独立的个体，不考虑在句子中的顺序，只关心词出现频次。\n应用场景：文本分析、情感分析\n代码实践 准备数据集 1 2 3 4 5 corpus = [\u0026#34;我特别特别喜欢看电影\u0026#34;, \u0026#34;这部电影真的是很好看的电影\u0026#34;, \u0026#34;今天天气真好是难得的好天气\u0026#34;, \u0026#34;我今天去看了一部电影\u0026#34;, \u0026#34;电影院的电影都很好看\u0026#34;] 对数据集进行分词 1 2 3 4 import jieba corpus_tokenized = [list(jieba.cut(sentence)) for sentence in corpus] print(corpus_tokenized) 分词完成的结果如下：\n1 [[\u0026#39;我\u0026#39;, \u0026#39;特别\u0026#39;, \u0026#39;特别\u0026#39;, \u0026#39;喜欢\u0026#39;, \u0026#39;看\u0026#39;, \u0026#39;电影\u0026#39;], [\u0026#39;这部\u0026#39;, \u0026#39;电影\u0026#39;, \u0026#39;真的\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;很\u0026#39;, \u0026#39;好看\u0026#39;, \u0026#39;的\u0026#39;, \u0026#39;电影\u0026#39;], [\u0026#39;今天天气\u0026#39;, \u0026#39;真好\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;难得\u0026#39;, \u0026#39;的\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;天气\u0026#39;], [\u0026#39;我\u0026#39;, \u0026#39;今天\u0026#39;, \u0026#39;去\u0026#39;, \u0026#39;看\u0026#39;, \u0026#39;了\u0026#39;, \u0026#39;一部\u0026#39;, \u0026#39;电影\u0026#39;], [\u0026#39;电影院\u0026#39;, \u0026#39;的\u0026#39;, \u0026#39;电影\u0026#39;, \u0026#39;都\u0026#39;, \u0026#39;很\u0026#39;, \u0026#39;好看\u0026#39;]] 创建词汇表 1 2 3 4 5 6 7 8 word_dict = {} # 初始化词汇表 # 遍历分词后的语料库 for sentence in corpus_tokenized: for word in sentence: # 如果词汇表中没有该词，则将其添加到词汇表中 if word not in word_dict: word_dict[word] = len(word_dict) # 分配当前词汇表索引 print(\u0026#34; 词汇表：\u0026#34;, word_dict) # 打印词汇表 获取到如下结果，其中 key 为对应的词，value 为词出现的频率。\n1 词汇表： {\u0026#39;我\u0026#39;: 0, \u0026#39;特别\u0026#39;: 1, \u0026#39;喜欢\u0026#39;: 2, \u0026#39;看\u0026#39;: 3, \u0026#39;电影\u0026#39;: 4, \u0026#39;这部\u0026#39;: 5, \u0026#39;真的\u0026#39;: 6, \u0026#39;是\u0026#39;: 7, \u0026#39;很\u0026#39;: 8, \u0026#39;好看\u0026#39;: 9, \u0026#39;的\u0026#39;: 10, \u0026#39;今天天气\u0026#39;: 11, \u0026#39;真好\u0026#39;: 12, \u0026#39;难得\u0026#39;: 13, \u0026#39;好\u0026#39;: 14, \u0026#39;天气\u0026#39;: 15, \u0026#39;今天\u0026#39;: 16, \u0026#39;去\u0026#39;: 17, \u0026#39;了\u0026#39;: 18, \u0026#39;一部\u0026#39;: 19, \u0026#39;电影院\u0026#39;: 20, \u0026#39;都\u0026#39;: 21} 生成词袋表示 1 2 3 4 5 6 7 8 9 10 11 12 # 根据词汇表将句子转换为词袋表示 bow_vectors = [] # 初始化词袋表示 # 遍历分词后的语料库 for sentence in corpus_tokenized: # 初始化一个全 0 向量，其长度等于词汇表大小 sentence_vector = [0] * len(word_dict) for word in sentence: # 将对应词的索引位置加 1，表示该词在当前句子中出现了一次 sentence_vector[word_dict[word]] += 1 # 将当前句子的词袋向量添加到向量列表中 bow_vectors.append(sentence_vector) print(\u0026#34; 词袋表示：\u0026#34;, bow_vectors) # 打印词袋表示 输出如下结果：\n1 2 3 4 5 6 7 [ [1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1] ] 输出结果需要稍微理解一下\n数组中的每一行表示语料中的一句话 每个元素表示该词在当前句子中出现的次数。例如第一行的第二个元素 2 为“特别”，说明“特别”在第一句话“我特别特别喜欢看电影”总一共出现了两次。 每一行一共有 22 个元素，说明所有的语料一共有 22 个词。 可以看到整个的输出结果为一个稀疏矩阵，尤其是当词汇量变大后，矩阵会更加稀疏。\n计算余弦相似度 该步骤需要有一点数学基础。\n余弦相似度：用来衡量两个向量的相似程度。值在 -1 到 1 之间，值越接近 1，两个向量越相似。越接近 -1，表示两个向量越不相似。值为 0 时，表示没有明显的相似性。\n公式如下：\n1 (A * B) / (||A|| * ||B||) (A * B) 为向量的点积，||A|| 和 ||B|| 表示向量的长度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 导入 numpy 库，用于计算余弦相似度 import numpy as np # 定义余弦相似度函数 def cosine_similarity(vec1, vec2): dot_product = np.dot(vec1, vec2) # 计算向量 vec1 和 vec2 的点积 norm_a = np.linalg.norm(vec1) # 计算向量 vec1 的范数 norm_b = np.linalg.norm(vec2) # 计算向量 vec2 的范数 return dot_product / (norm_a * norm_b) # 返回余弦相似度 # 初始化一个全 0 矩阵，用于存储余弦相似度 similarity_matrix = np.zeros((len(corpus), len(corpus))) # 计算每两个句子之间的余弦相似度 for i in range(len(corpus)): for j in range(len(corpus)): similarity_matrix[i][j] = cosine_similarity(bow_vectors[i], bow_vectors[j]) 可视化余弦相似度 1 2 3 4 5 6 7 8 9 10 11 12 13 # 导入 matplotlib 库，用于可视化余弦相似度矩阵 import matplotlib.pyplot as plt plt.rcParams[\u0026#34;font.family\u0026#34;]=[\u0026#39;SimHei\u0026#39;] # 用来设定字体样式 plt.rcParams[\u0026#39;font.sans-serif\u0026#39;]=[\u0026#39;SimHei\u0026#39;] # 用来设定无衬线字体样式 plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;]=False # 用来正常显示负号 fig, ax = plt.subplots() # 创建一个绘图对象 # 使用 matshow 函数绘制余弦相似度矩阵，颜色使用蓝色调 cax = ax.matshow(similarity_matrix, cmap=plt.cm.Blues) fig.colorbar(cax) # 条形图颜色映射 ax.set_xticks(range(len(corpus))) # x 轴刻度 ax.set_yticks(range(len(corpus))) # y 轴刻度 ax.set_xticklabels(corpus, rotation=45, ha=\u0026#39;left\u0026#39;) # 刻度标签 ax.set_yticklabels(corpus) # 刻度标签为原始句子 plt.show() # 显示图形 最终获取到如下的结果： 每个单元格表示两个句子之间的相似度。\n总结 缺点：\n采用了稀疏矩阵，每个单词是一个维度，计算效率较低。 忽略了文本的上下文信息。 ","date":"2025-02-06T00:00:00Z","permalink":"/post/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%9B%8F%E5%BD%A2-%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B/","title":"语言模型雏形 - 词袋模型"},{"content":"在 2022 年的上半年我有了属于自己的北京新能源指标，终于可以抛弃掉倍受限制的外地牌照，实现出门自由的目标了，有种翻身当家做主人般的无奈喜悦。\n摆在我面前的第一个问题就是该换哪款新能源汽车，车对我而言仅仅是个代步工具而已，能满足出行需求即可，因此买车的原则比较简单：\n买最大众化的销量最高的车型。当时销量最高的品牌当属比亚迪和特斯拉，单车型销量最高的当属特斯拉 Model Y，甚至 Model Y 火热到需要等至少两个月才交付，而且还在涨价。 后排座椅可以放倒，可以在车里睡觉，作为一个行走的床使用，Model Y 也正好可以满足需求。 我在苦苦观望了半年之久，等过了特斯拉的两拨降价后，在 2023 年元旦前夕选择了 Model Y 标准续航版。我担心过了 2023 年元旦后就没有补贴了，会额外增加一部分购车成本。事实证明我的判断是完全错误的，补贴不仅没有取消，而且一个月后居然降价两万多，肠子都要悔青了。\n到目前为止，已经开了两年的时间，一共跑了 2 万多公里。虽然我对车并不感兴趣，但油车也开过，多少还是有些话语权的，作为一个单纯的用户，我来聊聊我的用车感受。\n整体感受 驾驶 开车后的第一感受就是悬挂偏硬。虽然采用了双叉臂 + 五连杆悬挂，但不得不说 Model Y 还是非常颠的。在换车之后，家人们都觉得车变贵了，肯定会变舒服了，殊不知颠簸感要强于之前的大众朗逸，但颠簸相比朗逸会有一种高级感。\n其次是加速度。我对于加速基本无追求，长期在舒适模式下驾驶，运动模式对我而言提速过于激烈了一些。即使是在舒适模式下，也经常会让后排乘客晕车，加速度仍然是很快。但要是自己独自开车，驾驶感受还是挺不错的。尤其是绿灯起步时，往往能一骑绝尘，超越同一起跑线的其他车辆。\n单踏板真香。特斯拉的单踏板一直在被外界诟病，但我一直是单踏板的模式运行，日常的通行中，很少踩刹车，基本上一个踏板就够了，大大减轻了右腿的压力。抛开第一性原理不谈，产品的设计本来就是服务人类的，以简化操作作为设计目标。\n车子质量 特斯拉是一家对于利润极度压榨的公司，在这种原则下造车，往往车关键部位的质量非常好，但次要的零件由于要追求性价比，不会用最好的供应商，质量就会有些堪忧，反正坏了给你换，只要不坏就是赚到了，整体而言，特斯拉还是赚的。\n在用车的短短一年内，我遇到了两次故障：\n空调坏掉一次。在一次超充充电完成后，空调完全不能制冷，去售后维修认定是空调压缩机坏掉了。 离开车后不能自动锁车。经过检查是主驾驶座椅下传感器坏掉了，检测不到驾驶员已经离开车。 内饰 ​相比于国内品牌的冰箱彩电大沙发的豪华配置，特斯拉无遗是毛坯房。我个人对内饰也没有过多的要求，也喜欢极简风，倒是可以接受。\n座椅加热功能在冬天还是非常的必要，但夏天就比较难受了，居然没有座椅通风。开一路车后，后背非常容易湿透。不过网上倒是有可以通过点烟器供电的外置通风座椅垫，我暂时没有体验过。\n车顶的大玻璃对我而言影响不大，第一年的时候我买了一个手动遮阳帘挂上去。但第二年我就干脆没有挂上去，也不影响用车。在夏天的太阳下，虽然大玻璃会比较热， 但如果不用手去触摸，坐在车内头部是基本感受不到上面热量的。\n空间 作为大号鼠标，​Model Y 的空间还是非常给力的。我车上长期放着一整套的露营装备，除了露营车外，其他的装备全部可以放到前备箱和后备箱的下方储物格中，这些空间完全是比油车多出的储物空间。\n值得一提的是，我曾经搬过一次家，全程没有找搬家公司，家里所有的物品全部是我蚂蚁搬家一点点通过 Model Y 搬完的，虽然断断续续得搬了近两个月。\n后排放倒后，在车里​睡觉完全不是问题。​略微不爽的就是不能完全放平，稍微有个角度，但在车里睡觉倒是没啥影响。配合着露营模式，我个人也曾在车里睡过很多次的午觉。\n相比之下地盘离地高度就是个大大的劣势了，当然这是电车的通用设计理念。我曾经拿着手机测量过，最矮处跟手机的高度差不多。原先的油车偶尔还会应急停在马路牙子上，Model Y 我是一次也没敢往马路牙子上停。走在马路上过个坑都生怕磕到电池，怕跟保险公司扯上不解的缘分。（此处求 Model 3 车主的心理阴影面积）\n续航 相比油车而言，电池无遗是电车的最大短板了。\n刚买来时，车机显示满电续航为 435km，目前仅能显示 415km，掉了 20 km，属于在可接受的范围内，如果不追求数字，实际用车也感受不明显。要是跑高速，满打满算 350km 问题不大。\n冬天是磷酸铁锂电池的噩梦，充电速度变慢，续航衰减明显，再加上还需要方向盘加热、座椅加热、空调加热这些功能，电耗会进一步提升。电池的回收动能变差，单踏板的减速效果会大大折扣。在零下十度的天气里，刚开启动的时候，动能回收甚至是完全失效的，开一段时间热起来后动能回收才能逐渐恢复一些。\n充电方面，虽然有购车和推荐送的充电里程，但是一直没有舍得用完，大部分的时间都在外面的充电桩充电。在北京城市里面，充电还是非常方便的，方圆两公里内就有非常多的充电桩。时间久了后，我基本上会选择在出租车师傅长出没的充电桩充电了，而且大部分会选择在晚上十一点后，充电的价格要便宜挺多，我正好睡的晚，也不影响作息。\n最怕的就是节假日期间的高速上了，很多情况下都是要排队充电的。再就是去偏远的地方，甚至北京周边河北境内的很多地方都很难找到充电桩，在张家口市沽源县城，地图上只能找到一家充电桩，而且晚上还不营业，我迫不得已要跑到高速的服务区去充电。\n额外吐槽一下，不得不说，特斯拉是家特别会营销的公司。前两年的时候，4680 电池都快要让媒体吹上了天，能量密度提升了多少多少，啥时候量产，最近一年，几乎已经听不到 4680 电池的新闻了。（当然也有可能是我没关注到，顺便 Q 下比亚迪的刀片电池）\n软件 特斯拉的 OTA 升级还是比较频繁的，差不多每两个月一次升级。但两年回顾下来，真正有用的软件功能升级并不多。比如下面这次更新，看着就像是个把刀架在程序员脖子上写的更新简介。 特斯拉做产品非常的轴，跟苹果有的一拼。有的功能明明非常难用，却愣是不更新。另外就是因为国内的缘故，很多在国外可用的功能，在国内却无法使用，导致使用体验又大打折扣。\n相比之下，国内的新能源厂商的车机体验却一路猛追，应该说早已超越了特斯拉的自研车机。当然这么比并不公平，毕竟特斯拉的主战场不是中国，而中国却是新能源厂商的主战场。中国的新能源厂商出海后，车机的体验也一定会大打折扣，有大量的本地化、安全合规等需要适配。\n导航 导航功能刚开始用的时候弱爆了，但凡走陌生的路我都要打开手机上的高德导航。上班路上有一段特别拥堵，为了躲避拥堵，我都是刻意的多走一段路，但当我已经偏离了导航很长一段距离后，导航仍然傻到不断提醒让我前方掉头。\n经过了两年的迭代，导航已经多次改版，已经好用了很多，车道级导航、红绿灯倒计时这些最近两年才在导航软件上有的新功能都已经陆陆续续引入了。\n但相比手机上的导航软件还是有一定的差距，如：在计算拥堵方面还不够精确，显示的拥堵程度总是令我半信半疑，特斯拉可获取的数据量也不足以计算出拥堵的精度。\n语音 国内的新势力厂商的语音交互早已经非常流畅，跟车机交流起来毫无障碍。而特斯拉的语音功能只能说是智障，以至于我跟车机的交互都会选择手动点击屏幕。使用语音的唯一场景就是导航时输入目的地了，剩下的下发指令类的场景完全没有使用过，没有通过点击屏幕来的便捷。\n摄像头 特斯拉通过摄像头将物理环境通过视觉算法转换为内部的模型，并以此为基础来做更上层的数据输入来源。\n但在目前国内的版本上，在一些极端的场景下，摄像头的识别能力还是不行的。比如在下面的场景下，明明前面有一个非常粗的杆子，在车机上却视而不见。\n车上有那么多的摄像头，却没有 360 全景功能，而是给用户展示出经过处理的模型，这明明是你内部的实现细节，暴露给用户有啥用。在摄像头的识别率还不能到百分百的情况下，其实直接将全景交给人是个更好的选择。\n在刚交付时，车身前后的雷达还是可以使用的，在停车时可以在车机上显示出距离障碍物的距离，对停车还是非常有帮助的。但后来特斯拉一直在宣称要基于纯视觉，雷达和纯视觉变为了二选一。\n辅助驾驶 我没有花钱购买额外的服务，用的最多的功能就是车道居中和自适应巡航了，不过这个功能在普通的油车上都快成为标配了。另外，车道居中功能在走山路 S 弯的时候根本没法使用，弱爆了！国外的 FSD V12 和 V13 快吹上天了，而国内的辅助驾驶还处于智障水平。\n相比于普通的油车，稍微高级点的辅助驾驶功能就只有两个：\n当车速太快，且没有踩刹车的情况下，车机会判断出是否会追尾，并发出滴滴的提示音。这一点还是挺有用的，可以大大降低追尾的概率。 车道偏离后的被车机紧急控制。我刚买车的时候，曾经有一次向右侧变道，右侧的车在后视镜的盲区内，变道过程中就被车机紧急接管方向盘，避免了一次剐蹭的事故。 当然辅助驾驶还是比较弱的，在很多的极端情况下，还是无能为力的。分享我曾经遇到过两个案例。\n案例一： Your browser does not support the video tag. 在右转刚开始，外卖小哥还在马路对面，在车内是很难观察的，即使看到了也预判不了外卖小哥的风骚走位。当车头调转 90 度后，正准备加速时，外卖小哥突然冲到了车身前面，没有任何一个摄像头看到了外面小哥的存在，车机也没有发出告警，位置正好卡在了摄像头的盲区。\n好在这次事故后，外卖小哥没啥事，扶起电动车看看外卖无碍后头也不回的就走了，佩服小哥的敬业精神！而我的小 Y 就没那么幸运了，车牌凹进去了一大块，车牌上的漆被碰掉了很多。\n这次事故后，我每次右转都要提心吊胆，尤其是在转方向时不做过快的加速，否则可能会超出对方的预判，尤其是视时间为金钱的外卖骑手们的预判。\n案例二：\nYour browser does not support the video tag. 在等红绿灯的时候，左侧车道为摩托车。绿灯起步时，摩托车要右转到辅路。摩托车以为其他车道的车肯定没他加速快，忽略了有一种车叫 Model Y。由于带着头盔，头也没法向后扭，就直接右转到辅路。\n摩托车右转到我车前时，还突然减了一下速，车机都完全没反应过来，没有任何告警。幸好我当时反应快，踩了一脚刹车，要不然后果不堪设想。估计事后摩托车也不知道自己离事故这么近。\n上面两个危险的经历告诉了我们两个道理：\n远离电动车和摩托车，能离多远离多远。 特斯拉的辅助驾驶是完全靠不住的。 周边产品 因为特斯拉有 OpenAPI 可以供第三方调用，因此有一些第三方的软件我也在使用。据说后续 OpenAPI 要收费了，不知道后续围绕 OpenAPI 的一些应用是否会有调整。\nTeslaMate 可以在电脑上运行的软件，会持续调用 OpenAPI，并将车的运行数据都记录在本地的数据库中，并通过 grafana 来展出出来。通过该软件，可以方便的查看历史的行车轨迹。\n比如今年夏天的时候去达达线、热阿线的路线图，可以精准的绘制出来。\n小特钥匙 小特钥匙是 Apple Watch 上的应用，可以实现通过 Apple Watch 来打开车门，而且走的是蓝牙协议。主要的场景就是当手机没电时，可以通过手表了开锁，多了一条开门的途径。\n因为是收费软件，而且功能非常单一，很多人都觉得该应用很鸡肋，我也是在一次手机没电开不了车门后下定决心购买的。\n另外，特斯拉的官方 Apple Watch 版本的应用也快要上线了。\n总结 开了两年的车，整体而言还是比较满意的。本来也不可能有百分百满足自己的产品，只有适合自己的产品。\n一件东西也就在刚开始买的时候会比较纠结，各种比较权衡，一旦拥有了，即使有缺点也是可以忍受的，除非忍无可忍。人类真是个奇怪的动物。\n","date":"2024-12-18T00:00:00Z","permalink":"/post/%E7%89%B9%E6%96%AF%E6%8B%89-model-y-%E4%B8%A4%E4%B8%87%E5%85%AC%E9%87%8C%E7%9A%84%E7%94%A8%E8%BD%A6%E6%84%9F%E5%8F%97/","title":"特斯拉 Model Y 两万公里的用车感受"},{"content":"交换机堆叠：是指将一台以上的交换机组合起来工作，从逻辑上虚拟成一台交换机，作为一个整体参与数据转发。\n交换机的堆叠可以通过DAC高速线缆，光模块或者专门用于堆叠的线缆来实现。\n交换机堆叠包括堆叠主交换机和堆叠备交换机，一主多备工作模式。堆叠主交换机存储整个交换机堆栈的运行配置文件，并通过堆叠主交换机对所有的堆叠交换机进行管理。如果主交换机发生故障，堆叠系统会从备交换机中选择新的堆叠主交换机，且不会影响整个网络的性能。\n堆叠的协议都是私有的，因此不同品牌的交换机无法实现堆叠。\n资料 交换机堆叠知识：概述，配置与常见问题解答 什么是堆叠？ ","date":"2024-12-01T00:00:00Z","permalink":"/post/%E4%BA%A4%E6%8D%A2%E6%9C%BA%E5%A0%86%E5%8F%A0/","title":"交换机堆叠"},{"content":"阿里巴巴的财年为每年4月1日至次年3月31日，而4月1日至9月30日为财年的上半年。大部分的团队每半个财年要完成一次述职，而眼下的 9 月 30 日前又到了述职的日子。\n在我过往的工作经历中，并没有述职的习惯，绩效的考核也相对随意一些，绩效的好坏跟主管的主管判断关系比较大，但通常绩效的好坏跟个人的成绩也是成正比的，不会出现较大的偏差。\n在阿里绩效的绩效考核会非常被重视，希望能够通过公平的方法来分配高低绩效，降低主管的主观判断，当然主观的判断仍然是无法避免的，而且也会仍然占较大的比重。\n述职可以作为一种常见的职场总结方式，也是绩效考核的一个重要的参考依据。\n记得刚来阿里的时候，团队处于业务的爆发期，述职并不是太被重视，尤其是形式方面。某一年的述职中，我仅准备了可怜的三页 PPT，仅完成了做的事情的简单罗列，缺少一些自己的思考总结，当时就被喷的比较惨。当然述职中没有体现，并不代表平常就没有思考，即使平时没有思考也不代表就会影响实际的工作。\n一到述职，自己也曾经非常头疼，经过了这几年的洗礼，自己对职场的述职也有了一点点自己的总结，希望能帮助到大家。\n为什么要做述职？ 存在即合理，我认为的述职有这么几个好处：\n对自己过去做的事情的非常好的总结，帮助自己重新认识自己。这也是最关键的一点。大家肯定有这样的体会，如果让你回想过去几个月甚至一年做的事情，恐怕很难一下子想全。在现代职场中，大家都是匆匆忙忙的赶路人，任务一个接一个的完成，很难像在学校读书一样有温故而知新的机会。而述职恰恰是一个非常好的机会，回顾过去一段时间内自己做的事情，找找自己的闪光点，反思一下如果一件事情再重新做，会不会做的更好更高效。人总是在不断的总结经验中成长，述职是个很不错的职场成长机会。 让自己了解别人做的事情。别人做的最好的事情、吃的亏、踩过的坑都已经帮你总结好了，这是多好的一个学习机会。也许在吸取别人经验的过程中，会迸发出一些新的灵感。 让别人了解自己做的事情。自己做的申请同样需要传播给其他的同学，或许别人才能发现自己的闪光点，是个提升自己影响力的好时机（当然前提是自己要有货）。自己身上的不足也能够让别人发现，进而别人来帮助自己成长。 作为主管绩效考核的依据。这一点我倒认为并没有那么关键，自己做的事情自己的能力在日常工作中已经足够体现，不需要在述职的时候才做出判断，最多也就是作为判断的依据而已。 提升自己的演讲能力。演讲是一种能力，需要刻意练习。述职中要面对不同的人群，在有效的时间内将自己的事情给别人讲明白属实不是一件容易的事情。 通过述职这种形式化的方式来间接的促进员工在日常中工作的积极性。对于一部分员工，如果缺少了述职这种形式，在日常工作中就少了一份压力，工作中可能就会稍显懈怠。 述职的常见误区有哪些？ 述职也并非百害而无一利，同样是一把双刃剑，如果能利用的好，会对自己有较大的帮助。如果利用不好，就容易反噬。\n不要搞的花里胡哨太抽象。也是述职中最常见的问题，也是最让人诟病的。我见到过不少搞得花里胡哨的述职材料，一件很简单的事情非要说的很复杂，让人感觉很高大上，殊不知包装后，能听懂的人群更加少了。尽量少用 PPT，直接使用文档形式就非常不错，省去了大量排版的工作。图片也尽可能的简介，不要为了美观而消耗太多的时间。简简单单的真实就是最好的述职。 不要报喜不报忧。这也是个非常常见的问题。这种虚假的汇报，或许可以玩转一次述职，下次述职就很难遮盖过去了。或许一部分人可以糊弄过去，另外一部分人就未必。一个谎言的背后就得需要无数个谎言来弥补，实事求是在述职中非常的关键，严禁弄虚作假。 不要为了述职而述职。很多人都比较头疼述职，为了述职不得不做准备。其实这种心态就不太对，为了做而做，那么一定做不好。 不要消耗过多的时间来准备。如果一次要准备上好几天的时间就有点不值了，毕竟一年真正有效的工作时间才能有多少天呢，述职本身并不会产生业务价值，不要本末倒置。 不要陷在自己的主观世界中。要考虑到参加述职的人群，给别人讲明白自己做的事情。 述职的形式是什么样子的？ 职场中的很多事情都有固定的套路，尤其是在一家公司内部。最好的方式就是参考老员工的述职报告，找到固定的套路，或者按照主管要求的套路来。最好不要试图找到一条新的方法，前辈们总结的路基本都不会错。\n述职的内容基本要遵循 STAR 法则（即背景 Situation、任务 Task、行为 Action、结果 Result），主要包含几部分内容：\nSituation：事情的背景。 Task：面临的挑战和遇到的困难。项目干系人，自己在其中承担的角色。 Action：自己的解决方案。 Result：获得的业务结果。能量化的一些业务结果最好要量化，这样会非常直观。 其中也可以讲讲自己的总结思考和个人成长等内容。\n另外在讲述的时候要符合金字塔原理，核心的原则：\n结论先行。 每层结论下面的论据不要超过7个。 每一个论点要言之有物，有明确的思想。 述职的材料该如何准备？ 这里有几个建议：\n如果有写周报的习惯，看看周报的内容。 看看各种群的聊天记录的内容。 看看自己的代码提交记录。 看看自己的文档。 当然，上面的一些前提都是平常要有一些积累，如果平常积累的多，那么述职的材料准备起来就比较简单快速。\n最后，述职是一种管理员工的手段，作为员工在无法取消述职这种形式的时候，还是要尽可能将述职作为一种促进自己提升的手段，不要为了述职而述职。\n","date":"2024-09-20T00:00:00Z","permalink":"/post/%E6%88%91%E7%90%86%E8%A7%A3%E7%9A%84%E8%81%8C%E5%9C%BA%E8%BF%B0%E8%81%8C/","title":"我理解的职场述职"},{"content":"容器中的执行top、free等命令展示出来的CPU，内存等信息是从/proc目录中的相关文件里读取出来的。而容器并没有对/proc，/sys等文件系统做隔离，因此容器中读取出来的CPU和内存的信息是宿主机的信息，与容器实际分配和限制的资源量不同。\n1 2 3 4 5 6 /proc/cpuinfo /proc/diskstats /proc/meminfo /proc/stat /proc/swaps /proc/uptime lxcfs是一个常驻进程运行在宿主机上，从而来自动维护宿主机cgroup中容器的真实资源信息与容器内/proc下文件的映射关系。\nlxcfs实现的基本原理是通过文件挂载的方式，把cgroup中容器相关的信息读取出来，存储到lxcfs相关的目录下，并将相关目录映射到容器内的/proc目录下，从而使得容器内执行top,free等命令时拿到的/proc下的数据是真实的cgroup分配给容器的CPU和内存数据。\n类别 容器内目录 宿主机lxcfs目录 cpu /proc/cpuinfo /var/lib/lxcfs/proc/cpuinfo 内存 /proc/meminfo /var/lib/lxcfs/proc/meminfo /proc/diskstats /var/lib/lxcfs/proc/diskstats /proc/stat /var/lib/lxcfs/proc/stat /proc/swaps /var/lib/lxcfs/proc/swaps /proc/uptime /var/lib/lxcfs/proc/uptime /proc/loadavg /var/lib/lxcfs/proc/loadavg /sys/devices/system/cpu/online /var/lib/lxcfs/sys/devices/system/cpu/online 在每个容器内仅需要挂载 lxcfs 在宿主机上的目录到容器中的目录即可。\n","date":"2024-08-31T00:00:00Z","permalink":"/post/lxcfs-%E6%8A%80%E6%9C%AF/","title":"lxcfs 技术"},{"content":"多年前，我曾经阅读过两本当时非常流行的书籍《人性的弱点》和《人性的优点》，作者均为卡耐基。 而前段时间参加过一场[卡内基的培训](卡内基训练 (carnegiechina.com))，自己就有点傻傻的分不清楚此“卡”是否为彼“卡”。 于是搜索了一下资料，发现两“卡”均为 戴尔·卡耐基(Dale Carnegie，1888年11月24日—1955年11月1日)，美国的作家和演讲者。只是因为翻译的偏差，导致出现了卡内基和卡耐基两种称呼。 1912 年创办了卡内基训练（Dale Carnegie Training）教导人们人际沟通及如何处理压力。 1936 年完成书籍《人性的弱点》又名《如何赢取友谊与影响他人》（How to Win Friends and Influence People）一书。\n之所以会出现混淆，还有另外的名字也是跟卡耐基相关的：\n安德鲁·卡内基（Andrew Carnegie，1835年11月25日—1919年8月11日）：美国著名的钢铁大王，其名称要远大于戴尔·卡耐基。 卡内基·梅隆大学（Carnegie Mellon University）：由钢铁大王安德鲁·卡内基创办，全球大学排名上在前60名以内。 ","date":"2024-08-21T00:00:00Z","permalink":"/post/%E5%8D%A1%E5%86%85%E5%9F%BA%E4%B8%8E%E5%8D%A1%E8%80%90%E5%9F%BA/","title":"卡内基与卡耐基"},{"content":" 本文为对《GPT 图解 - 大模型是怎样构建的》一书的学习笔记，所有的例子和代码均来源于本书。\n基本介绍 N-Gram 模型为语言模型的雏形，诞生于 1948 年。\n基本思想为：一个词出现的概率仅依赖其前面的 N-1 个词。即通过有限的 N-1 个词来预测第 N 个词。\n以 \u0026ldquo;我爱吃肉\u0026rdquo; 举例，分词为 [”我“, \u0026ldquo;爱\u0026rdquo;, \u0026ldquo;吃\u0026rdquo;, \u0026ldquo;肉\u0026rdquo;]。\n当 N=1 时，对应的序列为[”我“, \u0026ldquo;爱\u0026rdquo;, \u0026ldquo;吃\u0026rdquo;, \u0026ldquo;肉\u0026rdquo;]，又成为 Unigram。 当 N=2 时，对应的序列为[”我爱“, \u0026ldquo;爱吃\u0026rdquo;, \u0026ldquo;吃肉\u0026rdquo;]，又成为 Bigram。 当 N=3 时，对应的序列为[”我爱吃“, \u0026ldquo;爱吃肉\u0026rdquo;]，又成为 Trigram。 2-Gram 的代码实践 将语料拆分为分词 将给定的语料库，拆分为以一个的分词。在真实的场景中需要使用分词函数，这里简单起见，使用了一个汉字一个分词的方式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 corpus = [ \u0026#34;我喜欢吃苹果\u0026#34;, \u0026#34;我喜欢吃香蕉\u0026#34;, \u0026#34;她喜欢吃葡萄\u0026#34;, \u0026#34;他不喜欢吃香蕉\u0026#34;, \u0026#34;他喜欢吃苹果\u0026#34;, \u0026#34;她喜欢吃草莓\u0026#34;] # 定义一个分词函数，将文本转换为单个字符的列表 def tokenize(text): return [char for char in text] # 将文本拆分为字符列表 # 对每个文本进行分词，并打印出对应的单字列表 print(\u0026#34;单字列表:\u0026#34;) for text in corpus: tokens = tokenize(text) print(tokens) 得到如下的结果：\n1 2 3 4 5 6 7 单字列表: [\u0026#39;我\u0026#39;, \u0026#39;喜\u0026#39;, \u0026#39;欢\u0026#39;, \u0026#39;吃\u0026#39;, \u0026#39;苹\u0026#39;, \u0026#39;果\u0026#39;] [\u0026#39;我\u0026#39;, \u0026#39;喜\u0026#39;, \u0026#39;欢\u0026#39;, \u0026#39;吃\u0026#39;, \u0026#39;香\u0026#39;, \u0026#39;蕉\u0026#39;] [\u0026#39;她\u0026#39;, \u0026#39;喜\u0026#39;, \u0026#39;欢\u0026#39;, \u0026#39;吃\u0026#39;, \u0026#39;葡\u0026#39;, \u0026#39;萄\u0026#39;] [\u0026#39;他\u0026#39;, \u0026#39;不\u0026#39;, \u0026#39;喜\u0026#39;, \u0026#39;欢\u0026#39;, \u0026#39;吃\u0026#39;, \u0026#39;香\u0026#39;, \u0026#39;蕉\u0026#39;] [\u0026#39;他\u0026#39;, \u0026#39;喜\u0026#39;, \u0026#39;欢\u0026#39;, \u0026#39;吃\u0026#39;, \u0026#39;苹\u0026#39;, \u0026#39;果\u0026#39;] [\u0026#39;她\u0026#39;, \u0026#39;喜\u0026#39;, \u0026#39;欢\u0026#39;, \u0026#39;吃\u0026#39;, \u0026#39;草\u0026#39;, \u0026#39;莓\u0026#39;] 计算每个 2-Gram（BiGram） 在语料库中的词频 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 定义计算 N-Gram 词频的函数 from collections import defaultdict, Counter # 导入所需库 def count_ngrams(corpus, n): ngrams_count = defaultdict(Counter) # 创建一个字典，存储 N-Gram 计数 for text in corpus: # 遍历语料库中的每个文本 tokens = tokenize(text) # 对文本进行分词 for i in range(len(tokens) - n + 1): # 遍历分词结果，生成 N-Gram ngram = tuple(tokens[i:i+n]) # 创建一个 N-Gram 元组 prefix = ngram[:-1] # 获取 N-Gram 的前缀 token = ngram[-1] # 获取 N-Gram 的目标单字 ngrams_count[prefix][token] += 1 # 更新 N-Gram 计数 return ngrams_count bigram_counts = count_ngrams(corpus, 2) # 计算 bigram 词频 print(\u0026#34;bigram 词频：\u0026#34;) # 打印 bigram 词频 for prefix, counts in bigram_counts.items(): print(\u0026#34;{}: {}\u0026#34;.format(\u0026#34;\u0026#34;.join(prefix), dict(counts))) 计算获取到如下的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 bigram 词频： 我: {\u0026#39;喜\u0026#39;: 2} 喜: {\u0026#39;欢\u0026#39;: 6} 欢: {\u0026#39;吃\u0026#39;: 6} 吃: {\u0026#39;苹\u0026#39;: 2, \u0026#39;香\u0026#39;: 2, \u0026#39;葡\u0026#39;: 1, \u0026#39;草\u0026#39;: 1} 苹: {\u0026#39;果\u0026#39;: 2} 香: {\u0026#39;蕉\u0026#39;: 2} 她: {\u0026#39;喜\u0026#39;: 2} 葡: {\u0026#39;萄\u0026#39;: 1} 他: {\u0026#39;不\u0026#39;: 1, \u0026#39;喜\u0026#39;: 1} 不: {\u0026#39;喜\u0026#39;: 1} 草: {\u0026#39;莓\u0026#39;: 1} 即当第一个词为 ”我“，第二个词为”喜“在整个语料库中出现了 2 次。\n如果为 3-Gram（TriGram），此时输出结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 bigram 词频： 我喜: {\u0026#39;欢\u0026#39;: 2} 喜欢: {\u0026#39;吃\u0026#39;: 6} 欢吃: {\u0026#39;苹\u0026#39;: 2, \u0026#39;香\u0026#39;: 2, \u0026#39;葡\u0026#39;: 1, \u0026#39;草\u0026#39;: 1} 吃苹: {\u0026#39;果\u0026#39;: 2} 吃香: {\u0026#39;蕉\u0026#39;: 2} 她喜: {\u0026#39;欢\u0026#39;: 2} 吃葡: {\u0026#39;萄\u0026#39;: 1} 他不: {\u0026#39;喜\u0026#39;: 1} 不喜: {\u0026#39;欢\u0026#39;: 1} 他喜: {\u0026#39;欢\u0026#39;: 1} 吃草: {\u0026#39;莓\u0026#39;: 1} 如果为 1-Gram（UniGram），此时输出结果如下：\n1 2 bigram 词频： : {\u0026#39;我\u0026#39;: 2, \u0026#39;喜\u0026#39;: 6, \u0026#39;欢\u0026#39;: 6, \u0026#39;吃\u0026#39;: 6, \u0026#39;苹\u0026#39;: 2, \u0026#39;果\u0026#39;: 2, \u0026#39;香\u0026#39;: 2, \u0026#39;蕉\u0026#39;: 2, \u0026#39;她\u0026#39;: 2, \u0026#39;葡\u0026#39;: 1, \u0026#39;萄\u0026#39;: 1, \u0026#39;他\u0026#39;: 2, \u0026#39;不\u0026#39;: 1, \u0026#39;草\u0026#39;: 1, \u0026#39;莓\u0026#39;: 1} 可以看到已经退化为了每个单词在整个语料库中出现的次数。\n计算每个 2-Gram 出现的概率 即给定前一个词，计算下一个词出现的概率。\n1 2 3 4 5 6 7 8 9 10 11 # 定义计算 N-Gram 出现概率的函数 def ngram_probabilities(ngram_counts): ngram_probs = defaultdict(Counter) # 创建一个字典，存储 N-Gram 出现的概率 for prefix, tokens_count in ngram_counts.items(): # 遍历 N-Gram 前缀 total_count = sum(tokens_count.values()) # 计算当前前缀的 N-Gram 计数 for token, count in tokens_count.items(): # 遍历每个前缀的 N-Gram ngram_probs[prefix][token] = count / total_count # 计算每个 N-Gram 出现的概率 return ngram_probs bigram_probs = ngram_probabilities(bigram_counts) # 计算 bigram 出现的概率 print(\u0026#34;\\nbigram 出现的概率 :\u0026#34;) # 打印 bigram 概率 for prefix, probs in bigram_probs.items(): print(\u0026#34;{}: {}\u0026#34;.format(\u0026#34;\u0026#34;.join(prefix), dict(probs))) 输出结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 bigram 出现的概率 : 我: {\u0026#39;喜\u0026#39;: 1.0} 喜: {\u0026#39;欢\u0026#39;: 1.0} 欢: {\u0026#39;吃\u0026#39;: 1.0} 吃: {\u0026#39;苹\u0026#39;: 0.3333333333333333, \u0026#39;香\u0026#39;: 0.3333333333333333, \u0026#39;葡\u0026#39;: 0.16666666666666666, \u0026#39;草\u0026#39;: 0.16666666666666666} 苹: {\u0026#39;果\u0026#39;: 1.0} 香: {\u0026#39;蕉\u0026#39;: 1.0} 她: {\u0026#39;喜\u0026#39;: 1.0} 葡: {\u0026#39;萄\u0026#39;: 1.0} 他: {\u0026#39;不\u0026#39;: 0.5, \u0026#39;喜\u0026#39;: 0.5} 不: {\u0026#39;喜\u0026#39;: 1.0} 草: {\u0026#39;莓\u0026#39;: 1.0} 给定一个前缀，输出连续的文本 根据前面学习的语料信息，给定一个前缀，即可生成对应的文本内容。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 定义生成下一个词的函数 def generate_next_token(prefix, ngram_probs): if not prefix in ngram_probs: # 如果前缀不在 N-Gram 中，返回 None return None next_token_probs = ngram_probs[prefix] # 获取当前前缀的下一个词的概率 next_token = max(next_token_probs, key=next_token_probs.get) # 选择概率最大的词作为下一个词 return next_token # 定义生成连续文本的函数 def generate_text(prefix, ngram_probs, n, length=6): tokens = list(prefix) # 将前缀转换为字符列表 for _ in range(length - len(prefix)): # 根据指定长度生成文本 # 获取当前前缀的下一个词 next_token = generate_next_token(tuple(tokens[-(n-1):]), ngram_probs) if not next_token: # 如果下一个词为 None，跳出循环 break tokens.append(next_token) # 将下一个词添加到生成的文本中 return \u0026#34;\u0026#34;.join(tokens) # 将字符列表连接成字符串 # 输入一个前缀，生成文本 generated_text = generate_text(\u0026#34;我\u0026#34;, bigram_probs, 2) print(\u0026#34;\\n 生成的文本：\u0026#34;, generated_text) # 打印生成的文本 给定了文本”我“，可以生成出如下结果：\n1 生成的文本： 我喜欢吃苹果 总结 N-Gram 为非常简单的语言模型，可以根据给定的词来生成句子。\n缺点：无法捕捉到距离较远的词之间的关系。\n","date":"2024-08-19T00:00:00Z","permalink":"/post/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%9B%8F%E5%BD%A2-n-gram/","title":"语言模型雏形 N-Gram"},{"content":"在字节的 AI 发布平台扣子中提供了创建机器人的功能，并且可以直接对接微信公众号，使用在微信公众号中回复消息，由大模型直接回复的效果。\n操作步骤 整体操作步骤非常简单，需要申请一个 Coze 的账号和开通微信公众号的开发者功能。\n微信公众号的开发者功能在这里配置： 点击创建 Bot 按钮，输入 Bot 名称后点击确认。 在机器人设置页面，可以配置模型、选择自己训练的知识库等操作。 设置完成后，点击发布，选择微信公众号（订阅号）配置功能，设置对应的微信公众号的 AppID。 体验 访问公众号直接输入内容，可以看到自动回复内容。 ","date":"2024-06-19T00:00:00Z","permalink":"/post/%E5%BE%AE%E4%BF%A1%E5%85%AC%E4%BC%97%E5%8F%B7%E9%80%9A%E8%BF%87%E6%89%A3%E5%AD%90%E6%8E%A5%E5%85%A5%E5%A4%A7%E6%A8%A1%E5%9E%8B/","title":"微信公众号通过扣子接入大模型"},{"content":"无意间发现了开源项目 mi-gpt，该项目可以将家里的小爱音箱接入到 GPT 中，增强小爱音箱的功能。在跟小爱音箱对话的过程中，可以根据特定的提示词走 GPT 来回答，而不是用小爱音箱原生的回复。\n必备条件：\n必须有一个小米音箱。 必须要有可以长期运行的服务器，可以是树莓派等设备。 要有一个 OpenAI 的账号，也可以用兼容 ChatGPT API 的国内大模型。 部署 部署比较简单，下面为我的部署过程，供大家参考。更详细的信息大家可以直接参考 github 项目中的相关文档。\n创建配置文件 .migpt.js： 参考项目中的文件 .migpt.example.js内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 // 小爱音箱扮演角色的简介 const botProfile = ` 性别：女 性格：乖巧可爱 爱好：喜欢搞怪，爱吃醋。 `; // 小爱音箱主人（你）的简介 const masterProfile = ` 性别：男 性格：善良正直 其他：总是舍己为人，是傻妞的主人。 `; export default { bot: { name: \u0026#34;傻妞\u0026#34;, profile: botProfile, }, master: { name: \u0026#34;陆小千\u0026#34;, profile: masterProfile, }, speaker: { // 小米 ID userId: \u0026#34;12345\u0026#34;, // 注意：不是手机号或邮箱，请在「个人信息」-「小米 ID」查看 // 账号密码 password: \u0026#34;xxx\u0026#34;, // 小爱音箱 ID 或在米家中设置的名称 did: \u0026#34;Redmi小爱触屏音箱8\u0026#34;, // 当消息以下面的关键词开头时，会调用 AI 来回复消息 callAIKeywords: [\u0026#34;请\u0026#34;, \u0026#34;你\u0026#34;, \u0026#34;傻妞\u0026#34;], // 当消息以下面的关键词开头时，会进入 AI 唤醒状态 wakeUpKeywords: [\u0026#34;打开\u0026#34;, \u0026#34;进入\u0026#34;, \u0026#34;召唤\u0026#34;], // 当消息以下面的关键词开头时，会退出 AI 唤醒状态 exitKeywords: [\u0026#34;关闭\u0026#34;, \u0026#34;退出\u0026#34;, \u0026#34;再见\u0026#34;], // 进入 AI 模式的欢迎语 onEnterAI: [\u0026#34;你好，我是傻妞，很高兴认识你\u0026#34;], // 退出 AI 模式的提示语 onExitAI: [\u0026#34;傻妞已退出\u0026#34;], // AI 开始回答时的提示语 onAIAsking: [\u0026#34;让我先想想\u0026#34;, \u0026#34;请稍等\u0026#34;], // AI 结束回答时的提示语 onAIReplied: [\u0026#34;我说完了\u0026#34;, \u0026#34;还有其他问题吗\u0026#34;], // AI 回答异常时的提示语 onAIError: [\u0026#34;啊哦，出错了，请稍后再试吧！\u0026#34;], // 无响应一段时间后，多久自动退出唤醒模式（默认 30 秒） exitKeepAliveAfter: 30, // TTS 指令，请到 https://home.miot-spec.com 查询具体指令 ttsCommand: [3, 1], // 设备唤醒指令，请到 https://home.miot-spec.com 查询具体指令 wakeUpCommand: [3, 2], // 是否启用流式响应，部分小爱音箱型号不支持查询播放状态，此时需要关闭流式响应 streamResponse: false, // 查询是否在播放中指令，请到 https://home.miot-spec.com 查询具体指令 playingCommand: [2, 1, 1], // 默认无需配置此参数，播放出现问题时再尝试开启 }, }; 必须要修改的内容涉及到如下字段，其他字段可以根据含义来定义：\nspeaker.userId：小米的用户 ID。 speaker.password：小米的账号密码。 speaker.did：小爱音箱的 ID 或者小爱音箱在米家的设备名字。 ttsCommand：需要设置。如果设置不正常，会导致小爱音箱无法播放 GPT 回复内容的情况。 wakeUpCommand：需要设置。 streamResponse：在某些音箱设备上需要关闭。我的设备因为无法读完完整的句子，选择了关闭该功能，相关参考：小爱音箱没有读完整个句子，总是戛然而止。 ttsCommand 和 wakeUpCommand 需要在 https://home.miot-spec.com 页面搜索对应的音箱型号 点击规格后任选一个，选择 Intelligent Speaker，其中的 [3, 1] 对应的为 ttsCommand，[3, 2] 对应的为 wakeUpCommand。 创建配置文件 .env 该文件中需要配置 OPENAI 的账号信息，我这里直接采用了阿里云的通义千问大模型服务，API 是完全兼容的。\n参考文档《开通DashScope并创建API-KEY》 阿里云上开通大模型服务，获取到 API-KEY 参考项目中的文件.env.example\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # OpenAI（也支持通义千问、MoonShot、DeepSeek 等模型） OPENAI_MODEL=qwen-turbo OPENAI_API_KEY=获取到的API_KEY OPENAI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1 # Azure OpenAI Service（可选） # OPENAI_API_VERSION=2024-04-01-preview # AZURE_OPENAI_API_KEY=你的密钥 # AZURE_OPENAI_ENDPOINT=https://你的资源名.openai.azure.com # AZURE_OPENAI_DEPLOYMENT=你的模型部署名，比如：gpt-35-turbo-instruct # 提示音效（可选，一般不用填，你也可以换上自己的提示音链接试试看效果） # AUDIO_SILENT=静音音频链接，示例：https://example.com/slient.wav # AUDIO_BEEP=默认提示音链接，同上 # AUDIO_ACTIVE=唤醒提示音链接，同上 # AUDIO_ERROR=出错了提示音链接，同上 # Doubao TTS（可选，用于调用第三方 TTS 服务，比如：豆包） # TTS_DOUBAO=豆包 TTS 接口 # SPEAKERS_DOUBAO=豆包 TTS 音色列表接口 主要修改如下两个值：\nOPENAI_API_KEY：即为上文获取到阿里云模型服务灵积的 API-KEY。 OPENAI_MODEL：支持的模型，可以在《支持的模型列表》中查询模型列表。 启动服务 提供了两种方式：docker 和 宿主机 Node.js 方式运行，我自然会选择更加简洁的 docker 方式。\n执行 docker run -d --name mi-gpt --env-file $(pwd)/.env -v $(pwd)/.migpt.js:/app/.migpt.js idootop/mi-gpt:3.1.0 即可本地运行。\n功能演示 提问小爱同学：“请问一下太阳的重量是多少”，小爱同学可以顺利的回答出答案。\n通过 docker logs mi-gpt -f 可以看到如下的输出日志：\n1 2 3 4 5 6 7 2024/06/12 16:15:38 Speaker 🔥 请问一下太阳的重量是多少 2024/06/12 16:15:38 Speaker 🔊 让我先想想 2024/06/12 16:15:41 Open AI ✅ Answer: 傻妞: 哦，太阳的重量可大了，它是个恒星，比我们的地球重得多。科学家们用的是质量而不是重量来衡量，太阳的质量大约是地球的333,000倍，真是个超级大块头，想想如果它能变成棉花糖，那得多软多亮啊！不过，太阳对我们来说太遥远了，它的重量咱们还是别去抱了，哈哈。 2024/06/12 16:15:41 Speaker 🔊 傻妞: 哦，太阳的重量可大了，它是个恒星，比我们的地球重得多。科学家们用的是质量而不是重量来衡量，太阳的质量大约是地球的333,000倍，真是个超级大块头，想想如果它能变成棉花糖，那得多软多亮啊！不过，太阳对我们来说太遥远了，它的重量咱们还是别去抱了，哈哈 ","date":"2024-06-13T00:00:00Z","permalink":"/post/%E4%BD%BF%E7%94%A8-mi-gpt-%E5%B0%86%E5%B0%8F%E7%88%B1%E9%9F%B3%E7%AE%B1%E6%8E%A5%E5%85%A5%E5%A4%A7%E6%A8%A1%E5%9E%8B/","title":"使用 mi-gpt 将小爱音箱接入大模型"},{"content":"在 k8s 中包含两类用户：\nServiceAccount。又称服务账号，在运行 pod 时必须绑定 ServiceAccount，如果没有指定，则使用当前 namespace 下的 ServiceAccount default。是针对程序而言，用于 pod 中的程序访问 kube-apiserver。 普通用户。在 k8s 中并没有使用单独的对象来存储，而是通过了分发证书、外部用户认证系统等方式实现，是针对用户而言。 1. X509 证书认证 使用场景：使用 kubectl 访问 k8s 集群即通过 X509 证书认证方式，kubeconfig 本质上是个证书文件。\n客户端使用证书中的 Common Name 作为请求的用户名，organization 作为用户组的成员信息。\n证书的签发可以使用 openssl、cfssl 等工具来签发，也可以使用 k8s 自带的 CertificateSigningRequest 对象来实现签发。\n1.1. CertificateSigningRequest 签发证书 创建私钥信息：\n1 2 openssl genrsa -out myuser.key 2048 openssl req -new -key myuser.key -out myuser.csr -subj \u0026#34;/CN=myuser\u0026#34; 创建如下的 CertificateSigningRequest 对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: myuser spec: # value 使用命令 cat myuser.csr | base64 | tr -d \u0026#34;\\n\u0026#34; 获取 request: xxx # 固定值 signerName: kubernetes.io/kube-apiserver-client # 过期时间 expirationSeconds: 86400 # one day usages: - client auth EOF 查看 csr 处于 Pending 状态：\n1 2 3 $ kubectl get csr NAME AGE SIGNERNAME REQUESTOR REQUESTEDDURATION CONDITION myuser 12s kubernetes.io/kube-apiserver-client kubernetes-admin 24h Pending 批准给证书签发请求：\n1 kubectl certificate approve myuser 签发完成后的证书会存放到 status.certificate 字段中，至此证书签发完成。\n2. ServiceAccount 使用场景：该方式使用较为常见，用于 pod 中访问 k8s apiserver。\n原理：pod 可以通过 spec.serviceAccountName 字段来指定要使用的 ServiceAccount，如果没有指定则使用 namespace 下默认的 default ServiceAccount。kube-controller-manager 中的 ServiceAccount 控制器会在拉起的 pod 中自动注入如下的信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 spec: volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-j6vpz readOnly: true volumes: - name: kube-api-access-j6vpz projected: defaultMode: 420 sources: - serviceAccountToken: expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace 即将信息注入到 pod 的 /var/run/secrets/kubernetes.io/serviceaccount 目录下，目录结构如下：\n1 2 3 4 /var/run/secrets/kubernetes.io/serviceaccount |-- ca.crt -\u0026gt; ..data/ca.crt |-- namespace -\u0026gt; ..data/namespace `-- token -\u0026gt; ..data/token token 为 JWT 认证，对其格式解密后如下：\n1 2 3 4 { \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, \u0026#34;kid\u0026#34;: \u0026#34;u7rF5JCtJRNiMzSUOFAYvDpCwPqUII-N-OtxR59cnQ0\u0026#34; } 1 2 3 4 5 6 7 8 { \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;ingress-token-s9gtm\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;ingress\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;19ce4f11-7105-43ce-b189-f3d71a2ffc74\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:default:ingress\u0026#34; } 2.1. Secret 存放 token 在 1.22 版本及之前版本中，token 以 Secret 的形式存在于 pod 所在的 namespace 下，且 token 不会过期。Secret 的名字存在于 ServiceAccount 的 spec 中，格式如下：\n1 2 3 4 apiVersion: v1 kind: ServiceAccount secrets: - name: nginx-token-scjvn 而 Secret 通过 Annotation kubernetes.io/service-account.name 指定了关联的 ServiceAccount。\n在后续版本中，为了兼容当前方案，如果 ServiceAccount 关联了 Secret，则认为仍然使用 Secret 中存放 token 的方式。如果 Secret 已经很长时间没有使用，则自动回收 Secret。\n如果要手工创建一个 token Secret，可以创建如下的 Secret，k8s 自动会为 Secret 产生 token：\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: build-robot-secret annotations: # 带有 annotation kubernetes.io/service-account.name: build-robot type: kubernetes.io/service-account-token 2.2. TokenRequest API 产生 token 在 1.22 之后的版本中，kubelet 使用 TokenRequest API 获取有时间限制的临时 token，该 token\n会在 pod 删除或者 token 生命周期（默认为 1h）结束后失效。\n可以使用 kubectl create token default 来为 ServiceAccount default 创建 token，该命令实际上向 kube-apiserver 发送了请求 /api/v1/namespaces/default/serviceaccounts/default/token：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \u0026#34;kind\u0026#34;: \u0026#34;TokenRequest\u0026#34;, \u0026#34;apiVersion\u0026#34;: \u0026#34;authentication.k8s.io/v1\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;creationTimestamp\u0026#34;: null }, \u0026#34;spec\u0026#34;: { \u0026#34;audiences\u0026#34;: null, \u0026#34;expirationSeconds\u0026#34;: null, \u0026#34;boundObjectRef\u0026#34;: null }, \u0026#34;status\u0026#34;: { \u0026#34;token\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;expirationTimestamp\u0026#34;: null } } kube-apiserver 支持如下参数：\n\u0026ndash;service-account-key-file：用来验证服务账号的 token。 \u0026ndash;service-account-issuer：ServiceAccount token 的签发机构。 \u0026ndash;service-account-signing-key-file：ServiceAccount token 的签发私钥。 3. 用户伪装 一个用户通过 Http Header Impersonation- 的方式来扮演另外一个用户的身份。\n场景：跨 k8s 集群访问的网关服务\n支持的 Http Header 如下：\nImpersonate-User：要伪装的用户名。 Impersonate-Group：要伪装的组名。该 Header 可以为多个，即支持多个组。 4. bootstrap token 使用 kube-apiserver 参数 --enable-bootstrap-token-auth=true 启用功能，引导 token 以 Secret 的形式存放在 kube-system 下。\n该功能仅用于节点初始化时加入到 k8s 集群中。\n资料 用户认证 ","date":"2024-04-22T00:00:00Z","permalink":"/post/k8s-authenticate/","title":"k8s 中的用户认证方式"},{"content":"client 端限流 在 client-go 中会默认对客户端进行限流，并发度为 5。可以通过修改 rest.Conifg 来修改并发度。\nMaxInFlightLimit 限流 通过如下参数来控制：\n\u0026ndash;max-requests-inflight：代表只读请求的最大并发量 \u0026ndash;max-mutating-requests-inflight：代表写请求的最大并发量 该实现为单个 kube-apiserver 层面的，可以针对所有的请求。\nEventRateLimit 用来对 Event 类型的对象进行限制，可以通过 kube-apiserver 的参数 \u0026ndash;admission-control-config-file 来指定配置文件，文件格式如下：\n1 2 3 4 5 6 apiVersion: apiserver.config.k8s.io/v1 kind: AdmissionConfiguration plugins: - name: EventRateLimit path: eventconfig.yaml ... 其中 EventRateLimit 为对 Event 的限制，eventconfig.yaml 文件为详细的对 Event 的限流策略，可以精确到 Namespace 和 User 信息。\n1 2 3 4 5 6 7 8 9 10 apiVersion: eventratelimit.admission.k8s.io/v1alpha1 kind: Configuration limits: - type: Namespace qps: 50 burst: 100 cacheSize: 2000 - type: User qps: 10 burst: 50 API 优先级和公平性 版本状态：\nalpha：1.18 通过 kube-apiserver 的参数 --enable-priority-fairness 来控制是否开启 APF 特性。\n资料 kubernetes apiserver限流方案 API 优先级和公平性 ","date":"2024-04-07T15:34:37Z","permalink":"/post/k8s-apiserver-%E7%9A%84%E9%99%90%E6%B5%81/","title":"k8s apiserver 的限流"},{"content":"默认的情况下，k8s 对于 pod 在单个节点的资源分配并不会考虑到 NUMA 架构。比如 cpu 默认会采用 cgroup CFS 来做资源的分配，并未考虑到 NUMA 架构。为了提升 pod 的性能，需要 pod 在分配资源时感知 NUMA 架构。\n为此，k8s 在 kubelet 中通过 CPU Manager、Memory Manager、Device Manager、Topology Manager 等特性对 NUMA 做了支持，支持的 pod QoS 类型要求为 Granteed pod。\n各特性的支持版本情况如下：\n特性 alpha beta stable CPU Manager 1.12 1.26 Memory Manager 1.21 1.22 - Topology Manager 1.16 1.18 - CPU Manager 在 k8s 中使用 cgroup 的 CFS 配额来执行 pod 的 CPU 约束，在 CFS 的模式下，pod 可能会运行在不同的核上，会导致 pod 的缓存失效的问题。对于性能要求非常高的 pod，为了提升性能，可以通过 cgroup 中的 cpuset 绑核的特性来提升 pod 的性能。\n在 k8s 中仅针对如下的 pod 类型做了绑核操作：\n必须为 guaranteed pod 类型。即 pod 需要满足如下两个条件： pod 中的每个容器都必须指定cpu 和 内存的 request 和 limit。 pod 中的每个容器的 cpu 和内存的 request 和 limit 必须相等。 pod 的 cpu request 和 limit 必须为整数。 kubelet 的参数配置 在 k8s 中仅通过 kubelet 来支持 pod 的绑核操作，跟其他组件无关。\nkubelet 通过参数 --cpu-manager-policy 或者在 kubelet 的配置文件中参数cpuManagerPolicy 来配置，支持如下值：\nnone：默认策略。即不执行绑核操作。 static：允许为节点上的某些特征的 pod 赋予增强的 cpu 亲和性和独占性。 kubelet 通过参数 --cpu-manager-reconcile-period 来指定内存中的 cpu 分配跟 cgroupfs 一致。\nkubelet 通过参数 --cpu-manager-policy-options来微调。该特性通过特性门控 CPUManagerPolicyOptions 来控制。\n模式之间切换 默认的 kubelet cpuManagerPolicy 配置为 none，策略配置位于文件 /var/lib/kubelet/cpu_manager_state，文件内容如下：\n1 {\u0026#34;policyName\u0026#34;:\u0026#34;none\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;checksum\u0026#34;:1353318690} 修改 kubelet 的cpuManagerPolicy 为 static，将模式从 none 切换为 static，重启 kubelet。发现 kubelet 会启动失败，kubelet 并不能支持仅修改参数就切换模式，报如下错误：\n1 2 Mar 06 15:00:02 iZt4nd5yyw9vfuxn3q2g3tZ kubelet[102800]: E0306 15:00:02.463939 102800 cpu_manager.go:223] \u0026#34;Could not initialize checkpoint manager, please drain node and remove policy state file\u0026#34; err=\u0026#34;could not restore state from checkpoint: configured policy \\\u0026#34;static\\\u0026#34; differs from state checkpoint policy \\\u0026#34;none\\\u0026#34;, please drain this node and delete the CPU manager checkpoint file \\\u0026#34;/var/lib/kubelet/cpu_manager_state\\\u0026#34; before restarting Kubelet\u0026#34; Mar 06 15:00:02 iZt4nd5yyw9vfuxn3q2g3tZ kubelet[102800]: E0306 15:00:02.463972 102800 kubelet.go:1392] \u0026#34;Failed to start ContainerManager\u0026#34; err=\u0026#34;start cpu manager error: could not restore state from checkpoint: configured policy \\\u0026#34;static\\\u0026#34; differs from state checkpoint policy \\\u0026#34;none\\\u0026#34;, please drain this node and delete the CPU manager checkpoint file \\\u0026#34;/var/lib/kubelet/cpu_manager_state\\\u0026#34; before restarting Kubelet\u0026#34; 将文件 /var/lib/kubelet/cpu_manager_state 删除后，kubelet 即可启动成功，新创建的 /var/lib/kubelet/cpu_manager_state 文件内容如下：\n1 {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-3\u0026#34;,\u0026#34;checksum\u0026#34;:611748604} 可以看到已经存在了绑核的 pod。\n如果节点上已经存在符合绑核条件的 pod，在修改配置并重启 kubelet 后即可绑核生效。\n绑核实践 配置 kubelet 的cpuManagerPolicy值为 static，创建 guaranteed pod：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 resources: limits: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; requests: memory: \u0026#34;200Mi\u0026#34; cpu: \u0026#34;2\u0026#34; 在 pod 调度的节点上，进入到 /sys/fs/cgroup/cpuset/kubepods.slice 目录下，跟绑核相关的设置均在该目录下，该目录的结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ ll /sys/fs/cgroup/cpuset/kubepods.slice -rw-r--r-- 1 root root 0 Mar 6 10:31 cgroup.clone_children -rw-r--r-- 1 root root 0 Mar 6 10:31 cgroup.procs -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.cpu_exclusive -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.cpus -r--r--r-- 1 root root 0 Mar 6 10:31 cpuset.effective_cpus -r--r--r-- 1 root root 0 Mar 6 10:31 cpuset.effective_mems -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.mem_exclusive -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.mem_hardwall -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.memory_migrate -r--r--r-- 1 root root 0 Mar 6 10:31 cpuset.memory_pressure -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.memory_spread_page -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.memory_spread_slab -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.mems -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.sched_load_balance -rw-r--r-- 1 root root 0 Mar 6 10:31 cpuset.sched_relax_domain_level drwxr-xr-x 2 root root 0 Mar 6 10:31 kubepods-besteffort.slice drwxr-xr-x 10 root root 0 Mar 6 10:31 kubepods-burstable.slice drwxr-xr-x 4 root root 0 Mar 6 15:09 kubepods-pod4dc3ad18_5bad_4728_9f79_59f2378de46e.slice -rw-r--r-- 1 root root 0 Mar 6 10:31 notify_on_release -rw-r--r-- 1 root root 0 Mar 6 10:31 pool_size -rw-r--r-- 1 root root 0 Mar 6 10:31 tasks 其中 kubepods-besteffort.slice 和 kubepods-burstable.slice 分别对应的 besteffort 和 burstable 类型的 pod 配置，因为这两种类型的 pod 并不执行绑核操作，所有的子目录下的 cpuset.cpus 文件均绑定的 cpu 核。\nkubepods-pod4dc3ad18_5bad_4728_9f79_59f2378de46e.slice 目录为要绑核的 pod 目录，其中 4dc3ad18_5bad_4728_9f79_59f2378de46e 根据 pod 的 uid 转换而来，pod 的 metadata.uid 字段的值为 4dc3ad18-5bad-4728-9f79-59f2378de46e，即目录结构中将-转换为_。\n目录结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ ll /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-pod4dc3ad18_5bad_4728_9f79_59f2378de46e.slice -rw-r--r-- 1 root root 0 Mar 6 15:09 cgroup.clone_children -rw-r--r-- 1 root root 0 Mar 6 15:09 cgroup.procs -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.cpu_exclusive -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.cpus -r--r--r-- 1 root root 0 Mar 6 15:09 cpuset.effective_cpus -r--r--r-- 1 root root 0 Mar 6 15:09 cpuset.effective_mems -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.mem_exclusive -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.mem_hardwall -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.memory_migrate -r--r--r-- 1 root root 0 Mar 6 15:09 cpuset.memory_pressure -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.memory_spread_page -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.memory_spread_slab -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.mems -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.sched_load_balance -rw-r--r-- 1 root root 0 Mar 6 15:09 cpuset.sched_relax_domain_level drwxr-xr-x 2 root root 0 Mar 6 15:09 cri-containerd-75f8e4b4185f673869604d300629ec2de934cf253244bf91c577f9fc0ba0f14a.scope drwxr-xr-x 2 root root 0 Mar 6 15:09 cri-containerd-cce0a338829921419407fcdc726d1a4bd5d4489da712b49a4834a020131ce718.scope -rw-r--r-- 1 root root 0 Mar 6 15:09 notify_on_release -rw-r--r-- 1 root root 0 Mar 6 15:09 pool_size -rw-r--r-- 1 root root 0 Mar 6 15:09 tasks 其中 cri-containerd-75f8e4b4185f673869604d300629ec2de934cf253244bf91c577f9fc0ba0f14a.scope 和 cri-containerd-cce0a338829921419407fcdc726d1a4bd5d4489da712b49a4834a020131ce718.scope 为 pod 的两个容器，其中一个为 pause 容器，另外一个为 nginx 容器。\n1 2 $ crictl pods | grep nginx-deploy cce0a33882992 6 hours ago Ready nginx-deployment-67778646bb-mgcpg default 0 (default) 其中第一列的 cce0a33882992 为 pod id，cri-containerd-cce0a338829921419407fcdc726d1a4bd5d4489da712b49a4834a020131ce718.scope 对应的为 pause 容器，查看该目录下的 cpuset.cpus 文件，绑定了所有的核，并未做绑核操作。\n1 2 $ crictl ps | grep nginx-deploy 75f8e4b4185f6 e4720093a3c13 6 hours ago Running nginx 0 cce0a33882992 nginx-deployment-67778646bb-mgcpg 其中第一列的 75f8e4b4185f6 为 container id，cri-containerd-75f8e4b4185f673869604d300629ec2de934cf253244bf91c577f9fc0ba0f14a.scope 对应的为 nginx 容器。查看 /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-pod4dc3ad18_5bad_4728_9f79_59f2378de46e.slice/cri-containerd-75f8e4b4185f673869604d300629ec2de934cf253244bf91c577f9fc0ba0f14a.scope/cpuset.cpus 对应的值为2-3，说明绑核成功。\n/var/lib/kubelet/cpu_manager_state 文件内容如下，跟 cgroup cpuset 实际配置可以完全对应：\n1 {\u0026#34;policyName\u0026#34;:\u0026#34;static\u0026#34;,\u0026#34;defaultCpuSet\u0026#34;:\u0026#34;0-1\u0026#34;,\u0026#34;entries\u0026#34;:{\u0026#34;4dc3ad18-5bad-4728-9f79-59f2378de46e\u0026#34;:{\u0026#34;nginx\u0026#34;:\u0026#34;2-3\u0026#34;}},\u0026#34;checksum\u0026#34;:3689800814} kubelet 内部实现 在 kubelet 内部，采用 cpu manager 模式实现绑核功能。\n总结 只能 by 节点配置，不能按照 pod 来灵活的配置。 无法适用于所有类型的 pod，pod 必须为 Guaranteed 时才允许开启。 修复配置较为麻烦，一旦配置变更后，需要删除文件 /var/lib/kubelet/cpu_manager_state 后才可生效，而且对现有的 pod 均有影响。 Memory Manager kubelet 中参数配置 k8s 1.21 版本引入，需要使用 featuregate 开启，参数 --feature-gates=MemoryManager=true。在k8s 1.22 版本为 beta 版本，默认开启。\nkubelet 通过参数 --memory-manager-policy 来配置内存管理策略，支持如下值：\nnone：默认策略，不执行任何内存分配的策略。 static：仅针对 Guaranteed pod 生效，对于 Guaranteed pod，会返回跟 NUMA 相关的 topology hint 信息。在该模式下，会修改 cgroup cpuset.mems 的配置为对应的 cpu core。 kubelet 将已经分配的 pod 的内存绑定信息位于文件 /var/lib/kubelet/memory_manager_state 中，文件格式为 json。\nTopology Manager Topology Manager 特性理解起来比较抽象。举个例子说明：上述的 CPU Manager 和 Memory Manager 的特性，在 kubelet 的实现中是完全独立的，可能会导致 cpu 和内存被分配到了不同的 numa 节点上，CPU Manager 通过 cgroup 的 cpuset.cpus 来控制容器要绑定的 cpu，而 Memory Manager 则通过 cgroup 的 cpuset.mems 来控制容器要使用的 NUMA node 内存。如果两者的信息不匹配，则会导致跨 NUMA Node 的内存访问，从而会对于性能要求高的应用产生影响。\nTopology Manager 是 kubelet 中的一部分功能，通过 Hint Providers 来发送和接收各个模块的 NUMA 拓扑信息，比如接收 CPU Manager 和 Memory Manager 的 NUMA Node以及 NUMA Node 分配的优先级。\nkubelet 的参数配置 kubelet 通过参数--topology-manager-scope来指定作用域：\ncontainer：默认值，按照容器级别分配到共同的 NUMA node 集合上。 pod：将 pod 内的所有 container 分配到共同的 NUMA node 集合上。 kubelet 通过参数 --topology-manager-policy 来设置 NUMA 的分配策略：\nnone：默认值，不执行任何的拓扑对齐。 best-effort：优先选择首选亲和性的 NUMA node，如果亲和性不满足，pod 仍然可以调度成功。 restricted：选择首选亲和性的 NUMA node，如果亲和性不满足，pod 调度失败。 single-numa-node：通过 Hint Provider 返回的结果，判断单 NUMA 节点的亲和性是否，如果不满足，则 pod 调度失败。 kubelet 中的实现 在 kubelet 中定义了接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // TopologyHint is a struct containing the NUMANodeAffinity for a Container type TopologyHint struct { // 记录了 NUMA Node 满足资源请求的位掩码 NUMANodeAffinity bitmask.BitMask // Preferred is set to true when the NUMANodeAffinity encodes a preferred // allocation for the Container. It is set to false otherwise. // 亲和性的结果是否为首选的 Preferred bool } // HintProvider is an interface for components that want to collaborate to // achieve globally optimal concrete resource alignment with respect to // NUMA locality. type HintProvider interface { // GetTopologyHints returns a map of resource names to a list of possible // concrete resource allocations in terms of NUMA locality hints. Each hint // is optionally marked \u0026#34;preferred\u0026#34; and indicates the set of NUMA nodes // involved in the hypothetical allocation. The topology manager calls // this function for each hint provider, and merges the hints to produce // a consensus \u0026#34;best\u0026#34; hint. The hint providers may subsequently query the // topology manager to influence actual resource assignment. GetTopologyHints(pod *v1.Pod, container *v1.Container) map[string][]TopologyHint // GetPodTopologyHints returns a map of resource names to a list of possible // concrete resource allocations per Pod in terms of NUMA locality hints. GetPodTopologyHints(pod *v1.Pod) map[string][]TopologyHint // Allocate triggers resource allocation to occur on the HintProvider after // all hints have been gathered and the aggregated Hint is available via a // call to Store.GetAffinity(). Allocate(pod *v1.Pod, container *v1.Container) error } CPU Manager、Memory Manager 和 Device Manager 均实现了该接口。在 Topology Manager 中根据各个 Manager 返回的 TopologyHint 数据，从而决定最终的 NUMA Node 分配，并调用各个 Manager 的 Allocate 来做最终的 NUMA Node 分配。\n资料 ACK CPU拓扑感知调度 控制节点上的 CPU 管理策略 配置 Pod 的服务质量 Kubelet之Topology Manager分析 ","date":"2024-03-22T20:08:26Z","permalink":"/post/k8s-%E4%B8%AD%E7%9A%84-numa-%E4%BA%B2%E5%92%8C%E6%80%A7/","title":"k8s 中的 numa 亲和性"},{"content":"kubeconfig 文件结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 apiVersion: v1 kind: Config clusters: - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJME1ETXlNVEE0TXpZMU1Wb1hEVE0wTURNeE9UQTRNelkxTVZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTFBaCjZUcUFNMVhiRkxIbnVvd1ZNT1FHeWQ2SzVBcGdwcmhYSlBvclVKdStoazBKd3BQQlNRZGNSdnJjYy9wNTNDbnQKTTBXWVN4dThTd1Z6a0dHajh0cHNSNjRkMWMxdFk1djYzYnlkVXN5M3JwME1OWUt1ckJPNEY2aVFLK01oL3R6UAp4eUdZei9BUnhheDdXNysvWEV6Y2FsdFp5T1JZZk9ISUR5ZjN5R3V2T1htYmw5ZEJmRHFBMFlSMGxOTFFlUEcrCm5lZkVGb1dUQncxUytiZEl0MDBRZnl3MVlvRXpkOFd6UDRBTzFlV3AxK0tJdFhLaUxyaWFBbjkzNTJhbnVIT2YKQ3h0U0NCbEwwRThCL3dGKzhTd0RQbDlUYkNhZU1nTUJpY2hsQzlYSjNrV2k2cElOVnVEeEpRUy82cnlwQWhLbgpHK21RWlJIdmhsUk1wTjhEbDBzQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZLay8zWDh0Q1dhZEpKN011SjNRNE5DL2xHd3pNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBSDR1K05zbE9ORWJVZXB1M0JaRQozL1hnQitFNDZVZ3N2d2R6UzhEWXF3YzBWQ28vUFc0RUNreW9IUkJINHJkdTVpaTBhVUhjZUxpNjhZQjlvQ3hpCmlkbHp3bUNGV1g1dEtGMUJTRFJ1YSt5MjVrOGZuSGtodm5IVG9CQ2c5ZlMzdFBOekNUdEtMSUhyVFpQVDhWU3IKdVYyZkdOcXMyT2djWjc4TmY4M2pnVVBXbFVPZkVsTDBrYjNYVHo5M29DdnF0RS9tRi9VOWhmOUdiRU5Lai9BSAovUHA4QWNmellkVGxGNTBva09temVFdnJnalZmaXFKMitGd2I5Lys1SUljSWFMR3IwZjVUMUVINzdxVEdEWElWCncwWGlKYmVTL1JTYTYrZFM0b1dRMS82Ryt1SUdRQWx0eTRSTXdqbG4rMTMvdStPelowTHErNzBlQTdUSm5mU0MKRVhBPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== server: https://127.0.0.1:55282 name: kind-kind - cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURsRENDQW55Z0F3SUJBZ0lVQVFVeTNtWi80eWZmbEhjTi9WblNIcUYwL0NRd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lqRUxNQWtHQTFVRUJoTUNWVk14Q3pBSkJnTlZCQWdUQWtOQk1SSXdFQVlEVlFRSEV3bFRkVzV1ZVhaaApiR1V4RURBT0JnTlZCQW9UQjB0MVltVmFiMjh4Q3pBSkJnTlZCQXNUQWtOQk1STXdFUVlEVlFRREV3cExkV0psCmNtNWxkR1Z6TUI0WERUSTBNRE15TVRBNE16a3dNRm9YRFRJNU1ETXlNREE0TXprd01Gb3dZakVMTUFrR0ExVUUKQmhNQ1ZWTXhDekFKQmdOVkJBZ1RBa05CTVJJd0VBWURWUVFIRXdsVGRXNXVlWFpoYkdVeEVEQU9CZ05WQkFvVApCMHQxWW1WYWIyOHhDekFKQmdOVkJBc1RBa05CTVJNd0VRWURWUVFERXdwTGRXSmxjbTVsZEdWek1JSUJJakFOCkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXlNNGo2d1NSdTUrcFpwckk1WkhxdmYycXpJdTUKSlZkYzRFUEp3cGY5eU9LN1FPZFM1SHJRUzVNK3ZEMFMzOUNJVFVYY2YxZUNJZy83SEt4TmVFbVk0TVdCMVlBWQpZOVRjMkZlR3JMMVBFR0lwNER6TnRRMkhvcWFxU2pJd0d0bnh3RjV5OGRRUGJkQ3JOdllDRVl5QlR2S0VtUXVoClFmdDhJR1NmaWJ0M3gwa2ZaaUFqZTJ5SDJabFNyMnBRSzRWWFdWUU5UV0hOQnlMS29Lb05Yazl2UTQ4dHhYbVUKcFRDWUxjcGdZSC9tU0lpY1FOcDQwRjRaOUUraGFjdTVkYVFVakIzZzQxWEVvYXBzL2xSa3d0bVFlV1gwVjR2VQp3cUlCS0doZmJ3dmluUjAvTmJGOVVLVldaVlpVL2R0NHR6TXQxVHg4L2tmcExmL0xnNXdoOThUSkJ3SURBUUFCCm8wSXdRREFPQmdOVkhROEJBZjhFQkFNQ0FRWXdEd1lEVlIwVEFRSC9CQVV3QXdFQi96QWRCZ05WSFE0RUZnUVUKWXp4aGFlN2V3TXNIeEJ5YnQ4U0Ntb01zVGg0d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFFWk5uUjI5SDVjRwpJSFBKdDlNMk5YZGt1NzFuU2VZb2s3SFBVdnJ6V0JOdFJjMUJPZkJRc25zb0RRcDQ1SmQxUnZsOEdLRHJjOCtICm5yaXZGQXpPZFRPQlhib3RMcFdGc0U1WU5VcGlUbWU2aW9pUVRVTnQ2WCtLd29CV00xNUwyWlJBYXdQQ0FBZ0sKeWRRU2lZaHVZMS93ekltWW1LenczRjVYb1BJcHVjTjhNam1MM3ZPNlFPaW51OFQrcW9wbWlmRFMraUVzRjcwSApwaW9mVXFJR3Zmbm5uVFFTWnFrRFAybXZ2VHFqT1lCbjU1dHRsL2JYQzh2ZHlwakNGVWRGOXNjUFM5R28ycTRuClkySUUvdHIvUFhaODUvUVVsdVQ2UEd5VUMycDNhZmNhdXRBbC9hQUZvM1ltK3BNbW03WUhad2JjcUp6U3BmRXIKbnJaekRNZFNGS2c9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K server: https://127.0.0.1:6443 name: zoo contexts: - context: cluster: kind-kind user: kind-kind name: kind-kind - context: cluster: zoo user: zoo-admin name: zoo current-context: kind-kind preferences: {} users: - name: kind-kind user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJSkNhSHFyUUl0b0l3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBek1qRXdPRE0yTlRGYUZ3MHlOVEF6TWpFd09ETTJOVEphTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXFNS1VOcStyTGNPTkxDWmMKTjA4SjBpYzhBdnhjdmlZNG40SVN1UFFleVViYUhLTVpRNnJpNjA4SlZoNUZxc3B3N3BoWTkrYVBMTHJQaG9uWApCQWhQdmVSSVVxeDdaTkJhODNuVUIrTXIrOXIrazAwTk1yNDBTdkg5aGpnTXVZUjFvN051OWRzN1k3U0pOcXVsCnlwOVN6YzUwUTNBbzh1cHBTRFlFRW5Hd3I0VVBidlp0ZTVlQXo2T2hDYy9hazZuZGlFcU9hMkdJRzhlUmEyWTkKM2hBQjl6V3YvVldGTkxFNXh2Mm5oS3JDQVl6TzBrVmJwTkNlSmkxRGFvNTIzQURWajhGQjRFbDFVNCtvTmM2Vwo5VHdRbDFIUklxZXJ6MUFMTFl1THhadGFFY2IwYW04N2dTcEtyaXcrYWVBL3h6L29tQ1BLY1IrQTBSTkpBcVdWCnduTzhDUUlEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JTcFA5MS9MUWxtblNTZXpMaWQwT0RRdjVScwpNekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBbjVCalc3VEZvYlBxWk5obS82cEJPVUdGUmVncHFKYVcrUFN1CktUVU5qNXdDU1ZjTWg3V2RmQWtBSGxrYzQ5YWZlaHpWZmVwUXJEUEpOam96eCsxOGc2ZmtNalJRdEhRaW9yYSsKck82UklBYnVJR0pqTXBKVGNGL25OMkY4amFzdFlrdkZ1cjNtdlVldzhsWmtEZEdYMFFaU0J2Y0xqVGdvZURmSQpLTmhjMGVaQ2QrMStGeVJYajZwaUs4Y3pBWlkvTTVsVHJSZTVQUmJSaHpMeVo0Wm1nMHZjOUZjZFcxbThVZ2hDCkVOWkZCVDA0WTl2bHRGTHJaSG9IRlFERlNKRUxTSnl5VG95dTVnYVh6OElZUm41Y0k0b1RPZXlKN2JvQ3hvdzEKa3VXbWdxbVZHdjZwUWJqRm0zaTBTZXFRMkQ5bU1SaDhsYW0vMlhiaVlyWlBjQUlaRnc9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== client-key-data: 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 - name: zoo-admin user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQyRENDQXNDZ0F3SUJBZ0lVYlBFdGx4LzRTYm9TckRvZTd1djllS1YrckZNd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1lqRUxNQWtHQTFVRUJoTUNWVk14Q3pBSkJnTlZCQWdUQWtOQk1SSXdFQVlEVlFRSEV3bFRkVzV1ZVhaaApiR1V4RURBT0JnTlZCQW9UQjB0MVltVmFiMjh4Q3pBSkJnTlZCQXNUQWtOQk1STXdFUVlEVlFRREV3cExkV0psCmNtNWxkR1Z6TUI0WERUSTBNRE15TVRBNE16a3dNRm9YRFRJMU1ETXlNVEE0TXprd01Gb3dhVEVMTUFrR0ExVUUKQmhNQ1ZWTXhDekFKQmdOVkJBZ1RBa05CTVJJd0VBWURWUVFIRXdsVGRXNXVlWFpoYkdVeEZ6QVZCZ05WQkFvVApEbk41YzNSbGJUcHRZWE4wWlhKek1SQXdEZ1lEVlFRTEV3ZExkV0psV205dk1RNHdEQVlEVlFRREV3VmhaRzFwCmJqQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU1PYnpoeDBOTW5ubTdHbmFjb2sKWHJ1RmxrRVBXc01la2xLNnBtbWFDRUVVUTNocHdsUWw2Z09UekpwaUVYVGZMZU1Uc29ZZ1NtNGFySWdjdVlTcwo3cDdzdWhhUi9RN1g3SUZjMXJEMzFtODB5amZEVUhYZi9jSWhDVmp0NDhLY1JLL2QrK2NzZkU5eHpEdlVBdDVxCmxmNFNvMy9YSnJyYzdtdURiUXdYdDgvaGQ4RnVZYUFWN2YraXVMMW5DRTB3b01IaFNPSkpEWC9TQ1pMakxTS2wKYU9aM2lOL0dDRXo4cldPUmVBbGUwQzRwazRwc0xmN1h6UlJpcnFTT21lQ3JTZzJoNlU1OS8xTHN5dXVqeTVFWQpxT3RsZ0R0Z3l5RDhBRWZhanM5R0NNRnFFOStHZWR6NkpuRmFlUlpOZThlVWNhVU1GWFVSWjYxb0pqT2UraUZ4CjU3TUNBd0VBQWFOL01IMHdEZ1lEVlIwUEFRSC9CQVFEQWdXZ01CMEdBMVVkSlFRV01CUUdDQ3NHQVFVRkJ3TUIKQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CQWY4RUFqQUFNQjBHQTFVZERnUVdCQlRPL0JxUE9HcVU1V0JYSW9wQgoxajdxYVBvTkh6QWZCZ05WSFNNRUdEQVdnQlJqUEdGcDd0N0F5d2ZFSEp1M3hJS2FneXhPSGpBTkJna3Foa2lHCjl3MEJBUXNGQUFPQ0FRRUF0SVRaN0FUVVhManB6UWtQVGM0RVFKYi9IZlJTcklOV3pSMlpRcVI1Q2J1dXEzR08KUktDZFphenVrakJjMCtXVkhGcVo4SEtNNUR2YThKbzZCUXgycXoxZ0ptVE1oYUdRMFlyNWFFcmxFZWJCWUcwVgpLMGh4LzJuMmNmMkt3N3VBOWdkeUJKSVFJbnY2RFJPUmt6VVNuQXJEd21TNitUNXdKK0lTUGlPdHVGRnQzazRyClZsZ1N3bjE5WHRSRnJ4OEI4dWRkSlZ4VEloN3FPa09hbURQaFJrTGJKNFdOUk5hYlpVMFVxdmxUQzVnQkhnS2QKSm1FaVQybHNoUENjS2lRZWsrTVNzNU9mNG9ZQml0TXM0eW1iendXb1hVaGcvVnRTbTdtT2MzUmo2ZGhrQXl2NwozQTJoZW1xUENKQ1JwbndzUDlpd0VrWUE0SlFZMG53aEUrd1h2Zz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K client-key-data: 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 cluster 字段 certificate-authority-data 字段：服务端的 ca 证书，用来验证 kube-apiserver 证书的正确性。\n当使用 kubectl 发送请求到 kube-apiserver 时，kube-apiserver 会返回通过参数 --tls-cert-file 配置的证书文件，kubectl 通过 kubeconfig 中的 certificate-authority-data 字段来校验 kube-apiserver 返回证书的有效性。\n当 kubectl 指定了参数 --insecure-skip-tls-verify=true，即可跳过对 kube-apiserver 证书的校验。\nusers name：用户名称 client-certificate-data：kubectl 连接 kube-apiserver 时使用的客户端证书，会发送给 kube-apiserver。内容经过 base64 编码。 client-key-data：客户端私钥信息。内容经过 base64 编码。 client-certificate-data 对应的为用户的公钥信息，使用命令 echo 'xx' | base64 -d \u0026gt; /tmp/client.crt; openssl x509 -in /tmp/client.crt -noout -text可对证书的内容进行解密。解密完成后的证书内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 Certificate: Data: Version: 3 (0x2) Serial Number: 2604918601715070594 (0x242687aab408b682) Signature Algorithm: sha256WithRSAEncryption Issuer: CN = kubernetes Validity Not Before: Mar 21 08:36:51 2024 GMT Not After : Mar 21 08:36:52 2025 GMT Subject: O = system:masters, CN = kubernetes-admin Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:a8:c2:94:36:af:ab:2d:c3:8d:2c:26:5c:37:4f: 09:d2:27:3c:02:fc:5c:be:26:38:9f:82:12:b8:f4: 1e:c9:46:da:1c:a3:19:43:aa:e2:eb:4f:09:56:1e: 45:aa:ca:70:ee:98:58:f7:e6:8f:2c:ba:cf:86:89: d7:04:08:4f:bd:e4:48:52:ac:7b:64:d0:5a:f3:79: d4:07:e3:2b:fb:da:fe:93:4d:0d:32:be:34:4a:f1: fd:86:38:0c:b9:84:75:a3:b3:6e:f5:db:3b:63:b4: 89:36:ab:a5:ca:9f:52:cd:ce:74:43:70:28:f2:ea: 69:48:36:04:12:71:b0:af:85:0f:6e:f6:6d:7b:97: 80:cf:a3:a1:09:cf:da:93:a9:dd:88:4a:8e:6b:61: 88:1b:c7:91:6b:66:3d:de:10:01:f7:35:af:fd:55: 85:34:b1:39:c6:fd:a7:84:aa:c2:01:8c:ce:d2:45: 5b:a4:d0:9e:26:2d:43:6a:8e:76:dc:00:d5:8f:c1: 41:e0:49:75:53:8f:a8:35:ce:96:f5:3c:10:97:51: d1:22:a7:ab:cf:50:0b:2d:8b:8b:c5:9b:5a:11:c6: f4:6a:6f:3b:81:2a:4a:ae:2c:3e:69:e0:3f:c7:3f: e8:98:23:ca:71:1f:80:d1:13:49:02:a5:95:c2:73: bc:09 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Authority Key Identifier: A9:3F:DD:7F:2D:09:66:9D:24:9E:CC:B8:9D:D0:E0:D0:BF:94:6C:33 Signature Algorithm: sha256WithRSAEncryption Signature Value: 9f:90:63:5b:b4:c5:a1:b3:ea:64:d8:66:ff:aa:41:39:41:85: 45:e8:29:a8:96:96:f8:f4:ae:29:35:0d:8f:9c:02:49:57:0c: 87:b5:9d:7c:09:00:1e:59:1c:e3:d6:9f:7a:1c:d5:7d:ea:50: ac:33:c9:36:3a:33:c7:ed:7c:83:a7:e4:32:34:50:b4:74:22: a2:b6:be:ac:ee:91:20:06:ee:20:62:63:32:92:53:70:5f:e7: 37:61:7c:8d:ab:2d:62:4b:c5:ba:bd:e6:bd:47:b0:f2:56:64: 0d:d1:97:d1:06:52:06:f7:0b:8d:38:28:78:37:c8:28:d8:5c: d1:e6:42:77:ed:7e:17:24:57:8f:aa:62:2b:c7:33:01:96:3f: 33:99:53:ad:17:b9:3d:16:d1:87:32:f2:67:86:66:83:4b:dc: f4:57:1d:5b:59:bc:52:08:42:10:d6:45:05:3d:38:63:db:e5: b4:52:eb:64:7a:07:15:00:c5:48:91:0b:48:9c:b2:4e:8c:ae: e6:06:97:cf:c2:18:46:7e:5c:23:8a:13:39:ec:89:ed:ba:02: c6:8c:35:92:e5:a6:82:a9:95:1a:fe:a9:41:b8:c5:9b:78:b4: 49:ea:90:d8:3f:66:31:18:7c:95:a9:bf:d9:76:e2:62:b6:4f: 70:02:19:17 其中的 Subject 中的 O 对应的 k8s 中的 Group，CN 对应的 k8s 中的 User。kube-apiserver 会通过证书的 O 和 CN 获取到 User 和 Group 信息。在 k8s 系统中，实际上并没有存储 Group 和 User 信息，而是完全依赖该证书中的信息。\nkubeconfig 文件生成 kubeconfig 文件本质上是个证书，包含了 ca、证书公钥和证书私钥，在有了证书后可以通过 kubectl 命令生成新的 kubeconfig 文件\n1 2 3 4 kubectl --kubeconfig ~/.kube/111111.kubeconfig config set-cluster hello --certificate-authority=/tmp/ca.pem --embed-certs=true --server=https://127.0.0.1:6443 kubectl --kubeconfig ~/.kube/111111.kubeconfig config set-credentials hello-admin --client-certificate=/tmp/tls.crt --client-key=/tmp/tls.key --embed-certs=true kubectl --kubeconfig ~/.kube/111111.kubeconfig config set-context hello --cluster=hello --user=hello-admin kubectl --kubeconfig ~/.kube/111111.kubeconfig config use-context hello 使用 curl 命令直接访问 kube-apiserver 由于 kube-apiserver 开启了双向认证，使用 curl 命令访问 kube-apiserver 时，curl 需要指定证书信息，证书信息可以使用 kubeconfig 中的证书信息。\n1 2 3 4 5 6 7 8 WORK_DIR=/tmp KUBECONFIG=~/.kube/config CONTEXT=kind-kind server=`yq eval \u0026#39;.clusters.[]|select(.name==\u0026#34;\u0026#39;$CONTEXT\u0026#39;\u0026#34;)|.cluster.server\u0026#39; $KUBECONFIG` yq eval \u0026#39;.users.[]|select(.name==\u0026#34;\u0026#39;$CONTEXT\u0026#39;\u0026#34;)|.user.client-certificate-data\u0026#39; $KUBECONFIG | base64 --decode \u0026gt; ${WORK_DIR}/client.crt yq eval \u0026#39;.users.[]|select(.name==\u0026#34;\u0026#39;$CONTEXT\u0026#39;\u0026#34;)|.user.client-key-data\u0026#39; ~/.kube/config | base64 --decode \u0026gt; ${WORK_DIR}/client.key yq eval \u0026#39;.clusters.[]|select(.name==\u0026#34;\u0026#39;$CONTEXT\u0026#39;\u0026#34;)|.cluster.certificate-authority-data\u0026#39; $KUBECONFIG | base64 --decode \u0026gt; ${WORK_DIR}/ca.crt curl --cert ${WORK_DIR}/client.crt --key ${WORK_DIR}/client.key --cacert ${WORK_DIR}/ca.crt \u0026#34;$server/apis/apiextensions.k8s.io/v1/customresourcedefinitions?limit=500\u0026amp;resourceVersion=0\u0026#34; 资料 kubeconfig文件全解析 生成kubeconfig常规的两种方法 ","date":"2024-03-22T19:58:22Z","permalink":"/post/kubeconfig/","title":"kubeconfig"},{"content":"设置评论系统 我的博客系统 Hexo 之前使用 Gitment 作为评论：《hexo添加gitment评论系统》，后来年久失修，了解了下基于 Github Discussions 的 giscus 用的比较多，我也采用该方案。\n在配置 giscus 之间需要满足如下三个条件：\n该仓库是公开的，否则访客将无法查看 discussion。 giscus app 已安装，否则访客将无法评论和回应。 Discussions 功能已在你的仓库中启用。 在 Github 仓库中安装 gitcus 我这里直接选择了我的 hexo 仓库 kuring/kuring.github.io 作为了 git 仓库，因为该仓库本来的用途本来就是博客相关的内容，权限也是 public 的。\n接下来安装 Github app giscus，访问 https://github.com/apps/giscus，在安装时仅选择需要的 repo 就可以了，不需要所有的 repo 都放开。\n在 Github 仓库中打开 Discussions 功能 进入到 Github 项目的 Settings -\u0026gt; General -\u0026gt; Features -\u0026gt; Discussions 即可打开该功能。\n获取到 Github 项目相关的 gitcus 配置 访问页面 https://giscus.app/zh-CN，在配置章节中设置相关的配置，可以获取到类似下面内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;kuring/kuring.github.io\u0026#34; data-repo-id=\u0026#34;MDEwOlJlcG9zaXRvcnkyODM4MzQ0NTk=\u0026#34; data-category=\u0026#34;Announcements\u0026#34; data-category-id=\u0026#34;DIC_kwDOEOr4W84CdeTU\u0026#34; data-mapping=\u0026#34;url\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;preferred_color_scheme\u0026#34; data-lang=\u0026#34;zh-CN\u0026#34; data-loading=\u0026#34;lazy\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 在 Hexo 中配置 gitcus 因为 Hexo 我使用的为 Next 主题，我使用了 Next 的插件 hexo-next-giscus。\n在 Git 仓库目录下执行 npm install hexo-next-giscus 即可安装 giscus。\n修改 Git 仓库下的 _config.yml 文件，在最后面增加如下的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 giscus: enable: false repo: # Github repository name repo_id: # Github repository id category: # Github discussion category category_id: # Github discussion category id # Available values: pathname | url | title | og:title mapping: pathname # Available values: 0 | 1 reactions_enabled: 1 # Available values: 0 | 1 emit_metadata: 1 # Available values: light | dark | dark_high_contrast | transparent_dark | preferred-color-scheme theme: light # Available values: en | zh-CN lang: en # Place the comment box above the comments input_position: bottom # Load the comments lazily loading: lazy 其中很多的字段值来自上个章节获取到的 json 结果。\n至此，即完成了整个 gitcus 的配置。\n资料 giscus GitHub Discussions 快速入门 hexo-next-giscus ","date":"2024-02-24T11:18:00Z","permalink":"/post/hexo-%E4%BD%BF%E7%94%A8-giscus-%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/","title":"Hexo 使用 giscus 评论系统"},{"content":"在前序文章中，实现了将树莓派 5 刷成了 Android TV 电视盒子，基本的功能已经完备，本文将来介绍如何使用遥控器来控制 Android TV。\n我要看电视 - 投影仪和电视盒子选型 我要看电视 - 将树莓派 5 打造成 Android TV 电视盒子折腾记（1） 树莓派的定位是用作小型计算机使用，而作为电视盒子却必须要具备遥控器来控制的功能。要想通过遥控器来控制树莓派有多种方式可供选择，下面列一下我自己尝试过的方案。\n使用手机遥控 Android TV 这是最简单成本最低的纯软方案，只要在手机上安装软件就可以来控制 Android TV 了，手机跟 Android TV 在同一个局域网下即可。我使用到了 IOS 上的 App Remote TV，软件免费，但有广告。\n使用上功能完备，比如开关 Android TV 的功能也是完备的。Android TV 的关系操作并非真正的关机，实际上还是在低功耗运行，网络还是可以连接的。也就意味着通过 App 实际上是可以开机的。\n不方便的地方在于，每次操作需要额外拿起手机打开 App 后才能操作，没有硬件的遥控器来的方便。\n使用投影仪的遥控器 我使用了爱普生的 TZ2800 投影仪作为播放设备，投影仪的 HDMI 已经支持了 HDMI CEC 功能。这里吐槽下传统企业爱普生，在官方产品页面中很难找到关于该款投影仪的 HDMI 是否支持 CEC 的介绍，或者存在某个地方，但总归我找不到。\nHDMI CEC（Consumer Electronics Control） 功能可以实现通过 HDMI 连接让多个设备之间相互控制，可以实现同一个遥控器来控制投影仪和 Android TV 的功能：\n当关闭投影仪的时候也可以关闭 Android TV。 当打开投影仪的时候可以打开 Android TV。 要想使用 HDMI CEC 功能，需要在 Android TV 系统中开启相关的功能。\n下面的界面用来选择 HDMI CEC 的支持接口，只能支持一个 HDMI 接口。\n由于跟投影仪共用一个遥控器，功能上还是有所受限。比如音量的大小功能，只能操作投影仪，而不能再设置 Android TV 的音量大小。但绝大多数的功能已经具备。\n使用蓝牙遥控器 家里正巧有个办移动宽带送的电视盒子，该遥控器为蓝牙遥控器。在遥控器的背面正巧有蓝牙配对的方法。\n打开 Android Tv 系统中的 设置 -\u0026gt; 系统 -\u0026gt; 遥控器和手柄 功能。同时按住菜单键和返回键即可蓝牙配对。\n蓝牙配对成功后，使用过程中功能正常。但发现个致命的缺点：蓝牙会自动断开，而且无法自动连接，下次只能重新配对。我不太确定是否为遥控器的问题，但单这一点已经否定了我使用蓝牙遥控器的方案。\n使树莓派设备支持遥控器控制 树莓派通过 24 个 GPIO 引脚提供了强大的扩展性，可以接入外部硬件设备来接收遥控器的信号。\n我们来了解下常见的遥控器的实现原理：\n红外遥控器：最为普遍的遥控器，使用红外线发射信号，接收端需要有对应的红外接收模块。在发射信号时需要对准接收端。 无线遥控器：通常采用了 2.4 GHz 无线频段作为传输介质，接收端需要有可以匹配的设备接收信号，比如很多无线鼠标提供了一个很小的 USB 接收端。 蓝牙遥控器：使用蓝牙技术通讯，而蓝牙技术同样采用了 2.4 GHz 的无线频段，但好处在于很多的设备都支持蓝牙协议，不需要接收端再有一个定制化的硬件。 接收红外信号 我家里正巧有几个废弃的红外遥控器，因此可以作为树莓派的遥控器来使用。剩下的事情首先需要树莓派通过 GPIO 引脚接收到红外信号。\n在万能的淘宝上，花了几块钱买到了红外接收器和杜邦线（用来连接红外接收头和 GPIO 引脚），杜邦线需要注意为母对母规格。\n红外接收器连接到树莓派 在默认情况下，树莓派 GPIO 引脚的功能并未开启。打开 Android TV 系统中的 Raspberry Pi settings -\u0026gt; IR -\u0026gt; Infrared remote开关。可以看到接收信号的为 GPIO 18 引脚。\n在红外接收器上包含了三个引脚，依次为：OUT（输出信号）、GND（接地信号）、VCC（工作电压），其中红外接收器的 OUT 引脚对应的 GPIO 18 引脚。\n树莓派提供了 3.3V 和 5V 两种工作电压，查看红外信号接收器的工作电压在 2.7V ~ 5.5V 之间，我直接使用了 GPIO 的 5V 的引脚。\n查看树莓派的各个引脚线路图：\n即可得到红外信号接收器跟 GPIO 引脚的对应关系：\n红外信号接收器引脚 GPIO 引脚 OUT 18 GND 9（就近原则） VCC 2（4 和 6 被风扇电源占用） 在将硬件连接好后就可以启动树莓派了。\n遥控器编码配对 家里可能有多个红外遥控器，那么为什么红外遥控器之间不会出现相互冲突，本来想打开电视，却打开了空调的情况呢？原因是红外遥控器上的每个按键都对应了一个编码，而不同设备的遥控器对应的编码是不同的。电视遥控器发出的开机信号虽然被空调接收到了，但是并不会对该按键的编码做处理，空调也就不会被开机。\n我从家里随便找了一个红外遥控器也就不可能直接用来控制 Android TV 了，因此需要让 Android TV 可以识别到我的红外遥控器对应的按键编码。\n要想识别到红外遥控器的编码，需要使用到 Android TV 系统中命令行工具 ir-keytable，需要以 ssh 的方式连接 Android TV。Android TV 的连接方式可以参考我之前文章《我要看电视 - 将树莓派 5 打造成 Android TV 电视盒子折腾记（1）》中的 SSH 章节部分。\n在命令行执行 ir-keytable -p all -t，并按下遥控器的按键，在命令行中可以获取到类似如下的输出：\n1 2 3 4 5 6 7 991.988018: lirc protocol(necx): scancode = 0x835590 991.988023: event type EV_MSC(0x04): scancode = 0x835590 991.988023: event type EV_SYN(0x00). 992.104019: lirc protocol(necx): scancode = 0x835590 992.104025: event type EV_MSC(0x04): scancode = 0x835590 992.104025: event type EV_SYN(0x00). 其中 916.968014 表示的事件发生的时间戳。necx 表示使用的为 NEC 扩展协议，NEC 为一种常见的红外编码协议。而 scancode 字段即为我们需要获取的按键编码。EV_SYN 表示一组时间的结束。\n在上面的命令中，按下遥控器的按键一次，发送出了两个相同信号，也就打印出了两个相同日志块。\n打开 Github 项目：https://github.com/lineage-rpi/android_external_ir-keytable/tree/lineage-18.1/rc_keymaps，可以看到里面有很多编码跟对应键之间的映射关系，因为我的红外遥控器没有数字键，我挑选了一个跟遥控器键比较相似的文件进行重新修改，将其中的编码修改我遥控器对应的编码，并保存到本地文件 rc_keymap.txt 中。我的文件类似如下，其中 KEY_PREVIOUSSONG 和 KEY_PLAYPAUSE 两个按键我的遥控器上没有，我这里未做修改。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # table allwinner_ba10_tv_box, type: NEC 0x646dca KEY_UP 0x646d81 KEY_VOLUMEDOWN 0x217 KEY_NEXTSONG 0x646ddc KEY_POWER 0x646dc5 KEY_BACK 0x646dce KEY_OK 0x646dd2 KEY_DOWN 0x646d80 KEY_VOLUMEUP 0x254 KEY_PREVIOUSSONG 0x255 KEY_PLAYPAUSE 0x646d82 KEY_MENU 0x646d88 KEY_HOMEPAGE 0x646dc1 KEY_RIGHT 0x646d99 KEY_LEFT 需要注意的是，第一行看似是个注释，实际不是，不要删掉，保留原文件中的格式不变。\nAndroid TV 系统开机时加载配置 有了遥控器按键和编码之间的映射关系后，需要将其放到 Android TV 系统的 /boot/rc_keymap.txt 文件中，让 Android TV 开启自动加载该配置。该文件默认情况下不存在需要创建。\n分区 /boot 默认为只读模式，不允许在其中增加文件：\n1 2 127|:/boot # echo \u0026#34;123\u0026#34; \u0026gt; aa sh: can\u0026#39;t create aa: Read-only file system 将 /boot 目录重新 mount 为读写模式：\n1 2 3 4 5 6 7 $ mount | grep \u0026#39;/boot \u0026#39; /dev/block/mmcblk0p1 on /boot type vfat (ro,relatime,fmask=0000,dmask=0000,allow_utime=0022,codepage=437,iocharset=ascii,shortname=mixed,errors=remount-ro) $ mount -o remount,rw /boot $ mount | grep \u0026#39;/boot \u0026#39; /dev/block/mmcblk0p1 on /boot type vfat (rw,relatime,fmask=0000,dmask=0000,allow_utime=0022,codepage=437,iocharset=ascii,shortname=mixed,errors=remount-ro) 创建并将配置写入到文件 /boot/rc_keymap.txt 中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026lt;\u0026lt; EOF \u0026gt; /boot/rc_keymap.txt # table allwinner_ba10_tv_box, type: NEC 0x646dca KEY_UP 0x646d81 KEY_VOLUMEDOWN 0x217 KEY_NEXTSONG 0x646ddc KEY_POWER 0x646dc5 KEY_BACK 0x646dce KEY_OK 0x646dd2 KEY_DOWN 0x646d80 KEY_VOLUMEUP 0x254 KEY_PREVIOUSSONG 0x255 KEY_PLAYPAUSE 0x646d82 KEY_MENU 0x646d88 KEY_HOMEPAGE 0x646dc1 KEY_RIGHT 0x646d99 KEY_LEFT EOF 重新将 /boot 分区挂载为只读模式：\n1 mount -o remount,ro /boot 在执行完后，重新启动系统。在顺利的情况下，即可以通过红外遥控器直接操作 Android TV 系统。\n总结 到目前为止，我主要在使用的方式为红外遥控器模式，毕竟一个电视盒子还是应该有属于自己的遥控器。Good Luck！\n","date":"2024-02-24T00:41:46Z","permalink":"/post/%E5%B0%86%E6%A0%91%E8%8E%93%E6%B4%BE-5-%E6%89%93%E9%80%A0%E6%88%90-android-tv-%E7%94%B5%E8%A7%86%E7%9B%92%E5%AD%90%E6%8A%98%E8%85%BE%E8%AE%B0-2-%E9%81%A5%E6%8E%A7%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE/","title":"将树莓派 5 打造成 Android TV 电视盒子折腾记 （2）- 遥控器的配置"},{"content":"随着处理器数量的增加，所有的处理器均通过同一个北桥来访问内存，导致内存访问延迟增加，内存带宽成为瓶颈。\nNUMA（Non-Uniform Memory Access）非统一内存访问，是一种针对多处理器系统的组织结构。处理器被分配到不同的节点，每个节点有自己的本地内存，处理器可以访问本地内存和其他节点的内存，但访问本地内存的速度要远远快于访问其他节点的内存。\n几个概念：\nSocket：一颗物理 CPU。 Numa Node：逻辑概念，对 CPU 分组的抽象，一个 Node 即为一个分组，一个分组下可以包含多个 CPU。每个 Node 都有自己的本地资源，包括内存和 IO。Numa Node 之间不共享 L3 cache。一个 Numa Node 内的不同 core 之间共享 L3. Core：一颗物理 CPU 的物理核，每个 Core 有独立的 L1 和 L2，Core 之间共享 L3。 Thread/Processor：一颗物理 CPU 的逻辑核，用超线程的方式模拟出两个逻辑核。Processor 之间共享 L1 和 L2 cache，在开启超线程后单核的性能会下降一些。 一个 NUMA Node 可以有一个 Socket，每个 Socket 包含一个或者多个物理 Core。\n常用命令 查看物理核数 Socket：lscpu | grep Socket 每个 Socket 包含的物理核 Core：lscpu | grep 'Core(s) per socket' 每个 Socket 包含的 Thead/Processor 数量：lscpu | grep 'Thread(s) per core' 查看所有的 Processor 数量：cat /proc/cpuinfo | grep \u0026quot;processor\u0026quot; | wc -l\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ numactl --hardware available: 4 nodes (0-3) node 0 cpus: 0 1 2 3 4 5 6 7 32 33 34 35 36 37 38 39 node 0 size: 128470 MB node 0 free: 88204 MB node 1 cpus: 8 9 10 11 12 13 14 15 40 41 42 43 44 45 46 47 node 1 size: 129019 MB node 1 free: 67982 MB node 2 cpus: 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55 node 2 size: 129019 MB node 2 free: 38304 MB node 3 cpus: 24 25 26 27 28 29 30 31 56 57 58 59 60 61 62 63 node 3 size: 128965 MB node 3 free: 45689 MB node distances: node 0 1 2 3 0: 10 16 28 22 1: 16 10 22 28 2: 28 22 10 16 3: 22 28 16 10 如果没有开启 numa，默认只会看到一个 node，类似如下：\n1 2 3 4 5 6 7 8 $ numactl --hardware available: 1 nodes (0) node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 node 0 size: 772904 MB node 0 free: 510590 MB node distances: node 0 0: 10 通过 numastat 命令可以看到 numa 的 miss 等情况，对于排查性能问题非常有帮助。\n1 2 3 4 5 6 7 8 $ numastat node0 node1 node2 node3 numa_hit 6010986094 3863188690 7861549559 9798877648 numa_miss 0 0 113102530 0 numa_foreign 0 0 0 113102530 interleave_hit 17976 17846 17965 17839 local_node 6010748681 3862984578 7861301902 9798751537 other_node 237411 204112 113350178 126111 资料 CPU 拓扑：从 SMP 谈到 NUMA （理论篇） ","date":"2024-02-21T18:04:26Z","permalink":"/post/numa/","title":"numa"},{"content":"在上一篇文章 《我要看电视 - 投影仪和电视盒子选型》中，提到了用树莓派 5 来作为电视盒子，需要刷 Android TV 系统，本文将详细介绍折腾经历。\n在这之前我并没有接触过树莓派，Android 系统倒是曾经刷过机，但也了解不深。文中如有不正确的地方，欢迎指正。\n准备工作 要想折腾树莓派，还是需要一点点设备，我使用到的设备清单如下：\n一台 Mac 笔记本电脑：用来下载镜像、烧录景象、远程连接电视盒子等。其他操作系统的电脑均可。 一个 32G SD 卡：用来作为树莓派的存储系统 RAM。 一个 SD 读卡器：用来读取 SD 卡，供电脑烧录系统使用。 一个 U 盘：该 U 盘并非树莓派的 SD 卡，而是作为树莓派的额外存储，用来向树莓派中复制文件。 一台显示器：用来显示树莓派的内容，也可以是电视或者投影仪等设备。 一个 USB 键盘：用来连接树莓派进行操作。 一个 USB 鼠标：用来连接树莓派进行操作。 一个蓝牙遥控器：用来连接树莓派的 Android TV 系统。我直接使用了家里办宽带送的移动电视盒子的遥控器。 一根 HDMI 数据线：其中一头为 Micro HDMI，用来连接树莓派，另外一头为标准 HDMI 口，用来连接显示器。 另外：\n网络需要能够访问 Google 等网站。 寻找 Android TV 系统 找到树莓派对应的 Android TV 镜像是非常重要的前提，找到合适的 ROM 永远是 Android 系统刷机中非常重要的一环，一般每个设备总有那么几个大神在提供各类 ROM。\n查找一个设备的资源最快的方式就是去 github.com 上查找 awesome，树莓派的项目地址为：awesome-raspberry-pi。在 OS Images 章节中包含了支持树莓派的多种 OS，单纯搜索 Android TV 并不能找到对应的 OS，但实际上 KonstaKANG 对应就是 Android 镜像，这是文档不好的地方，并没有将简介写清楚。 KonstaKANG 并非一个系统，而是一个网站，包含了多种设备的 OS 镜像。对应的 Raspberry 5 的页面包含了 AOSP 和 LineageOS 两种 OS 镜像，AOSP 和 LineageOS 均为 android 的发型版。但只提供了 LineageOS 20 一个 Android TV 的版本，剩下的两个为 Android 版本。Android TV 和 Android 并非同一个系统，Android TV 是针对电视使用的系统，而 Android 是针对智能手机和平板使用的系统，用户体验上还是有较大区别。 因此，LineageOS 20 Android TV (Android 13) 即变为了目前唯一一个可以在树莓派 5 上使用 Android TV 系统，除此之外，别无他选。无论该系统是否完善，这就是目前的唯一选择了。在文章中介绍了非常多的系统方面的支持，建议精读一遍，包括文章后面的评论。\n文中的两个镜像，一个是原始镜像，另外一个是 ota 补丁包，两个均需要安装。 用到的文件 由于在国内下载非常不稳定，通常需要魔法下载，我将需要用到的文件上传到云盘供下载使用。我对部分软件的用途一知半解，但我知道安装了应该没有坏处，选择全部安装。\nlineage-20.0-20240112-UNOFFICIAL-KonstaKANG-rpi5-atv.zip 提取码：rgad lineage-20.0-20240112-UNOFFICIAL-KonstaKANG-rpi5-atv-ota.zip 提取码: 3t4m MindTheGapps-13.0.0-arm64-ATV-full-20240104_210039.zip 提取码：33wj lineage-20.0-rpi-resize.zip 提取码：c6ja lineage-20.0-rpi-magisk-v25.2.zip lineage-20.0-rpi-widevine.zip Magisk-v25.2.apk lineage-20.0-rpi-gsfaid.zip 刷 LineageOS 到树莓派 树莓派官方提供了镜像烧录工具 Imager，支持 Windows、Ubuntu、Mac，刷机非常方便。 将树莓派的 SD 插入到电脑，在 Raspberry Pi Device 中选择 RASPBERRY PI 5，在请选择需要写入的操作系统中使用 Use custom 选项选择已经下载好的文件 lineage-20.0-20240112-UNOFFICIAL-KonstaKANG-rpi5-atv.zip，选择对应的 SD 卡，再点击 Next 即可开始将镜像写入到SD 卡中。\n值得一提的是，对于树莓派其他的系统，都不需要事先下载，选择系统后，该工具可以自动下载最新版本，这个功能还是非常值得👍。\nLineageOS 初步体验及初步设置 在将系统刷入 SD 卡后，即可将 SD 卡放到树莓派 5 中，在树莓派中连接好键盘和鼠标。 下面步骤中可能会用到键盘的一些快捷键：F1 = Home, F2 = Back, F3 = Multi-tasking, F4 = Menu, F5 = Power, F11 = Volume down, and F12 = Volume up 开机后即可看到 LineageOS 的开机动画。 接下来就会看到查找蓝牙设备的界面，这里可以选择等待一会后自动跳过。 接下来就来到了 Welcome 的界面，点击 Start。 接下来会进入到选择语言、连接 WIFI 等操作，完成后即可进入到操作系统界面。操作系统界面可谓简洁到不能再精简，只有一个文件的应用。左上角的语音和搜索功能均不能使用。 但设置功能作为 Android 系统的核心，这部分功能一点都不会少。在设置中连接 Wifi 后发现会持续断开，不知道是否为系统的问题。\n打开 Recovery 模式：设置 -\u0026gt; 系统 -\u0026gt; Buttons，打开右侧的 Advanced restart。 打开开发者模式：设置 -\u0026gt; 系统 -\u0026gt; 关于 -\u0026gt; Android TV 操作系统版本，连续点击键盘的回车键，会提示开发者模式已打开。 打开 Rooted debug：设置 -\u0026gt; 系统 -\u0026gt; 开发者选项 -\u0026gt; 打开 USB 调试、Rooted debugging、ADB over network 三个选项。\n打开树莓派的 SSH 服务：设置 -\u0026gt; 系统 -\u0026gt; Raspberry Pi Settings 中将 SSH 服务打开。 刷入 ota 包 接下来选择刷入 OTA 包 lineage-20.0-20240112-UNOFFICIAL-KonstaKANG-rpi5-atv-ota.zip，ota 包不会用到树莓派镜像烧录器，而是要通过Android Recovery 模式刷入。需要事先准备好 OTA 包，并将其复制到 U 盘中，并将 U 盘插入到树莓派中。\n在 Android 系统设置 -\u0026gt; 系统 -\u0026gt; 重新启动中选择 Recovery，此时系统会重启进入到 Recovery 模式。 点击 Install，并选择右下角的 Select Storage 按钮，选择 USB 存储，即刚插入的 U 盘。 在 U 盘中选择要刷入的 ota 补丁，取消 Zip signature verification。 在 Swipe to confirm Flash处向右滑动鼠标，即可进入到安装 ota 补丁的界面。ota 补丁安装完成后，即可自动重启进入到 Android TV 系统中。重新进入系统后，发现系统的界面没有任何变化，不知道该 ota 补丁的具体影响功能。 系统存储空间设置 在默认的情况下，打开设置 -\u0026gt; 存储空间，可以看到内部共享存储空间仅为 4.7 GB，这里的存储空间为 /data 目录挂载的设备，并没有将 U 盘的空间全部使用起来，U 盘剩余的磁盘空间处于未分配状态。 这里使用 lineage-20.0-rpi-resize.zip 工具来修改存储空间，按照刷入 ota 包相同的方式，进入 Recovery 模式下刷入该包，重新进入系统后即发现存储空间已经变大。 安装 adb 工具 前置条件：开发者选项中的 ADB over network 必须为开启状态。\nadb 命令需要安装到电脑上，在 mac 下使用 brew install android-platform-tools即可安装完成。\n在 android tv 上查看当前的 ip 地址，我这里为 192.168.31.167。执行 adb connect 192.168.31.167 后即可获取到 USB 调试信息，并选中一律允许使用这台计算机进行调试处后点击允许。 此时在终端中即可显示出连接成功的信息：already connected to 192.168.31.167:5555。\n在安装完 adb 工具后，即可以通过 adb 命令来远程访问 Android TV 系统了。例如：\nadb install 可以安装 apk 包。 adb shell settings list global 命令来查看 Android 系统的所有global 配置。 adb logcat 查看 android 的日志。 远程 ssh 连接 前置条件：\nssh 连接需要首先在 Raspberry Pi Settings 中的 Remote access 中打开 SSH。 adb connect 可以连接到 Android TV。 在开启 SSH 服务后，ssh 连接需要使用对应的私钥信息，而私钥信息需要通过 adb 命令获取。\n1 2 3 4 5 6 7 8 9 adb connect 192.168.31.166 adb root # 可查看 /data/ssh 目录下的 ssh 秘钥信息 adb shell ls /data/ssh # 将秘钥信息放到本地 adb pull /data/ssh/ssh_host_ed25519_key ~/.ssh/android_tv.key chmod 600 ~/.ssh/android_tv.key # 成功 ssh 连接 ssh -i ~/.ssh/android_tv.key root@192.168.31.166 安装 Google Apps 注意：此处要求网络具备魔法，可以访问 Google\n刷入 Google Apps 包 Google Apps 包为必须用到的包，通过 Recovery 模式刷入包 MindTheGapps-13.0.0-arm64-ATV-full-20240104_210039.zip。包安装完成后，重新进入系统发现界面发生了变化，多出了应用 Google Play Store，左上角的语音和搜索功能虽然不可以用，但是点击后提示信息已经发生了变化。\n原来的文件应用在这里消失不见了，实际上在所有应用中还可以找到。\n在系统中使用 Google 账号登录 Google Play Store，即使在可以访问 Google 的网络下，发现也一直会失败。\n查询并注册 Android ID 因为该 Android TV 设备并不被信任，需要将 Android ID 在 Android 网站注册。如果不注册，那么 Google 账号登录不成功。\n将树莓派的 SD 卡插入到笔记本，查看 SD 卡中的文件 gsf-android_id.txt，该文件对应的内容即为 Android ID。\n还有另外一种办法可以获取到 Android ID，在笔记本上通过 adb 命令查询到当前设备的 Android ID。\n1 2 3 4 5 6 7 adb root # 找到 google service 的 sqlite3 数据库文件 adb shell \u0026#39;find /data -name \u0026#34;gservices.db\u0026#34;\u0026#39; # 通过数据库查询到 android id # 其中 sqlite3 命令后的为上面步骤查询出的文件路径，如果查询出多个，可以任选一个 adb shell \u0026#39;sqlite3 /data/data/com.google.android.gsf/databases/gservices.db \u0026#34;select * from main where name = \\\u0026#34;android_id\\\u0026#34;;\u0026#34;\u0026#39; 将上述 Android ID 在网站进行注册，网址：https://www.google.com/android/uncertified/ 使用 Google 账号登录 重新进入 Recovery 模式，点击 Wipe -\u0026gt; Factory reset，此时机器会进行重启。该操作会清空系统中的 /data/media 下的内容。(还不太清楚该步骤是否为必须操作) 机器重新后会重新进入一遍系统的初始化，但现在的初始化界面跟最初的 LineageOS 的初始化有所不同，进入到了 GMS 的开机引导，该步骤中必须要登录到 Google 账号，而且无法跳过。如果 Android ID 没有注册，此时登录 Google 账号一直会失败，导致无法进入到系统中。 登录完成后在首页可以看到了更多的一些信息： 通过 Recovery 刷入其他包 widevine 是在 Android 生态下跟数字版权相关的包，通过 Recovery 模式刷入包 lineage-20.0-rpi-widevine.zip。\n通过 Recovery 模式刷入包 lineage-20.0-rpi-magisk-v25.2.zip，进入到 Android TV 系统后通过文件工具安装包 Magisk-v25.2.apk。\n解决网络连接受限 如果本地的网络无法访问 Google，默认情况下，网络会提示网络连接受限，原因主要还是跟访问不了 Google 的域名有关，以至于 Android TV 系统不能识别出可以连接互联网。 网络连接受限状态的 WIFI，经测试机器重启后无法自动连接，需要每次都手工连接 WIFI。\n执行如下的命令来系统进行设置：\n1 2 3 4 5 6 7 8 9 10 adb connect 192.168.31.166 # 设置时间服务器 adb shell settings put global ntp_server ntp1.aliyun.com # 该值默认为 0 adb shell settings put global captive_portal_detection_enabled 1 # 默认没有这两个值 adb shell settings put global captive_portal_https_url https://connect.rom.miui.com/generate_204 设置完成后重启系统，即可看到网络的连接受限已经消除，并且 WIFI 已经可以自动连接了。\n常用 apk 软件安装 我这里使用了 https://kxsw.gitbook.io/tv/ 中的方法安装了 File Commands 和 Clash 软件。File Commands 可以用来管理本地的文件，甚至可以提供 HTTP Server，供远程来下载或者上传文件。\n国内的常见应用在 Google Play Store 中并不存在，而且通过 Google Play Store 直接安装应用很可能会失败，跟使用的网络有很大关系。国内的应用我直接使用了当贝市场来安装电视应用即可。\n其他问题 听不到声音 在播放视频时发现听不到声音，原因是因为默认情况下使用了 hdmi0 接口来输入声音，通过 hdmi1 只能输出视频信号，没有声音。将 hdmi 线切换到 hdmi0 口后并重启系统后，声音即正常。\n后续 到目前为止，树莓派已经具备了完整的 Android TV 系统的功能，而且使用起来还比较稳定，这一点超出了我的预期，毕竟树莓派 5 比较新，该系统出来的时间比较短。\n后面我计划使用新的文章来记录使用体验，以及新的折腾经历，比如：如何通过遥控器实现正常的开关机。\n敬请期待。。。\n资料 https://blog.csdn.net/u013120422/article/details/132107317 https://konstakang.com/devices/rpi5/LineageOS20-ATV/ ","date":"2024-02-17T11:23:07Z","permalink":"/post/%E6%88%91%E8%A6%81%E7%9C%8B%E7%94%B5%E8%A7%86-%E5%B0%86%E6%A0%91%E8%8E%93%E6%B4%BE-5-%E6%89%93%E9%80%A0%E6%88%90-android-tv-%E7%94%B5%E8%A7%86%E7%9B%92%E5%AD%90%E6%8A%98%E8%85%BE%E8%AE%B01/","title":"我要看电视 - 将树莓派 5 打造成 Android TV 电视盒子折腾记（1）"},{"content":"去年装修的时候预留了投影仪和幕布的位置，搬家已经有几个月的时间了，最近开始考虑安装投影仪。作为技术男，已经有多年不看电视，在决策前自然要考察下市场，了解下当前的技术，综合考虑多种技术因素。\n投影仪的选型 家装投影仪分为了两大阵营：\n以传统厂商为代表的 3LCD 阵营。典型的厂家为爱普生、明基。优点为清晰度高、对比度高。缺点为噪音大、灯泡寿命短、个头偏大。 以国产新势力厂商为代表的 DLP 阵营。典型厂家为极米、当贝、坚果，无论其市场分布，还是技术形态，都像极了新能源领域的新势力”蔚小理“。优点为自带 android 系统、更加人性化、寿命长。缺点为亮度低。 我的需求优先级依次如下：\n亮度高，抛开亮度和对比度谈体验就本末倒置了，电视的最基本需求就是看得见，看得清楚。 最好不要带 android 系统，时间长了肯定会卡，不希望跟 android 系统绑定。 开机不要有广告。 经过对比后，选择了爱普生的 3LCD 投影仪，亮度完全满足需求，即使在白天不拉窗帘的情况下，清晰度也完全足够。选择了其优点，自然要忍受其缺点，噪音大也并没有大到不能忍受，一般播放片源后就可以将风扇的声音盖住，在投影仪旁实测噪音在 40 分贝左右。灯泡寿命短的缺点暂时忽略，毕竟官方说明灯泡的寿命在三年到六年之间，而且更换灯泡的费用并不高。\n电视盒子的选型 有了投影仪后，还缺一个电视盒子用于提供视频源，电视盒子的选择比投影仪更加丰富。\nApple TV 第一优先级考虑为地表最强电视盒子 Apple TV，无论其流畅度、清晰度、用户体验都堪称完美。主要的优点：\n强大的 Infuse 软件用来查看蓝光光源、杜比视界，绝对不是 1080P 的片源可以比的。 苹果一如既往丝滑般的体验。无论是操作系统，还是遥控器，其用户体验都能吊打一众安卓盒子。 可在 Netflix、Disney+、Apple TV+ 追剧。要想在设备上播放 Netflix，是需要授权的，国内的电视盒子一概不支持。 无广告。 虽然想体验一把最好的看电视体验，但存在如下两个问题导致我最终放弃了 Apple TV：\n无法看国内的”爱优腾“。投影仪的主要使用方并非我自己，而更多的是家人，没有国内的”爱优腾“，查看片源过于复杂，使用体验将大打折扣。 Netflix 和 Disney+ 并非强需求。Apple TV 用来追剧非常方便，但我个人并没有太多的时间用来追剧，而且还需要魔法，追剧还需要付费，使用成本整体并不低。 国产电视盒子 网上可以买到各式各样的电视盒子，当贝、小米、腾讯极光等，整体特点：\n均基于 Android TV 系统，又叠加了自己的桌面应用。 厂商均不靠硬件赚钱，而是靠 VIP 会员赚钱，因此所有的电视盒子均需要充值 VIP，没有了 VIP 也就剩下一个盒子了，基本上看不了多少片源。 各种开机广告、开屏广告满天飞。 下单销量排名靠前的当贝盒子后，体验一番后，大失所望。虽然如宣传的一般没有了开机广告，但进入系统后没有 VIP 寸步难行，每个视频右下角都会带一个灰色的 VIP logo。我的选择逻辑，既然”爱优腾“的会员在所难免，毕竟还有手机端 app 看视频的需求，再买一个盒子 VIP 的意义何在。如果盒子 VIP 不买，那盒子自身系统提供的片源很多就是个摆设。实在无法容忍，遂退之。\n国产的电视盒子没有一个能打的，整个产品的定位全跑偏了。如果能有一个干净、用户体验好的电视盒子可以给到用户，哪怕卖得贵一点（毕竟就要靠卖硬件赚钱了），我相信也是有市场的。\n树莓派 看了下 Android TV 的原生系统后，更加符合自己的需求，干净清爽，而且还可以刷 Google 服务包，用来安装 Google 全家桶应用。所以就开始考虑自己刷机使用 Android TV。要想刷机必须先有盒子，盒子的选型有如下几个：\n国产电视盒子。可在国产电视盒子的硬件上刷 Android TV 系统。 其他 Arm 架构的盒子。比如非常流行的斐讯 N1 盒子，曾经用于挖矿，现在却在软路由领域焕发出了新的生机。 开发版。最为流行的树莓派，还有友善 NanoPi 开发版、Banana Pi，均为基于 ARM 架构。 我最终选择了树莓派 5 作为电视盒子，主要考虑如下：\n性能强大，用来作为电视盒子性能是过剩的。 可玩性强。即使作为电视盒子翻车了，还可以用来作为小型服务器，跑 Home Assiant、TeslaMate 等应用，甚至可以作为 NAS 使用。 用户基数大。自己踩到的坑更容易找到解决方案。 总结 投影仪完成后，且电视盒子也选好了，接下来将另起一篇文章讲解下我的树莓派刷 Android TV 系统的折腾经历，自己选择的路线，查再多的资料，刷再多的机，也要折腾成功。\n","date":"2024-02-17T11:20:50Z","permalink":"/post/%E6%88%91%E8%A6%81%E7%9C%8B%E7%94%B5%E8%A7%86-%E6%8A%95%E5%BD%B1%E4%BB%AA%E5%92%8C%E7%94%B5%E8%A7%86%E7%9B%92%E5%AD%90%E9%80%89%E5%9E%8B/","title":"我要看电视 - 投影仪和电视盒子选型"},{"content":"在上篇文章《红米路由器 AX6000 解锁 SSH》中，解锁了红米路由器 AX6000 的 SSH 功能，本文在此基础上安装 clash，用来科学上网。\n安装 clash 通过 ssh 命令连接到红米路由器，执行如下 shell 命令，即可下载并安装 clash 软件：\n1 2 3 4 mkdir -p /data/openclash \u0026amp;\u0026amp; cd /data/openclash export url=\u0026#39;https://raw.fastgit.org/juewuy/ShellClash/master\u0026#39; curl -kfsSl $url/install.sh \u0026gt; install.sh sh install.sh 会出现如下的提示： install.sh 会将 ShellCrash 安装到 /data/ShellCrash 目录下。\nclash 配置 在安装完 clash 后，输入 clash 命令即可对 clash 进行管理，该操作有点类似于打客户电话的操作。 clash UI 通过上述 clash 命令可以安装 clash UI，可以通过网址 http://192.168.31.1:9999/ui 访问。UI 的功能较 clash 的客户端更为简单，但也可以弥补上述 clash 命令的很多不足之处。 资料 https://www.youtube.com/watch?v=e90UWDpAlxA\n","date":"2024-02-11T01:12:10Z","permalink":"/post/%E7%BA%A2%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8-ax6000-%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/","title":"红米路由器 AX6000 科学上网"},{"content":"半年前购买了红米路由器 AX6000，用起来一直非常稳定。因为最近在折腾树莓派，准备将红米路由器上开启 SSH 的功能。最早是准备将路由器刷成 OpenWRT 系统，当一旦路由器能够开启 SSH 功能后，刷 OpenWRT 系统的必要性就不是太大了。\n声明：本文下面操作步骤均为从网络上获取的现有操作步骤。\n在电脑的浏览器上登录红米路由器的管理页面，红米路由器管理页面为：https://miwifi.com/，或者 http://192.168.31.1/。如果本地设置过其他的 DNS 服务器，需要使用 ip 地址的形式访问。 登录后可以获取到当前红米路由器的版本，我的已经是最新的 1.0.67，该版本的固件可以支持开启 SSH 协议。 在登录红米路由器管理页面后，查看浏览器的地址栏，https://miwifi.com/cgi-bin/luci/;stok=5e55c58e949d5419c001bce8288e5a27/web/home#router，可以看到参数 stok 5e55c58e949d5419c001bce8288e5a27 即我们需要用到该值。需要注意的是参数 stok 在每次登录时均会发生改变。\n用浏览器打开下面的网址，其中 stok 为上面步骤获取到的值。\n1 http://192.168.31.1/cgi-bin/luci/;stok=5e55c58e949d5419c001bce8288e5a27/api/misystem/set_sys_time?timezone=%20%27%20%3B%20zz%3D%24%28dd%20if%3D%2Fdev%2Fzero%20bs%3D1%20count%3D2%202%3E%2Fdev%2Fnull%29%20%3B%20printf%20%27%A5%5A%25c%25c%27%20%24zz%20%24zz%20%7C%20mtd%20write%20-%20crash%20%3B%20 浏览器如果返回{\u0026quot;code\u0026quot;:0}，说明该步骤执行成功。\n在浏览器执行路由器重启操作，返回 {\u0026quot;code\u0026quot;:0}，说明该步骤执行成功，此时路由器会发生重启。\n1 http://192.168.31.1/cgi-bin/luci/;stok=5e55c58e949d5419c001bce8288e5a27/api/misystem/set_sys_time?timezone=%20%27%20%3b%20reboot%20%3b%20 重新登录红米路由器，获取到新的 stok 值 221bcd3ba09b3e4597b0308cc1df18a2。 在浏览器继续访问如下的地址，用来开启 telnet，成功后仍然返回 {\u0026quot;code\u0026quot;:0}。\n1 http://192.168.31.1/cgi-bin/luci/;stok=221bcd3ba09b3e4597b0308cc1df18a2/api/misystem/set_sys_time?timezone=%20%27%20%3B%20bdata%20set%20telnet_en%3D1%20%3B%20bdata%20set%20ssh_en%3D1%20%3B%20bdata%20set%20uart_en%3D1%20%3B%20bdata%20commit%20%3B%20 再继续执行重启命令\n1 http://192.168.31.1/cgi-bin/luci/;stok=221bcd3ba09b3e4597b0308cc1df18a2/api/misystem/set_sys_time?timezone=%20%27%20%3b%20reboot%20%3b%20 在重启完成后，路由器的 telnet 服务已经开启。在终端中执行 telnet 192.168.31.1 即可远程连接到小米路由器，而且不需要密码。\n在终端中执行如下的命令，用来设置 root 密码，开启并固化 ssh 服务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # 修改 root 密码，其中 xxx 为要修改的密码 echo -e \u0026#39;xxx\\nxxx\u0026#39; | passwd root # 开启 ssh 服务 nvram set ssh_en=1 nvram set telnet_en=1 nvram set uart_en=1 nvram set boot_wait=on nvram commit sed -i \u0026#39;s/channel=.*/channel=\u0026#34;debug\u0026#34;/g\u0026#39; /etc/init.d/dropbear /etc/init.d/dropbear restart # 设置 ssh 服务开机自启动 mkdir /data/auto_ssh cd /data/auto_ssh curl -O https://fastly.jsdelivr.net/gh/lemoeo/AX6S@main/auto_ssh.sh chmod +x auto_ssh.sh uci set firewall.auto_ssh=include uci set firewall.auto_ssh.type=\u0026#39;script\u0026#39; uci set firewall.auto_ssh.path=\u0026#39;/data/auto_ssh/auto_ssh.sh\u0026#39; uci set firewall.auto_ssh.enabled=\u0026#39;1\u0026#39; uci commit firewall # 设置时区 uci set system.@system[0].timezone=\u0026#39;CST-8\u0026#39; uci set system.@system[0].webtimezone=\u0026#39;CST-8\u0026#39; uci set system.@system[0].timezoneindex=\u0026#39;2.84\u0026#39; uci commit # 关闭开发模式 mtd erase crash reboot 在终端中执行 ssh -o HostkeyAlgorithms=+ssh-rsa -o PubkeyAcceptedKeyTypes=+ssh-rsa root@192.168.31.1即可 ssh 连接到路由器。\n需要注意的是，每次重启路由器后 ssh key 会刷新，需要在本地执行 ssh-keygen -R 192.168.31.1 命令。\n进入到系统后我们可以看到系统为基于 Linux 5.4 内核版本的 XiaoQiang 系统，至于名字为什么叫 XiaoQiang 我还不得而知。\n1 2 root@XiaoQiang:~# uname -a Linux XiaoQiang 5.4.150 #0 SMP Mon Jan 30 09:23:25 2023 aarch64 GNU/Linux 整个磁盘分区的使用情况如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 root@XiaoQiang:~# df -h Filesystem Size Used Available Use% Mounted on /dev/root 15.5M 15.5M 0 100% / tmpfs 240.0M 9.9M 230.1M 4% /tmp ubi1_0 40.4M 7.6M 30.7M 20% /data ubi1_0 40.4M 7.6M 30.7M 20% /userdisk /dev/root 15.5M 15.5M 0 100% /userdisk/data ubi1_0 40.4M 7.6M 30.7M 20% /etc/config ubi1_0 40.4M 7.6M 30.7M 20% /etc/datacenterconfig ubi1_0 40.4M 7.6M 30.7M 20% /etc/smartcontroller ubi1_0 40.4M 7.6M 30.7M 20% /etc/parentalctl ubi1_0 40.4M 7.6M 30.7M 20% /etc/smartvpn ubi1_0 40.4M 7.6M 30.7M 20% /etc/ppp ubi1_0 40.4M 7.6M 30.7M 20% /etc/crontabs tmpfs 512.0K 12.0K 500.0K 2% /dev 使用 top 命令查看整体的系统负载情况，cpu 利用率还是非常低。\n1 2 3 Mem: 325692K used, 165808K free, 11208K shrd, 12652K buff, 76388K cached CPU: 0.3% usr 1.3% sys 0.0% nic 97.8% idle 0.0% io 0.0% irq 0.4% sirq Load average: 1.05 1.06 1.12 2/147 28107 1 资料 https://www.youtube.com/watch?v=u5Qg4zqj_V0 https://uzbox.com/tech/openwrt/ax6000.html https://github.com/kjfx/AX6000/releases/tag/RedmiAX6000\n","date":"2024-02-11T00:20:15Z","permalink":"/post/%E7%BA%A2%E7%B1%B3%E8%B7%AF%E7%94%B1%E5%99%A8-ax6000-%E8%A7%A3%E9%94%81-ssh/","title":"红米路由器 AX6000 解锁 SSH"},{"content":"术语 host cluster：virtual cluster 中的宿主 k8s 集群，承载了所有的计算资源。也会被叫做 super cluster。 virtual cluster：virtual cluster 中的租户 k8s 集群，通常简写 vc。也会被叫做 tenant cluster。 vCluster：k8s virtual cluster 的实现之一，即本文中要介绍的方案。 项目简介 k8s 的多租功能 k8s 自身在多租的能力上支持较差，提供了 namespace 级别的隔离，不同的租户使用不同的 namespace，但该隔离功能较弱。很多组件部署在同一个 workload 会存在诸多问题：\n使用全局对象存在冲突，比如 CRD。 存在诸多安全性问题，比如多个租户之间的 pod 完全可以互访，没有任何隔离机制。 不同组件对于 k8s 的版本不统一。 为了解决多租的问题，最简单的思路就是使用多 k8s 集群，业界的 KubeFed v2、karmada、clusternet、OCM 等均为多 k8s 集群的实现。但多 k8s 集群因为存在独立的控制面和计算资源，存在资源消耗过多的问题。\n还有一个中间思路为仅做 k8s 的控制面隔离，计算资源仍然共享，即 pod 也可以解决很多的多租隔离问题。k8s 的控制面隔离又存在两个主要方案：\n独立的 kube-apiserver 和 etcd、kube-controller-manager， kube-scheduler 共享 host cluster。该方案中有独立的 kube-apiserver 组件，这里的 etcd 可以被 sqllite、mysql 等存储取代。该方案统称为 virtual cluster，简称为 vc。 独立的 proxy apiserver，kube-apiserver、kube-controller-manager、kube-scheduler 共享 host cluster。访问 k8s 的请求先到 proxy apiserver，proxy apiserver 转发到 host cluster 的 kube-apiserver。可以在 proxy apiserver 中提供独立的 RBAC 机制，实现一定程度的隔离。该方案在开源中未看到具体的实现。 vCluster 介绍 vCluster 为 virtual cluster 的开源实现之一，由 Loft Labs 提供，Github Star 3.7K，代码行数 4 万行。除了开源版本外，还提供了商业版本 vCluster PRO。\nvCluster 设计原则：\n最小化资源占用。在实现上使用了单 pod 的 k3s 作为 k8s 的控制面。 复用 host cluster 的计算、存储和网络资源。 降低对 host cluster 的请求。 简单灵活。 不需要 host cluster 的管理员权限。 vCluster 多个 namespace 下的对象映射到 host cluster 的同一个 namespace 下。同时也可以支持 vCluster 的一个 namespace 对应 host cluster 的一个 namespace。 易清理。 Getting Started k8s 集群准备 k8s 集群这里使用了 kind 方案，kind 配置 kind.conf 如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: vcluster nodes: - role: control-plane # 如果需要 ingress，则需要指定该参数 kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \u0026#34;ingress-ready=true\u0026#34; extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP # 指定 k8s 版本，默认不指定 # image: kindest/node:v1.23.17 - role: worker - role: worker - role: worker networking: apiServerPort: 6443 执行 kind create cluster --config kind.conf 即可创建 k8s 集群，包含了一个 control-plane 节点，三个 worker 节点。\n安装 vcluster vCluster 提供了使用 vcluster cli、helm 和 kubectl 三种安装方式，使用 vcluster cli 最为简单，其底层同样采用 helm chart 的方式部署，下面采用 vcluster cli 的方式进行安装。 安装 vcluster cli 工具：\n1 curl -L -o vcluster \u0026#34;https://github.com/loft-sh/vcluster/releases/latest/download/vcluster-darwin-arm64\u0026#34; \u0026amp;\u0026amp; sudo install -c -m 0755 vcluster /usr/local/bin \u0026amp;\u0026amp; rm -f vcluster 或者执行 brew install vcluster安装 vcluster 命令行工具。\n执行命令 vcluster create my-vcluster 创建 virtual cluster。会在 host cluster 上创建 namespace vcluster-my-vcluster，该 namespace 下创建如下对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl get all -n vcluster-my-vcluster NAME READY STATUS RESTARTS AGE pod/coredns-68559449b6-l5whx-x-kube-system-x-my-vcluster 1/1 Running 2 (5m45s ago) 3d pod/my-vcluster-0 1/1 Running 2 (5m45s ago) 3d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns-x-kube-system-x-my-vcluster ClusterIP 10.96.67.119 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 3d service/my-vcluster NodePort 10.96.214.58 \u0026lt;none\u0026gt; 443:30540/TCP,10250:31621/TCP 3d service/my-vcluster-headless ClusterIP None \u0026lt;none\u0026gt; 443/TCP 3d service/my-vcluster-node-vcluster-control-plane ClusterIP 10.96.69.115 \u0026lt;none\u0026gt; 10250/TCP 3d NAME READY AGE statefulset.apps/my-vcluster 1/1 3d 可以看到在该 namespace 下创建了 coredns 和 StatefulSet my-vcluster。每个租户有独立的 coredns 组件，用来做域名解析。my-vcluster 为 vCluster 的管控面组件，包括了 k8s controller plane 和 syncer 组件。\n执行 vcluster connect my-vcluster 后会在本地启动代理，并自动切换本地的 kubeconfig context，将 context 切换到 virtual cluster。执行 kubectl 命令即可连接到对应的 k8s 集群。virtual cluster 集群中的信息如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl get all -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-68559449b6-jg2bs 1/1 Running 1 (20m ago) 51m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-system service/kube-dns ClusterIP 10.96.120.59 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 51m default service/kubernetes ClusterIP 10.96.35.53 \u0026lt;none\u0026gt; 443/TCP 51m NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 1/1 1 1 51m NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-68559449b6 1 1 1 51m 在 virtual cluster 可以看到仅包含了 coredns 组件。\n在 virtual cluster 和在 host cluster 中的 node 信息：\n1 2 3 4 5 6 7 8 9 10 $ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME vcluster-worker3 Ready \u0026lt;none\u0026gt; 51m v1.27.3 10.96.118.228 \u0026lt;none\u0026gt; Debian GNU/Linux 11 (bullseye) 5.10.76-linuxkit containerd://1.7.1 $ kubectl get node -o wide --context kind-vcluster NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME vcluster-control-plane Ready control-plane 86m v1.27.3 172.19.0.3 \u0026lt;none\u0026gt; Debian GNU/Linux 11 (bullseye) 5.10.76-linuxkit containerd://1.7.1 vcluster-worker Ready \u0026lt;none\u0026gt; 85m v1.27.3 172.19.0.2 \u0026lt;none\u0026gt; Debian GNU/Linux 11 (bullseye) 5.10.76-linuxkit containerd://1.7.1 vcluster-worker2 Ready \u0026lt;none\u0026gt; 85m v1.27.3 172.19.0.5 \u0026lt;none\u0026gt; Debian GNU/Linux 11 (bullseye) 5.10.76-linuxkit containerd://1.7.1 vcluster-worker3 Ready \u0026lt;none\u0026gt; 85m v1.27.3 172.19.0.4 \u0026lt;none\u0026gt; Debian GNU/Linux 11 (bullseye) 5.10.76-linuxkit containerd://1.7.1 可以看到在 virtual cluster 和 host cluster 中的 node 名字相同，这是因为 node 在 vCluster 中并没有做隔离，而是从 host cluster 中做了同步。但 virtual cluster 中的 node 节点仅包含 pod 在 host cluster 中已经使用的 node 节点，未使用的节点并不会在 virtual cluster 上。 同时可以看到 vc 和 host cluster 中的 node ip 地址并不相同，vc 中的 node ip 地址跟 host cluster 中的对应 service clusterip 相同，在 host cluster 中的对应 service 名字为 $vClusterName-node-$hostClusterNodeName。\n1 2 3 $ kubectl get svc --context kind-vcluster -n vcluster-my-vcluster my-vcluster-node-vcluster-worker3 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE my-vcluster-node-vcluster-worker3 ClusterIP 10.96.118.228 \u0026lt;none\u0026gt; 10250/TCP 3h54m 在 virtual cluster 中创建 k8s 对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 在 virtual cluster 创建 namespace $ kubectl create ns demo-nginx namespace/demo-nginx created # 创建 Deployment $ kubectl create deployment nginx-deployment -n demo-nginx --image=nginx # 在 virtual cluster 上创建出了 pod $ kubectl get pod -n demo-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-66fb7f764c-dn59g 1/1 Running 0 11m 10.244.0.7 vcluster-control-plane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 由于 pod 调度了 host cluster 新节点，在 virtual cluster 中可以看到新的 k8s node，k8s node 为刚刚创建 $ kubectl get node NAME STATUS ROLES AGE VERSION vcluster-worker2 Ready \u0026lt;none\u0026gt; 2m45s v1.27.3 vcluster-worker3 Ready \u0026lt;none\u0026gt; 57m v1.27.3 在 host cluster 中看到如下对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 在 host cluster 上并没有对应的 namespace demo-nginx $ kubectl get ns --context kind-vcluster demo-nginx Error from server (NotFound): namespaces \u0026#34;demo-nginx\u0026#34; not found # 在 host cluster 上并没有对应的 deployment nginx-deployment $ kubectl get deploy -A --context kind-vcluster NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE ingress-nginx ingress-nginx-controller 1/1 1 1 90m kube-system coredns 2/2 2 2 91m local-path-storage local-path-provisioner 1/1 1 1 91m # 但在 host cluster 上却看到了对应的 pod，位于 vcluster 统一的 namespace vcluster-my-vcluster 之下 $ kubectl get pod -n vcluster-my-vcluster --context kind-vcluster NAME READY STATUS RESTARTS AGE coredns-68559449b6-jg2bs-x-kube-system-x-my-vcluster 1/1 Running 1 (26m ago) 57m my-vcluster-0 1/1 Running 1 (26m ago) 86m nginx-deployment-66fb7f764c-sffqt-x-demo-nginx-x-my-vcluster 1/1 Running 0 2m22s 可以看到仅 pod 在 host cluster 中同步存在，而 namespace、deployment 这些对象仅存在于 virtual cluster 中。在 virtual cluster 中创建的多个不同 namespace pod 仅会存在于 host cluster 的同一个 namespace 下。\n验证 service 在 virtual cluster 中创建 service 对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: v1 kind: Service metadata: labels: app: nginx kubernetes.io/cluster-service: \u0026#34;true\u0026#34; name: nginx namespace: demo-nginx spec: ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: app: nginx-deployment type: ClusterIP 在 virtual cluster 中包含如下的 Service 对象：\n1 2 3 $ kubectl get svc -n demo-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 10.96.239.53 \u0026lt;none\u0026gt; 80/TCP 2m11s 在 host cluster 中会同步创建如下的 Service 对象，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: v1 kind: Service metadata: annotations: vcluster.loft.sh/object-name: nginx vcluster.loft.sh/object-namespace: demo-nginx vcluster.loft.sh/object-uid: 5ab7aa9c-90b6-46f9-a162-9ea9ca9826f3 creationTimestamp: \u0026#34;2023-11-28T09:32:22Z\u0026#34; labels: vcluster.loft.sh/label-my-vcluster-x-a172cedcae: nginx vcluster.loft.sh/label-my-vcluster-x-d9125f8911: \u0026#34;true\u0026#34; vcluster.loft.sh/managed-by: my-vcluster vcluster.loft.sh/namespace: demo-nginx name: nginx-x-demo-nginx-x-my-vcluster namespace: vcluster-my-vcluster ownerReferences: - apiVersion: v1 controller: false kind: Service name: my-vcluster uid: 463a503e-d889-49a7-94e0-0cba5299dd47 resourceVersion: \u0026#34;12344\u0026#34; uid: fc9ea383-996e-471c-9c27-ee1c22fec7a3 spec: clusterIP: 10.96.239.53 clusterIPs: - 10.96.239.53 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: http port: 80 protocol: TCP targetPort: 80 selector: vcluster.loft.sh/label-my-vcluster-x-a172cedcae: nginx-deployment vcluster.loft.sh/managed-by: my-vcluster vcluster.loft.sh/namespace: demo-nginx sessionAffinity: None type: ClusterIP status: loadBalancer: {} 可以看到 host cluster 中的 service 的 ClusterIP 跟 virutal cluster 一致，但 spec.selector 字段已经被 syncer 修改，以便可以匹配到正确的 pod。\n验证 Ingress 默认情况下 Ingress 不会同步到 host cluster，需要通过开关的方式启动。创建文件 values.yaml，内容如下：\n1 2 3 sync: ingresses: enabled: true 执行 vcluster create my-vcluster --upgrade -f values.yaml 即可修改现在 vcluster 集群配置。\n在 virtual cluster 中创建如下的 Ingress 对象：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: labels: app: nginx name: nginx namespace: demo-nginx spec: rules: - host: nginx.aa.com http: paths: - backend: service: name: nginx port: number: 80 path: / pathType: ImplementationSpecific 查看 host cluster 中的 Ingress 信息如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 $ kubectl get ingress --context kind-vcluster -n vcluster-my-vcluster -o yaml nginx-x-demo-nginx-x-my-vcluster apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: vcluster.loft.sh/object-name: nginx vcluster.loft.sh/object-namespace: demo-nginx vcluster.loft.sh/object-uid: 4b8034a6-1513-4ccd-b80a-66807d862b4e creationTimestamp: \u0026#34;2023-11-28T10:07:52Z\u0026#34; generation: 1 labels: vcluster.loft.sh/label-my-vcluster-x-a172cedcae: nginx vcluster.loft.sh/managed-by: my-vcluster vcluster.loft.sh/namespace: demo-nginx name: nginx-x-demo-nginx-x-my-vcluster namespace: vcluster-my-vcluster ownerReferences: - apiVersion: v1 controller: false kind: Service name: my-vcluster uid: 463a503e-d889-49a7-94e0-0cba5299dd47 resourceVersion: \u0026#34;16292\u0026#34; uid: 1d0de02b-5aad-4858-a8cf-2caa345ca85b spec: rules: - host: nginx.aa.com http: paths: - backend: service: name: nginx-x-demo-nginx-x-my-vcluster port: number: 80 path: / pathType: ImplementationSpecific status: loadBalancer: ingress: - hostname: localhost 可以看到 Ingress 中对应的 Service 名字已经修改了 host cluster 中对应的 Service 名字。\n销毁 在使用完成后执行如下命令即可销毁 virtual cluster：\n1 2 3 4 # 切换本地的 context vcluster disconnect # 删除 vcluster vcluster delete my-vcluster 架构 组件 整个架构中，有两大核心组件：k8s Control Plane 和 syncer。其中 StatefulSet my-vcluster 中容器 syncer，默认情况下在该容器中同时启动了 k3s 容器作为 vCluster 控制平面和 vCluster 的 syncer 进程。\n1 2 3 4 5 6 $ kubectl exec -it --context kind-vcluster -n vcluster-my-vcluster my-vcluster-0 -- ps -ef Defaulted container \u0026#34;syncer\u0026#34; out of: syncer, vcluster (init) PID USER TIME COMMAND 1 root 2:57 /vcluster start --name=my-vcluster --kube-config=/data/k3s 17 root 18:35 /k3s-binary/k3s server 46 root 0:00 ps -ef controller plane 控制平面默认使用 k3s，存储使用 sqllite，也可以使用 etcd、mysql、postgresql。k8s 发行版也可以使用 k0s、Vanilla（标准 k8s）、第三方镜像等。控制平面由如下几个组件组成：\nk8s apiserver。 数据存储，比如 sqllite、etcd 等。 kube-controller-manager kube-scheduler：可选组件，默认使用 host cluster 调度器。 在 Pro 版本中，允许控制面跟 pod 部署在不同的 host cluster。\nsyncer virtual cluster 中并不包含实际的计算、存储和网络资源，syncer 的职责为将对象从 virtual cluster 同步到 host cluster，也有少部分对象需要从 host cluster 同步到 virtual cluster。 vCluster 将 k8s 对象划分为 low level 和 high level，其中 high level 的对象仅存在于 virtual cluster 中，比如 Deployment、CRD 等对象。low level 的对象会通过 syncer 模块同步到 host cluster 上，包括 Pod、ConfigMap、Secret 等。low level 的对象在 virutal cluster 为多个 namespace，但均会映射到 host cluster 的一个 namespace 下。另外，vCluster 也支持将 virtual cluster 的多个 namespace 映射到 host cluster 的多个 namespace，该特性目前处于 alpha 状态。 vCluster 可以通过配置的方式来定制资源的同步，更复杂的同步规则提供了插件机制实现。 vCluster 默认支持的同步资源列表：https://www.vcluster.com/docs/syncer/core_resources。\n已经创建完成的 syncer 配置，可以通过 vcluster create my-vcluster --upgrade -f values.yaml 的方式修改，该命令会调用 helm update，helm update 命令最终会修改 StatefulSet syncer 的配置，并触发 pod 的重启。\nk8s node 同步 支持多种 node 的同步行为，通过修改 syncer 的启动参数：\nFake Node：默认行为。根据 pod 中的 spec.nodeName 创建 Fake Node。Fake Node 为 syncer 服务自动创建。如果没有 pod 调度到 Fake Node 上，则 Fake Node 会自动删除。 Real Node：根据 pod 中的 spec.nodeName 创建 Real Node，Real Node 的信息从 host cluster 同步。如果没有 pod 调度到 Real Node 上，则 Real Node 会自动删除。 Real Node All：同步 host cluster 的所有 node 到 virtual cluster。如果要使用 DaemonSet，需要使用该模式。 Real Nodes Label Selector：仅同步 label selector 匹配的 host node 到 virtual cluster 中。 Real Nodes + Label Selector：仅同步包含在 pod spec.nodeName 且 Label selector 可以选中的 host cluster node 到 virtual cluster 中。 pod 调度 默认情况下，virtual cluster 中的 pod 调度会使用 host cluster 的调度，但存在如下的问题：\n在 virtual cluster node 上的 label 对于 pod 调度不会生效。 drait、trait 命令对于 virtual cluster 上的 pod 没有影响。 virtual cluster 中使用自定义调度器不生效。 基于上述限制，vCluster 支持如下两种方案：\n支持在 virtual cluster 中使用独立的调度器。可以给 virtual cluster 上的 node 增加标签、污点等信息，pod 的调度在 virtual cluster 中的调度器实现，syncer 组件仅将已经调度完成的 pod 同步到 host cluster。 仍然复用 host cluster 调度器，但做了部分功能的增强：在 syncer 服务中指定仅同步部分 host node 到 virtual cluster 中，这样 pod 就仅会调度到 host cluster 的特定 node 上。 网络 virutal cluster 中无独立的 pod 网络和 service 网络，完全复用 host cluster 的网络。\nService 网络 会从 virtual cluster 同步到 host cluster，两者的 clusterip 一致。 允许将一些 host cluster 中的 service 同步到 virtual cluster 中，同时指定service 的名字。 允许将 virtual cluster 中的 service 同步到 host cluster 中，同时指定service 的名字。 Ingress 网络 允许将 virtual cluster 中的 Ingress 同步到 host cluster，以便复用 host cluster 中的 Ingress Controller。\nDNS 解析 在 virtual cluster 中部署了单独的 coredns 组件，默认情况下，在 vritual cluster 中的域名仅能解析内部的域名，不能解析 host cluster 上的域名。可以通过开关的方式，将 virtual cluster 中的域名解析转发到 host cluster 的 coredns。\n在 PRO 版本中，coredns 组件可以集成到 syncer 组件内部，以便节省资源。\nNetworkPolicy 默认情况下，vcluster 中会忽略 virtual cluster 中的 NetworkPolicy 资源。可以通过开关的方式打开该配置，即可将 NetworkPolicy 规则同步到 host cluster。\n存储 默认情况下，host StorageClass 不会同步到 vc，可以通过开关的方式打开同步。 默认情况下，pv 不会从 vc 同步到 host cluster，可以通过开关的方式打开。\n可观测性 monitoring metrics-server 用来监控 k8s 的 Deployment、StatefulSet 等对象，metrics-server 可以复用 host cluster 中的，但需要启用 metrics server proxy 功能。也可以在 vc 中单独部署一套 metrics server。 在 vc 集群中，由于每个 k8s node 的 ip 地址为 host cluster 中的 service clusterip，在 vc 中网络是可达的，可以获取到对应的监控信息。\nlogging 需要用到Hostpath Mapper组件，该组件为 DaemonSet 的形式。后续即可以部署 loki 等组件。\n安全 隔离模式 在启动的时候指定--isolate，在该模式下对 workload 做了多种限制。\n对 vcluster pod 的 Pod Security 做限制，不符合规范的 pod 不会同步到 host cluster。 可以对 vc 中 pod 的总资源量做限制。 在 host cluster 上通过 NetworkPolicy 做隔离。 virtual cluster 集群的创建 目前仅能通过 vcluster cli、helm 的方式来创建，底层均为 helm chart 的方式来管理，缺少服务化功能。\nvirtual cluster 集群对外暴露方法 获取 kubeconfig vcluster connect 命令 该命令可以修改本地的 kubeconfig 文件，并将 context 切换为 virtual cluster context。默认为 virutual cluster 的管理员权限，可以指定使用特定的 ServiceAccount。\nhost cluster secret 中获取到 kubeconfig 在 host cluster 中，在 vc 的 namespace 下，存在一个以 vc- 开头的 Secret，该 Secret 中保存了 kubeconfig 完整信息。\nvc 集群中的 apiserver 的暴露地址 可以在 syncer 启动的时候指定获取的 kubeconfig 中的 endpoint 地址。endpoint 地址即为 vc 集群中的 kube-apiserver 的地址，该 kube-apiserver 的地址可以通过 host cluster 中的 Ingress、LoadBalancer Service、NodePort Service 等方式对外暴露。\n高可用设计 control plane 高可用 k3s 可以支持高可用架构，在创建 vc 的时候通过指定的副本的方式来设置高可用。其他的 k8s 发行版同样类似的实现。\n备份与恢复 vCluster 本身并没有提供对于 vc 集群的数据备份与恢复功能，可以通过通用的 velero 方式实现备份与恢复功能。\n总结 未做网络隔离，容器网络和 service 网络仍然在同一个平面，要想相互隔离，必须使用 NetworkPolicy。\n其他 获取 helm chart 到本地 1 2 3 helm repo add lofts https://charts.loft.sh/ helm fetch lofts/vcluster `` ","date":"2023-11-29T20:23:05Z","permalink":"/post/k8s-virtual-cluster-%E6%96%B9%E6%A1%88-vcluster/","title":"k8s virtual cluster 方案 - vCluster"},{"content":"CSI：Container Storage Interface (CSI)\nVolume 的三个阶段：\nProvision and Delete：负责卷的创建以及销毁。 Attaching and Detaching：将卷设备挂载到本地或者从本地卸载。 Mount and Umount：将 Attaching 的块设备以目录形式挂载到 pod中，或者从 pod 中卸载块设备。 开发 CSI 插件分为三个部分：\nCSI Identity：用来获取 CSI 的身份信息 CSI Controller CSI Node 参考 k8s 官方的 hostpath 项目：https://github.com/kubernetes-csi/csi-driver-host-path\n为了方便开发，在每个阶段 k8s 官方均实现了对应的 SideCarSet 容器。要想研发，仅需要实现 grpc server，又 SideCarSet 容器调用自研的容器。\n自研的容器需要实现如下的接口即可。\nCSI Identity 1 2 3 4 5 6 7 8 9 10 service Identity { // 返回插件名字以及版本号 rpc GetPluginInfo(GetPluginInfoRequest) returns (GetPluginInfoResponse) {} // 返回插件的包含的功能 rpc GetPluginCapabilities(GetPluginCapabilitiesRequest) returns (GetPluginCapabilitiesResponse) {} rpc Probe (ProbeRequest) returns (ProbeResponse) {} } External provisioner 会调用该接口。\nCSI Controller 实现 Volume 中的 Provisioning and Deleting 和 Attaching and Detaching 两个阶段的功能。只有块存储 CSI 插件才需要 Attach 功能。 该部分以中心化组件的方式部署，比如 Deployment。Provision 功能对应的 SidecarSet 为 external-provisioner，Attach 对应的 SidecarSet 为 external-attacher。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 service Controller { // Provisioning， External provisioner调用 // hostPath 实现中会调用 fallocate 实现 rpc CreateVolume (CreateVolumeRequest) returns (CreateVolumeResponse) {} // Deleting， External provisioner调用 rpc DeleteVolume (DeleteVolumeRequest) returns (DeleteVolumeResponse) {} // Attaching, 只有块设备才需要实现，比如云盘，由External attach 调用 rpc ControllerPublishVolume (ControllerPublishVolumeRequest) returns (ControllerPublishVolumeResponse) {} // Detaching，External attach 调用 rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest) returns (ControllerUnpublishVolumeResponse) {} rpc ValidateVolumeCapabilities (ValidateVolumeCapabilitiesRequest) returns (ValidateVolumeCapabilitiesResponse) {} rpc ListVolumes (ListVolumesRequest) returns (ListVolumesResponse) {} rpc GetCapacity (GetCapacityRequest) returns (GetCapacityResponse) {} rpc ControllerGetCapabilities (ControllerGetCapabilitiesRequest) returns (ControllerGetCapabilitiesResponse) {} // 创建快照功能 rpc CreateSnapshot (CreateSnapshotRequest) returns (CreateSnapshotResponse) {} // 删除快照功能 rpc DeleteSnapshot (DeleteSnapshotRequest) returns (DeleteSnapshotResponse) {} rpc ListSnapshots (ListSnapshotsRequest) returns (ListSnapshotsResponse) {} rpc ControllerExpandVolume (ControllerExpandVolumeRequest) returns (ControllerExpandVolumeResponse) {} rpc ControllerGetVolume (ControllerGetVolumeRequest) returns (ControllerGetVolumeResponse) { option (alpha_method) = true; } rpc ControllerModifyVolume (ControllerModifyVolumeRequest) returns (ControllerModifyVolumeResponse) { option (alpha_method) = true; } } CSI Node 实现 Volume 中的Mount 和 Umount 阶段，由 kubelet 负责调用。 该部分以 DaemonSet 的形式部署在每个 k8s node 上，对应的 SidecarSet 容器为 node-driver-registrer。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 service Node { // 针对块存储类型，将块设备格式化后先挂载到一个临时全局目录 rpc NodeStageVolume (NodeStageVolumeRequest) returns (NodeStageVolumeResponse) {} rpc NodeUnstageVolume (NodeUnstageVolumeRequest) returns (NodeUnstageVolumeResponse) {} // 如果是块存储设备，在执行完 NodeStageVolume 后，使用 linux 的 bind mount 技术将全局目录挂载到pod 中的对应目录 rpc NodePublishVolume (NodePublishVolumeRequest) returns (NodePublishVolumeResponse) {} rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest) returns (NodeUnpublishVolumeResponse) {} rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest) returns (NodeGetVolumeStatsResponse) {} rpc NodeExpandVolume(NodeExpandVolumeRequest) returns (NodeExpandVolumeResponse) {} rpc NodeGetCapabilities (NodeGetCapabilitiesRequest) returns (NodeGetCapabilitiesResponse) {} rpc NodeGetInfo (NodeGetInfoRequest) returns (NodeGetInfoResponse) {} } 资料 协议：https://github.com/container-storage-interface/spec/blob/master/spec.md\n","date":"2023-11-23T22:56:40Z","permalink":"/post/k8s-csi/","title":"k8s CSI"},{"content":"重装过电脑操作系统的同学大概知道操作系统的安装流程如下：\n在 BIOS 中将系统设置为光驱/USB开机优先模式 以DVD或者 U 盘中的操作系统开机，进入到装机界面 完成一系列的装机初始化，比如磁盘分区、语言选择等 重启进入新安装的操作系统 以上过程必须要手工才能完成，安装一台电脑还可以，但如果要大批量安装一批机器就不适用了。为此，Intel 公司研发了 PXE(Pre-boot Execution Environment) 技术，可以通过网络的方式批量安装操作系统。\nPXE 基于 C/S 架构，分为PXE client 和PXE server，其中 PXE client 为要安装操作系统的机器，PXE server 用来提供安装操作系统必须的镜像等信息。要想实现从网络上安装操作系统，必须要解决如下几个问题：\n因为还没有安装操作系统，此时并不存在 ip 地址，在装机之前必须要获取到一个 ip 地址。 安装操作系统需要的 boot loader 和操作系统镜像如何获取。 为了解决 PXE client 的 ip 地址问题，PXE 中采用了 DHCP 协议来给 client 分配 ip 地址，这就要求 PXE server 必须要运行 dhcp server。为了解决 PXE server 可以提供 boot loader 和操作系统基线，PXE server 通过 tftp 协议的方式对 client 提供服务。\nclient 端需要 DHCP client 和 tftp client 的功能，为此 PXE 协议中将该功能以硬件的方式内置在网卡 ROM 中。当启动时，BIOS 会加载内置在网卡中的 ROM，从而该机器具备了 DHCP client 和 tftp client 的功能。\n优点：\n规模化：可以批量实现多台服务器的安装 自动化：可以自动化安装 远程实现：不用本地的光盘来安装 OS 客户机的前提条件：\n网络必须要支持 PXE 协议 主板支持网络引导，一般在 BIOS 中可以配置 服务端：\nDHCP 服务，用来给客户机分配 ip 地址 TFTP 服务：用来提供操作系统文件的下载 ","date":"2023-10-12T20:12:55Z","permalink":"/post/pxe/","title":"PXE"},{"content":"\nTCP Fast Open（TFO）是一种TCP协议的扩展，旨在加快建立TCP连接的速度和降低延迟。传统的TCP连接需要进行三次握手（SYN-SYN/ACK-ACK）才能建立连接，而TFO允许在第一个数据包中携带连接建立的请求。\nTFO的工作原理如下：\n客户端在首次建立TCP连接时，在发送的SYN包中插入一个加密的Cookie。这个Cookie由服务器生成并发送给客户端。 当客户端发送带有TFO Cookie的SYN包到服务器时，服务器会验证Cookie的有效性。 如果Cookie有效，服务器会立即发送带有SYN+ACK标志的数据包，这样客户端就可以立即发送数据而无需等待ACK响应。 客户端收到带有SYN+ACK标志的数据包后，发送带有ACK标志的数据包，建立完整的TCP连接。 相关内核参数 net.ipv4.tcp_fastopen 支持如下值：\n0：关闭 1: 作为客户端可以使用 TFO 功能 2: 作为服务端可以使用 TFO 功能 3: 作为客户端和服务端均可使用 TFO net.ipv4.tcp_fastopen_key 用来产生 TFO 的 Cookie。\n","date":"2023-09-30T23:46:40Z","permalink":"/post/tcp-fast-open/","title":"TCP Fast Open"},{"content":"zone 设置 DNS 记录的 zone 信息为全局配置，配置地方包括 kubelet 和 coredns 两部分。\nkubelet 的启动参数 通过 kubelet 的 yaml 配置文件的 clusterDomain 字段。 通过 kubelet 的参数 --cluster-domain。 设置了 kubelet 的启动参数后，会设置容器的 /etc/resolv.conf 中的 search 域为如下格式：\n1 2 3 search default.svc.cluster.local svc.cluster.local cluster.local tbsite.net nameserver 10.181.48.10 options ndots:5 single-request-reopen 其中 search 域中的 cluster.local 为 kubelet 的配置。\ncoredns 的配置文件 coredns controller 需要 watch k8s 集群中的 pod 和 service，将其进行注册，因此 coredns 需要知道集群的 zone 配置。该配置信息位于 coredns 的配置文件 ConfigMap kube-system/coredns 中，默认的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } 其中 cluster.local 为对应的 k8s zone。\n域名注册 在 k8s 中，Service 和 Pod 对象会创建 DNS 记录，用于 k8s 集群内部的域名解析。\nPod 域名注册 规则一：\n每个 k8s pod 都会创建 DNS 记录： \u0026lt;pod_ip\u0026gt;.\u0026lt;namespace\u0026gt;.pod.\u0026lt;cluster-domain\u0026gt;。其中 \u0026lt;pod_ip\u0026gt; 为 pod ip 地址，但需要将 ip 地址中的 . 转换为 -。\n比如 pod nginx-deployment-57d84f57dc-cpgkc 会创建 A 记录 10-244-3-8.default.pod.cluster.local\n1 2 3 $ kubectl get pod -o wide nginx-deployment-57d84f57dc-cpgkc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-57d84f57dc-cpgkc 1/1 Running 0 2m59s 10.244.3.8 vc-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 规则二：\npod 如何同时指定了 spec.hostname 和 spec.subdomain，则会创建 A 记录：\u0026lt;hostname\u0026gt;.\u0026lt;subdomain\u0026gt;.\u0026lt;namespace\u0026gt;.svc.cluster.local，而不是 \u0026lt;pod_ip\u0026gt;.\u0026lt;namespace\u0026gt;.pod.\u0026lt;cluster-domain\u0026gt;。对于 Statefulset 类型的 pod 会自动设置 spec.hostname 为 pod 的名字，spec.subdomain 为 StatefulSet 的 spec.serviceName。\n比如 pod nginx-statefulset-0 会创建 A 记录 nginx-statefulset-0.nginx.default.svc.cluster.local。\n1 2 3 $ kubectl get pod nginx-statefulset-0 -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-statefulset-0 1/1 Running 0 62m 10.244.3.7 vc-worker2 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Deployment/DaemonSet 管理的 pod 使用 Deployment/DaemonSet 拉起的 pod，k8s 会创建额外的 DNS 记录：\u0026lt;pod_ip\u0026gt;.\u0026lt;deployment-name/daemonset-name\u0026gt;.\u0026lt;namespace\u0026gt;.svc.\u0026lt;cluster-domain\u0026gt;。\nService 域名注册 普通 Service 除了 headless service 之外的其他 service 会在 DNS 中生成 my-svc.my-namespace.svc.cluster-domain.example 的 A 或者 AAAA 记录，A 记录指向 ClusterIP。\nheadless service 会在 DNS 中生成 my-svc.my-namespace.svc.cluster-domain.example 的 A 或者 AAAA 记录，但指向的为 pod ip 地址集合。\nk8s 在 pod 的 /etc/resolv.conf 配置如下：\n1 2 3 nameserver 10.32.0.10 search \u0026lt;namespace\u0026gt;.svc.cluster.local svc.cluster.local cluster.local options ndots:5 对于跟 pod 同一个 namespace 下的 service，要访问可以直接使用 service 名字接口。跟 pod 不在同一个 namespace 下的 service，访问 service 必须为 service name.service namespace。\nExternalName Service service 的 spec.type 为 ExternalName，该种类型的服务会向 dns 中注册 CNAME 记录，CNAME 记录指向 externalName 字段。例子如下：\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com 当访问 my-service.prod.svc.cluster.local 时，DNS 服务会返回 CNAME 记录，指向地址为 my.database.example.com。\nexternalIPs 字段 可以针对所有类型的 Service 生效，用来配置多个外部的 ip 地址（该 ip 地址不是 k8s 分配），kube-proxy 会设置该 ip 地址的规则，确保在 k8s 集群内部访问该 ip 地址时，可以路由到后端的 pod。效果就跟访问普通的 ClusterIP 类型 Service 没有区别。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app.kubernetes.io/name: MyApp ports: - name: http protocol: TCP port: 80 targetPort: 49152 externalIPs: - 198.51.100.32 当在集群内部访问 198.51.100.32:80 时，流量会被 kube-proxy 路由到当前 Service 的 Endpoint。\n该字段存在中间人攻击的风险，不推荐使用。Detect CVE-2020-8554 – Unpatched Man-In-The-Middle (MITM) Attack in Kubernetes\nHeadless Service 翻译成中文又叫无头 Service，显式的将 Service spec.clusterIP 设置为 \u0026quot;None\u0026quot;，表示该 Service 为 Headless Service。此时，该 Service 不会分配 clusterIP。因为没有 clusterIP，因此 kube-proxy 并不会处理该 service。\nHeadless Service 按照是否配置了 spec.selector 在实现上又有不同的区分。\n未配置 spec.selector 的 Service，不会创建 EndpointSlice 对象，但是会注册如下的记录：\n对于 ExternalName Service，配置 CNAME 记录。 对于非 ExternalName Service，配置 A/AAAA 记录，指向 EndPoint 的所有 ip 地址。如果未配置 Endpoint，但配置了 externalIPs 字段，则指向 externalIPs。 配置 spec.selector 的 Service，会创建 EndpointSlice 对象，并修改 DNS 配置返回 A 或者 AAAA 记录，指向 pod 的集合。\n域名查询 待补充\n资料 k8s 官方 Service 文档 k8s 官方 Service 与 Pod 的 DNS Kubernetes DNS-Based Service Discovery ","date":"2023-09-12T12:05:42Z","permalink":"/post/k8s-%E9%9B%86%E7%BE%A4%E5%86%85-dns-%E8%A7%84%E8%8C%83/","title":"k8s 集群内 dns 规范"},{"content":"json patch 该规范定义在 RFC 6902，定义了修改 json 格式的规范，同时还可以配合 http patch 请求一起使用，实例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 PATCH /my/data HTTP/1.1 Host: example.org Content-Length: 326 Content-Type: application/json-patch+json If-Match: \u0026#34;abc123\u0026#34; [ { \u0026#34;op\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;foo\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;value\u0026#34;: [ \u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34; ] }, { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;value\u0026#34;: 42 }, { \u0026#34;op\u0026#34;: \u0026#34;move\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/d\u0026#34; }, { \u0026#34;op\u0026#34;: \u0026#34;copy\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/a/b/d\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/e\u0026#34; } ] 支持add、remove、replace、move、copy和 test 六个patch动作。\n协议规范 add 格式如下：\n1 { \u0026#34;op\u0026#34;: \u0026#34;add\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;value\u0026#34;: [ \u0026#34;foo\u0026#34; ] } 规范：\n如果原始 json 中不存在 key \u0026ldquo;/hello\u0026rdquo;，则会全新创建 key。 如果原始 json 存在 key \u0026ldquo;/hello\u0026rdquo;，则会直接覆盖；即使\u0026quot;/hello\u0026quot;为数组，也不会在原先的基础上追加，而是直接强制覆盖； 原始 json 如下：\n1 2 3 { \u0026#34;hello\u0026#34;: [\u0026#34;123\u0026#34;] } 执行后结果如下：\n1 2 3 4 5 { \u0026#34;hello\u0026#34;: [ \u0026#34;world\u0026#34; ] } remove 用来删除某个 key，格式如下：\n1 [{ \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/hello\u0026#34; }] replace 用来替换某个 key，跟 add 动作的差异是，如果 key 不存在，则不会创建 key。\n1 { \u0026#34;op\u0026#34;: \u0026#34;replace\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;value\u0026#34;: 42 } 如果原始 json 格式为: {}，执行完成后，输出 json 格式仍然为：{}。\nmove 用来修改 key 的名称，格式如下：\n1 { \u0026#34;op\u0026#34;: \u0026#34;move\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/hello2\u0026#34; } 如果 key 不存在，则不做任何修改。\ncopy 用来复制某个 key，格式如下：\n1 { \u0026#34;op\u0026#34;: \u0026#34;copy\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/hello2\u0026#34; } 如果原始 key 不存在，则不复制；如果目标 key 已经存在，则仍然会复制。\n原始 json 如下：\n1 2 3 4 { \u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;, \u0026#34;hello2\u0026#34;: \u0026#34;world2\u0026#34; } 执行完成后的 json 如下：\n1 2 3 4 { \u0026#34;hello\u0026#34;: \u0026#34;world\u0026#34;, \u0026#34;hello2\u0026#34;: \u0026#34;world\u0026#34; } test 用来测试 key 对应的 value 是否相等，该操作并不常用\n1 { \u0026#34;op\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/a/b/c\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;foo\u0026#34; } 工具 JSON Patch Builder Online 在线工具，可根据原始 json 和 patch 完成后的 json，产生 json patch jsonpatch.me 在线工具，可根据原始 json 和 json patch，产生 patch 完成后的 json 总结 通过上述协议可以发现如下缺点：\n对于数组的处理不是太理想，如果要删除数组中的某个元素，或者在数组中追加某个元素，则无法表达。 该协议对于人类并不友好。 json merge patch 定义在 RFC 7386，由于patch 能力比较有限，使用场景较少。\n同样可以配合 http patch 方法一起使用，http 请求如下：\n1 2 3 4 5 6 7 8 9 10 PATCH /target HTTP/1.1 Host: example.org Content-Type: application/merge-patch+json { \u0026#34;a\u0026#34;:\u0026#34;z\u0026#34;, \u0026#34;c\u0026#34;: { \u0026#34;f\u0026#34;: null } } 下面结合具体的实例来说明 json merge patch 的功能。 原始 json 格式如下：\n1 2 3 4 5 6 7 8 9 { \u0026#34;title\u0026#34;: \u0026#34;Goodbye!\u0026#34;, \u0026#34;author\u0026#34; : { \u0026#34;givenName\u0026#34; : \u0026#34;John\u0026#34;, \u0026#34;familyName\u0026#34; : \u0026#34;Doe\u0026#34; }, \u0026#34;tags\u0026#34;:[ \u0026#34;example\u0026#34;, \u0026#34;sample\u0026#34; ], \u0026#34;content\u0026#34;: \u0026#34;This will be unchanged\u0026#34; } patch json 格式如下：\n1 2 3 4 5 6 7 8 { \u0026#34;title\u0026#34;: \u0026#34;Hello!\u0026#34;, \u0026#34;phoneNumber\u0026#34;: \u0026#34;+01-123-456-7890\u0026#34;, \u0026#34;author\u0026#34;: { \u0026#34;familyName\u0026#34;: null }, \u0026#34;tags\u0026#34;: [ \u0026#34;example\u0026#34; ] } 其中 null 用来表示该 key 需要删除。对于数组类型，则直接覆盖数组中的值。 patch 完成后的 json 如下：\n1 2 3 4 5 6 7 8 9 { \u0026#34;title\u0026#34;: \u0026#34;Hello!\u0026#34;, \u0026#34;author\u0026#34; : { \u0026#34;givenName\u0026#34; : \u0026#34;John\u0026#34; }, \u0026#34;tags\u0026#34;: [ \u0026#34;example\u0026#34; ], \u0026#34;content\u0026#34;: \u0026#34;This will be unchanged\u0026#34;, \u0026#34;phoneNumber\u0026#34;: \u0026#34;+01-123-456-7890\u0026#34; } 通过上述实例可以发现如下的功能缺陷：\n如果某个 json 的 key 对应的值为 null，则无法表达，即不可以将某个 key 对应的value 设置为 null。 对于数组的处理非常弱，是直接对数组中所有元素的替换。 k8s strategic merge patch 该协议的资料较少，官方参考资料只有两篇文章，最好结合着 k8s 的代码才能完全理解：\nUpdate API Objects in Place Using kubectl patch Strategic Merge Patch Pod Spec 定义 背景 无论是 json patch，还是 json merge patch 协议，对于数组元素的支持都不够友好。 比如对于如下的 json：\n1 2 3 4 spec: containers: - name: nginx image: nginx-1.0 期望能够 patch 如下的内容\n1 2 3 4 spec: containers: - name: log-tailer image: log-tailer-1.0 从而可以实现 containers中包含两个元素的情况，无论是 json patch 还是 json merge patch，其行为是对数组元素的直接替换，不能实现追加的功能。\n协议规范 为了解决 json merge patch 的功能缺陷，strategic merge patch 通过如下两种方式来扩展功能：\njson merge patch 的 json 语法增强，增加一些额外的指令 通过增强原始 json 的 struct 结构实现，跟 golang 语言强绑定，通过 golang 中的 struct tag 机制实现。这样的好处是不用再扩充 json merge patch 的 json 格式了。支持如下 struct tag： patchStrategy： 指定策略指令，支持：replace、merge 和 delete。默认的行为为 replace，保持跟 json merge patch 的兼容性。 patchMergeKey: 数组一个子 map 元素的主键，类似于关系型数据库中一行记录的主键。 支持如下指令：\nreplace merge delete replace 支持 go struct tag 和 在 json patch 中增加指令两种方式。 replace 是默认的指令模式，对于数组而言会直接全部替换数组内容。\n如下指令用来表示，\n1 2 3 4 $patch: replace # recursive and applies to all fields of the map it\u0026#39;s in containers: - name: nginx image: nginx-1.0 delete 删除数组中的特定元素，下面例子可以删除数组中包含 name: log-tailer 的元素。\n1 2 3 4 5 containers: - name: nginx image: nginx-1.0 - $patch: delete name: log-tailer # merge key and value goes here 删除 map 的特定 key，如下实例可以删除 map 中的 key rollingUpdate。\n1 2 rollingUpdate: $patch: delete merge 该指令仅支持 go struct tag 模式，格式为：$deleteFromPrimitiveList/\u0026lt;keyOfPrimitiveList\u0026gt;: [a primitive list]。\ndeleteFromPrimitiveList 删除数组中的某个元素 go struct 定义如下：\n1 Finalizers []string `json:\u0026#34;finalizers,omitempty\u0026#34; patchStrategy:\u0026#34;merge\u0026#34; protobuf:\u0026#34;bytes,14,rep,name=finalizers\u0026#34;` 原始 yaml 如下：\n1 2 3 4 5 finalizers: - a - b - c - b patch yaml 如下，用来表示删除finalizers中的所有元素 b 和 c\n1 2 3 4 5 6 # The directive includes the prefix $deleteFromPrimitiveList and # followed by a \u0026#39;/\u0026#39; and the name of the list. # The values in this list will be deleted after applying the patch. $deleteFromPrimitiveList/finalizers: - b - c 最终得到结果：\n1 2 finalizers: - a setElementOrder 用于数组中的元素排序\n简单数组排序例子 原始内容如下：\n1 2 3 4 finalizers: - a - b - c 设置排序顺序：\n1 2 3 4 5 6 # The directive includes the prefix $setElementOrder and # followed by a \u0026#39;/\u0026#39; and the name of the list. $setElementOrder/finalizers: - b - c - a 最终得到排序顺序：\n1 2 3 4 finalizers: - b - c - a map 类型数组排序例子 其中 patchMergeKey 为 name 字段\n1 2 3 4 5 6 7 containers: - name: a ... - name: b ... - name: c ... patch 指令的格式：\n1 2 3 4 5 # each map in the list should only include the mergeKey $setElementOrder/containers: - name: b - name: c - name: a 最终获得结果：\n1 2 3 4 5 6 7 containers: - name: b ... - name: c ... - name: a ... retainKeys 用来清理 map 结构中的 key，并指定保留的 key 原始内容：\n1 2 3 union: foo: a other: b patch 内容：\n1 2 3 4 5 6 union: retainKeys: - another - bar another: d bar: c 最终结果，可以看到 foo 和 other 因为不在保留列表中已经被清楚了。同时新增加了字段 another 和 bar，新增加的是字段是直接 patch 的结果，同时这两个字段也在保留的列表内。\n1 2 3 4 union: # Field foo and other have been cleared w/o explicitly set them to null. another: d bar: c strategic merge patch 在 k8s 中应用 kubectl patch 命令通过\u0026ndash;type 参数提供了几种 patch 方法。\n1 --type=\u0026#39;strategic\u0026#39;: The type of patch being provided; one of [json merge strategic] json：即支持 json patch 协议，例子：kubectl patch pod valid-pod --type='json' -p='[{\u0026quot;op\u0026quot;: \u0026quot;replace\u0026quot;, \u0026quot;path\u0026quot;: \u0026quot;/spec/containers/0/image\u0026quot;, \u0026quot;value\u0026quot;:\u0026quot;newimage\u0026quot;}] merge：对应的为 json merge patch 协议。 stategic：k8s 特有的 patch 协议，在 json merge patch 协议基础上的扩展，可以解决 json merge patch 的缺点。 对于 k8s 的 CRD 对象，由于不存在 go struct tag，因此无法使用 stategic merge patch。\nTODO：待补充具体例子\nkubectl patch、replace、apply之间的区别 patch kubectl patch 命令的实现比较简单，直接调用 kube-apiserver 的接口在 server 端实现 patch 操作。\nreplace 如果该命令使用 \u0026ndash;force=true 参数，则会先删除对象，然后再提交，相当于是全新创建。\napply apply 的实现相对比较复杂，逻辑也比较绕，可以实现 map 结构中的字段增删改操作，数组中数据的增删改操作。实现上会将多份数据进行merge 后提交，数据包含：\n要重新 apply 的 yaml 对象的annotation kubectl.kubernetes.io/last-applied-configuration 包含的内容 运行时的 k8s 对象 具体的操作步骤：\n要重新 apply 的 yaml 跟annotation kubectl.kubernetes.io/last-applied-configuration 包含的内容比较，获取到要删除的字段。 要重新 apply 的 yaml 跟运行时的 k8s 对象进行比较，获取到要增加的字段。 上述两个结果再进行一次 merge 操作，最终调用 kube-apiserver 的接口实现 patch 操作。 为什么一定需要用到kubectl.kubernetes.io/last-applied-configuration的数据呢？ 在 yaml 对象提交到 k8s 后，k8s 会自动增加一些字段，也可以通过额外的方式修改对象增加一些字段。如果 patch 内容仅跟运行时结果比较，会导致一些运行时的k8s 自动增加的字段或者手工更新的字段被删除掉。\n试验 上次提交 last-applied-configuration 运行时 patch 内容 结果 结果分析 试验一 label1: first label1: first label2: second label2: second 1. patch 内容跟上次内容比较，发现要删除字段 label1\n2. patch 内容跟运行时比较，发现新增加了字段 label2 3. 最终 label1 被删除，仅保留 label2 试验二 label1: first label1: first label2: second label1: first label1: first label2: second 1. patch 内容跟上次内容比较，发现结果无变化\n2. patch 内容跟运行时比较，发现要新增加字段 label2\n3. 最终新增加字段 label2 引用 json patch RFC 规范 https://jsonpatch.com/ ","date":"2023-01-29T17:04:14Z","permalink":"/post/json-patch/","title":"json patch"},{"content":"本文将通过 kubeadm 实现单 master 节点模式和集群模式两种部署方式。\n所有节点均需初始化操作 所有节点均需做的操作。\n主机准备 1 2 3 4 5 6 7 8 cat \u0026gt; /etc/sysctl.d/kubernets.conf \u0026lt;\u0026lt;EOF net.ipv4.ip_forward=1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF sysctl --system modprobe br_netfilter 安装containerd 由于 dockerd 从 k8s 1.24 版本开始不再支持，这里选择 containerd。\n手工安装 安装 containerd，containerd 的版本可以从这里获取 https://github.com/containerd/containerd/releases\n1 2 3 4 5 wget https://github.com/containerd/containerd/releases/download/v1.6.11/containerd-1.6.11-linux-amd64.tar.gz tar Cxzvf /usr/local containerd-1.6.11-linux-amd64.tar.gz wget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service -P /usr/local/lib/systemd/system/ systemctl daemon-reload systemctl enable --now containerd 安装 runc\n1 2 wget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64 install -m 755 runc.amd64 /usr/local/sbin/runc yum 源安装 1 2 3 4 5 yum install -y yum-utils yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install containerd.io -y systemctl daemon-reload systemctl enable --now containerd 通过 yum 安装的containerd 没有启用 cri，在其配置文件 /etc/containerd/config.toml 中包含了 disabled_plugins = [\u0026quot;cri\u0026quot;] 配置，需要将配置信息注释后并重启 containerd。\n1 2 sed -i \u0026#39;s/disabled_plugins/#disabled_plugins/\u0026#39; /etc/containerd/config.toml systemctl restart containerd 安装 kubeadm/kubelet/kubectl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 cat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\\$basearch enabled=1 gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF # Set SELinux in permissive mode (effectively disabling it) sudo setenforce 0 sudo sed -i \u0026#39;s/^SELINUX=enforcing$/SELINUX=permissive/\u0026#39; /etc/selinux/config sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes sudo systemctl enable --now kubelet 单 master 节点模式 节点 角色 172.21.115.190 master 节点 kubeadm 初始化 创建文件 kubeadm-config.yaml，文件内容如下：\n1 2 3 4 5 6 apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.25.4 kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd 执行命令：\n1 2 3 kubeadm init --config kubeadm-config.yaml kubeadm config print init-defaults --component-configs KubeletConfiguration \u0026gt; cluster.yaml kubeadm init --config cluster.yaml 接下来初始化 kubeconfig 文件，这样即可通过 kubectl 命令来访问 k8s 了。\n1 2 3 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 安装网络插件 刚部署完成的节点处于NotReady 的状态，原因是因为还没有安装网络插件。\ncilim 网络插件 cilim 网络插件比较火爆，下面介绍其安装步骤：\n1 2 3 4 5 6 7 8 9 10 11 # 安装 cilium 客户端 CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/master/stable.txt) CLI_ARCH=amd64 if [ \u0026#34;$(uname -m)\u0026#34; = \u0026#34;aarch64\u0026#34; ]; then CLI_ARCH=arm64; fi curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} # 网络插件初始化 cilium install 在安装完网络插件后，node 节点即可变为 ready 状态。\n查看环境中包含如下的 pod:\n1 2 3 4 5 6 7 8 9 10 11 $ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cilium-7zj7t 1/1 Running 0 82s kube-system cilium-operator-bc4d5b54-kvqqx 1/1 Running 0 82s kube-system coredns-565d847f94-hrm9b 1/1 Running 0 14m kube-system coredns-565d847f94-z5kwr 1/1 Running 0 14m kube-system etcd-k8s002 1/1 Running 0 14m kube-system kube-apiserver-k8s002 1/1 Running 0 14m kube-system kube-controller-manager-k8s002 1/1 Running 0 14m kube-system kube-proxy-bhpqr 1/1 Running 0 14m kube-system kube-scheduler-k8s002 1/1 Running 0 14m k8s 自带的 bridge 插件 在单节点的场景下，pod 不需要跨节点通讯，k8s 自带的 bridge 插件也可以满足单节点内的 pod 相互通讯，类似于 docker 的 bridge 网络模式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 mkdir -p /etc/cni/net.d cat \u0026gt; /etc/cni/net.d/10-mynet.conf \u0026lt;\u0026lt;EOF { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;mynet\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;isGateway\u0026#34;: true, \u0026#34;ipMasq\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;172.19.0.0/24\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ] } } EOF 如果 k8s 节点已经部署完成，需要重启下 kubelet 进程该配置即可生效。\n添加其他节点 1 2 kubeadm join 172.21.115.189:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:457fba2c4181a5b02d2a4f202dfe20f9ce5b9f2274bf40b6d25a8a8d4a7ce440 此时即可以将节点添加到 k8s 集群中\n1 2 3 4 $ kubectl get node NAME STATUS ROLES AGE VERSION k8s002 Ready control-plane 79m v1.25.4 k8s003 Ready \u0026lt;none\u0026gt; 35s v1.25.4 节点清理 清理普通节点 1 2 3 4 5 kubectl drain \u0026lt;node name\u0026gt; --delete-emptydir-data --force --ignore-daemonsets kubeadm reset # 清理 iptabels 规则 iptables -F \u0026amp;\u0026amp; iptables -t nat -F \u0026amp;\u0026amp; iptables -t mangle -F \u0026amp;\u0026amp; iptables -X kubectl delete node \u0026lt;node name\u0026gt; 清理 control plan 节点 1 kubeadm reset 集群模式部署 待补充，参考文档：Creating Highly Available Clusters with kubeadm | Kubernetes\n资料 Bootstrapping clusters with kubeadm ","date":"2022-12-07T00:07:20Z","permalink":"/post/%E4%BD%BF%E7%94%A8-kubeadm-%E5%AE%89%E8%A3%85-k8s-%E9%9B%86%E7%BE%A4/","title":"使用 kubeadm 安装 k8s 集群"},{"content":"\n题图为望京傍晚的天气，夕阳尽情散发着落山前的最后余光，层次分明的云朵映射在建筑物的上熠熠生辉。上班族结束了一天紧张的工作，朝着地铁站的方向奔向自己的家，这才是城市生活该有的模样。不过可惜的是，对于很多打工族而言，一天的工作还远未结束，晚饭后仍要坐在灯火通明的写字楼内或为生活或为梦想挥霍着自己的时光。\n资源 kaniko Google开源的一款可以在容器内部通过Dockerfile构建docker镜像的工具。\n我们知道docker build命令可以根据Dockerfile构建出docker镜像，但该操作实际上是由docker daemon进程完成。如果docker build命令在docker容器中执行，由于容器中并没有docker daemon进程，因此直接执行docker build肯定会失败。\nkaniko则重新实现了Dockerfile构建镜像的功能，使得构建镜像不再依赖docker daemon。随着gitops的技术普及，CI工具也正逐渐on k8s部署，kaniko正好可以在k8s的环境中根据Dockerfile完成镜像的打包过程，并将镜像推送到镜像仓库中。\narc42 技术人员在写架构文档的时候，遇到最多的问题是该如何组织技术文档的结构，arc42 提供了架构文档的模板，将架构文档分为了 12 个章节，每个章节又包含了多个子章节，用来帮助技术人员更好的编写架构文档。\n相关链接：https://topic.atatech.org/articles/205083?spm=ata.21736010.0.0.18c23b50NAifwr#tF1lZkHm\nCarina 国内云厂商博云发起的一款基于 Kubernetes CSI 标准实现的存储插件，用来管理本地的存储资源，支持本地磁盘的整盘或者LVM方案来管理存储。同时，还包含了Raid管理、磁盘限速、容灾转移等高级特性。\n相关链接：一篇看懂 Carina 全貌\nkube-capacity k8s的命令行工具kubectl用来查看集群的整体资源情况往往操作会比较复杂，可能需要多条命令配合在一起才能拿得到想要的结果。kube-capacity命令行工具用来快速查看集群中的资源使用情况，包括node、pod维度。\n相关链接：Check Kubernetes Resource Requests, Limits, and Utilization with Kube-capacity CLI\nKubeprober 在k8s集群运维的过程中，诊断能力非常重要，可用来快速的定位发现问题。Kubeprober为一款定位为k8s多集群的诊断框架，提供了非常好的扩展性来接入诊断项，诊断结果可以通过grafana来统一展示。\n社区里类似的解决方案还有Kubehealthy和Kubeeye。\n相关链接：用更云原生的方式做诊断｜大规模 K8s 集群诊断利器深度解析\nOpen Policy Agent OPA为一款开源的基于Rego语言的通用策略引擎，CNCF的毕业项目，可以用来实现一些基于策略的安全防护。比如在k8s中，要求pod的镜像必须为某个特定的registry，用户可以编写策略，一旦pod创建，OPA的gatekeeper组件通过webhook的方式来执行策略校验，一旦校验失败从而会导致pod创建失败。\n比如 阿里云的ACK的gatekeeper 就是基于OPA的实现。\ndocker-squash docker-squash为一款docker镜像压缩工具。在使用Dockerfile来构建镜像时，会产生很多的docker镜像层，当Dockerfile中的命令过多时，会产生大量的docker镜像层，从而导致docker镜像过大。该工具可以将镜像进行按照层合并压缩，从而减小镜像的体积。\nFlowUs FlowUs为国内研发的一款在线编辑器，支持文档、表格和网盘功能，该软件可以实现笔记、项目管理、共享文件等功能，跟蚂蚁集团的产品《语雀》功能比较类似。但相比语雀做的好的地方在于，FlowUs通过”块编辑器“的方式，在FlowUs看来所有的文档形式都是”块“，作者可以在文档中随意放置各种类型的”块“，在同一个文档中即可以有功能完善的表格，也可以有网盘。而语雀要实现一个相对完整的表格，需要新建一种表格类型的文档，类似于Word和Excel。\nk8tz k8s中的pod默认的时区跟pod的镜像有关，跟pod宿主机所在的时区没有关系。很多情况下，用户都期望pod里看到的时区能够跟宿主机的保持一致。用户的一种实现方式是将宿主机的时区文件挂载到pod中，但需要修改pod的yaml文件。本工具可以通过webhook的方式自动化将宿主机的时区文件挂载到pod中。\n文章 中美云巨头歧路，中国云未来增长点在哪？ 文章结合全球的云计算行业，对国内的云计算行业做了非常透彻的分析。”全球云，看中美；中美云，看六大云“，推荐阅读。\n程序员必备的思维能力：结构化思维 结构化思维不仅对于程序员，对于职场中的很多职业都非常重要，无论是沟通、汇报、晋升，还是写代码结构化思维都非常重要。本文深度剖析了金字塔原理以及如何应用，非常值得一读。文章的作者将公众号的文章整理为了《程序员底层思维》一书，推荐大家阅读。\n中文技术文档的写作规范 阮一峰老师的中文技术文档写作规范，写技术文档的同学可以参考。\n书籍 《程序员的底层思维》 通过书名中的“程序员”来看有点初级，但实际上书中的内容适合所有软件行业的从业者，甚至同样适合于其他行业的从业者，因为底层思维本来就是共性的东西，万变不离其宗。作者曾在阿里巴巴有过很长一段工作经历，书中结合着工作中的实践经验介绍了16种思维能力，讲解浅显易懂，部分内容上升到了哲学的角度来讲解。\n作为软件行业从业者的我，实际上书中的大部分思维能力在工作中都有应用，但却没有形成理论来总结。阅读本书，有助于对工作的内容进行总结，找到工作的理论基础。另一方面，有了书中的理论总结，也可以更好的指导工作。\n","date":"2022-07-23T16:09:08Z","permalink":"/post/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E7%AC%AC16%E6%9C%9F/","title":"技术分享第16期"},{"content":"ipv6的优势 拥有更大的地址空间。 点对点通讯更方便。由于ipv6地址足够多，可以不再使用NAT功能，避免NAT场景下的各种坑。 ip配置方便。每台机器都有一个唯一48位的mac地址，如果再增加一个80位的网段前缀即可组成ipv6地址。因此，在分配ip地址的时候，只需要获取到网段前缀即可获取到完整的ipv6地址了。 局域网内更安全。去掉了arp协议，而是采用Neighbor Discovery协议。 ipv6的数据包格式 分为报头、扩展报头和上层协议数据单元（PDU）三部分组成。\n报头 固定为40个字节，相比于 ipv4 的可变长度报文更简洁。\n版本：固定为4bit，固定值6。\n业务流类别：8bit，用来表明数据流的通讯类别或者优先级。\n流标签：20bit，标记ipv6路由器需要特殊处理的数据流，目的是为了让路由器对于同一批数据报文能够按照同样的逻辑来处理。目前该字段的应用场景较少。\n净荷长度：16bit，扩展头和上次协议数据单元的字节数，不包含报头的固定 40 字节。\n下一个头：8bit，每个字段值有固定含义，用来表示上层协议。如果上层协议为 tcp，那么该字段的值为 4。\n跳限制：8bit，即跳数限制，等同于ipv4中的ttl值。\n源ip地址：128bit\n目的ip地址：128bit\n扩展报头 扩展报头的长度任意\nipv6地址 地址表示方法 采用16进制表示法，共128位，分为8组，每组16位，每组用4个16进制表示。各组之间使用:分割。例如：1080:0:0:0:8:800:200C:417A。 地址中出现连续的0，可以使用::来代替连续的0，一个地址中只能出现一次连续的0。例如上述地址可以表示为：1080::8:800:200C:417A。本地的回环地址可使用 ::1 表示。 如果ipv6的前面地址全部为0，可能存在包含ipv4地址的场景，可以使用ipv4的十进制表示方法。例如：0:0:0:0:0:0:61.1.133.1或者::61.1.133.1。 ip地址结构包含了64位的网络地址和64位的主机地址，其中64位的网络地址又分为了48位的全球网络标识符和16位的本地子网标识符。\n网段表示方法 在ipv6中同样有网段的概念，如 2001:0:0:CD30::/60 ，其中前60位为前缀长度，后面的所有位表示接口 ID，使用 :: 表示，但前面的两个0不能使用 :: 表示。\n地址分类 包括了如下地址类型，但跟 ipv4 不同的地方在于没有广播地址。\n单播地址 单播地址又可以分为如下类型：\n链路本地地址（LLA） 类似于 ipv4 的私网地址段，但比 ipv4 的私网地址段范围更小，仅可以用于本地子网内通讯，不可被路由。以fe80::/10开头的地址段。设备要想支持 ipv6，必须要有链路本地地址，且只能设置一个。\n在设备启动时，通常该地址会自动设置，也可以手动设置。自动生成的地址通常会根据 mac 地址有关，因为每个设备的 mac 地址都是唯一的。\n公网单播地址（GUA） 类似于 ipv4 的公网地址。\n地址范围：2000::3 - 3fff::/16，即以 2 或者 3 开头，用总 ipv6 地址空间的1/8。\n通常情况下，公网路由前缀为 48 位，子网 id 为 16 位，接口 id 为 64 位。\n本地唯一单播地址（ULA） 范围：fc00::/7，当前唯一有效的前缀为fd00::/8。只能在私网内部使用，不能在公网上路由。\n其全网 id 的部分采用伪随机算法，可以尽最大可可能确保全局的唯一性，从而在两个网络进行并网的时候减少地址冲突的概率。\nloopback地址 即 ::1，等同于 ipv4 的 127.0.0.1/8\n未指定单播地址 即 ::。\n多播地址（Multicast） 又叫组播地址，标识一组节点，目的为组播地址的流量会转发到组内的所有节点，类似于 ipv4 的广播地址。地址范围：FF00::/8。\n任意播地址（Anycast） 标识一组节点，所有节点的接口分配相同的 ip 地址，目的为组播地址的流量会转发到组内的就近节点。任意播地址没有固定的前缀。\nDNS服务和ipv6 双栈请求域名请求顺序 在开启ipv4/ipv6双栈的情况下，域名解析会同时发出A/AAAA请求，发出请求的先后顺序由/etc/resolv.conf的option中的inet6选项决定。\nipv6与 /etc/hosts文件 通常在/etc/hosts文件中包含了如下的回环地址\n1 ::1 localhost6.localdomain6 localhost6 检测域名是否支持ipv6 dig aaaa方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 dig aaaa ipv6.google.com.hk ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; aaaa ipv6.google.com.hk ;; global options: +cmd ;; Got answer: ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, status: NOERROR, id: 50248 ;; flags: qr rd ra; QUERY: 1, ANSWER: 6, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4000 ;; QUESTION SECTION: ;ipv6.google.com.hk.\tIN\tAAAA ;; ANSWER SECTION: ipv6.google.com.hk.\t21600\tIN\tCNAME\tipv6.google.com. ipv6.google.com.\t21600\tIN\tCNAME\tipv6.l.google.com. ipv6.l.google.com.\t300\tIN\tAAAA\t2607:f8b0:4003:c0b::71 ipv6.l.google.com.\t300\tIN\tAAAA\t2607:f8b0:4003:c0b::64 ipv6.l.google.com.\t300\tIN\tAAAA\t2607:f8b0:4003:c0b::65 ipv6.l.google.com.\t300\tIN\tAAAA\t2607:f8b0:4003:c0b::8b ;; Query time: 83 msec ;; SERVER: 30.30.30.30#53(30.30.30.30) ;; WHEN: Mon Jun 20 10:45:37 CST 2022 ;; MSG SIZE rcvd: 209 使用网站测试 使用该网址，将域名更换为对应的域名：http://ipv6-test.com/validate.php?url=http://www.microsoft.com\n启用ipv6 ipv6特性可以设置在整个系统级别或者单个网卡上，默认启用。\n系统级别 系统级别可以通过内核参数 net.ipv6.conf.all.disable_ipv6 来查看是否启用，如果输出结果为0，说明启用。可以修改该内核参数的值来开启或者关闭系统级别的ipv6特性。\n也可以通过修改grub的内核参数来选择开启或者关闭，在 /etc/default/grub 中GRUB_CMDLINE_LINUX追加如下内容，其中xxxxx代表当前已经有的参数：\n1 GRUB_CMDLINE_LINUX=\u0026#34;xxxxx ipv6.disable=1\u0026#34; 网卡级别 可以通过 ifconfig ethx 命令来查看网卡信息，如果其中包含了inet6，则说明该网卡启用了ipv6特性。\n也可以通过 sysctl net.ipv6.conf.ethx.disable_ipv6 来查看网卡是否启用。其中ethx为对应的网卡名称。\nnginx支持ipv6 默认情况下，nginx不支持ipv6，要支持ipv6，需要在编译的时候指定 \u0026ndash;with-ipv6 选项。\n在编译完成后，通过nginx -V 查看需要包含 \u0026ndash;with-ipv6 选项。\n配置 ipv6 ifconfig eth0 inet6 add 2607:f8b0:4003:c0b::71\n参考资料 Linux有问必答：如何在Linux下禁用IPv6\n","date":"2022-07-14T10:24:00Z","permalink":"/post/ipv6/","title":"ipv6"},{"content":"2012 年，Heroku 创始人 Adam Wiggins 发布十二要素应用宣言，又被称为微服务十二要素。\n内容 基准代码 一个应用一个代码仓库，不要出现一个代码仓库对应多个应用的情况 如果多个应用共享一份代码，那么需要将该代码拆分为独立的类库 依赖 规范：\n应用必须通过配置显式声明出所有的依赖项，即使依赖的工具在所有的操作系统上都存在。比如 python 可以使用 pip 的 requirements.txt 文件声明出所有的依赖包及其版本信息。 在运行时需要通过隔离手段确保应用不会调用未显式声明的依赖项。比如 python 应用可以通过virtualenv 技术来确保隔离性。 优点：\n简化开发者的环境配置，通过构建命令即可安装所有的依赖项。 配置 规范：\n代码和配置单独管理，不要将配置放到代码仓库中管理。 配置传递给应用的方式之一为配置文件，另外一种为环境变量。 后端服务 规范：\n后端服务分为本地服务和第三方服务，对应用而言都是后端服务，不应该区别对待。 构建、发布和运行 规范：\n严格区分从代码到部署的三个阶段：构建编译打包代码；将构建结果和部署需要的配置放到环境中；指定发布版本，在环境中启动应用。 每个发布版本必须对应一个唯一的 id 标识。 反模式：\n不可以直接修改环境中的代码，可能会导致非常难同步会构建步骤。 进程 规范：\n应用的进程需要持久化的数据一定要保存到后端服务中，保持应用进程的无状态。 本地的内存和磁盘可以作为应用的缓存数据。 反对使用基于 session 的粘滞技术（某一个用户的请求通过一致性 hash 等技术请求到同一个后端服务，而后端服务将用户数据缓存在内存中），推荐将用户数据缓存到Redis 等分布式缓存中。很多互联网公司都会采用 session 粘滞技术，都违背了这一原则。 端口绑定 要求应用自己通过监听端口的方式来对外提供服务。\n并发 规范：\n允许通过多进程或者多线程的方式来处理高并发 应用不需要守护进程或者写入 PID 文件，可以借助如 systemd 等工具来实现。 易处理 规范：\n进程启动速度尽可能快，以便于更好的支持弹性伸缩、部署应用。 进程在接收到SIGTERM 信号后需要优雅停止。比如对于 nginx 而言，要等到处理完所有的连接后才可以退出。 进程要能够处理各种异常情况。 开发环境与生产环境等价 核心就只有一点，尽可能保证开发环境与生产环境的一致性。\n日志 规范：\n应用程序将日志直接输出到标准输出，标准输出的内容由日志收集程序消费。 反模式：\n现实中应用程序将日志直接写到了日志文件，并且自己来管理日志文件。 管理进程 规范：\n仅需要执行一次的进程，跟普通的进程一样的管理思路，比如版本、执行环境等。 参考 12要素应用 ","date":"2022-07-01T22:21:10Z","permalink":"/post/12%E8%A6%81%E7%B4%A0%E5%BA%94%E7%94%A8/","title":"12要素应用"},{"content":"CRI为k8s提供的kubelet扩展接口，用来支持多种容器运行时。CRI协议为protobuf格式，kubelet作为客户端，容器运行时作为服务端，两者通过gRpc协议通讯。下面主要解读 CRI的协议定义\nspec解读 RuntimeService 包括了Pod和容器相关的操作。\nPod相关的操作包括：\n启：RunPodSandbox 停：StopPodSandbox 删：RemovePodSandbox 查：PodSandboxStatus、ListPodSandbox、ListPodSandboxStats、PodSandboxStats 容器相关的操作：\n增、启：CreateContainer、StartContainer 停：StopContainer 删：RemoveContainer 查：ListContainers、ContainerStatus、 改：UpdateContainerResources 控：ReopenContainerLog、ExecSync、Exec、Attach、PortForward 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 // Runtime service defines the public APIs for remote container runtimes service RuntimeService { // Version returns the runtime name, runtime version, and runtime API version. rpc Version(VersionRequest) returns (VersionResponse) {} // RunPodSandbox creates and starts a pod-level sandbox. Runtimes must ensure // the sandbox is in the ready state on success. rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {} // StopPodSandbox stops any running process that is part of the sandbox and // reclaims network resources (e.g., IP addresses) allocated to the sandbox. // If there are any running containers in the sandbox, they must be forcibly // terminated. // This call is idempotent, and must not return an error if all relevant // resources have already been reclaimed. kubelet will call StopPodSandbox // at least once before calling RemovePodSandbox. It will also attempt to // reclaim resources eagerly, as soon as a sandbox is not needed. Hence, // multiple StopPodSandbox calls are expected. rpc StopPodSandbox(StopPodSandboxRequest) returns (StopPodSandboxResponse) {} // RemovePodSandbox removes the sandbox. If there are any running containers // in the sandbox, they must be forcibly terminated and removed. // This call is idempotent, and must not return an error if the sandbox has // already been removed. rpc RemovePodSandbox(RemovePodSandboxRequest) returns (RemovePodSandboxResponse) {} // PodSandboxStatus returns the status of the PodSandbox. If the PodSandbox is not // present, returns an error. rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {} // ListPodSandbox returns a list of PodSandboxes. rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {} // CreateContainer creates a new container in specified PodSandbox rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {} // StartContainer starts the container. rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {} // StopContainer stops a running container with a grace period (i.e., timeout). // This call is idempotent, and must not return an error if the container has // already been stopped. // The runtime must forcibly kill the container after the grace period is // reached. rpc StopContainer(StopContainerRequest) returns (StopContainerResponse) {} // RemoveContainer removes the container. If the container is running, the // container must be forcibly removed. // This call is idempotent, and must not return an error if the container has // already been removed. rpc RemoveContainer(RemoveContainerRequest) returns (RemoveContainerResponse) {} // ListContainers lists all containers by filters. rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {} // ContainerStatus returns status of the container. If the container is not // present, returns an error. rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {} // UpdateContainerResources updates ContainerConfig of the container. rpc UpdateContainerResources(UpdateContainerResourcesRequest) returns (UpdateContainerResourcesResponse) {} // ReopenContainerLog asks runtime to reopen the stdout/stderr log file // for the container. This is often called after the log file has been // rotated. If the container is not running, container runtime can choose // to either create a new log file and return nil, or return an error. // Once it returns error, new container log file MUST NOT be created. rpc ReopenContainerLog(ReopenContainerLogRequest) returns (ReopenContainerLogResponse) {} // ExecSync runs a command in a container synchronously. rpc ExecSync(ExecSyncRequest) returns (ExecSyncResponse) {} // Exec prepares a streaming endpoint to execute a command in the container. rpc Exec(ExecRequest) returns (ExecResponse) {} // Attach prepares a streaming endpoint to attach to a running container. rpc Attach(AttachRequest) returns (AttachResponse) {} // PortForward prepares a streaming endpoint to forward ports from a PodSandbox. rpc PortForward(PortForwardRequest) returns (PortForwardResponse) {} // ContainerStats returns stats of the container. If the container does not // exist, the call returns an error. rpc ContainerStats(ContainerStatsRequest) returns (ContainerStatsResponse) {} // ListContainerStats returns stats of all running containers. rpc ListContainerStats(ListContainerStatsRequest) returns (ListContainerStatsResponse) {} // PodSandboxStats returns stats of the pod sandbox. If the pod sandbox does not // exist, the call returns an error. rpc PodSandboxStats(PodSandboxStatsRequest) returns (PodSandboxStatsResponse) {} // ListPodSandboxStats returns stats of the pod sandboxes matching a filter. rpc ListPodSandboxStats(ListPodSandboxStatsRequest) returns (ListPodSandboxStatsResponse) {} // UpdateRuntimeConfig updates the runtime configuration based on the given request. rpc UpdateRuntimeConfig(UpdateRuntimeConfigRequest) returns (UpdateRuntimeConfigResponse) {} // Status returns the status of the runtime. rpc Status(StatusRequest) returns (StatusResponse) {} } ImageService 跟镜像相关的操作，包括了\n增：PullImage 查：ListImages、ImageStatus和ImageFsInfo 删：RemoveImage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // ImageService defines the public APIs for managing images. service ImageService { // ListImages lists existing images. rpc ListImages(ListImagesRequest) returns (ListImagesResponse) {} // ImageStatus returns the status of the image. If the image is not // present, returns a response with ImageStatusResponse.Image set to // nil. rpc ImageStatus(ImageStatusRequest) returns (ImageStatusResponse) {} // PullImage pulls an image with authentication config. rpc PullImage(PullImageRequest) returns (PullImageResponse) {} // RemoveImage removes the image. // This call is idempotent, and must not return an error if the image has // already been removed. rpc RemoveImage(RemoveImageRequest) returns (RemoveImageResponse) {} // ImageFSInfo returns information of the filesystem that is used to store images. rpc ImageFsInfo(ImageFsInfoRequest) returns (ImageFsInfoResponse) {} } CRI CLI CRI 自带了命令行工具 crictl。\nLinux 下安装脚本：\n1 2 3 4 VERSION=\u0026#34;v1.26.0\u0026#34; # check latest version in /releases page wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin rm -f crictl-$VERSION-linux-amd64.tar.gz ","date":"2022-06-29T22:20:52Z","permalink":"/post/container-runtime-interfacecri/","title":"Container Runtime Interface(CRI)"},{"content":"今年六月份入职阿里正好三周年，在阿里入职三年被称为\u0026quot;三年醇\u0026quot;，三年才称之为真正的阿里人。我个人在这三年的时间里也变化了许多，值得反思总结一下。\n阿里缘起 三年前，从上家公司离开。当时可选择的机会还是比较多的，基本上职位比较match的面试都通过了，甚至有公司经过了七面。之所以最终选择了阿里，主要如下两个原因：\n阿里的技术在国内的知名度是响当当的。从技术深度和广度而言，阿里的技术都可圈可点，进入阿里我想多接触一下牛人，技术更进一层。 过往工作经历没有一线大厂，需要丰富下阅历。大厂的经历还是比较关键，不光是为了给自己的职业生涯中增加一份光鲜的经历，更多的是想看大厂是如何运作的，同样一件事情，在不同规模的公司有着不同的处理思路。 主要是基于上述原因，即使不是最好选择，即使做出了部分牺牲，毅然决定加入阿里云，加入了国内最领先的云计算厂商之一。\n工作内容及感受 上家公司为互联网公司，处理的业务为高并发的在线业务，在互联网公司摸爬滚打积累了大量经验。来到阿里云后，加入了混合云，场景变为了专有云，虽然工作内容同样为云原生领域，但本质上为离线业务，跟互联网的在线业务在工作内容上有了较大的区别。互联网业务在线化，仅需要几套在线环境即可满足需求，专有云场景却需要维护大量的测试环境和客户的离线环境，从而导致业务复杂度急剧上升。主要体现在如下方面：\n专有云场景强化对版本的概念。由于大量环境的存在，要想做到统一的管理和运维，必须用版本来强管理，否则维护大量环境将成为灾难。 专有云对自动化部署的要求变高。几套环境的情况下，考虑到ROI，并不需要很高的自动化程度，相反引入自动化会给变更引入额外的复杂性。但如果成百上千套环境，自动化部署成为了强需求，混合云的很多能力都是围绕着自动化部署展开的。 专有云场景对自动化运维的要求变高。运维场景包括了升级、扩缩容、故障处理等，由于是离线的场景，运维的流程变成了：客户 -\u0026gt; 驻场运维 -\u0026gt; 远程运维支持 -\u0026gt; 研发，相比在线业务的支持流程会更长，更难找到bug的真正owner。一方面需要对支持链路上的人员进行相关的培训，更为重要的是需要通过增强平台的自动化运维能力来降低运维成本。 bug在客户环境的问题收敛非常慢。由于有大量的客户环境存在，一旦发现bug，需要在客户现场打hotfix，由于绝大多数环境无法远程，需要人工在客户现场操作，受限于人力、客户变更窗口等限制，hotfix patch的速度很难快起来。从而导致一个bug，会在多个客户现场环境爆发。 墨菲定律凸显，只要是 bug 暴露出来的概率明显增加。由于大量环境的存在，且客户现场运行着多个历史版本，非常难以拉齐版本，哪怕是多年前的旧 bug，在客户现场也极有可能会被发现。 专有云场景下忌架构变更大，架构复杂。专有云场景下经常出现的问题是，一个组件在版本一的架构，还没等一线的运维熟悉架构，在版本二下更换为了另外一套方案，导致运维性特别差。在引入“高大上”的高级特性时也一定要慎重，避免将架构搞复杂，将学习成本变高，专有云的试错成本太高。专有云场景下做好的架构解决方案是，使用尽可能通用简洁的方案来解决复杂的业务问题。 虽然两份工作内容都是云原生领域，但因为业务场景的不同，却对工作的重心产生了质的差异，量变产生了质变。正是因为质变，导致用互联网的“快糙猛”、“短平快”的思路来做需要精心打磨的专有云业务，路子非常容易走偏。\n从工作量上来看，虽然上一份工作也不怎么轻松，但跟当前工作相比，却是小巫见大巫。尤其是前两年的时间，正处在业务从0到1的阶段，基本上一直处于高度运转的状态，每天都会有大量的钉钉消息和大量的会议，不是在救火就是在救火的路上，钉钉消息泛滥到惨不忍睹。能休息的周末也非常少，基本上周末都用来加班干一周攒下来的工作了。下面为相对卷的轻一些的2021年度的钉钉报告，一年光钉钉的会议就有三万多分钟，平均一天有两个小时，还不包含阿里内部的阿里郎会议和会议室中参加的线下会议。\n成长 最大的成长是在工作方法上有了比较大的进步。阿里有一套工作处事的方法论，“搞不定就上升问题”，“责任边界”，“总结思考”等基本上是行走江湖必备技能，在网络上大家也可以看到大量的带有“阿里味”的文章，虽然处理起工作我还是比较喜欢按照自己的方式，但实在搞不定的时候可作为兜底处理思路的，在阿里的江湖里要想风生水起除了靠技术实力，做事方法也极为关键。\n文档能力有了很大提升。在过往的工作经历中，很多事情想好了直接就开干了，反正使用方也不会很多，导致写的文档比较少，毕竟写一篇文档有可能花费时间比写代码的时间还要长。但目前我基本上已经养成了写文档的好习惯，写文档是一个整理思路的好方法，同时也可以将方案分享出去。在阿里的这几年深切体会到文档的重要性，因为业务方特别多，哪怕是一个技术点也会被多个人频繁提问，这时候扔一篇文档是比较好的减负方法。\n另一个比较大的转变是很少失眠了。上一份工作经历大概是处理在线业务的原因，或者是工作不够饱和，经常出现失眠的情况。来阿里之前还比较担心失眠的问题发生，顶不住阿里高强度的工作压力。结果后来发现失眠根本就不存在，当每天都处理N件事情后，超卖非常严重，已经从原来的脑力劳动变成了体力加脑力劳动，根本不存在失眠的情况。到现在为止，即使工作没有刚加入阿里时那么饱和，失眠的情况也比较少了。\n不足 面向钉钉的工作方式没有改掉。刚加入阿里的时候，看到同事的钉钉怎么那么多群，而且都把消息清一色设置为了屏蔽状态，我还特别的不理解。入职没多长时间就渐渐习惯了，每天总会加入几个群，刚开始的时候还会关注群消息，后来发现大部分群都是跟自己无关的。由于会随时被@，所以就把消息提醒给关掉了，否则会特别影响注意力。关掉消息提醒后，消息的查看方式就从push模式变了定期pull模式，基本上每隔几分钟就会打开一次钉钉。虽然大部分情况下都能看到新的消息要处理，但也养成了一个坏习惯，一旦不打开钉钉，总感觉心里不舒服。我不知道这个坏习惯算不算一种医学上的疾病，至少我扣手指的坏习惯在医学上找到了对应的名称“强迫性皮肤剥离症”。这个坏习惯一度尝试修正，比如采用“番茄工作法”的方式，每隔十五分钟再打开一次钉钉，但一直没有强有力的推动下去，总是忍不住打开钉钉看一眼消息。钉钉俨然成为了我工作效率的最大杀手，这句话也不完全对，因为钉钉本来就是我工作的一部分。如果这个问题可以解掉，我的工作效率会上一个大大的台阶。\n能力陷阱问题突出。工作中有大量时间是作为客服来答疑解惑各类问题，前期阶段不管我是不是owner，只要是我力所能及的都尽可能提供帮助，直到我实在扛不住。虽然一方面服务了大量的客户，但另一方面我自己可支配的时间变的非常少。后面渐渐将不是我的问题路由到其他同学，自己才释放出一些时间。但路由器其实不是那么好当的，平台有大量组件，而且不同的组件往往归属不同的owner，有很多问题其实并不能直观看出具体是哪个组件，很多情况下都是多个组件配合的问题，因此还需要充当问题初步定位的工作以及没有owner问题的默认路由。\n自身的学习投入比重偏少。工作时间多的最突出问题就是没有太多的精力来投入到新技术新知识的学习，虽然最近一年有很大的改善，但相比上家公司学习的技术的投入是完全不能比的。更别提，刚工作那几年曾经看过满满一书架的技术书籍了，现在想来真是一种奢侈。\n前瞻性不够。每天都有很多问题在排队的后果就是规划能力不够。阿里还是特别强调抬头看路，但现实中，我却大部分的时间消耗在了低头解问题，思考的时间偏少。长此以往，渐渐发现自己的创造性是不够的。多花时间独立思考，这条路永远是正确的。\n写在最后 自己对未来有很多的期待，这里最想提升的点为：打造个人品牌影响力。个人影响力方面过去不够重视，期望将来能够行动起来。要想做到这一点并非易事，需要很多基础能力，比如需要大量的知识积累，需要大量的工作实践等。期望借着阿里的平台，能够最大化个人影响力，未来可期！\n","date":"2022-06-28T21:58:02Z","permalink":"/post/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%85%A5%E8%81%8C%E4%B8%89%E5%91%A8%E5%B9%B4%E6%84%9F%E6%82%9F/","title":"阿里巴巴入职三周年感悟"},{"content":"OCI（Open Container Initiative）为Linux基金会下的子项目，成立于2015年，由docker、coreos等公司发起，用来制定开放的容器和容器镜像标准，同时docker将其镜像格式和容器运行时runc捐献给了OCI，因此OCI中包含了容器镜像标准和容器运行时标准两部分。\n容器镜像的发展历史 2013年，docker横空出世，docker的核心能力之一为将应用封装为镜像，便于打包、发布。\n2014年，docker将镜像格式定位为docker镜像规范v1版本。\n2016年，docker制定了新的镜像格式规范v2，解决v1版本的部分设计缺陷。\n2017年，OCI发布了OCI镜像规范1.0版本，整个规范以docker镜像规范v2为基础，两者的规范比较类似。\n容器镜像标准 官方定义：OCI Image Format Specification\n容器镜像标准包含了镜像索引（可选）、manifest、层文件和配置文件四部分内容。包含了容器的构建、分发和准备可以运行的镜像。\nskopeo为一个容器镜像和镜像仓库的命令行操作工具，可以使用该工具来学习OCI容器镜像规范，可以直接使用 yum install skopeo -y 进行安装。\n使用如下命令可以将docker的镜像转换为oci格式，并将其保存到/tmp/local_nginx目录下。\n1 skopeo copy docker://nginx oci:/tmp/local_nginx /tmp/local_nginx的目录包含如下结构，参考链接：OCI Image Layout Specification\nblobs 目录：包含了镜像Manifest、镜像层和镜像配置信息，均已sha256值命名的文件形式存储，文件可以为文本文件，也可以为gzip压缩的二进制文件。 oci-layout 文件：定义了当前目录结构的版本信息 index.json 文件：定义了镜像索引信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . ├── blobs │ └── sha256 │ ├── 4a7307612456a7f65365e1da5c3811df49cefa5a2fd68d8e04e093d26a395d60 │ ├── 67e9751bc5aab75bba375f0a24702d70848e5b2bea70de55e50f21ed11feed14 │ ├── 8f46223e4234ce76b244c074e79940b9ee0a01b42050012c8555ebc7ac59469e │ ├── 935cecace2a02d2545e0c19bd52fe9c8c728fbab2323fc274e029f5357cda689 │ ├── b85a868b505ffd0342a37e6a3b1c49f7c71878afe569a807e6238ef08252fcb7 │ ├── efb56228dbd26a7f02dafc74a2ca8f63d5e3bb6df9046a921f7d8174e5318387 │ ├── f4407ba1f103abb9ae05a4b2891c7ebebaecab0c262535fc6659a628db25df44 │ └── fe0ef4c895f5ea450aca17342e481fada37bf2a1ee85d127a4473216c3f672ea ├── index.json └── oci-layout 2 directories, 10 files 镜像索引 非必须部分。如果包含镜像索引，用来解决多架构问题。不同的平台上，可以使用同一个镜像tag，即可以获取到对应平台的镜像。\n镜像索引为 json 格式的文件，查看 index.json 文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:15beb598b14fca498f13a46923de0614a17012abf675ba06e364904d642d8a61\u0026#34;, \u0026#34;size\u0026#34;: 1183 } ] } 镜像索引文件可以包含多种架构。其中digest对应的sha256值指向 blobs/sha256 下的文件，其文件为Manifest文件。\n需要注意的是：docker 可以使用 docker manifest命令来查看镜像的manifest信息，但格式并非为OCI Manifest格式，而更类似于OCI index的信息，下面的命令中，可以看到rancher镜像为多镜像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ docker manifest inspect rancher/rancher { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.list.v2+json\u0026#34;, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 4732, \u0026#34;digest\u0026#34;: \u0026#34;sha256:b8f1fdb8228d32ae5fc6f240503cd8e22b214fcfd4ad2a8a0b03274f3ead4e95\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 4731, \u0026#34;digest\u0026#34;: \u0026#34;sha256:ae0fa74e8dce9b72bdc6611815deb16bbddc8fe0a555052ccc8127fdc1b76980\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;arm64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 4519, \u0026#34;digest\u0026#34;: \u0026#34;sha256:94c03afba43e81885c3cd2f5065032d1b7f8f540860fcc1fce1bbd7f1068d3db\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;s390x\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } } ] } Manifest 参考：OCI Image Manifest Specification\nManifest为json格式的描述文件，包含了如下三个用途：\n每一个镜像都有一个唯一的 id 标识 对于同一个镜像tag，可以支持多架构镜像 可以直接转换为OCI的运行时规范 查看 blobs/sha256/15beb598b14fca498f13a46923de0614a17012abf675ba06e364904d642d8a61 内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.manifest.v1+json\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.config.v1+json\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:67e9751bc5aab75bba375f0a24702d70848e5b2bea70de55e50f21ed11feed14\u0026#34;, \u0026#34;size\u0026#34;: 6567 }, \u0026#34;layers\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:b85a868b505ffd0342a37e6a3b1c49f7c71878afe569a807e6238ef08252fcb7\u0026#34;, \u0026#34;size\u0026#34;: 31379408 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:f4407ba1f103abb9ae05a4b2891c7ebebaecab0c262535fc6659a628db25df44\u0026#34;, \u0026#34;size\u0026#34;: 25354178 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:4a7307612456a7f65365e1da5c3811df49cefa5a2fd68d8e04e093d26a395d60\u0026#34;, \u0026#34;size\u0026#34;: 603 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:935cecace2a02d2545e0c19bd52fe9c8c728fbab2323fc274e029f5357cda689\u0026#34;, \u0026#34;size\u0026#34;: 893 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:8f46223e4234ce76b244c074e79940b9ee0a01b42050012c8555ebc7ac59469e\u0026#34;, \u0026#34;size\u0026#34;: 666 }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.oci.image.layer.v1.tar+gzip\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:fe0ef4c895f5ea450aca17342e481fada37bf2a1ee85d127a4473216c3f672ea\u0026#34;, \u0026#34;size\u0026#34;: 1394 } ] } 包含了config 和 layers 两部分信息，其中 config 信息为运行镜像的配置，layers 为镜像中的层信息，其中gzip说明镜像的层为gzip压缩格式，每个层一个压缩文件。\n镜像配置 参考：OCI Image Configuration\n通过 manifest 中的config信息，可以找到镜像的配置信息 blobs/sha256/67e9751bc5aab75bba375f0a24702d70848e5b2bea70de55e50f21ed11feed14：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.820503805Z\u0026#34;, \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;ExposedPorts\u0026#34;: { \u0026#34;80/tcp\u0026#34;: {} }, \u0026#34;Env\u0026#34;: [ \u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;NGINX_VERSION=1.23.0\u0026#34;, \u0026#34;NJS_VERSION=0.7.5\u0026#34;, \u0026#34;PKG_RELEASE=1~bullseye\u0026#34; ], \u0026#34;Entrypoint\u0026#34;: [ \u0026#34;/docker-entrypoint.sh\u0026#34; ], \u0026#34;Cmd\u0026#34;: [ \u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34; ], \u0026#34;Labels\u0026#34;: { \u0026#34;maintainer\u0026#34;: \u0026#34;NGINX Docker Maintainers \u0026lt;docker-maint@nginx.com\u0026gt;\u0026#34; }, \u0026#34;StopSignal\u0026#34;: \u0026#34;SIGQUIT\u0026#34; }, \u0026#34;rootfs\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;layers\u0026#34;, \u0026#34;diff_ids\u0026#34;: [ \u0026#34;sha256:08249ce7456a1c0613eafe868aed936a284ed9f1d6144f7d2d08c514974a2af9\u0026#34;, \u0026#34;sha256:d5b40e80384bb94d01a8d2d8fb2db1328990e7088640132c33d3f691dd8a88ee\u0026#34;, \u0026#34;sha256:b2f82de68e0d9246de01fa8283876427af5d6f3fe21c4bb04785892d5d071aef\u0026#34;, \u0026#34;sha256:41451f050aa883f9102df03821485fc2e27611da05689c0ba25f69dcda308988\u0026#34;, \u0026#34;sha256:44193d3f4ea2bae7a5ae5983f2562f551618b787751a6abfb732b6d17393bb88\u0026#34;, \u0026#34;sha256:e7344f8a29a34b4861faf6adcf072afb26fadf6096756f0e3fc4c289cdefb7c2\u0026#34; ] }, \u0026#34;history\u0026#34;: [ { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T00:20:27.020952309Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ADD file:8adbbab04d6f84cd83b5f4205b89b0acb7ecbf27a1bb2dc181d0a629479039fe in / \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T00:20:27.337378745Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) CMD [\\\u0026#34;bash\\\u0026#34;]\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:05.737870066Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) LABEL maintainer=NGINX Docker Maintainers \u0026lt;docker-maint@nginx.com\u0026gt;\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:05.834940798Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV NGINX_VERSION=1.23.0\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:05.931909571Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV NJS_VERSION=0.7.5\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:06.026686816Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENV PKG_RELEASE=1~bullseye\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:23.901038357Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c set -x \u0026amp;\u0026amp; addgroup --system --gid 101 nginx \u0026amp;\u0026amp; adduser --system --disabled-login --ingroup nginx --no-create-home --home /nonexistent --gecos \\\u0026#34;nginx user\\\u0026#34; --shell /bin/false --uid 101 nginx \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get install --no-install-recommends --no-install-suggests -y gnupg1 ca-certificates \u0026amp;\u0026amp; NGINX_GPGKEY=573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62; found=\u0026#39;\u0026#39;; for server in hkp://keyserver.ubuntu.com:80 pgp.mit.edu ; do echo \\\u0026#34;Fetching GPG key $NGINX_GPGKEY from $server\\\u0026#34;; apt-key adv --keyserver \\\u0026#34;$server\\\u0026#34; --keyserver-options timeout=10 --recv-keys \\\u0026#34;$NGINX_GPGKEY\\\u0026#34; \u0026amp;\u0026amp; found=yes \u0026amp;\u0026amp; break; done; test -z \\\u0026#34;$found\\\u0026#34; \u0026amp;\u0026amp; echo \u0026gt;\u0026amp;2 \\\u0026#34;error: failed to fetch GPG key $NGINX_GPGKEY\\\u0026#34; \u0026amp;\u0026amp; exit 1; apt-get remove --purge --auto-remove -y gnupg1 \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* \u0026amp;\u0026amp; dpkgArch=\\\u0026#34;$(dpkg --print-architecture)\\\u0026#34; \u0026amp;\u0026amp; nginxPackages=\\\u0026#34; nginx=${NGINX_VERSION}-${PKG_RELEASE} nginx-module-xslt=${NGINX_VERSION}-${PKG_RELEASE} nginx-module-geoip=${NGINX_VERSION}-${PKG_RELEASE} nginx-module-image-filter=${NGINX_VERSION}-${PKG_RELEASE} nginx-module-njs=${NGINX_VERSION}+${NJS_VERSION}-${PKG_RELEASE} \\\u0026#34; \u0026amp;\u0026amp; case \\\u0026#34;$dpkgArch\\\u0026#34; in amd64|arm64) echo \\\u0026#34;deb https://nginx.org/packages/mainline/debian/ bullseye nginx\\\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/nginx.list \u0026amp;\u0026amp; apt-get update ;; *) echo \\\u0026#34;deb-src https://nginx.org/packages/mainline/debian/ bullseye nginx\\\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/nginx.list \u0026amp;\u0026amp; tempDir=\\\u0026#34;$(mktemp -d)\\\u0026#34; \u0026amp;\u0026amp; chmod 777 \\\u0026#34;$tempDir\\\u0026#34; \u0026amp;\u0026amp; savedAptMark=\\\u0026#34;$(apt-mark showmanual)\\\u0026#34; \u0026amp;\u0026amp; apt-get update \u0026amp;\u0026amp; apt-get build-dep -y $nginxPackages \u0026amp;\u0026amp; ( cd \\\u0026#34;$tempDir\\\u0026#34; \u0026amp;\u0026amp; DEB_BUILD_OPTIONS=\\\u0026#34;nocheck parallel=$(nproc)\\\u0026#34; apt-get source --compile $nginxPackages ) \u0026amp;\u0026amp; apt-mark showmanual | xargs apt-mark auto \u0026gt; /dev/null \u0026amp;\u0026amp; { [ -z \\\u0026#34;$savedAptMark\\\u0026#34; ] || apt-mark manual $savedAptMark; } \u0026amp;\u0026amp; ls -lAFh \\\u0026#34;$tempDir\\\u0026#34; \u0026amp;\u0026amp; ( cd \\\u0026#34;$tempDir\\\u0026#34; \u0026amp;\u0026amp; dpkg-scanpackages . \u0026gt; Packages ) \u0026amp;\u0026amp; grep \u0026#39;^Package: \u0026#39; \\\u0026#34;$tempDir/Packages\\\u0026#34; \u0026amp;\u0026amp; echo \\\u0026#34;deb [ trusted=yes ] file://$tempDir ./\\\u0026#34; \u0026gt; /etc/apt/sources.list.d/temp.list \u0026amp;\u0026amp; apt-get -o Acquire::GzipIndexes=false update ;; esac \u0026amp;\u0026amp; apt-get install --no-install-recommends --no-install-suggests -y $nginxPackages gettext-base curl \u0026amp;\u0026amp; apt-get remove --purge --auto-remove -y \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* /etc/apt/sources.list.d/nginx.list \u0026amp;\u0026amp; if [ -n \\\u0026#34;$tempDir\\\u0026#34; ]; then apt-get purge -y --auto-remove \u0026amp;\u0026amp; rm -rf \\\u0026#34;$tempDir\\\u0026#34; /etc/apt/sources.list.d/temp.list; fi \u0026amp;\u0026amp; ln -sf /dev/stdout /var/log/nginx/access.log \u0026amp;\u0026amp; ln -sf /dev/stderr /var/log/nginx/error.log \u0026amp;\u0026amp; mkdir /docker-entrypoint.d\u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.128160562Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) COPY file:65504f71f5855ca017fb64d502ce873a31b2e0decd75297a8fb0a287f97acf92 in / \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.233980553Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) COPY file:0b866ff3fc1ef5b03c4e6c8c513ae014f691fb05d530257dfffd07035c1b75da in /docker-entrypoint.d \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.337299368Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) COPY file:0fd5fca330dcd6a7de297435e32af634f29f7132ed0550d342cad9fd20158258 in /docker-entrypoint.d \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.441125652Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) COPY file:09a214a3e07c919af2fb2d7c749ccbc446b8c10eb217366e5a65640ee9edcc25 in /docker-entrypoint.d \u0026#34; }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.534829205Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) ENTRYPOINT [\\\u0026#34;/docker-entrypoint.sh\\\u0026#34;]\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.627520512Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) EXPOSE 80\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.724935944Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) STOPSIGNAL SIGQUIT\u0026#34;, \u0026#34;empty_layer\u0026#34;: true }, { \u0026#34;created\u0026#34;: \u0026#34;2022-06-23T04:13:24.820503805Z\u0026#34;, \u0026#34;created_by\u0026#34;: \u0026#34;/bin/sh -c #(nop) CMD [\\\u0026#34;nginx\\\u0026#34; \\\u0026#34;-g\\\u0026#34; \\\u0026#34;daemon off;\\\u0026#34;]\u0026#34;, \u0026#34;empty_layer\u0026#34;: true } ] } 其中包含了如下几个关键信息：\nconfig：运行镜像的参数，比如entrypoint、labels等，跟通过 docker inspect 命令看到的信息比较类似。 rootfs：镜像的层信息 history：镜像的历史构建信息，如果empty_layer的值为true，说明未产生新的层 镜像层 镜像层同样存在于 blobs/sha256 目录下，且以压缩格式存储，一个层一个压缩文件。manifests文件中的 application/vnd.oci.image.layer.v1.tar+gzip 说明镜像层的压缩格式为gzip。\n容器运行时标准 用来定义容器的配置、运行环境和声明周期。runc为容器运行时的官方实现，其主要代码来源为docker的容器运行时，kara-containers也有对应的OCI实现。\n参考文档：opencontainers/runtime-spec\n容器配置 定义在config.json文件中，定义了创建容器的字段。由于runc更具体的操作系统环境有关，其中部分的规范是跟具体操作系统有关的。执行runc spec可以获取到默认的config.json文件，文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 { \u0026#34;ociVersion\u0026#34;: \u0026#34;1.0.2-dev\u0026#34;, \u0026#34;process\u0026#34;: { \u0026#34;terminal\u0026#34;: true, \u0026#34;user\u0026#34;: { \u0026#34;uid\u0026#34;: 0, \u0026#34;gid\u0026#34;: 0 }, \u0026#34;args\u0026#34;: [ \u0026#34;sh\u0026#34; ], \u0026#34;env\u0026#34;: [ \u0026#34;PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34;, \u0026#34;TERM=xterm\u0026#34; ], \u0026#34;cwd\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;bounding\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ], \u0026#34;effective\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ], \u0026#34;permitted\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ], \u0026#34;ambient\u0026#34;: [ \u0026#34;CAP_AUDIT_WRITE\u0026#34;, \u0026#34;CAP_KILL\u0026#34;, \u0026#34;CAP_NET_BIND_SERVICE\u0026#34; ] }, \u0026#34;rlimits\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;RLIMIT_NOFILE\u0026#34;, \u0026#34;hard\u0026#34;: 1024, \u0026#34;soft\u0026#34;: 1024 } ], \u0026#34;noNewPrivileges\u0026#34;: true }, \u0026#34;root\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;rootfs\u0026#34;, \u0026#34;readonly\u0026#34;: true }, \u0026#34;hostname\u0026#34;: \u0026#34;runc\u0026#34;, \u0026#34;mounts\u0026#34;: [ { \u0026#34;destination\u0026#34;: \u0026#34;/proc\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;proc\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;proc\u0026#34; }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;tmpfs\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;tmpfs\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;strictatime\u0026#34;, \u0026#34;mode=755\u0026#34;, \u0026#34;size=65536k\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev/pts\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;devpts\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;devpts\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;newinstance\u0026#34;, \u0026#34;ptmxmode=0666\u0026#34;, \u0026#34;mode=0620\u0026#34;, \u0026#34;gid=5\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev/shm\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;tmpfs\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;shm\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34;, \u0026#34;mode=1777\u0026#34;, \u0026#34;size=65536k\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/dev/mqueue\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;mqueue\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;mqueue\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/sys\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;sysfs\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;sysfs\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34;, \u0026#34;ro\u0026#34; ] }, { \u0026#34;destination\u0026#34;: \u0026#34;/sys/fs/cgroup\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cgroup\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;cgroup\u0026#34;, \u0026#34;options\u0026#34;: [ \u0026#34;nosuid\u0026#34;, \u0026#34;noexec\u0026#34;, \u0026#34;nodev\u0026#34;, \u0026#34;relatime\u0026#34;, \u0026#34;ro\u0026#34; ] } ], \u0026#34;linux\u0026#34;: { \u0026#34;resources\u0026#34;: { \u0026#34;devices\u0026#34;: [ { \u0026#34;allow\u0026#34;: false, \u0026#34;access\u0026#34;: \u0026#34;rwm\u0026#34; } ] }, \u0026#34;namespaces\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;pid\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;network\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;ipc\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;uts\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;mount\u0026#34; } ], \u0026#34;maskedPaths\u0026#34;: [ \u0026#34;/proc/acpi\u0026#34;, \u0026#34;/proc/asound\u0026#34;, \u0026#34;/proc/kcore\u0026#34;, \u0026#34;/proc/keys\u0026#34;, \u0026#34;/proc/latency_stats\u0026#34;, \u0026#34;/proc/timer_list\u0026#34;, \u0026#34;/proc/timer_stats\u0026#34;, \u0026#34;/proc/sched_debug\u0026#34;, \u0026#34;/sys/firmware\u0026#34;, \u0026#34;/proc/scsi\u0026#34; ], \u0026#34;readonlyPaths\u0026#34;: [ \u0026#34;/proc/bus\u0026#34;, \u0026#34;/proc/fs\u0026#34;, \u0026#34;/proc/irq\u0026#34;, \u0026#34;/proc/sys\u0026#34;, \u0026#34;/proc/sysrq-trigger\u0026#34; ] } } 运行时和生命周期 在config.json文件中可以声明跟容器生命周期相关的部分，比如prestart、poststop等。\n定义了很多子命令，比如状态查询的state \u0026lt;container-id\u0026gt;，删除容器的delete \u0026lt;container-id\u0026gt;等，这些子命令runc部分均有实现。通过 runc state mycontainerid 来查看的输出结果如下：\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;ociVersion\u0026#34;: \u0026#34;1.0.2-dev\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;mycontainerid\u0026#34;, \u0026#34;pid\u0026#34;: 40805, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;bundle\u0026#34;: \u0026#34;/mycontainer\u0026#34;, \u0026#34;rootfs\u0026#34;: \u0026#34;/mycontainer/rootfs\u0026#34;, \u0026#34;created\u0026#34;: \u0026#34;2022-06-29T13:51:54.795617419Z\u0026#34;, \u0026#34;owner\u0026#34;: \u0026#34;\u0026#34; } 镜像分发规范 pull pull manifest 接口定义：GET /v2/\u0026lt;name\u0026gt;/manifests/\u0026lt;reference\u0026gt;\n\u0026lt;name\u0026gt;： 镜像的 namespace。 \u0026lt;reference\u0026gt;：镜像的 tag 或者摘要信息。\npull bolb 接口定义：GET /v2/\u0026lt;name\u0026gt;/blobs/\u0026lt;digest\u0026gt;\npush POST /v2/\u0026lt;name\u0026gt;/blobs/uploads/?digest=\u0026lt;digest\u0026gt; PUT /v2/\u0026lt;name\u0026gt;/manifests/\u0026lt;reference\u0026gt;\nlist tag 接口定义：GET /v2/\u0026lt;name\u0026gt;/tags/list\n返回格式如下：\n1 2 3 4 5 6 7 8 { \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;name\u0026gt;\u0026#34;, \u0026#34;tags\u0026#34;: [ \u0026#34;\u0026lt;tag1\u0026gt;\u0026#34;, \u0026#34;\u0026lt;tag2\u0026gt;\u0026#34;, \u0026#34;\u0026lt;tag3\u0026gt;\u0026#34; ] } list references 接口定义： GET /v2/\u0026lt;name\u0026gt;/referrers/\u0026lt;digest\u0026gt;\ndelete tag 接口定义： DELETE /v2/\u0026lt;name\u0026gt;/manifests/\u0026lt;tag\u0026gt;\ndelete manifest 接口定义： DELETE /v2/\u0026lt;name\u0026gt;/manifests/\u0026lt;digest\u0026gt;\ndelete blobs 接口定义： DELETE /v2/\u0026lt;name\u0026gt;/blobs/\u0026lt;digest\u0026gt;\nk8s支持情况 K8s可以通过 pod 的 spec.runtimeClassName 来指定 oci runtime 的实现方式。\n参考 Understanding OCI Image Spec （简单明了，推荐优先阅读） https://opencontainers.org/ 云原生制品那些事(2)：OCI 镜像规范 skopeo 解读 OCI Image Spec ","date":"2022-06-23T22:25:29Z","permalink":"/post/oci/","title":"OCI"},{"content":"多架构镜像 查看镜像的多架构信息 可以使用 docker manifest inspect $image 命令来查看，manifest为docker的体验特性，在Linux系统下开启，需要在本地创建 ~/.docker/config.json 文件，内容如下：\n1 2 3 { \u0026#34;experimental\u0026#34;: \u0026#34;enabled\u0026#34; } 最好的方式为开启docker daemon的特性，修改 /etc/docker/daemon.json 文件：\n1 2 3 { \u0026#34;experimental\u0026#34;: true } 例如执行 docker manifest inspect golang:alpine 可以看到golang 官方的docker镜像包含了多架构信息，每个架构下会对应一个sha256值。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 { \u0026#34;schemaVersion\u0026#34;: 2, \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.list.v2+json\u0026#34;, \u0026#34;manifests\u0026#34;: [ { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1365, \u0026#34;digest\u0026#34;: \u0026#34;sha256:725f8fd50191209a4c4a00def1d93c4193c4d0a1c2900139daf8f742480f3367\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;amd64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1365, \u0026#34;digest\u0026#34;: \u0026#34;sha256:5adcff3a3e757841a6c7b07f1986b2a36cb0afaf47025e78bb17358eda2d541a\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;arm\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;variant\u0026#34;: \u0026#34;v6\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1365, \u0026#34;digest\u0026#34;: \u0026#34;sha256:84a1e4174b934fbf8f1dfe9f7353a5be449096b6f2273d6af5a364ffd6bf8f15\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;arm\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;variant\u0026#34;: \u0026#34;v7\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1365, \u0026#34;digest\u0026#34;: \u0026#34;sha256:86cfea5046e196f5061324c93f25ef05e1df58ba96721e0c0b42cc6e0cf22e49\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;arm64\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34;, \u0026#34;variant\u0026#34;: \u0026#34;v8\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1365, \u0026#34;digest\u0026#34;: \u0026#34;sha256:5ad072476cb8b51dddaf4142789f1528c7d48a3a0c31941a5ce21177c8e47259\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;386\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1365, \u0026#34;digest\u0026#34;: \u0026#34;sha256:fca6cbe2f1fb9095eac2669c0be58b482135f9cf7196d51ac7338ea3e7c556c7\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;ppc64le\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } }, { \u0026#34;mediaType\u0026#34;: \u0026#34;application/vnd.docker.distribution.manifest.v2+json\u0026#34;, \u0026#34;size\u0026#34;: 1365, \u0026#34;digest\u0026#34;: \u0026#34;sha256:3f7ac24ca4b3ce61b51439cb59b57a8151ba60bd73a0e33cc06020dda6b692cb\u0026#34;, \u0026#34;platform\u0026#34;: { \u0026#34;architecture\u0026#34;: \u0026#34;s390x\u0026#34;, \u0026#34;os\u0026#34;: \u0026#34;linux\u0026#34; } } ] } gcr.io 可以在 console 上直接看到信息，比如: nginx镜像\n多架构镜像的构建 可以使用docker buildx命令，比如 docker buildx build -t \u0026lt;image-name\u0026gt; --platform=linux/arm64,linux/amd64 . --push 可以同时构建出arm64和amd64的镜像。\n查看镜像的构建历史 可以使用 docker history --no-trunc ${image} 来查看镜像的每层构建命令\n通过代理拉取镜像 创建或者修改/etc/docker/daemon.json文件，文件内容如下：\n1 2 3 4 5 6 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34; ] } 重启docker后通过docker info命令查看输出结果：\n1 2 3 4 5 $ docker info Registry Mirrors: https://hub-mirror.c.163.com/ https://mirror.baidubce.com/ Live Restore Enabled: false 常用基础镜像 nicolaka/netshoot：包含了丰富的网络命令，排查网络问题非常方便 参考资料 Image Manifest V 2, Schema 2 ","date":"2022-06-16T22:03:09Z","permalink":"/post/docker%E9%95%9C%E5%83%8F/","title":"docker镜像"},{"content":"/etc/resolv.conf文件为Linux主机的DNS配置文件，在 Linux 主机上可以执行 man resolv.conf 查看帮助信息。\n配置文件说明 整个配置文件分为了4个部分：nameserver、search、sortlist和options\nnameserver 用来配置DNS服务器地址。支持ipv4和ipv6地址。如果要配置多个DNS服务器，可以增加多条配置，域名按照DNS服务器配置的顺序来解析。配置多条的例子如下：\n1 2 nameserver 100.100.2.136 nameserver 100.100.2.138 search 在默认情况下，如果要查找的域名中不包含domain信息（即不包含.字符），此时会以当前hostname的domain信息来进行查找。可以通过search字段来修改该行为，即可以通过在域名的后面增加配置的后缀来进行查找。\n如果存在多条search记录，则以最后一条search记录为准。\n比如k8s的pod中的search域格式如下，如果要查找defaultnamespace下的service的svc1，此时系统会自动追加 svc1.default.svc.cluster.local来进行查找：\n1 search default.svc.cluster.local svc.cluster.local cluster.local sortlist 该字段极少场景下会用到。在做域名解析时，如果返回的A记录为多条，可以对结果进行排序，排序的依据为当前的字段配置。\n1 sortlist 130.155.160.0/255.255.240.0 130.155.0.0 options 该配置必须加载一行中，格式如下，其中option部分可以为多个，多个之间用空格分割：\n1 options option ... inet6: 应用程序在执行系统调用 gethostbyname 时，会优先执行 AAAA 记录的查询。如果查询不到 ipv6 地址，再去查询 ipv4 地址。 实验 实验一 修改/resolv.conf配置文件的内容如下：\n1 nameserver 8.8.8.8 可以看到该nameserver是生效的，但是访问map域名是不生效的，因为没有map这个域名.\n1 2 3 4 5 6 [vagrant@localhost ~]$ ping map.baidu.com PING map.n.shifen.com (119.75.222.71) 56(84) bytes of data. 64 bytes from 119.75.222.71: icmp_seq=1 ttl=63 time=4.22 ms [vagrant@localhost ~]$ ping map ping: unknown host map 实验二 修改文件内容如下：\n1 2 3 nameserver 8.8.8.8 search baidu.com google.com 此时可以ping通map域名，解析到了跟map.baidu.com相同的域名.如果map.baidu.com的域名没有解析到，会继续解析map.google.com的域名。\n1 2 3 [vagrant@localhost ~]$ ping map PING map.n.shifen.com (112.80.248.48) 56(84) bytes of data. 64 bytes from 112.80.248.48: icmp_seq=1 ttl=63 time=28.6 ms domain的作用跟search类似，作为search的默认值，因为search可以使用多个域。\n参考资料 resolv.conf(5) - Linux manual page (man7.org) ","date":"2022-06-16T21:57:43Z","permalink":"/post/etc/resolv.conf%E6%96%87%E4%BB%B6/","title":"/etc/resolv.conf文件"},{"content":"什么是CNCF Landscape？ CNCF（Cloud Native Computing Foundation）为云原生计算基金会的英文缩写，致力于云原生技术的普及和推广。\nCNCF Landscape为CNCF的一个重要项目，为了帮助企业和个人开发者快速了解云原生技术的全貌，该项目维护在GitHub。CNCF Landscape的最重要的两个产出物为路线图和愿景图。\nCNCF Landscape路线图 路线图的目的是指导用户使用云原生技术的路径和开源项目。其中包括了10个步骤。\nCNCF Landscape愿景图 将云原生技术进行了分层分类，可以非常清晰的将云原生技术展示给用户。类似于软件架构中，又称“方块图”、“砌砖图”。 精简版图形如下：\n供给层（provisioning） 愿景图的第一层，是云原生平台和云原生应用的基础。\n自动化配置 关键词：IaC（基础设施即代码）、声明式配置\n用来加速计算机资源的创建和配置，资源包括虚拟机、网络、防火墙规则、负载均衡等。只需要点击下按钮，即可自动化创建对应的资源，用来降低人工的维护。\n典型工具代表：Terraform、Chef、Ansible、Puppet\nCNCF项目列表：\nCNCF项目 项目阶段 项目介绍 Akri 沙箱 CDK for Kubernetes 沙箱 Cloud Custodian 沙箱 KubeDL 沙箱 KubeEdge 孵化 华为开源的边缘计算项目 Metal3-io 沙箱 OpenYurt 沙箱 阿里云开源的边缘计算项目 SuperEdge 沙箱 Tinkerbell 沙箱 容器镜像仓库 用来存储和拉取容器镜像，最简单的项目如docker官方的单机版的docker registry，所有的公有云厂商也都有自己的工具。\nCNCF项目包括：\nCNCF项目 项目阶段 项目介绍 Harbor 毕业 由vmware打造的镜像仓库，提供了很多企业级的特性，广受企业内部使用。 Dragonfly 孵化 阿里开源的基于P2P技术做镜像分发项目，在大规模集群的场景下效果会比较明显。 安全合规 关键词：镜像扫描、镜像签名、策略管理、审计、证书管理、代码扫描、漏洞扫描、网络层安全\n用来监控、增强应用和平台的安全性，包括从容器到k8s的运行时环境均会涉及到。\nCNCF项目包括：\nCNCF项目 项目阶段 项目介绍 cert-manager 沙箱 证书签发工具，部署在k8s之上，通过抽象CRD Certificate来管理证书，也可以用来管理Ingress的证书 Confidential Containers 沙箱 Curiefense 沙箱 Dex 沙箱 Falco 孵化 in-toto 孵化 Keylime 沙箱 Kyverno 沙箱 基于k8s的策略引擎工具，通过抽象CRD ClusterPolicy的方式来声明策略，在运行时通过webhook的技术来执行策略。相比于opa \u0026amp; gatekeeper，更加k8s化，但却没有编程语言的灵活性。 Notary 孵化 Open Policy Agent (OPA) 毕业 基于Rego语言的策略引擎，编程能力非常强大 Parsec 沙箱 [The Update Framework (TUF)](The Update Framework) 毕业 秘钥和身份管理 关键词：秘钥、身份、Secret、访问控制、认证、授权\nCNCF项目包括：\nCNCF项目 项目阶段 项目介绍 Athenz 沙箱 SPIFFE 孵化 SPIRE 孵化 Teller 沙箱 运行时层（Runtime） 云原生存储 关键词：PV、CSI、备份和恢复\n云原生架构下，存储类的工具主要涉及到如下几个方面：\n为容器提供云原生的存储。由于容器具有灵活、弹性的特点，云原生的存储相比传统存储会更复杂。 需要有统一的接口。这块基本都会使用k8s的CSI接口。另外，minio提供了S3协议的接口。 备份和还原功能。例如Velero可以用来备份k8s本身和容器使用到的存储。 CNCF项目包括：\nCNCF项目 项目阶段 项目介绍 CubeFS 孵化 K8up 沙箱 Longhorn 孵化 OpenEBS 沙箱 ORAS 沙箱 Piraeus Datastore 沙箱 Rook 毕业 Vineyard 沙箱 容器运行时 容器运行时的三个主要特征：标准化、安全、隔离性。Containerd和CRI-O为容器运行时的标准实现方案，业界类似KataContainer的方式为将VM作为容器运行时，gVisor方案则在OS和容器中间增加了额外的安全层。\n发展趋势：\n基于 MicroVM 的安全容器技术，通过虚拟化和容器技术的结合，可以提升更高的安全性。比如 KataContainer。 操作系统的虚拟化程度进一步增加。Linux 4.5 版本的 CGroup V2 技术逐渐成熟，进一步提升了容器的隔离能力。Docker 提供了 rootless 技术，可以以非 root 用户运行，提升了容器的安全性。 WebAssembly技术作为跨平台的容器技术可能会作为新的挑战者出现。 CNCF项目 项目阶段 项目介绍 Containerd 毕业 CRI-O 孵化 k8s的CRI的轻量级实现，支持runc和kata作为容器运行时 Inclavare Containers 沙箱 rkt 归档 CoreOS公司主导研发的容器引擎，昔日的Docker竞争对手，目前已经没落，已经被CNCF归档 WasmEdge Runtime 沙箱 云原生网络 关键词：SDN、CNI、Overlay网络\n云原生网络具体指的是容器网络，k8s定义了CNI的网络规范，开源项目只需要实现CNI的网络规范即可，社区有非常多的CNI项目可以选择，比如Calico、Flannel、Weave Net等。网络分为Underlay和Overlay网络，其中容器网络为SDN网络，以overlay网络居多。\nCNCF项目 项目阶段 项目介绍 Antrea 沙箱 Cilium 孵化 CNI-Genie 沙箱 华为云开源的多网络平面的项目，该项目并非具体的网络插件实现，而是k8s的CNI和具体网络插件实现的中间层，可以实现同一个节点上有多种网络插件的实现，支持同一个pod中有多个网卡。 CNI（Container Network Interface） 孵化 k8s的容器网络规范，指的一提的是在[plugins](containernetworking/plugins: Some reference and example networking plugins, maintained by the CNI team. (github.com))项目中，提供了很多k8s内置的简单容器插件，比如macvlan、bandwidth等 Kube-OVN 沙箱 Network Service Mesh 沙箱 Submariner 沙箱 Rancher公司开源的项目，用来解决k8s的多集群场景下的跨集群互通问题 编排和管理层（Orchestration \u0026amp; Management） 调度和编排 在单机的系统中，操作系统会来调度系统中运行的所有的进程，允许某个时间点调度某个进程到某个cpu上面。在集群的环境中，同样需要调度容器在某个时间点运行在某台主机的某个cpu上。\n在云原生社区基本上形成以k8s为生态的调度和编排，主要的发展方向为：\n扩展k8s自身的功能。 k8s的多集群方向。 CNCF项目 项目阶段 项目介绍 Crossplane 孵化 Fluid 沙箱 Karmada 沙箱 华为云开源的k8s多集群管理项目，用来管理多个k8s集群。自己实现了一套完整的apiserver、scheduler、controller-manager，用来多k8s集群的调度。 kube-rs 沙箱 Kubernetes 毕业 CNCF的最重要项目，在容器编排领域具有绝对的统治地位。俗称“现代数据中心的操作系统” Open Cluster Management 沙箱 Redhat主导的k8s多集群项目 Volcano 孵化 华为云开源的基于k8s的容器批量计算平台，常用于大数据、AI领域。k8s默认的Job设计较为简单，无法满足很多批处理场景。Volcano通过CRD扩展的方式定义了Queue、PodGroup、VolcanoJob等实现对批处理作业的抽象，并通过调度器扩展的方式来大幅提升pod的调度效率。 wasmCloud 沙箱 调协和服务发现 该领域主要包含两类工具：\n服务发现引擎。 域名解析服务。比如CoreDNS。 CNCF项目 项目阶段 项目介绍 CoreDNS 毕业 etcd 毕业 k8gb 沙箱 远程过程调用（RPC） 用于进程间通讯的框架，主要解决的问题：\n提供了框架，使开发者编码更简单。 提供了结构化的通讯协议。 CNCF项目 项目阶段 项目介绍 gRPC 孵化 业界使用较为广泛的RPC框架 服务代理 通常又称为负载均衡，从协议上来划分，可以分为四层负载均衡和七层负载均衡。\nCNCF项目 项目阶段 项目介绍 BFE 沙箱 Contour 孵化 Envoy 毕业 OpenELB 沙箱 API网关 相比于七层负载均衡，API网关还提供了更多高级特性，比如认证、鉴权、限流等。\nCNCF项目 项目阶段 项目介绍 Emissary-Ingress 孵化 服务网格 值得注意的是，业内最为流行的istio项目并不在该范围内。\nCNCF项目 项目阶段 项目介绍 Kuma 沙箱 Linkerd 毕业 流行的服务网格项目 Meshery 沙箱 Open Service Mesh 沙箱 Service Mesh Interface 沙箱 Service Mesh Performance 沙箱 应用定义和应用部署 数据库 CNCF项目 项目阶段 项目介绍 SchemaHero 沙箱 TiKV 毕业 国内公司PingCAP开源的分布式kv数据库 Vitess 毕业 用来扩展mysql集群的数据库解决方案，突破单mysql集群的性能瓶颈 流式计算和消息 CNCF项目 项目阶段 项目介绍 CloudEvents 孵化 仅描述了事件的数据规范，并非具体的实现 NATS 孵化 Pravega 沙箱 Strimzi 沙箱 Tremor 沙箱 应用定义和镜像构建 CNCF项目 项目阶段 项目介绍 Artifact Hub 沙箱 Backstage 孵化 Buildpacks 孵化 Devfile 沙箱 Helm 毕业 k8s的应用打包工具 Krator 沙箱 KubeVela 沙箱 阿里云开源的基于OAM的应用模型的实现，用来做应用的发布，同时支持多集群 KubeVirt 孵化 KUDO 沙箱 Nocalhost 沙箱 Operator Framework 孵化 用来开发基于k8s CRD的operator框架，功能跟k8s亲生的kubebuilder非常相似 Porter sealer 沙箱 阿里云开源的集群部署工具，理念比较先进，通过升维的方式可以通过类似Dockerfile的方式来构建集群镜像，并可以通过类似docker run的方式一键拉起完成的一套基于k8s的集群环境 Serverless Workflow Telepresence 持续集成和持续交付 CNCF项目 项目阶段 项目介绍 Argo 孵化 k8s上应用广泛的工作流引擎 Brigade 沙箱 Flux 孵化 gitops工具 Keptn 孵化 OpenGitOps 沙箱 OpenKruise 沙箱 基于k8s能力扩展的组件，通过CRD的方式定义了很多对象，用来增强k8s的workload能力。该组件放到该领域下有些不合适。 可观测和分析 监控 CNCF项目 项目阶段 项目介绍 Cortex 孵化 Fonio 沙箱 Kuberhealthy 沙箱 k8s的巡检工具，用来检查k8s的健康状态。支持以插件的方式接入巡检脚本。 OpenMetrics 沙箱 从Prometheus项目中发展出来的监控数据格式标准，该项目仅定义标准，非实现。 Pixie 沙箱 Prometheus 毕业 云原生领域事实上的监控标准 Skooner 沙箱 Thanos 孵化 prometheus的集群化方案 Trickster 沙箱 日志 CNCF项目 项目阶段 项目介绍 Fluentd 毕业 日志收集工具 分布式会话跟踪 CNCF项目 项目阶段 项目介绍 Jaeger 毕业 User开源的完整的分布式会话跟踪项目 OpenTelemetry 孵化 同时集成了监控、日志和分布式会话跟踪三个领域的数据收集工具，大有一统可观察性领域的趋势。 OpenTracing 归档 已经完全被OpenTelemetry取代 混沌引擎 CNCF项目 项目阶段 项目介绍 Chaos Mesh 孵化 Chaosblade 沙箱 Litmus 沙箱 参考资料 Cloud Native Landscape 云原生全景图详解系列（一）：带你了解云原生技术图谱 云原生全景图详解系列（二）：供应层 云原生全景图详解系列（三）：运行时层 云原生全景图详解系列（四）：编排和管理层 云原生全景图详解系列（五）：应用程序定义和开发层 云原生全景图详解（六）｜托管 Kubernetes 和 PaaS 解决什么问题 云原生全景图详解（七）：可观察性是什么，有哪些相关工具 ","date":"2022-06-10T00:14:00Z","permalink":"/post/cncf-landscape%E8%A7%A3%E8%AF%BB/","title":"CNCF Landscape解读"},{"content":"核心原理 Flux为一款GitOPS的发布工具，应用信息全部放到git仓库中，一旦git仓库中应用信息有新的提交，Flux即可在k8s中发布新的部署。支持kustomize和helm chart形式的应用。目前该项目为CNCF的孵化项目。\n快速开始 前置条件 已经存在k8s集群 可以获取到Github的个人token 安装flux cli 执行如下命令安装flux cli\n1 curl -s https://fluxcd.io/install.sh | sudo bash 将如下内容追到到文件~/.bash_profile\n1 . \u0026lt;(flux completion bash) bootstrap bootstrap操作会在k8s集群中安装flux到flux-system下，并且会通过git的方式来管理其自身。这里直接采用github，首先要获取到token，并设置为环境变量。\n1 export GITHUB_TOKEN=xxx 并执行如下命令开始bootstrap\n1 2 3 4 5 flux bootstrap github \\ --owner=kuring \\ --repository=flux-learn \\ --path=clusters/my-cluster \\ --personal 会自动创建github repo，并且会在main分支的clusters/my-cluster/flux-system下创建三个文件：\ngotk-components.yaml：flux的组件yaml gotk-sync.yaml kustomization.yaml：使用kustomization来个性化flux组件 会安装如下组件到k8s中\n1 2 3 4 5 6 $ k get deployment -n flux-system NAME READY UP-TO-DATE AVAILABLE AGE helm-controller 1/1 1 1 27m kustomize-controller 1/1 1 1 27m notification-controller 1/1 1 1 27m source-controller 1/1 1 1 27m 也可以使用更简单的方式flux install来安装flux，该命令不会将flux的安装信息存放到git仓库中，而是会直接安装。\n部署应用 将上述的github项目copy到本地，需要特别注意的是github不允许使用密码来认证，需要在输入密码的地方输入token。\n1 2 git clone https://github.com/kuring/flux-learn cd fleet-infra 使用如下命令在git仓库中创建podinfo-source.yaml文件和kustomization文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 flux create source git podinfo \\ --url=https://github.com/stefanprodan/podinfo \\ --branch=master \\ --interval=30s \\ --export \u0026gt; ./clusters/my-cluster/podinfo-source.yaml flux create kustomization podinfo \\ --target-namespace=default \\ --source=podinfo \\ --path=\u0026#34;./kustomize\u0026#34; \\ --prune=true \\ --interval=5m \\ --export \u0026gt; ./clusters/my-cluster/podinfo-kustomization.yaml podinfo-source.yaml文件内容如下：\n1 2 3 4 5 6 7 8 9 10 apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: GitRepository metadata: name: podinfo namespace: flux-system spec: interval: 30s ref: branch: master url: https://github.com/stefanprodan/podinfo podinfo-kustomization.yaml文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 kind: Kustomization metadata: name: podinfo namespace: flux-system spec: interval: 5m0s path: ./kustomize prune: true sourceRef: kind: GitRepository name: podinfo targetNamespace: default 执行如下命令将文件提供到git仓库\n1 2 git add -A \u0026amp;\u0026amp; git commit -m \u0026#34;Add podinfo GitRepository\u0026#34; git push 查看podinfo的部署状态，说明已经自动部署成功。\n1 2 3 4 $ flux get kustomizations --watch NAME REVISION SUSPENDED READY MESSAGE podinfo master/bf09377 False True Applied revision: master/bf09377 flux-system main/81f2115 False True Applied revision: main/81f2115 修改podinfo-kustomization.yaml文件内容如下，并重新提交到git仓库\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: kustomize.toolkit.fluxcd.io/v1beta2 kind: Kustomization metadata: name: podinfo namespace: flux-system spec: interval: 5m0s path: ./kustomize prune: true sourceRef: kind: GitRepository name: podinfo targetNamespace: default patches: - patch: |- apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: podinfo spec: minReplicas: 3 target: name: podinfo kind: HorizontalPodAutoscaler 过会可以看到环境中的hpa最小副本数已经变更为3.\n资料 https://fluxcd.io/ ","date":"2022-05-06T23:42:46Z","permalink":"/post/flux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"Flux（学习笔记）"},{"content":"在k8s的运维过程中，经常会有根据pid获取到pod名称的需求。比如某个pid占用cpu特别高，想知道是哪个pod里面的进程。\n操作步骤 查看进程的cgroup信息\n1 2 3 4 5 6 7 8 9 10 11 12 $ cat /proc/5760/cgroup 11:rdma:/ 10:hugetlb:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 9:freezer:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 8:pids:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 7:perf_event:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 6:blkio:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 5:devices:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 4:memory:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 3:cpuset,cpu,cpuacct:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 2:net_cls,net_prio:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 1:name=systemd:/apsara_k8s/kubepods/burstable/podf87c35b4-c170-4e1c-a726-d839e2fe6bea/17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 其中的 17e15f78a43b2ddc7caff93bcc03d5ca7f5249fbee4c2ade5c3c96a7279af0a3 即为容器的id，可以如下的命令直接获取到容器的pid\n1 2 CID=$(cat /proc/5760/cgroup | awk -F \u0026#39;/\u0026#39; \u0026#39;{print $6}\u0026#39;) echo ${CID:0:8} 继续执行如下命令查看docker的label\n1 docker inspect 17e15f --format \u0026#34;{{json .Config.Labels}}\u0026#34; 其中io.kubernetes.pod.namespace为pod所在的namespace，io.kubernetes.pod.name为pod的name。\n参考资料 Kubernetes 教程：根据 PID 获取 Pod 名称 ","date":"2022-05-06T14:45:20Z","permalink":"/post/%E6%A0%B9%E6%8D%AEpid%E8%8E%B7%E5%8F%96%E5%88%B0pod%E5%90%8D%E7%A7%B0/","title":"根据pid获取到pod名称"},{"content":"cue为使用golang编写的一款配置语言。\n安装 mac用户执行：brew install cue-lang/tap/cue，其他操作系统用户可以直接使用源码安装：go install cuelang.org/go/cmd/cue@latest\n命令行使用 创建如下文件first.cue\n1 2 3 4 5 6 7 8 9 a: 1.5 a: float b: 1 b: int d: [1, 2, 3] g: { h: \u0026#34;abc\u0026#34; } e: string cue fmt first.cue：对代码进行格式化 cue vet first.cue：校验语法的正确性 cue eval first.cue：获得渲染结果 cue export first.cue：将渲染结果以json格式的形式导出，如果指定参数\u0026ndash;out yaml，则可以以yaml方式导出。如果要导出某个变量，可以使用-e参数来指定变量。 语法 基础数据类型 支持的数据类型包括：float、int、string、array、bool、struct、null、自定义数据类型\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 // float a: 1.5 // int b: 1 // string c: \u0026#34;blahblahblah\u0026#34; // array d: [1, 2, 3, 1, 2, 3, 1, 2, 3] // bool e: true // struct f: { a: 1.5 b: 1 d: [1, 2, 3, 1, 2, 3, 1, 2, 3] g: { h: \u0026#34;abc\u0026#34; } } // null j: null // 表示两种类型的值，即可以是string，也可以是int h: string | int // 表示k的默认值为1，且类型为int k: *1 | int // 自定义数据类型 #abc: string 更复杂的自定义数据类型如下：\n1 2 3 4 5 6 7 8 #abc: { x: int y: string z: { a: float b: bool } } 定义cue模板 文件deployment.cue定义如下内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 // 自定义结构体类型 #parameter: { name: string image: string } template: { apiVersion: \u0026#34;apps/v1\u0026#34; kind: \u0026#34;Deployment\u0026#34; spec: { selector: matchLabels: { \u0026#34;app.oam.dev/component\u0026#34;: parameter.name } template: { metadata: labels: { \u0026#34;app.oam.dev/component\u0026#34;: parameter.name } spec: { containers: [{ name: parameter.name // 引用自定义结构体变量parameter image: parameter.image }] }}} } parameter:{ name: \u0026#34;mytest\u0026#34; image: \u0026#34;nginx:v1\u0026#34; } 执行 cue export deployment.cue -e template \u0026ndash;out yaml 可获取到渲染结果。\n引用 Getting Started CUE语言基础入门 Cuetorials ","date":"2022-05-04T23:24:34Z","permalink":"/post/cue%E8%AF%AD%E8%A8%80%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0/","title":"cue语言（个人笔记）"},{"content":"快速开始 部署 从https://github.com/tektoncd/cli/releases下载tkn工具，该工具为tekton的命令行工具。\n执行如下命令安装dashboard\n1 kubectl apply --filename https://github.com/tektoncd/dashboard/releases/latest/download/tekton-dashboard-release.yaml dashboard安装完成后仅申请了ClusterIP类型的Service，可以在本地通过kubectl端口转发的方式来对外提供服务，既可以通过ip:9097的方式来对外提供服务了。\n1 kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097:9097 --address 0.0.0.0 执行如下命令，在k8s中提交yaml\n1 kubectl apply --filename \\ https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml 会在k8s中提交如下两个deployment，此时tekton即安装完毕。\n1 2 3 4 $ kubectl get deploy -n tekton-pipelines NAME READY UP-TO-DATE AVAILABLE AGE tekton-pipelines-controller 1/1 1 1 4m tekton-pipelines-webhook 1/1 1 1 4m 创建task 创建如下yaml，提交一个任务定义，提交完成后任务并不会被执行\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: steps: - name: echo image: alpine script: | #!/bin/sh echo \u0026#34;Hello World\u0026#34; 继续提交如下的yaml，提交一次任务运行实例\n1 2 3 4 5 6 7 apiVersion: tekton.dev/v1beta1 kind: TaskRun metadata: name: hello-task-run spec: taskRef: name: hello 查看任务执行情况\n1 2 3 4 5 6 7 8 $ kubectl get taskrun hello-task-run NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME hello-task-run True Succeeded 45s 22s # 可以看到创建出了任务执行的pod $ kubectl get pod NAME READY STATUS RESTARTS AGE hello-task-run-pod 0/1 Completed 0 2m10s 创建流水线任务pipeline 创建新的task goodye\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: goodbye spec: steps: - name: goodbye image: ubuntu script: | #!/bin/bash echo \u0026#34;Goodbye World!\u0026#34; 提交如下yaml，创建pipeline定义\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: hello-goodbye spec: tasks: - name: hello taskRef: name: hello - name: goodbye runAfter: - hello taskRef: name: goodbye 提交如下yaml，创建pipeline实例。\n1 2 3 4 5 6 7 apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: hello-goodbye-run spec: pipelineRef: name: hello-goodbye 可以使用tkn来查看pipeline拉起的pod日志，该工具会将pod的日志合并到一起。\n1 2 3 4 $ tkn pipelinerun logs hello-goodbye-run -f -n default [hello : echo] Hello World [goodbye : goodbye] Goodbye World! ","date":"2022-04-16T22:56:52Z","permalink":"/post/tekton%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0/","title":"tekton（个人笔记）"},{"content":"操作系统级别 /proc/meminfo 其中的Buffers和Cached的迷惑性非常大，非常难理解。Buffers是指的磁盘数据的缓存，Cached是指对文件数据的缓存.\nfree 该命令实际上是通过读取/proc/meminfo文件得到的如下输出\n1 2 3 4 $ free -h total used free shared buff/cache available Mem: 30G 22G 412M 47M 7.6G 7.3G Swap: 0B 0B 0B 具体列含义：\ntotal: 总内存大小 used：已经使用的内存大小，包含共享内存 free：未使用的内存大小 shared：共享内存大小 buff/cache：缓存和缓冲区的大小 available：新进程可用的内存大小 vmstat 1 2 3 4 $ vmstat 1 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 4 0 0 1091484 1142360 34545436 0 0 146 265 0 0 8 5 86 1 0 容器级别 通过kubectl命令来查看内存使用 1 2 3 $ kubectl top pod nginx-ingress-controller-85cd6c7b5d-md6vc NAME CPU(cores) MEMORY(bytes) nginx-ingress-controller-85cd6c7b5d-md6vc 22m 502Mi 通过docker stats命令来查看容器 1 2 3 $ docker stats $container_id CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS 97d8bff3f89f k8s_nginx-ingress 6.33% 180.3MiB / 512MiB 35.21% 0B / 0B 0B / 0B 119 docker通过cgroup的统计数据来获取的内存值\n参考文档：https://docs.docker.com/engine/reference/commandline/stats/\n进程级别 具体的含义可以通过下文的/proc/$pid/status来查看，其他进程的内存含义\ncat /proc/$pid/status 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 cat status Name: nginx Umask: 0022 State: S (sleeping) Tgid: 46787 Ngid: 0 Pid: 46787 PPid: 33 TracerPid: 0 Uid: 1000 1000 1000 1000 Gid: 19062 19062 19062 19062 FDSize: 128 Groups: VmPeak: 559768 kB VmSize: 559120 kB VmLck: 0 kB VmPin: 0 kB VmHWM: 285240 kB VmRSS: 284752 kB RssAnon: 279016 kB RssFile: 3420 kB RssShmem: 2316 kB VmData: 371784 kB VmStk: 136 kB VmExe: 4716 kB VmLib: 6828 kB VmPTE: 900 kB VmSwap: 0 kB Threads: 33 SigQ: 1/123857 SigPnd: 0000000000000000 ShdPnd: 0000000000000000 SigBlk: 0000000000000000 SigIgn: 0000000040001000 SigCgt: 0000000198016eef CapInh: 0000001fffffffff CapPrm: 0000000000000400 CapEff: 0000000000000400 CapBnd: 0000001fffffffff CapAmb: 0000000000000000 NoNewPrivs: 0 Seccomp: 0 Speculation_Store_Bypass: vulnerable Cpus_allowed: 0001 Cpus_allowed_list: 0 Mems_allowed: 00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000001 Mems_allowed_list: 0 voluntary_ctxt_switches: 343464 nonvoluntary_ctxt_switches: 35061 具体字段含义可以查看man文档：https://man7.org/linux/man-pages/man5/proc.5.html，跟内存相关的字段如下：\nVmRSS：虚拟内存驻留在物理内存中的部分大小 VmHWM：使用物理内存的峰值 VmData：进程占用的数据段大小 top命令查看 1 2 3 $ top -p $pid PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 46787 www-data 20 0 559120 286544 5740 S 1.3 0.9 2:43.64 nginx 具体列含义如下：\nVIRT：进程使用虚拟内存总大小，即使没有占用物理内存，包括了进程代码段、数据段、共享内存、已经申请的堆内存和已经换到swap空间的内存等 RES: 进程实际使用的物理内存大小，不包括swap和共享内存 SHR：与其他进程的共享内存、加载的动态链接库、程序代码段的大小 MEM：进程使用的物理内存占系统总内存的百分比 ps命令查看 1 2 3 4 5 6 7 8 9 10 11 $ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND www-data 1 0.0 0.0 212 8 ? Ss Apr09 0:00 /usr/bin/dumb-init -- /nginx-ingress-controller www-data 6 0.9 0.3 813500 99644 ? Ssl Apr09 67:32 /nginx-ingress-controller www-data 33 0.0 0.7 458064 242252 ? S Apr09 0:53 nginx: master process /usr/local/nginx/sbin/nginx -c /etc/nginx/nginx.conf www-data 46786 0.0 0.7 459976 239996 ? S 17:57 0:00 rollback logs/eagleeye.log interval=60 adjust=600 www-data 46787 1.3 0.8 559120 284452 ? Sl 17:57 2:22 nginx: worker process www-data 46788 1.0 0.8 558992 285168 ? Sl 17:57 1:51 nginx: worker process www-data 46789 0.0 0.7 452012 237152 ? S 17:57 0:01 nginx: cache manager process www-data 46790 0.0 0.8 490832 267600 ? S 17:57 0:00 nginx: x www-data 47533 0.0 0.0 60052 1832 pts/2 R+ 20:50 0:00 ps aux RSS：虚拟内存中的常驻内存，即实际占用的物理内存，包括所有已经分配的堆内存、栈内存、共享内存，由于共享内存非独占，实际上进程独占的物理内存要少于RSS pmap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 # pmap -x 452021 452021: nginx: worker process Address Kbytes RSS Dirty Mode Mapping 0000000000400000 4716 1540 0 r-x-- tengine 0000000000a9a000 20 16 4 r---- tengine 0000000000a9f000 212 196 176 rw--- tengine 0000000000ad4000 248 60 60 rw--- [ anon ] 00002b0da6cb0000 136 4 0 r-x-- ld-2.17.so 00002b0da6cd2000 4 4 4 rw--- [ anon ] 00002b0da6cd3000 4 4 4 rw-s- zero (deleted) 00002b0da6cd7000 28 24 24 rw--- [ anon ] 00002b0da6cde000 64 8 8 rwx-- [ anon ] 00002b0da6ed1000 4 4 4 r---- ld-2.17.so 00002b0da6ed2000 4 4 4 rw--- ld-2.17.so 00002b0da6ed3000 4 4 4 rw--- [ anon ] 00002b0da6ed4000 8 0 0 r-x-- libdl-2.17.so 00002b0da70d8000 92 32 0 r-x-- libpthread-2.17.so 00002b0da72f0000 16 4 4 rw--- [ anon ] 00002b0da72f4000 32 0 0 r-x-- libcrypt-2.17.so 00002b0da72fc000 2044 0 0 ----- libcrypt-2.17.so 00002b0da74fb000 4 4 4 r---- libcrypt-2.17.so 00002b0da74fc000 4 4 4 rw--- libcrypt-2.17.so 00002b0da74fd000 184 0 0 rw--- [ anon ] 00002b0da752b000 1028 8 0 r-x-- libm-2.17.so 00002b0da7a35000 760 372 0 r-x-- libssl.so.1.1 00002b0da7d03000 2812 1124 0 r-x-- libcrypto.so.1.1 00002b0da7fc2000 2044 0 0 ----- libcrypto.so.1.1 00002b0da81c1000 172 172 172 r---- libcrypto.so.1.1 00002b0da81ec000 12 12 12 rw--- libcrypto.so.1.1 00002b0da81ef000 16 8 8 rw--- [ anon ] 00002b0da81f3000 84 40 0 r-x-- libgcc_s-4.8.5-20150702.so.1 00002b0da8409000 1808 316 0 r-x-- libc-2.17.so 00002b0da85cd000 2044 0 0 ----- libc-2.17.so 00002b0da87cc000 16 16 16 r---- libc-2.17.so 00002b0da87d0000 8 8 8 rw--- libc-2.17.so 00002b0da87d2000 20 12 12 rw--- [ anon ] 00002b0da87d7000 8 0 0 r-x-- libfreebl3.so 00002b0da87d9000 2044 0 0 ----- libfreebl3.so 00002b0da89d8000 4 4 4 r---- libfreebl3.so 00002b0da89d9000 4 4 4 rw--- libfreebl3.so 00002b0da8a00000 24576 21936 21936 rw--- [ anon ] 00002b0daa200000 8192 8180 8180 rw--- [ anon ] 00002b0daaa00000 8192 8192 8192 rw--- [ anon ] 00002b0dab200000 8192 8192 8192 rw--- [ anon ] 00002b0daba00000 8192 8172 8172 rw--- [ anon ] 00002b0dac200000 4096 4096 4096 rw--- [ anon ] 00002b0dac600000 4096 4096 4096 rw--- [ anon ] 00002b0daca00000 4096 12 12 rw-s- zero (deleted) 00002b0dace00000 1024 0 0 rw-s- zero (deleted) 00002b0dacf00000 1024 0 0 rw-s- zero (deleted) 00002b0dad000000 16384 16384 16384 rw--- [ anon ] 00002b0dae000000 10240 0 0 rw-s- zero (deleted) 00002b0db3f00000 24 16 0 r-x-- cjson.so 00002b0db3f06000 2048 0 0 ----- cjson.so 00002b0db4106000 4 4 4 r---- cjson.so 00002b0db4107000 4 4 4 rw--- cjson.so 00002b0db4108000 8 0 0 r-x-- librestychash.so 00002b0db410a000 2044 0 0 ----- librestychash.so 00002b0db4309000 4 4 4 r---- librestychash.so 00002b0db430a000 4 4 4 rw--- librestychash.so 00002b0db430b000 10240 1784 1784 rw-s- zero (deleted) 00002b0db4d0b000 10240 0 0 rw-s- zero (deleted) 00002b0db5800000 6144 6144 6144 rw--- [ anon ] 00002b0db5e00000 6144 6144 6144 rw--- [ anon ] 00002b0db6400000 8192 8192 8192 rw--- [ anon ] 00002b0db6c0b000 5120 8 8 rw-s- zero (deleted) 00002b0db7200000 8192 8192 8192 rw--- [ anon ] 00002b0dc2800000 1024 0 0 rw-s- zero (deleted) 00002b0dc2900000 10240 0 0 rw-s- zero (deleted) 00002b0dc3300000 10240 0 0 rw-s- zero (deleted) 00002b0dc861f000 2048 8 8 rw--- [ anon ] 00002b0dc881f000 4 0 0 ----- [ anon ] 00002b0dc8820000 2048 8 8 rw--- [ anon ] 00007fff60f6c000 136 44 44 rw--- [ stack ] 00007fff60fdd000 8 4 0 r-x-- [ anon ] ffffffffff600000 4 0 0 r-x-- [ anon ] ---------------- ------- ------- ------- total kB 558996 289376 285888 Mapping: 支持如下值 [anon]：分配的内存 ","date":"2022-04-14T20:33:17Z","permalink":"/post/%E5%86%85%E5%AD%98%E4%BD%BF%E7%94%A8%E5%88%86%E6%9E%90/","title":"内存使用分析"},{"content":"理论 待补充\n安装 下载kind命令，但不需要创建一个k8s集群。\n执行如下命令下载kn 二进制文件\n1 2 3 4 5 6 7 wget https://github.com/knative/client/releases/download/knative-v1.2.0/kn-linux-amd64 mv kn-linux-amd64 /usr/local/bin/kn chmod +x /usr/local/bin/kn # 自动补全 echo -e \u0026#34;\\n# kn\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;source \u0026lt;(kn completion bash)\u0026#39; \u0026gt;\u0026gt;~/.bash_profile 下载quickstart二进制文件\n1 2 3 4 5 6 7 wget https://github.com/knative-sandbox/kn-plugin-quickstart/releases/download/knative-v1.2.0/kn-quickstart-linux-amd64 mv kn-quickstart-linux-amd64 /usr/local/bin/kn-quickstart chmod +x /usr/local/bin/kn-quickstart # 如下两条命令可以得到相同的输出结果 kn quickstart --help kn-quickstart --help 执行 kn quickstart kind 命令即可创建出一个knative的k8s集群。\nknative serving serving的核心功能为提供弹性扩缩容能力。\nCRD Service：用来管理整个应用的生命周期。 Route：用来将流量分发到不同的Revision Configuration： Revision：\nkpa功能\n实践 最简单service 创建hello.yaml文件，内容如下，并执行 kubectl apply -f hello.yaml。其中service的名字为hello，revision的名字为world。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: metadata: # This is the name of our new \u0026#34;Revision,\u0026#34; it must follow the convention {service-name}-{revision-name} name: hello-world spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \u0026#34;World\u0026#34; 通过kn命令可以看到创建了一个service hello，并且有一个可以访问的url地址。\n1 2 3 # kn service list NAME URL LATEST AGE CONDITIONS READY REASON hello http://hello.default.127.0.0.1.sslip.io hello-world 154m 3 OK / 3 True knative抽象了Revision来标识该service对应的版本信息，可以使用kubectl命令，也可以使用kn命令来查看revision信息。\n1 2 3 4 5 6 $ k get revisions.serving.knative.dev NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON ACTUAL REPLICAS DESIRED REPLICAS hello-world hello 1 True 0 0 $ kn revision list NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-world hello 100% 1 6m33s 3 OK / 4 True 在宿主机上执行 curl http://hello.default.127.0.0.1.sslip.io 接口访问刚才创建的service。这里比较有意思的是为什么域名可以在宿主机上解析，该域名实际上是通过公网来解析的，域名服务器sslip.io负责该域名的解析。 本机的127.0.0.1的80端口实际是指向的是kind容器的31080端口，而31080为kourier-ingress对外暴露的服务。\n1 2 # kubectl get svc -A | grep 31080 kourier-system kourier-ingress NodePort 10.96.252.144 \u0026lt;none\u0026gt; 80:31080/TCP 3h16m kourier-ingress为knative使用的ingress服务，该ingress并非k8s原生的ingress对象，而是自定义的ingress networking.internal.knative.dev/v1alpha1。service hello在创建的时候会同步创建一个ingress对象。在该ingress对象中可以看到刚才访问的域名hello.default.127.0.0.1.sslip.io，同时可以看到该ingress将域名指向到了k8s的service hello-world.default。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 kubectl get ingresses.networking.internal.knative.dev hello -o yaml apiVersion: networking.internal.knative.dev/v1alpha1 kind: Ingress metadata: annotations: networking.internal.knative.dev/rollout: \u0026#39;{\u0026#34;configurations\u0026#34;:[{\u0026#34;configurationName\u0026#34;:\u0026#34;hello\u0026#34;,\u0026#34;percent\u0026#34;:100,\u0026#34;revisions\u0026#34;:[{\u0026#34;revisionName\u0026#34;:\u0026#34;hello-world\u0026#34;,\u0026#34;percent\u0026#34;:100}],\u0026#34;stepParams\u0026#34;:{}}]}\u0026#39; networking.knative.dev/ingress.class: kourier.ingress.networking.knative.dev serving.knative.dev/creator: kubernetes-admin serving.knative.dev/lastModifier: kubernetes-admin finalizers: - ingresses.networking.internal.knative.dev generation: 1 labels: serving.knative.dev/route: hello serving.knative.dev/routeNamespace: default serving.knative.dev/service: hello name: hello namespace: default ownerReferences: - apiVersion: serving.knative.dev/v1 blockOwnerDeletion: true controller: true kind: Route name: hello uid: 4c58e77b-4871-42cc-bfa0-aa9fda9646ed spec: httpOption: Enabled rules: - hosts: - hello.default - hello.default.svc - hello.default.svc.cluster.local http: paths: - splits: - appendHeaders: Knative-Serving-Namespace: default Knative-Serving-Revision: hello-world percent: 100 serviceName: hello-world serviceNamespace: default servicePort: 80 visibility: ClusterLocal - hosts: - hello.default.127.0.0.1.sslip.io http: paths: - splits: - appendHeaders: Knative-Serving-Namespace: default Knative-Serving-Revision: hello-world percent: 100 serviceName: hello-world serviceNamespace: default servicePort: 80 visibility: ExternalIP status: conditions: - lastTransitionTime: \u0026#34;2022-03-11T12:47:26Z\u0026#34; status: \u0026#34;True\u0026#34; type: LoadBalancerReady - lastTransitionTime: \u0026#34;2022-03-11T12:47:26Z\u0026#34; status: \u0026#34;True\u0026#34; type: NetworkConfigured - lastTransitionTime: \u0026#34;2022-03-11T12:47:26Z\u0026#34; status: \u0026#34;True\u0026#34; type: Ready observedGeneration: 1 privateLoadBalancer: ingress: - domainInternal: kourier-internal.kourier-system.svc.cluster.local publicLoadBalancer: ingress: - domainInternal: kourier.kourier-system.svc.cluster.local 在default namespace下可以看到有三个service，其中hello-world的ingress转发的service。\n1 2 3 4 # k get svc | grep hello hello ExternalName \u0026lt;none\u0026gt; kourier-internal.kourier-system.svc.cluster.local 80/TCP 155m hello-world ClusterIP 10.96.58.149 \u0026lt;none\u0026gt; 80/TCP 155m hello-world-private ClusterIP 10.96.54.163 \u0026lt;none\u0026gt; 80/TCP,9090/TCP,9091/TCP,8022/TCP,8012/TCP 155m 通过查看该serivce的yaml，并未定义serivce的selector。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: v1 kind: Service metadata: annotations: autoscaling.knative.dev/class: kpa.autoscaling.knative.dev serving.knative.dev/creator: kubernetes-admin labels: app: hello-world networking.internal.knative.dev/serverlessservice: hello-world networking.internal.knative.dev/serviceType: Public serving.knative.dev/configuration: hello serving.knative.dev/configurationGeneration: \u0026#34;1\u0026#34; serving.knative.dev/configurationUID: 13138d0f-ee5f-4631-94a5-6928546e504c serving.knative.dev/revision: hello-world serving.knative.dev/revisionUID: f3aaae74-6b79-4785-b60d-5607c0ab3bcf serving.knative.dev/service: hello serving.knative.dev/serviceUID: 79907029-32df-4f21-b14b-ed7d24e1a10e name: hello-world namespace: default ownerReferences: - apiVersion: networking.internal.knative.dev/v1alpha1 blockOwnerDeletion: true controller: true kind: ServerlessService name: hello-world uid: 3a6326c5-32b0-4074-bec2-4d3eed293b71 spec: clusterIP: 10.96.58.149 clusterIPs: - 10.96.58.149 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - name: http port: 80 protocol: TCP targetPort: 8012 sessionAffinity: None type: ClusterIP status: loadBalancer: {} 查看Service对应的Endpoint对象，可以看到Endpoint对象实际上指向到了knative-serving下的pod activator-85bd4ddcbb-6ms7n。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion: v1 kind: Endpoints metadata: annotations: autoscaling.knative.dev/class: kpa.autoscaling.knative.dev serving.knative.dev/creator: kubernetes-admin labels: app: hello-world networking.internal.knative.dev/serverlessservice: hello-world networking.internal.knative.dev/serviceType: Public serving.knative.dev/configuration: hello serving.knative.dev/configurationGeneration: \u0026#34;1\u0026#34; serving.knative.dev/configurationUID: 13138d0f-ee5f-4631-94a5-6928546e504c serving.knative.dev/revision: hello-world serving.knative.dev/revisionUID: f3aaae74-6b79-4785-b60d-5607c0ab3bcf serving.knative.dev/service: hello serving.knative.dev/serviceUID: 79907029-32df-4f21-b14b-ed7d24e1a10e name: hello-world namespace: default ownerReferences: - apiVersion: networking.internal.knative.dev/v1alpha1 blockOwnerDeletion: true controller: true kind: ServerlessService name: hello-world uid: 3a6326c5-32b0-4074-bec2-4d3eed293b71 subsets: - addresses: - ip: 10.244.0.5 nodeName: knative-control-plane targetRef: kind: Pod name: activator-85bd4ddcbb-6ms7n namespace: knative-serving resourceVersion: \u0026#34;809\u0026#34; uid: df916ac6-3161-4bc6-bf8c-47bb7c83cc4a ports: - name: http port: 8012 protocol: TCP 进一步查看knative-serving下有一个knative的组件activator。\n1 2 3 k get deploy -n knative-serving activator NAME READY UP-TO-DATE AVAILABLE AGE activator 1/1 1 1 3h35m 打开终端，执行一下 kubectl get pod -l serving.knative.dev/service=hello -w，重新执行 curl http://hello.default.127.0.0.1.sslip.io 发起新的请求，可以看到会有pod产生，且pod为通过deployment拉起。\n1 2 3 4 5 $ kubectl get pod -l serving.knative.dev/service=hello -w hello-world-deployment-7ff4bdb7fd-rqg96 0/2 Pending 0 0s hello-world-deployment-7ff4bdb7fd-rqg96 0/2 ContainerCreating 0 0s hello-world-deployment-7ff4bdb7fd-rqg96 1/2 Running 0 1s hello-world-deployment-7ff4bdb7fd-rqg96 2/2 Running 0 1s 过一段时间没有新的请求后，pod会自动被删除，同时可以看到deployment的副本数缩成0。该namespace下并没有对应的hpa产生，说明deployment副本数的调整并非使用k8s原生的hpa机制。\n1 2 3 4 5 hello-world-deployment-7ff4bdb7fd-rqg96 2/2 Terminating 0 63s hello-world-deployment-7ff4bdb7fd-rqg96 0/2 Terminating 0 93s $ k get deployments.apps NAME READY UP-TO-DATE AVAILABLE AGE hello-world-deployment 0/0 0 0 21h service的流量切分 重新提交如下的yaml文件，将service对应的revision更新为knative。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: metadata: name: hello-knative spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \u0026#34;Knative\u0026#34; 重新查看revision，可以看到revision已经变更为了knative，同时可以看到老的revision world并没有被删除，只是没有了流量转发。\n1 2 3 4 $ kn revision list NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 100% 2 49s 4 OK / 4 True hello-world hello 1 10m 3 OK / 4 True 重新apply新的Service，将流量切分为hello-world和hello-knative两份，重新执行curl请求，可以看到结果会随机返回。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: metadata: name: hello-knative spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \u0026#34;Knative\u0026#34; traffic: - latestRevision: true percent: 50 - revisionName: hello-world percent: 50 查看revision的信息，可以看到流量已经是50%的切分了。\n1 2 3 4 # kn revision list NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 50% 2 11m 3 OK / 4 True hello-world hello 50% 1 21m 3 OK / 4 True knative eventing Source：k8s的CR对象，产生Event Broker：用来分发Event Trigger：Event触发器 Sink：Event输出结果\n其中包含业务逻辑的可编程的部分在于Trigger部分，由于trigger实际上是无状态的服务，对于一些有状态的消息knative很难满足。比如同一类型的特定字段的event转发到特定的trigger上，broker实际上不具备可编程性，因此无法完成。\nkn quickstart 命令会安装一个broker到环境中\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 $ k get brokers.eventing.knative.dev example-broker -o yaml apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker eventing.knative.dev/creator: kubernetes-admin eventing.knative.dev/lastModifier: kubernetes-admin name: example-broker namespace: default spec: config: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing delivery: backoffDelay: PT0.2S backoffPolicy: exponential retry: 10 status: address: url: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker annotations: knative.dev/channelAPIVersion: messaging.knative.dev/v1 knative.dev/channelAddress: http://example-broker-kne-trigger-kn-channel.default.svc.cluster.local knative.dev/channelKind: InMemoryChannel knative.dev/channelName: example-broker-kne-trigger conditions: - lastTransitionTime: \u0026#34;2022-03-11T12:32:43Z\u0026#34; status: \u0026#34;True\u0026#34; type: Addressable - lastTransitionTime: \u0026#34;2022-03-11T12:32:43Z\u0026#34; message: No dead letter sink is configured. reason: DeadLetterSinkNotConfigured severity: Info status: \u0026#34;True\u0026#34; type: DeadLetterSinkResolved - lastTransitionTime: \u0026#34;2022-03-11T12:32:43Z\u0026#34; status: \u0026#34;True\u0026#34; type: FilterReady - lastTransitionTime: \u0026#34;2022-03-11T12:32:43Z\u0026#34; status: \u0026#34;True\u0026#34; type: IngressReady - lastTransitionTime: \u0026#34;2022-03-11T12:32:43Z\u0026#34; status: \u0026#34;True\u0026#34; type: Ready - lastTransitionTime: \u0026#34;2022-03-11T12:32:43Z\u0026#34; status: \u0026#34;True\u0026#34; type: TriggerChannelReady observedGeneration: 1 快速开始 创建如下的Service，该service通过环境变量 BROKER_URL 作为broker地址，可以看到其地址为 quickstart工具默认安装的example-broker。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: serving.knative.dev/v1 kind: Service metadata: name: cloudevents-player spec: template: metadata: annotations: autoscaling.knative.dev/min-scale: \u0026#34;1\u0026#34; spec: containers: - image: ruromero/cloudevents-player:latest env: - name: BROKER_URL value: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker 访问service页面 http://cloudevents-player.default.127.0.0.1.sslip.io/ 可以在界面上创建Event后，产生如下的Event内容，格式完全遵循社区的CloudEvent规范。点击发送，即可以将消息发送给broker，但由于borker没有配置任何的Trigger，消息在发送到broker后会被直接丢弃。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \u0026#34;root\u0026#34;: { \u0026#34;attributes\u0026#34;: { \u0026#34;datacontenttype\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;mediaType\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;manual\u0026#34;, \u0026#34;specversion\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;test-type\u0026#34; }, \u0026#34;data\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Hello CloudEvents!\u0026#34; }, \u0026#34;extensions\u0026#34;: {} } } 我们继续来给broker增加触发器，创建如下的yaml。该触发器定义了broker为example-broker，\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: cloudevents-trigger annotations: knative-eventing-injection: enabled spec: broker: example-broker subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: cloudevents-player 重新在页面上发送event，可以看到消息的状态为接收。\n参考 即学即会Serverless ","date":"2022-04-01T21:16:00Z","permalink":"/post/knative%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0-%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0/","title":"knative（个人笔记） - 个人笔记"},{"content":"内核参数项 以CentOS7 系统为例，可以看到有1088个内核参数项。\n1 2 3 4 $ uname -a Linux iZbp17o12gcsq2d87y7x1hZ 3.10.0-1160.36.2.el7.x86_64 #1 SMP Wed Jul 21 11:57:15 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux $ sysctl -a 2\u0026gt;/dev/null | wc -l 1088 Linux的内核参数均位于/proc/sys目录下，涉及到如下几个目录：\n分类 描述 abi crypto debug dev 用来配置特定设备，比如raid、scsi设备 fs 文件子系统 kernel 内核子系统 net 网络子系统 user vm 内存子系统 kernel子系统 参数 描述 kernel.panic 内核出现panic，重新引导前需要等待的时间，单位为秒。如果该值为0，，说明内核禁止自动引导 kernel.core_pattern core文件的存放路径 vm子系统 参数 描述 vm.min_free_kbytes 系统所保留空闲内存的最小值。该值通过公式计算，跟当前机器的物理内存相关。 vm.swappiness 用来控制虚拟内存，支持如下值：\n0：关闭虚拟内存 1：允许开启虚拟内存设置的最小值 10：剩余内存少于10%时开启虚拟内存 100：完全适用虚拟内存。\n该参数可以通过swapon 命令开启，swapoff关闭。\n参考链接：https://linuxhint.com/understanding_vm_swappiness/ net子系统 参数 描述 net.ipv4.ip_local_reserved_ports 随机端口的黑名单列表，系统在发起连接时，不使用该内核参数内的端口号 net.ipv4.ip_local_port_range 随机端口的白名单范围，网络连接可以作为源端口的最小和最大端口限制 net.ipv4.rp_filter 是否开启对数据包源地址的校验, 收到包后根据source ip到route表中检查是否否和最佳路由，否的话扔掉这个包。这次如下值：\n1. 不开启源地址校验\n2. 开启严格的反向路径校验。对每个进来的数据包，校验其反向路径是否是最佳路径。如果反向路径不是最佳路径，则直接丢弃该数据包。\n3. 开启松散的反向路径校验。对每个进来的数据包，校验其源地址是否可达，即反向路径是否能通（通过任意网口），如果反向路径不通，则直接丢弃该数据包。该内核参数 net.ipv4.conf.all.log_martians 可以来控制是否打开日志，日志打开后可以在/var/log/message中观察到。 TCP tcp 相关内核参数可以使用 man 7 tcp 查看。\n建连相关 syn 队列又称为半连接队列。服务端在接收到客户端的 SYN 包后，服务端向客户端发送 SYN + ACK 报文，此时会进入到半连接队列。\n相关文章：Linux TCP backlog\n断开连接相关 TCP TIME_WAIT\n文件子系统 fs.mount-max The value in this file specifies the maximum number of mounts that may exist in a mount namespace. The default value in this file is 100,000.\nLinux 4.19 内核引入。当 mount namespace 中加载的文件数超过该值后，会报错 \u0026ldquo;No space left on device\u0026rdquo;。\n内核参数在k8s的支持情况 大类 子类 备注 namespace内核参数 安全的内核参数 k8s默认支持的内核参数非常少，仅支持如下的内核参数：\n1. kernel.shm_rmid_forced\n2. net.ipv4.ip_local_port_range\n3. net.ipv4.tcp_syncookies（在内核4.4之前为非namespace化）\n4. net.ipv4.ping_group_range （从 Kubernetes 1.18 开始）\n5. net.ipv4.ip_unprivileged_port_start （从 Kubernetes 1.22 开始） namespace内核参数 非安全内核参数 默认禁用，pod可以调度成功，但会报错SysctlForbidden。修改kubelet参数开启 kubelet --allowed-unsafe-sysctls 'kernel.msg*,net.core.somaxconn' 非namespace内核参数 在容器中没有的内核参数 如：\nnet.core.netdev_max_backlog = 10000\nnet.core.rmem_max = 2097152\nnet.core.wmem_max = 2097152 非namespace隔离参数 直接修改宿主机的内核参数 在容器中需要开启特权容器来设置，如：\nvm.overcommit_memory = 2 vm.overcommit_ratio = 95 操作系统的namespace化的内核参数仅支持：\nkernel.shm*, kernel.msg*, kernel.sem, fs.mqueue.*, net.*（内核中可以在容器命名空间里被更改的网络配置项相关参数）。然而也有一些特例 （例如，net.netfilter.nf_conntrack_max 和 net.netfilter.nf_conntrack_expect_max 可以在容器命名空间里被更改，但它们是非命名空间的）。 k8s在pod中声明内核参数的方式如下：\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: sysctl-example spec: securityContext: sysctls: - name: kernel.shm_rmid_forced value: \u0026#34;0\u0026#34; 业界解决方案 ACK - 安全沙箱容器 阿里云ACK服务的安全沙箱容器，底层实现为runV，pod拥有独立的内核参数，相互之间不受影响。通过扩展pod的annotation来完成内核参数的修改：\n1 2 annotations: securecontainer.alibabacloud.com/sysctls: \u0026#34;net.bridge.bridge-nf-call-ip6tables=1,net.bridge.bridge-nf-call-iptables=1,net.ipv4.ip_forward=1\u0026#34; 阿里云ACK - 配置安全沙箱Pod内核参数\n弹性容器实例 完全利用k8s的功能，有限内核参数修改。 https://help.aliyun.com/document_detail/163023.html\n参考文档 ASI 配置Security Context ","date":"2022-04-01T14:31:38Z","permalink":"/post/linux%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/","title":"Linux内核参数（持续更新）"},{"content":"在RHEL7系统中，firewalld和iptabels两个防火墙的服务并存，firewalld通过调用iptables命令来实现，firewalld和iptables均用来维护系统的netfilter规则，底层实现为内核的netfilter模块。\nfirewalld为systemd项目的一部分，为python语言实现，跟iptables相比，使用上更加人性化，提供了更高层次的抽象，用户不用关心netfilter的链和表的概念。\n架构 最上层为用户界面层，提供了firewall-cmd和firewall-offline-cmd两个命令行工具，其中firewall-cmd为最主要的命令行工具。firewall-config为GUI工具。用户界面层通过firewalld提供的D-Bus接口进行通讯。\nfirewalld为daemon进程，其中zone、service、ipset等为firewalld抽象的概念。在接收到用户界面层的命令后，一方面需要将操作保存到本地文件，另外还需要调用更底层的如iptables、ipset、ebtables等命令来产生规则，最终在内核层的netfilter模块生效。\nfirewalld的管理 在RHEL7系统下，firewalld作为systemd家族的一员会默认安装。其service文件定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ cat /usr/lib/systemd/system/firewalld.service [Unit] Description=firewalld - dynamic firewall daemon Before=network-pre.target Wants=network-pre.target After=dbus.service After=polkit.service Conflicts=iptables.service ip6tables.service ebtables.service ipset.service Documentation=man:firewalld(1) [Service] EnvironmentFile=-/etc/sysconfig/firewalld ExecStart=/usr/sbin/firewalld --nofork --nopid $FIREWALLD_ARGS ExecReload=/bin/kill -HUP $MAINPID # supress to log debug and error output also to /var/log/messages StandardOutput=null StandardError=null Type=dbus BusName=org.fedoraproject.FirewallD1 KillMode=mixed [Install] WantedBy=multi-user.target Alias=dbus-org.fedoraproject.FirewallD1.service 查看firewall-cmd的运行状态\n1 2 $ firewall-cmd --state not running 在安装完firewalld后，会在/usr/lib/firewalld/目录下产生默认的配置，该部分配置不可修改。同时会在/etc/firewalld目录下产生firewalld的配置，该部分配置会覆盖/usr/lib/firewalld/下的配置。/etc/firewalld目录下的文件内容如下，其中firewalld.conf文件为最主要的配置文件。\n1 2 $ ls firewalld.conf helpers icmptypes ipsets lockdown-whitelist.xml services zones 概念 firewalld抽象了zone、service、ipset、helper、icmptypes几个概念。\nzone firewalld 将网络按照安全等级划分了不同的zone，zone的定义位于/usr/lib/firewalld/zones/目录下，文件格式为xml，包括了如下的zone。\ndrop：只允许出向，任何入向的网络数据包被丢弃，不会回复icmp报文。 block：任何入向的网络数据包均会被拒绝，会回复icmp报文。 public：公共区域网络流量。不信任网络上的流量，选择接收入向的网络流量。 external：不信任网络上的流量，选择接收入向的网络流量。 DMZ：隔离区域，内网和外网增加的一层网络，起到缓冲作用。选择接收入向的网络连接。 work：办公网络，信任网络流量，选择接收入向的网络流量。 home：家庭网络，信任网络流量，选择接收入向的网络流量。 internal：内部网络，信任网络流量，选择接收入向的网络流量。 trusted：信任区域。所有网络连接可接受。 firewalld的默认zone为public，zone目录下的public.xml的内容如下：\n1 2 3 4 5 6 7 8 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Public\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.\u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;dhcpv6-client\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;cockpit\u0026#34;/\u0026gt; \u0026lt;/zone\u0026gt; 可以看到Public zone关联了三个service对象 ssh、dhcpv6-client、cockpit。没有匹配到该zone的流量，默认情况下会被拒绝。\n在配置文件/etc/firewalld/firewalld.conf中，通过DefaultZone字段指定的默认zone为public。即如果开启了firewalld规则，那么默认仅会放行访问上述三个服务的流量。执行 iptables-save 命令实际上并为看到任何的iptables规则，说明firewalld是直接调用内核的netfilter来实现的。\nservice service为firewalld对运行在宿主机上的进程的抽象，并在zone文件中跟zone进行绑定。比如ssh.xml的内容如下：\n1 2 3 4 5 6 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;service\u0026gt; \u0026lt;short\u0026gt;SSH\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;Secure Shell (SSH) is a protocol for logging into and executing commands on remote machines. It provides secure encrypted communications. If you plan on accessing your machine remotely via SSH over a firewalled interface, enable this option. You need the openssh-server package installed for this option to be useful.\u0026lt;/description\u0026gt; \u0026lt;port protocol=\u0026#34;tcp\u0026#34; port=\u0026#34;22\u0026#34;/\u0026gt; \u0026lt;/service\u0026gt; 实践 向某个zone内增加端口号 执行 firewall-cmd --permanent --zone=public --add-port=80/tcp 即可向public zone内增加80端口号。同时可以在 /etc/firewalld/zones/public.xml 文件中看到新增加了80端口号。\n1 2 3 4 5 6 7 8 9 10 $ cat public.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;zone\u0026gt; \u0026lt;short\u0026gt;Public\u0026lt;/short\u0026gt; \u0026lt;description\u0026gt;For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.\u0026lt;/description\u0026gt; \u0026lt;service name=\u0026#34;ssh\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;dhcpv6-client\u0026#34;/\u0026gt; \u0026lt;service name=\u0026#34;cockpit\u0026#34;/\u0026gt; \u0026lt;port port=\u0026#34;80\u0026#34; protocol=\u0026#34;tcp\u0026#34;/\u0026gt; \u0026lt;/zone\u0026gt; 参数 --permanent 为固化到文件中，如果不增加该参数重启firewalld进程后配置会失效。\n获取当前zone信息 1 2 3 $ firewall-cmd --get-active-zones public interfaces: eth0 每个网络设备可以属于不同的zone，可以根据网络设备名来查询所属的zone\n1 2 $ firewall-cmd --get-zone-of-interface=eth0 public 设置当前默认zone 该命令会自动修改/etc/firewalld/firewalld.conf配置文件中的DefaultZone字段。\n1 $ firewall-cmd --set-default-zone=trusted 查看当前配置规则 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ firewall-cmd --list-all public (active) target: default icmp-block-inversion: no interfaces: eth0 sources: services: cockpit dhcpv6-client ssh ports: 80/tcp protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: 相关链接 firewalld官方文档 RHEL7: How to get started with Firewalld ","date":"2022-03-29T19:24:27Z","permalink":"/post/firewalld/","title":"firewalld"},{"content":"audit简介 audit为linux内核安全体系的重要组成部分，用来记录内核的系统调用，文件修改等事件，用于审计目的。\nauditctl: 面向用户的工具，类似于iptables命令 auditd: 负责将审计信息写入到/var/ 启动auditd服务 auditd作为单独的服务运行在系统上，Redhat系统使用systemctl start auditd启动服务，启动后通过 ps -ef | grep auditd查看进程是否启动成功。\nauditctl 查看auditd的运行状态\n1 2 3 4 5 6 7 8 9 $ auditctl -s enabled 1 failure 1 pid 638 rate_limit 0 backlog_limit 8192 lost 0 backlog 0 loginuid_immutable 0 unlocked 查看当前环境规则\n1 2 3 4 5 $ auditctl -l -w /tmp/hosts -p rwxa -w /proc/sys/net/ipv4/tcp_retries1 -p rwxa -w /proc/sys/net/ipv4/tcp_retries2 -p rwxa -w /proc/sys/net/ipv4/tcp_retries2 -p wa 删除所有的audit规则\n1 2 $ auditctl -D No rules 实践 监控文件变化 执行 auditctl -w $file -p wa 来监控文件，比如监控内核参数 auditctl -w /proc/sys/net/ipv4/tcp_retries2 -p wa，其中-p指定了监控文件的行为，支持rwxa。 查看文件 cat /proc/sys/net/ipv4/tcp_retries2。 使用vim打开文件 vim /proc/sys/net/ipv4/tcp_retries2。 执行 ausearch -f /proc/sys/net/ipv4/tcp_retries2 命令查看，可以看到如下的日志 1 2 3 4 5 6 7 8 $ ausearch -f /proc/sys/net/ipv4/tcp_retries1 ---- time-\u0026gt;Mon Mar 28 12:44:48 2022 type=PROCTITLE msg=audit(1648442688.159:6232591): proctitle=76696D002F70726F632F7379732F6E65742F697076342F7463705F7265747269657331 type=PATH msg=audit(1648442688.159:6232591): item=1 name=\u0026#34;/proc/sys/net/ipv4/tcp_retries1\u0026#34; inode=46629229 dev=00:03 mode=0100644 ouid=0 ogid=0 rdev=00:00 objtype=NORMAL cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0 type=PATH msg=audit(1648442688.159:6232591): item=0 name=\u0026#34;/proc/sys/net/ipv4/\u0026#34; inode=8588 dev=00:03 mode=040555 ouid=0 ogid=0 rdev=00:00 objtype=PARENT cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0 type=CWD msg=audit(1648442688.159:6232591): cwd=\u0026#34;/root\u0026#34; type=SYSCALL msg=audit(1648442688.159:6232591): arch=c000003e syscall=2 success=yes exit=3 a0=11687a0 a1=241 a2=1a4 a3=7ffe33dc14e0 items=2 ppid=8375 pid=8629 auid=0 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=pts1 ses=250225 comm=\u0026#34;vim\u0026#34; exe=\u0026#34;/usr/bin/vim\u0026#34; key=(null) 监控文件夹变化 监控文件夹同样采用跟上述文件相同的方式，但有个问题是如果文件夹下内容较多，会一起监控，从而导致audit的log内容过多。\n监控系统定期reboot 执行如下命令：\n1 2 auditctl -w /bin/systemctl -p rwxa -k systemd_call auditctl -a always,exit -F arch=b64 -S reboot -k reboot_call 待系统重启后执行如下命令：\n1 ausearch -f reboot 参考文档 RedHat auditd文档\n","date":"2022-03-28T11:57:28Z","permalink":"/post/linux-auditd/","title":"linux auditd"},{"content":"curl \u0026ndash;local-port：指定源端口号 \u0026ndash;proxy：指定本地代理，例如：http://127.0.0.1:52114 -d：指定body，如果body比较小，可以直接指定-d 'login=emma＆password=123'，也可以通过指定文件的方式 -d '@data.txt' history bash会将历史命令记录到文件.bash_history中，通过history命令可以查看到历史执行的命令。但history在默认情况下，仅会显示命令，不会展示出执行命令的时间。history命令可以根据环境变量HISTTIMEFORMAT来显示时间，要想显示时间可以执行如下的命令：\n1 HISTTIMEFORMAT=\u0026#39;%F %T \u0026#39; history lrzsz CentOS rpm包地址：https://rpmfind.net/linux/centos/7.9.2009/os/x86_64/Packages/lrzsz-0.12.20-36.el7.x86_64.rpm\nps 最常用的为ps -ef和ps aux命令，两者的输出结果差不多，其中ps -ef为System V Style风格，ps aux为BSD风格，现在ps命令两者均支持。\n1 2 3 4 5 6 7 8 9 10 11 $ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND www-data 1 0.0 0.0 212 8 ? Ss Apr09 0:00 /usr/bin/dumb-init -- /nginx-ingress-controller www-data 6 0.9 0.3 813500 100456 ? Ssl Apr09 67:20 /nginx-ingress-controller --publish-service www-data 33 0.0 0.7 458064 242252 ? S Apr09 0:53 nginx: master process /usr/local/nginx/sbin/nginx -c /etc/nginx/nginx.conf www-data 46786 0.0 0.7 459976 239996 ? S 17:57 0:00 rollback logs/eagleeye.log interval=60 adjust=600 www-data 46787 1.4 0.8 559120 283328 ? Sl 17:57 2:07 nginx: worker process www-data 46788 1.1 0.8 558992 284772 ? Sl 17:57 1:38 nginx: worker process www-data 46789 0.0 0.7 452012 237152 ? S 17:57 0:01 nginx: cache manager process www-data 46790 0.0 0.8 490832 267600 ? S 17:57 0:00 nginx: x www-data 47357 0.0 0.0 60052 1832 pts/2 R+ 20:21 0:00 ps aux 每个列的值如下：\n%MEM：占用内存百分比 VSZ: 进程使用的虚拟内存量（KB） RSS：进程占用的固定内存量，驻留在页中的（KB） STAT：进程的状态 TIME：进程实际使用的cpu运行时间 pssh 该工具的定位是在多台主机上批量执行pssh命令。\n将文件存放到文件中 /tmp/hosts 中，文件格式如下： 1 2 192.168.1.1 192.168.1.2 批量执行shell命令：pssh -h /tmp/hosts -A -i \u0026lsquo;uptime\u0026rsquo;。 具体参数说明如下：\n-A: 手工输入密码模式，如果未打通ssh免密，可以在执行pssh命令的时候手工输入主机密码，但要求所有主机密码必须保持一致 rsync 常用命令：\nrsync -avz -e 'ssh -p 50023' ~/git/arkctl root@100.67.27.224:/tm 常用参数：\n--delete: 本地文件删除时，同步删除远程文件 -e 'ssh -p 50023': 指定 ssh 端口号 --exclude=.git: 忽略同步本地的 git 目录 scp -P：指定端口号 strace 跟踪进程的系统调用\n-p：指定进程 -s：指定输出的字符串的最大大小 -f：跟踪由fork调用产生的子进程 wget -P: 当下载文件时，可以指定本地的下载的目录 split split [-bl] file [prefix]\n-b, \u0026ndash;bytes=SIZE：对file进行切分，每个小文件大小为SIZE。可以指定单位b,k,m。\n-C,\u0026ndash;bytes=SIZE：与-b选项类似，但是，切割时尽量维持每行的完整性。\nprefix：分割后产生的文件名前缀。\n拆分文件：split -b 200m ip_config.gzip ip_config.gzip\n文件合并：cat ip_config.gzip* \u0026gt; ip_config.gzip\npython 快速开启一个http server python -m SimpleHTTPServer 8080。\n在python3环境下该命令变更为：python3 -m http.server 8080\n格式化 json: cat 1.json | python -m json.tool\nawk 按照,打印出一每一列 awk -F, '{for(i=1;i\u0026lt;=NF;i++){print $i;}}'\ndocker registry 列出镜像：curl http://127.0.0.1:5000/v2/_catalog?n=1000 查询镜像的tag: curl http://127.0.0.1:5000/v2/nginx/tags/list，如果遇到镜像名类似aa/bb的情况，需要转移一下 curl http://127.0.0.1:5000/v2/aa\\/bb/tags/list socat 向本地的 socket 文件发送数据：echo \u0026quot;test\u0026quot; | socat - unix-connect:/tmp/unix.sock 通过交互的方式输入命令：socat - UNIX-CONNECT:/var/lib/kubelet/pod-resources/kubelet.sock git 删除远程分支 git push origin --delete xxx 强制更新远程分支：git push --force-with-lease origin feature/statefulset 删除本地分支：git branch -D local-branch 拉取远程分支并切换分支：git checkout -b develop origin/remote-branch develop为本地分支，origin/remote-branch为远程分支 给 Github 设置代理 1 2 3 4 5 # 设置代理 git config --global http.https://github.com.proxy socks5://127.0.0.1:13659 # 取消代理 git config --global --unset http.https://github.com.proxy rpm rpm -ivh xx.rpm：用来安装一个 rpm 包 rpm -qa：查看已经安装的包 rpm -ql: 查看已经安装的 rpm 的文件内容 rpm -qpR *.rpm: 查看rpm包的依赖 rpm -e *：要删除的rpm包 cloc 用来统计代码行数\ncloc .： 用来统计当前的代码行数 cloc . --exclude-dir vendor：忽略目录 vendor，--exclude-dir 仅能支持一级目录 cloc . --fullpath --not-match-d=pkg/apis/：用来忽略目录 pkg/apis 下的文件 jq 用来解析 json 格式\nkey 中包含特殊字符，假设文件 data.json 格式如下：\n1 2 3 4 5 { \u0026#34;data\u0026#34;: { \u0026#34;a.json\u0026#34;: \u0026#34;abc\u0026#34; } } 可以使用 cat data.json | jq '.data.\u0026quot;a.json\u0026quot;' 或者 cat data.json | jq '.data[\u0026quot;a.json\u0026quot;]' 的方式来解析其中的内容。\n-r 参数： echo '\u0026quot;{}\u0026quot;' | jq . 输出结果为 \u0026quot;{}\u0026quot; 即对应的格式为字符串。echo -r '\u0026quot;{}\u0026quot;' | jq -r . 输出结果为 {}，已经解析为标准的 json 格式。\n","date":"2022-03-28T11:20:03Z","permalink":"/post/%E5%B8%B8%E7%94%A8shell%E5%91%BD%E4%BB%A4%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/","title":"常用shell命令（持续更新）"},{"content":"\n题图为周末的公园露营区。一周前曾经下过一场雪，地上覆盖着厚厚的一层雪，而一星期过后，上面却扎满了帐篷。城市里的人们，在捂了一个冬季后，终于迎来了阳光明媚的春天。虽内心充满着诗和远方，疫情之下，能约上三五好友，在草地上吃上一顿野餐，亦或在帐篷里美美的睡上一觉已是一件很奢侈的事情。\n资源 1. Submariner Rancher开源的一款k8s集群之间的容器网络打通的工具。k8s社区的网络插件中以overlay的网络插件居多，因为overlay的网络对底层物理网络几乎很少有依赖，通常会采用vxlan、IPIP等协议来实现。虽然overlay的网络插件用起来比较方便，但是两个k8s集群的容器网络通常是无法直接通讯，在多k8s集群的应用场景下比较受限。Submariner提供了容器网络的互通方案。\n2. k8e k8e为Kubernetes Easy的简写。社区里有k3s和k0s项目来提供了k8s精简版，本项目在k3s的基础之上又进一步进行了裁剪，移除了一些边缘场景的特性。\n3. Rancher Desktop k8s的发行版SUSE Rancher提供的k8s的桌面客户端，目前已经发布了1.0.0版本。\n4. nginx config Nginx作为最流行的负载均衡配置软件之一，有自己的一套配置语法。DigitalOcean提供的nginx config工具可以通过UI直接进行配置，并最终可以一键生成nginx的配置文件。\n5. mizu 一款部署在k8s上的流量分析工具，可以认为是k8s版的tcpdump + wireshark。底层的实现也是基于libpcap抓包的方式，可以支持解析HTTP、Redis、Kafka等协议。\n6. kube-bench 互联网安全中心（Center for Internet Security）针对k8s版本提供了一套安全检查的规范，约有200多页的pdf文档，本项目为针对该规范的实现。仅需要向k8s环境中提交一个job，即可得到最终的安全结果。很多公有云厂商也有自己的实现，比如阿里云ACK的实现。\n7. virtual-kubelet virtual kubelet服务通过在k8s集群上创建虚拟node，当一个pod调度到虚拟node时，virtual kubelet组件以插件的形式提供了不同的实现，可以将pod创建在k8s集群之外。比如，在阿里云的场景下，可以将pod创建到弹性容器实例ECI上面，从而达到弹性的目的。该项目除了用于公有云一些弹性的场景外，还常用于边缘计算的场景。\n8. cloudevents CloudEvents定义了一种通用的方式描述事件数据的规范，由CNCF的Serverless工作组提出。阿里云的事件总线EventBridge基于此规范提供了比较好商业化产品。\n相关链接：EventBridge 事件总线及 EDA 架构解析\n9. kubectl-who-can 在k8s系统中，通常会通过RBAC的机制来配置某个账号拥有某种权限，但如果反过来要查询某个权限被哪些账号所拥有，就会麻烦很多。\n该工具是一个k8s的命令行小工具，可以用来解决上述需求。比如查询拥有创建namespace权限的ServiceAccount有哪些，可以直接执行 kubectl-who-can create namespace。\n10. kyverno Kyverno是一款基于k8s的策略引擎工具，通过抽象CRD ClusterPolicy的方式来声明策略，在运行时通过webhook的技术来执行策略。相比于opa \u0026amp; gatekeeper，更加k8s化，但却没有编程语言的灵活性。目前该项目为CNCF的孵化项目。\n文章 1. 进击的Kubernetes调度系统 该系列一共三篇文档，分别讲解了如下内容：\n第一篇：k8s 1.16版本引入的Scheduling Framework 第二篇：阿里云ACK服务基于Scheduling Framework实现的Gang scheduling 第三篇：阿里云ACK服务基于Scheduling Framework实现的支持批任务的Binpack Scheduling 2. 中国的云计算革命尚未开始 作者通过工业革命时代的电气化道路做类似，认为当前云计算的阶段仍然比较初级，并且首先要解决的是人的问题，而不是技术本身。\n","date":"2022-03-19T22:45:32Z","permalink":"/post/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E7%AC%AC15%E6%9C%9F/","title":"技术分享第15期"},{"content":"阿里云上有众多的云产品，本文主要分析云产品的功能以及应用场景。\n容器与中间件 该部分领域与我的工作重合度较高，为重点研究领域。\n事件总线EventBridge 很多服务会产生输出供下游的服务来消费，最常见的解决办法是数据生产者通过rpc调用的方式将消息发送给数据消费者。但假如消费者的数量不止一处，该模式下就需要数据生产者将数据重复发送给多个消费者，该方式对于生产者可配置的灵活性要求非常高。\n为了解决上述问题，将生产者和消费者解耦，解决办法就是引入一个中间层，这也是软件架构中最常见的解决复杂问题的方法。引入的中间层即为消息队列类的服务，比如Kafka。生成者仅负责生产数据到消息队列，消费者负责从消息队列消费数据，且可以存在多个消费者。生成者和消费者根本不用关心彼此，仅需要跟消息队列进行交互就可以了。\n阿里云上的某个产品新增加了一个新的操作，比如ECS主机完成了一次快照操作，都会产生事件。如果有服务要消费事件，可以使用该产品。\n阿里云上的很多产品会将新产生的事件发送到事件总线，另外也支持自定义事件源，通过编程的方式将事件推送到事件源上。\n数据的消费模块跟通常的消息队列有所不同，这里的数据消费需要由事件总线产品主动推送消息到对应的服务，具体要推送到哪些服务，则需要在创建总线的时候配置，该功能即消息路由。支持的消费端包括钉钉、消息队列、Serverless服务、HTTP Server等。可以看到数据的消费端除了HTTP Server外，基本不需要额外开发一个服务，也是阿里云的一些其他云产品。\n消息的规范完成遵循云原生社区CNCF的CloudEvent规则。\n相关链接：\nCloudEvents规范 弹性容器实例ECI 该产品的功能比较简单，相当于提供了管控页面来创建k8s的pod，具体pod部署在哪里用户不需要关心，提供了非常好的弹性能力，充分发挥了云的优势。\n具体在实现层面，实际上会以pod的形式部署在阿里云维护的公共k8s集群中，且容器的网络在用户指定的vpc中。\n除了给用户提供直接创建容器实例外，还有很大一部分功能是给Serverless Kubernetes（ASK）和容器服务（ACK）来提供弹性扩缩容的功能。\n企业级分布式应用服务EDAS 提供了应用托管和微服务的治理能力。\n在应用托管方面支持应用发布到虚拟机和k8s两种方式。\n微服务治理方面支持了Spring Cloud、Dubbo、HSF三种微服务框架。\nServerless应用引擎SAE 提供了类似于knative的serving功能，可以支持应用的托管，用户不需要关心底层的服务器资源，可以自动将用户的应用部署在托管的k8s集群中，且具备秒级的弹性扩缩容的功能。\nServerless工作流SWF ","date":"2022-03-19T22:19:24Z","permalink":"/post/%E9%98%BF%E9%87%8C%E4%BA%91%E4%BA%A7%E5%93%81%E5%88%86%E6%9E%90%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/","title":"阿里云产品分析（持续更新）"},{"content":"软件工程 康威定律 任何一个组织在设计一个系统的时候，这个系统的结构与这个组织的沟通结构是一致的。 工作了这么些年对此深有感触，即“组织架构决定软件架构”。\n布鲁克定律 在一个已经延期的项目中增加人手只会让项目延期更长。 我个人不是特别认可此定律，该定律肯定是项目而定的，这要看项目的协作复杂程度，如果是体力劳动居多的项目，那么堆人还是特别好使的。\n帕金森定律 一项工作会占用掉所有用来完成它的时间。即如果不给一个项目设置截止日期，那么该项目就永远完成不了。安排多少时间，就会有多少工作。\n冰山谬论 一款新软件的开发成本只占管理层预算的总成本的25%左右。\n其他 黄金圈法则 著名的营销顾问西蒙斯.涅克提出了一个“黄金圈”理论：三个同心圆，最里面的一个是Why,中间一层是How，最外面一层是What。\n大多数人的思维方式是想做什么（what）和怎么做（how），不太考虑为什么这么做（why）。\n本理论提倡的思维方式为：\nWhy：最内层——为什么，做一件事的原因或目的，也可以说是理念和宗旨，属于战略层面； How：中间层——怎么做，针对这个目的或理念的计划，也即如何去做好这件事情，属于战术层面； What：最外层——是什么，最终得到什么，或者要做哪些具体的事，这基本是事情的表象，主要是执行层面的东西。 该法则在软件行业的述职晋升等场景下非常适用。\nWhy：描述为什么做这个项目？ How：做这个项目遇到的挑战有哪些，是怎么解决的。挑战和解决方法可以一一对应起来。 What：项目的最终结果，最好有具体的可以量化的指标。 SWOT分析法 常见的战略分析方法，对研究对象进行全面、系统、准确的研究。\n金字塔原理 参见《金字塔原理总结》\n成功的面试 = 把握正确清晰的用人标准 + 挖掘真实匹配的应聘者信息 = 以素质模型去“发问” + 用STAR方式去“追问”\nSTAR行为面试法 STAR是业界公认的最为有效的面试方法之一，为背景（Situation）、任务（Task）、行为（Action）、结果（Result）的缩写。该方法不仅用于面试的场合，也会用于述职、晋升答辩等场景。\n任务（Task）描述在事情里的担任的角色和负责的任务。\n行为（Action）是最关键部分，要了解做了什么，展现出了哪些能力。\n结果（Result）部分通常需要虚实结合，且重点在实，围绕效率、效果、质量和成本四个维度量化评估。\nSTAR方法同样适用于述职汇报或者晋升中。\n奥克姆剃刀理论 如无必要，勿增实体。\n马斯洛需求层次理论 心理学中重要理论，将人类的需求分为五个层级：\n生理 安全 社交 尊重 自我实现 人类的需求为逐步递进的，在满足了基本需求后，就会去实现更高的需求和目标。\n在工作中，经常会用类似马斯洛需求层次理论中的金字塔结构来解释一些其他的有递进关系的场景 ，比如一个软件产品的设计目标。\nSMART 原则 确定目标的五原则，通常用在绩效考核中。\nS（Special）：目标必须是具体的 M（Measurable）：目标必须是可衡量的 A（Attainable）：目标必须是可实现的 R（Relevant）：与其他的目标有一定的相关性 T（Time-bound）：目标必须有完成的期限 常见名词 ROI：投入产出比\n","date":"2022-03-19T00:23:24Z","permalink":"/post/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E7%BB%8F%E5%B8%B8%E7%94%A8%E5%88%B0%E7%9A%84%E7%90%86%E8%AE%BA%E6%88%96%E6%B3%95%E5%88%99/","title":"工作中经常用到的理论或法则"},{"content":"pod状态 1. 前置检查 在排查异常状态的pod错误之前，可以先检查一下node状态，执行kubectl get node查看是否所有的node状态都正常。\n2. pod状态为CrashLoopBackOff 如果pod状态为CrashLoopBackOff状态，查看pod日志来定位原因，或者describe pod看一下。\n3. pod状态为Pending pod状态为Pending状态，说明调度失败，通常跟污点、标签、cpu、内存、磁盘等资源相关。\n可以通过kubectl describe pod -n {NAMESPACE} {POD_NAME}，可以在最后的Event部分找到原因。\n4. pod状态为Init:0/1 有些pod会有Init Containers，这些container是在pod的containers执行之前先执行。如果Init Container出现未执行完成的情况，此时pod处于Init状态。\n通过kubectl get pod -n {NAMESPACE} {POD_NAME} -o yaml 找到pod的Init Containers，并找到其中的name字段。执行kubectl logs -n {NAMESPACE} {POD_NAME} -c {INIT_CONTAINER_NAME}可以查看Init Container的日志来分析原因。\n5. pod状态为Terminating pod处于此种状态的原因大致可分为： 1、pod或其控制器被删除。 解决方法：查看pod控制器类型和控制器名称，查看其控制器是否正常。如果正常pod将会被重建，如果pod没有被重建，查看controller-manager是否正常。 2、pod所在节点状态NotReady导致。 解决方法：检查该节点的网络，cpu，内存，磁盘空间等资源是否正常。检查该节点的kubelet、docker服务是否正常。检查该节点的网络插件pod是否正常。\n最常见的pod处于Terminating状态的解决办法为强制删除 kubectl delete pods -n ${namespace} ${name} --grace-period=0 --force\n6. pod状态为Evicted pod处于Evicted的原因大致可分为： 1、kubelet服务启动时存在驱逐限制当节点资源可用量达到指定阈值（magefs.available\u0026lt;15%,memory.available\u0026lt;300Mi,nodefs.available\u0026lt;10%,nodefs.inodesFree\u0026lt;5%） 会优先驱逐Qos级别低的pod以保障Qos级别高的pod可用。 解决方法：增加节点资源或将被驱逐的pod迁移到其他空闲资源充足的节点上。 2、pod所在节点上被打上了NoExecute的污点，此种污点会将该节点上无法容忍此污点的pod进行驱逐。 解决方法：查看该节点上的NoExecute污点是否必要。或者pod是否可以迁移到其他节点。 3、pod所在的节点为NotReady状态\n通常可以通过kubectl describe pods ${pod} -n ${namespace}的底部的Events信息来找到一些问题的原因。例如下面例子中可以看到DiskPressure信息，说明跟磁盘相关。\n1 2 3 4 5 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Evicted 61s kubelet, acs.deploy The node had condition: [DiskPressure]. Normal Scheduled \u0026lt;invalid\u0026gt; default-scheduler Successfully assigned ark-system/bridge-console-bridge-console-554d57bb87-nh2vd to acs.deploy 或者根据pod的Message字段来找到原因\n1 2 3 4 5 6 7 8 9 10 11 12 Name: tiller-deploy-7f6456894f-22vgr Namespace: kube-system Priority: 0 Node: a36e04001.cloud.e04.amtest17/ Start Time: Mon, 08 Jun 2020 12:17:26 +0800 Labels: app=helm name=tiller pod-template-hash=7f6456894f Annotations: \u0026lt;none\u0026gt; Status: Failed Reason: Evicted Message: Pod The node had condition: [DiskPressure]. 由于pod驱逐的原因排查跟时间点相关，需要根据pod被驱逐的时间来分析当时的状态。 4、批量删除状态Evicted的pod，此操作会删除集群里面所有状态为Evicted的pod\n1 2 ns=`kubectl get ns | awk \u0026#39;NR\u0026gt;1 {print $1}\u0026#39;` for i in $ns;do kubectl get pods -n $i | grep Evicted| awk \u0026#39;{print $1}\u0026#39; | xargs kubectl delete pods -n $i ;done 7. pod状态为Unknown 通常该状态为pod对应节点的为NotReady，通过查看 kubectl get node 来查看node是否为NotReady。\n8. pod为running，但是Not Ready状态 1 argo-ui-56f4d67b69-8gshr 0/1 Running 0 10h 类似上面这种状态，此时说明pod的readiness健康检查没过导致的，需要先从pod的健康检查本身来排查问题。可以通过 kubectl get pods -n ${namespace} ${name} -o yaml 找到pod的健康检查部分，关键字为readiness，然后进入pod中执行对应的健康检查命令来测试健康检查的准确性。\n例如readiness的配置如下，需要进入pod中执行curl http://127.0.0.1/api/status，也可以在pod对应的node节点上执行curl [http://${pod_ip}/api/status](http://127.0.0.1/api/status)\n1 2 3 4 5 6 7 8 9 10 livenessProbe: failureThreshold: 3 httpGet: path: /api/status port: http scheme: HTTP initialDelaySeconds: 120 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 9. pod为ContainerCreating状态 通过kubectl describe pods -n ${namespace} ${name}的Events部分来分析原因，可能的原因如下：\n网络分配ip地址失败 10. Init:CrashLoopBackOff 通过kubectl get pod -n {NAMESPACE} {POD_NAME} -o yaml 找到pod的Init Containers，并找到其中的name字段。 执行kubectl logs -n {NAMESPACE} {POD_NAME} -c {INIT_CONTAINER_NAME}可以查看Init Container的日志来分析原因。 11. PodInitializing 需要查看initContainer的日志\n12. MatchNodeSelector 查看pod的status信息可以看到如下信息：\n1 2 3 4 5 status: message: Pod Predicate MatchNodeSelector failed phase: Failed reason: MatchNodeSelector startTime: \u0026#34;2022-03-15T05:07:57Z\u0026#34; 说明该pod没有调度成功，在predicate的MatchNodeSelector阶段失败了，没有匹配上node节点。\n在k8s 1.21之前的版本，存在bug，节点重启后可能遇到过问题，将pod delete后重新调度可以解决。https://github.com/kubernetes/kubernetes/issues/92067\n参考 常见的Pod异常状态及处理方式 ","date":"2022-03-17T15:54:47Z","permalink":"/post/k8s-pod%E5%BC%82%E5%B8%B8%E7%8A%B6%E6%80%81%E5%88%86%E6%9E%90/","title":"k8s pod异常状态分析"},{"content":"排查k8s上问题通常需要监控的配合，而k8s上的监控标准为prometheus，prometheus的dashboard最通用的为grafana。本文用来记录排查k8s问题时经常遇到的dashboard，dashboard监控的数据来源包括node-exporter、metrics-server、kube-state-metrics等最场景。\nnode top监控 下载链接\nnode上的pod监控 下载链接\n","date":"2022-03-14T23:09:31Z","permalink":"/post/%E7%94%A8%E6%9D%A5%E6%8E%92%E6%9F%A5k8s%E9%97%AE%E9%A2%98%E7%9A%84%E5%B8%B8%E7%94%A8grafana-dashboard/","title":"用来排查k8s问题的常用grafana dashboard"},{"content":"正向代理通常用在远程访问某个环境中的。常见的正向代理工具包括squid、nginx、3proxy。\nsquid 老牌的正向代理工具。\n安装：yum install squid \u0026amp;\u0026amp; systemctl start squid\nsquid默认会监听在3128端口号。\n缺点：如果修改了本地的/etc/hosts文件，则需要重启squid后才可以更新。\n3proxy 官方并没有提供yum的安装方式，比较简单的运行方式是以docker的形式。\n执行如下的命令，即可开启3128端口作为http代理，3129端口作为sock5代理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 mkdir /etc/3proxy cat \u0026gt; /etc/3proxy/3proxy.cfg \u0026lt;\u0026lt;EOF log /var/log/3proxy.log D logformat \u0026#34;- +_L%t.%. %N.%p %E %U %C:%c %R:%r %O %I %h %T\u0026#34; rotate 7 auth none flush allow somepu maxconn 200 # starting HTTP proxy with disabled NTLM auth ( -n ) proxy -p3128 -n # starting SOCKS proxy socks -p3129 -n EOF docker run -d --restart=always -p 3128:3128 -p 3129:3129 --net=host -v /var/log:/var/log -v /etc/3proxy/3proxy.cfg:/etc/3proxy/3proxy.cfg --name 3proxy 3proxy/3proxy 设置代理 终端设置代理 shell支持如下的代理环境变量:\n1 2 export http_proxy=http://localhost:1080 export https_proxy=http://localhost:1080 如果是 socks5 代理同样可以使用上述两个环境变量：\n1 2 export http_proxy=socks5://localhost:1080 export https_proxy=socks5://localhost:1080 相关链接 https://github.com/3proxy/3proxy ","date":"2022-03-12T20:42:50Z","permalink":"/post/%E6%AD%A3%E5%90%91%E4%BB%A3%E7%90%86/","title":"正向代理"},{"content":"ssh命令常用参数 指定私钥连接：ssh -i ~/.ssh/id_rsa ido@192.168.1.111 -p 7744\n免密登录 用来两台主机之间的ssh免密操作，步骤比较简单，主要实现如下两个操作：\n生成公钥和私钥 将公钥copy到要免密登录的服务器 生成公钥和私钥 执行 ssh-keygen -b 4096 -t rsa 即可在 ~/.ssh/目录下生成两个文件id_rsa和id_rsa.pub，其中id_rsa为私钥文件，id_rsa.pub为公钥文件。\n将公钥copy到要免密登录的服务器 执行 ssh-copy-id $user@$ip 即可将本地的公钥文件放到放到要免密登录服务器的 $HOME/.ssh/authorized_keys 文件中。至此，免密登录的配置就完成了。\n也可以不使用 ssh-copy-id 命令，而是手工操作完成。\n在客户端机器上执行 cat .ssh/id_rsa.pub 获取到公钥信息。 在服务端机器上执行 vim ~/.ssh/authorized_keys，将上面的公钥信息追加到文件的尾部。如果目录 .ssh 不存在，需要先创建。 执行 chmod 700 ~/.ssh; chmod 600 ~/.ssh/authorized_keys 给文件和目录设置合理的权限。 ssh隧道 动态端口转发：执行 ssh root@xxxx -ND 127.0.0.1:1080 即可在本机的1080端口开启一个ssh隧道。\nrsync rsync的常用命令：\n1 rsync -avzP --delete $local_idr $user@$remote:$remote_dir ","date":"2022-03-12T17:55:00Z","permalink":"/post/ssh%E5%8D%8F%E8%AE%AE-%E6%95%88%E7%8E%87/","title":"ssh协议 - 效率"},{"content":"OpenKruise是阿里云开源的一系列基于k8s的扩展组件的集合，其中包含了像增强版的workload、sidecar容器管理、高可用性防护等特性，包含了很多的“黑科技”。\n如果k8s的kube-controller-manager组件可以提供非常强的扩展能力，可以实现自定义的Deployment、StatefulSet的controller，而不是使用原生的kube-controller-manager的功能，类似于实现自定义的调度器扩展功能。那么很有可能OpenKruise的实现方案就不再会采用CRD扩展的方式，而是直接在原生的Deployment、StatefulSet等对象上通过annotation的方式来实现。\n安装 可以直接使用helm的方式安装\n1 2 helm repo add openkruise https://openkruise.github.io/charts/ helm install kruise openkruise/kruise --version 1.0.1 安装完成后，可以看到在kruise-system下创建了一个DeamonSet和一个Deployment。并且安装了很多的CRD和webhook组件。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ kubectl get pod -n kruise-system NAME READY STATUS RESTARTS AGE kruise-controller-manager-67878b65d-cv6f4 1/1 Running 0 92s kruise-controller-manager-67878b65d-jrmnd 1/1 Running 0 92s kruise-daemon-ktwvv 1/1 Running 0 92s kruise-daemon-nf84r 1/1 Running 0 92s kruise-daemon-rjs26 1/1 Running 0 92s kruise-daemon-vghw4 1/1 Running 0 92s $ kubectl get crd | grep kruise.io advancedcronjobs.apps.kruise.io 2022-03-05T13:21:39Z broadcastjobs.apps.kruise.io 2022-03-05T13:21:39Z clonesets.apps.kruise.io 2022-03-05T13:21:39Z containerrecreaterequests.apps.kruise.io 2022-03-05T13:21:39Z daemonsets.apps.kruise.io 2022-03-05T13:21:39Z imagepulljobs.apps.kruise.io 2022-03-05T13:21:39Z nodeimages.apps.kruise.io 2022-03-05T13:21:39Z podunavailablebudgets.policy.kruise.io 2022-03-05T13:21:39Z resourcedistributions.apps.kruise.io 2022-03-05T13:21:39Z sidecarsets.apps.kruise.io 2022-03-05T13:21:39Z statefulsets.apps.kruise.io 2022-03-05T13:21:39Z uniteddeployments.apps.kruise.io 2022-03-05T13:21:39Z workloadspreads.apps.kruise.io 2022-03-05T13:21:39Z $ kubectl get validatingwebhookconfigurations kruise-validating-webhook-configuration NAME WEBHOOKS AGE kruise-validating-webhook-configuration 17 17m $ kubectl get mutatingwebhookconfigurations kruise-mutating-webhook-configuration NAME WEBHOOKS AGE kruise-mutating-webhook-configuration 11 17m 功能 大类 子类 描述 通用工作负载 CloneSet 定位是用来代替k8s的Deployment，但做了很多能力的增强。增强的功能点：\n1. 支持声明pvc，给pod来申请pv。当pod销毁后，pvc会同步销毁。\n2. 指定pod来进行缩容。\n3. 流式扩容，可以指定扩容的步长等更高级的库容特性。4. 分批灰度。\n5. 通过partition回滚。\n6. 控制pod的升级顺序。\n7. 发布暂停。\n8. 原地升级自动镜像预热。\n9. 生命周期钩子。pod的多个声明周期之间可以读取finalizer，如果finalizer中有指定的值，则controller会停止。该行为作为k8s的一种hook方式，用户可以自定义controller来控制finalizer的行为。 通用工作负载 Advanced StatefulSet 用来取代k8s原生的StatefulSet，很多增强特性跟CloneSet比较类似。\n1. 原地升级。\n2. 升级顺序增强。\n3. 发布暂停。\n4. 原地升级自动预热。\n5. 序号跳过。StatefulSet创建的pod的后缀会从0开始依次累加，可以指定某个特定的序号跳过。\n6. 流式扩容。 通用工作负载 Advanced DaemonSet 用来取代k8s原生的DaemonSet。1. 热升级\n2. 暂停升级\n任务工作负载 BroadcastJob agent类型的job，每个节点上都会执行 任务工作负载 AdvancedCronJob 原生的CronJob的扩展版本，可以周期性创建BroadcastJob。 Sidecar容器管理 SidecarSet 用来管理Sidecar容器，其最核心的功能是支持在pod不重启的情况下Sidecar容器的热升级 多区域管理 WorkloadSpread 将workload按照不同的策略来打散，随着k8s功能不断完善，部分功能k8s已经具备。支持Deployment、ReplicaSet、CloneSet。 多区域管理 UnitedDeployment k8s集群内可能存在不同种类型的node，该特性通过UnitedDeployment对象来管理将一个workload的不同pod分发到不同类型的节点上，并且可以指定不同类型节点的pod副本数。 增强运维 重启一个pod中的某个容器 该特性依赖于kurise-daemon组件实现，通过将容器进程停掉，kubelet检测到容器停掉后会自动将容器拉起。停掉容器的方式跟kubelet实现一致。 增强运维 镜像预热 通过ImagePullJob CR提供操作入口，底层的实现通过调用CRI的pod image接口实现 增强运维 控制pod中容器的启动顺序 kruise创建一个ConfigMap，并在pod中注入来挂载该ConfigMap，每个容器使用ConfigMap中的不同key。kruise依次往CM中增加key来实现控制容器启动顺序的目的。 增强运维 资源分发ResourceDestribution 可以跨namespace来分发CM、Secret，保证每个namespace下均有 应用安全防护 资源删除防护 通过webhook技术实现 应用安全防护 PodUnavailableBudget 通过webhook实现的k8s原生的pdb能力的增强，覆盖pdb不具备的场景 资料 https://openkruise.io/zh/docs/installation 如何基于 OpenKruise 打破原生 Kubernetes 中的容器运行时操作局限？ ","date":"2022-03-08T21:14:34Z","permalink":"/post/openkruise%E8%B0%83%E7%A0%94/","title":"OpenKruise调研"},{"content":"阿里云容器服务是阿里云公有云基于Kubernetes企业级服务，在社区的Kubernetes版本基础上有能力增强， 本文用于调研社区增强功能，记录解决的问题、以及实现方法。ACK集群的增强功能有一部分是基于Kubernetes的api的prodiver实现，另外一部分是基于Kubernetes增加的额外组件，其中很多都已经开源，可以看到开源软件列表。\nACK的一些组件列表可以参见：组件概述\n产品形态 专有版Kubernetes：master和worker阶段均需要创建 托管版Kubernetes：只需要创建worker节点，master节点通过ack托管 Serverless Kubernetes：master节点和worker节点均不需要自己创建 节点管理 大类 特性 描述 节点 节点自动扩缩容 完全利用k8s的autoscaler实现，提供了白屏的配置功能。https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md?spm=a2c4g.11186623.0.0.5e09135f2AAa2u\u0026amp;file=FAQ.md 节点资源变配 master和worker节点的资源变配，通过调用ecs的变配规格接口来实现。 节点池 将节点进行了分组，同一个分组内的节点可以统一来管理。比如，可以统一设置标签污点、设置期望节点数。通过节点池实现，节点池跟弹性伸缩组为一对一关系。 弹性伸缩 类型 特性 描述 调度层 hpa 基于k8s的hpa功能实现 自定义指标的hpa 基于prometheus-adapter实现的自定义指标监控 vpa 适用于大型单体应用。k8s的vpa功能实现，基于cluster-autoscaler实现 CronHPA 定期对pod进行伸缩，组件已开源：kubernetes-cronhpa-controller ElasticWorkload 资源层 节点自动伸缩 k8s的node自动伸缩，基于社区的cluster-autoscaler实现 使用ECI弹性调度 通过virtual-kubelet实现，将ECI抽象为k8s的node节点 安全 大类 特性 详细描述 操作系统 基于Alibaba Cloud Linux 2支持等保2.0三级加固 基于Alibaba Cloud Linux 2支持CIS安全加固 基础设施 使用阿里云KMS提供Secret的落盘加密功能 在默认情况下，k8s的Secret是基于base64转码后明文存储的，存在一定的安全风险。k8s提供了使用外部的KMS provider来对Secret的数据进行加密的功能，apiserver和provider之间的通讯协议采用grpc接口。Secret中的数据在经过KMS provider加密后存储在etcd中。ACK借助阿里云的秘钥管理服务（KMS）提供了provider实现，该provider完全开源。 为pod动态配置阿里云产品白名单 ack-kubernetes-webhook-injector组件会自动将pod ip添加到阿里云产品的白名单中。 k8s审计日志白屏化展示 安全巡检功能 基于CIS Kubernetes基线的实现，用来校验k8s的安全性 容器 配置容器安全策略 基于opa实现的策略引擎，可以根据预置的规则对k8s中创建的资源进行校验，校验不通过会返回失败 通过巡检来检查集群中存在的安全隐患的pod 无 可观测性 大类 功能项 描述 日志 日志采集功能基于logtail实现，可以采集容器日志 类似于开源组件log-pilot，仅需要配置环境变量，即可对日志进行收集。也可以通过AliyunLogConfig CR旁路的对日志采集进行配置。 日志 coredns日志 收集coredns日志 监控 基于arms产品支持应用性能监控 监控 基于ahas产品实现的架构感知监控 监控 node节点异常监控 npd将节点异常信息产生k8s的event 监控 k8s event监控 kube-eventer收集k8s的event 操作系统 Container OS：为容器场景而生的操作系统，操作系统镜像大大精简，提供了安全加固能力，不支持单个软件包的升级，软件包只能跟操作系统一起原子升级。 安全容器katacontainer\n容器\u0026amp;镜像 大类 特性 描述 镜像 容器镜像服务ACR 使用公有云的容器镜像服务ACR 验证容器镜像 基于开源组件kritis的kritis-validation-hook组件通过webhook的方式对镜像进行验证，确保镜像安全 调度 大类 特性 解决问题 适用场景 具体实现 pod调度 使用Descheduler组件对Pod进行调度优化 k8s的调度为静态的，可以确保在pod调度时为最优的，但当集群运行一段时间后，集群中资源水位会发生变化，此处无法保证整个集群是最优的。 使用社区的开源组件Descheduler对pod进行重新调度 cpu调度 cpu拓扑感知调度 k8s的cpu manager特性解决的是pod调度到同一个节点上后，通过cpuset来隔离pod的cpu争抢。但却缺乏集群级别的资源视角，从而无法做到全局最优。该特性解决cpu密集型的pod调度到同一个节点上后的争抢问题，允许不是Guaranteed级别的pod实现cpuset特性。 cpu敏感型应用 通过调度器扩展来实现pod的cpu优化调度。ack-slo-manager agent负责实现每台机器上的绑核策略。 cpu Brust策略优化容器性能 k8s的cpu limit机制会在特定的时间段内将进程可以使用的时间片进行限制。但该特性不太适合一些突发类的应用，会导致这类应用在想要cpu的时间不够用，但不想用的时候却有空闲的时间片。 cpu突发型应用 ack通过slo-manager实现cpu brust的优化，该组件会监控容器的cpu throtteld状态，并且动态调整容器中的cgroup cfs quota限制。每个节点上会部署一个resource-controller的组件来动态修改pod的cgroup配置。 动态调整pod的资源上限 k8s中要想修改pod的limit配置，只能修改pod的yaml，此时一定会导致pod重建。对于想调整pod的limit限制，却又不想重启pod的场景。 通过一个Cgroups的CR来对pod使用的cpu、内存以及磁盘的上限进行动态调整，但并不会修改pod的yaml配置，因此不会导致pod的重建。每个节点上会部署一个resource-controller的组件来动态修改pod的cgroup配置。 通过控制L3 cache和MBA提高不同优先级任务的隔离能力 不同优先级的任务调度到同一台机器上存在L3 Cache和内存带宽的资源争抢的问题 cpu敏感型应用 底层利用Intel的RDT技术来实现，该技术可以跟踪和控制同一台机器上同时运行的多个应用程序的共享资源情况，比如L3 Cache、内存带宽。每个节点上会部署一个resource-controller的组件来应用RDT的控制。 控制动态水位提高不同优先级pod的资源利用效率 为了充分利用机器资源，通常将不同优先级的pod部署在同一个节点上，pod的优先级往往随着时间段而有所变化。在白天的时候，在线业务pod优先级高；晚上离线任务优先级高。该特定可以调整一个节点上pod可以使用的资源上限。 在线业务pod和离线业务pod混部 通过ConfigMap来配置一台机器上的在线业务和离线业务可以使用的资源比例。通过Cronjob的方式来修改ConfigMap的配置，从而达到变配的效果。 动态资源超卖 一台机器上的pod在设计request和limit的时候总会预留一部分buffer，如果将所有pod的buffer加起来就非常多，从而导致机器的资源利用率很难上去。 pod预留buffer 引入了reclaimed资源来解决，未完全理解实现。文档：动态资源超卖 gpu调度 GPU拓扑感知调度 共享GPU调度 GPU核共享 FPGA调度 暂未深入研究 任务调度 Gang scheduling Coscheduling将N个pod调度到M个节点上同时运行，需要部分pod同时启动该批处理任务即可运行。如果允许的部分pod为N时，则退化为Gang Scheduling。该场景要求pod要同时创建。 批处理任务 基于Scheduling Framework实现的自定义调度器，核心机制是借助了Permit插件的pod延迟绑定功能，等到同一个group下的pod都创建后再调度。参考文章：支持批任务的Coscheduling/Gang scheduling Binpack scheduling k8s默认的调度策略会将pod优先分配到空闲的节点上，但这样集群中的节点会存在资源碎片化的问题。Binpack调度策略会优先将节点的资源用完。 减少资源碎片化，尤其是GPU场景 基于Scheduling Framework实现的自定义调度器。参考文章：支持批任务的Binpack Scheduling Capacity Scheduling k8s支持的namespace ResourceQuota特性可以设置一个namespace下的pod可以使用的资源上限，但不够灵活。比如一个namespace下资源耗尽，另外一个namespace还有额外的资源，但这部分资源却不能给资源耗尽的namespace使用。 namespace ResourceQuota特性增强 使用ElasticQuotaTree的方式来定义每个namespace下可以使用的资源最小值和资源上限，配合Scheduling Framework扩展来实现。 弹性调度 ECI弹性调度 充分利用ECI的弹性功能，解决k8s的node资源不够灵活的问题。 将pod调度到ECI virtual kubelet技术将ECI抽象为k8s的node，并将pod指向调度到该node。调度到该node的pod最终会在ECI拉起容器。 自定义资源的优先级调度 k8s的调度器的策略采用固定的算法，会将所有node一视同仁，并不能针对某种类型的节点采用不同的策略。 自定义基于node调度策略 引入CRD ResourcePolicy用来定义节点调度的优先级。 负载感知调度 负载感知调度 在pod调度时，参考node节点历史的负载信息，优先将pod调度到负载较低的节点上，避免出现单个节点负载过高的情况。 避免node的资源使用率不均 通过调度器扩展实现，pod要开启该特性需要增加特性的annotation。 网络 大类 特性 描述 容器网络 terway网络 基于ENI实现，支持ipvlan模式，基于ipvlan和eBPF实现。NetworkPolicy基于eBPF实现。 terway网络的Hubble组件 基于eBPF实现的网络流量可视化 为pod挂载独立公网ip pod声明annotation k8s.aliyun.com/pod-with-eip:\u0026ldquo;true\u0026rdquo; Service网络 ccm跨集群部署服务 同一个vip可以挂载到两个k8s集群内部的Service Ingress 基于Nginx Ingress实现灰度发布 扩展Nginx Ingress实现 通过AHAS支持流控特性 基于Nginx Ingress实现流量复制 基于ALB实现了ALB Ingress 数据面直接使用ALB的七层负载均衡功能 DNS 引入kubernetes项目中的addon组件NodeLocal DNSCache来增加cache层 NodeLocal DNS Cache nodelocal dns cache位于kubernetes项目中，以DaemonSet的方式运行在k8s集群中 ExternalDNS服务 用来将DNS注册到外部的公共域名服务器 使用DNSTAP Analyser诊断异常 s使用CoreDNS DNSTAP Analyser组件来接收coredns的DNS解析报文格式dnstap协议，并最终可以输出到sls对异常的DNS解析报文进行分析。 存储 存储为ACK的一大亮点，借助云的丰富存储类型，通过k8s的CSI插件机制提供了块存储、文件存储、对象存储OSS和本地存储的支持。\n本地存储 LVM数据卷功能，基于lvm来动态创建pv QuotaPath，基于ext4的quota特性实现的本地存储的quota隔离功能 内存数据卷 持久化内存技术 云盘存储卷 支持云盘的在线扩容，k8s 1.16版本之前可以手工扩容磁盘，但是pvc保持必变。在k8s 1.16之后，在pvc修改后，可以自动完成云盘的扩容。 可根据磁盘的使用水位支持云盘的自动扩容，该功能通过额外的组件storage-operator来实现，策略存放到了额外的CRD StorageAutoScalerPolicy。跟k8s的hpa和vpa功能相比，该功能没有自动缩容的功能。 使用k8s的存储快照功能实现了存储快照。k8s定义了VolumeSnapshotContent（类似pv）、VolumeSnapshot（类似pvc）和VolumeSnapshotClass（类似StorageClass）三个类型来实现打快照功能，快照的恢复则借助pvc的spec.dataSource字段实现。 加密云盘功能，同样借助云上能力实现。使用时，仅需要在StorageClass的parameters参数中指定加密参数即可。 Serverless Kubernetes（ASK） 适用场景：\n业务要求高弹性，如互联网在线服务存在波峰波谷特别明显 ACK提供了k8s以及k8s的管理功能，其中k8s的master节点需要单独创建和维护，每个k8s集群使用的资源是完全独立的。在ASK中，用户仅需要创建k8s集群，而不需要关心k8s集群的核心组件具体是怎么创建的。\n在实际上，k8s的核心组件如etcd、kube-apiserver、kube-scheduler、kube-controller-manager可能是以pod的形式运行在另外的k8s集群之上，即所谓的k8s on k8s（KOK）的方案。而且这部分组件是多个k8s集群混部的，对用户完全屏蔽了实现细节，用户仅需要聚焦在如何使用k8s即可。\n对于k8s的node节点，用户同样不需要创建，可以认为k8s的节点资源是无限多的。ask采用了virtual kubelet的技术创建了一个虚拟的k8s node节点 virtual-kubelet-${region}-${zone}，整个k8s集群仅有一个节点。因为只有一个k8s节点，实际上k8s的管控作用会大大弱化，尤其是kube-scheduler，因为只有一个k8s节点，不存在pod调度的问题。\n用户创建的pod资源，实际上会部署在阿里云产品弹性容器实例ECI上。新创建ask集群完成后，再到ECI上即可看到有默认的容器创建出来。ECI了创建容器的功能，给用户暴露的功能是跟k8s无关的，而ask相当于是给用户提供了以k8s的方式来创建容器。\nask还集成了knative，用户可以使用knative的Serving和Eventing的功能来实现社区通用的serverless服务。\n相关链接：\n全新的网关能力增强 分布式云容器平台ACK One ack one提供了两个相对独立的功能，一个是第三方k8s集群的注册，另外一个是k8s多集群的管理和应用发布。两个功能的入口也未统一，其中一个是在ack界面，另外一个是在分布式云容器平台ack one。\n第三方k8s集群的注册，允许用户将自己的k8s集群注册到ack上，可以从ack上来管理用户的k8s集群，而且可以部署一些ack自己的组件到用户的k8s集群上面。需要在用户的k8s集群部署一个ack-cluster-agent的服务，在用户的k8s集群跟ack可达的情况下，即可完成用户k8s集群的托管。\nk8s多集群的管理功能，底层直接使用了kubevela项目，实现了k8s多集群的管理、多集群的应用发布功能。\n其他 集群成本分析\n","date":"2022-03-08T19:54:17Z","permalink":"/post/%E9%98%BF%E9%87%8C%E4%BA%91%E5%AE%B9%E5%99%A8%E6%9C%8D%E5%8A%A1ack%E6%8A%80%E6%9C%AF%E8%B0%83%E7%A0%94%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0/","title":"阿里云容器服务ack技术调研（个人笔记）"},{"content":"大页内存介绍 一个操作系统上一台机器的物理内存是有限的，而操作系统上却运行着大量的进程，为了对进程屏蔽物理内存的差异，操作系统引入了虚拟内存的概念。Linux系统中虚拟内存是按照页的方式来进行管理的，每个页的默认大小为4K。如果物理内存非常大，就会导致虚拟内存和物理内存映射的页表（TLB）非常大。为了减少页表，一个可行的方法为增加页的大小。 ​\n可以通过如下命令来查看默认页大小：\n1 2 $ getconf PAGE_SIZE 4096 在对内存要求特别严格的场景下，比如很多数据库的场景，都有巨页的需求，比如将页设置为1GB，甚至几十GB。 ​\n通常在大页内存的场景下，会将Linux的swap功能关闭，避免当页换出时导致的性能下降。 ​\n在Linux中Huge Page有2MB和1GB两种规则，其中2MB适用于GB级别的内存，1GB适用于TB级别的内存。 ​\n大页内存可以分为静态大页和透明大页两类。\n静态大页需要用户自行控制大页的分配、释放和使用。但该方式需要事先配置大页内存的量，因此用起来不够灵活。配置可以在系统启动的时候配置hugepages（页数量）和hugepagesz（页大小）两个参数。也可以使用修改内核参数的方式来预留。 透明大页由操作系统的后台内核线程khugepaged控制大页的分配、释放和使用。 静态大页内存操作 预留大页内存\n1 echo 20 \u0026gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages 可以通过/proc/meminfo文件系统看到大页内存的使用情况\n1 2 3 4 5 6 7 # cat /proc/meminfo | grep Huge AnonHugePages: 2037760 kB HugePages_Total: 20 # 预先分配的大页数量 HugePages_Free: 20 # 空闲大页数量 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB 挂载hugetlb文件系统\n1 mount -t hugetlbfs none /mnt/huge 在应用程序中mmap映射hugetlb文件系统即可使用，对内存的操作类似于访问文件。\n透明大页内存操作（待补充） cat /sys/kernel/mm/transparent_hugepage/enabled\n大页内存在k8s的支持情况 HugePage是k8s 1.9版本中引入的特性，1.10变为beta版本，1.23版本stable版本。k8s大于1.10版本该feature默认开启。\n在节点上配置了大页内存后，kubelet会自动感知到大页内存的配置，会修改node配置。会在hugepages-2Mi中有对应的大页内存容量，memory字段会去掉对应的大页内存量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 allocatable: cpu: \u0026#34;15\u0026#34; ephemeral-storage: 41152812Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: 40Mi memory: 63998924Ki pods: \u0026#34;110\u0026#34; capacity: cpu: \u0026#34;16\u0026#34; ephemeral-storage: 41152812Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: 40Mi memory: 64756684Ki pods: \u0026#34;110\u0026#34; pod在使用大页内存的时候，需要以volume的方式挂载到pod中，pod对大页内存的使用跟普通文件系统相同。\n","date":"2022-03-07T18:20:17Z","permalink":"/post/%E5%A4%A7%E9%A1%B5%E5%86%85%E5%AD%98/","title":"大页内存"},{"content":"自己断断续续花了几个月的时间，终于将《南渡北归》读完了。当我知道这本书的时候，迅速被这本书的内容给吸引住了。早高峰上班的地铁上基本全部花在了该书上，晚上躺在床上后临睡前也在刷该书，困的睁不开眼的时候再倒头就睡。\n中学的历史课本上，在讲解中国近现代史的时候实际上带有了比较浓厚的党的特色，过分的强调了自我，而对于当时的国民政府的描述相对较少。受限于篇幅的限制，历史课本对于历史人物提的也比较少一些。知识分子在中国历史上乘井喷状爆发的有两个阶段春秋时期和民国时期，本书以民国时期我们耳熟能详的知识分子们为主角，比如胡适、陈寅恪、梅贻琦、闻一多，讲解了大师们在乱世中颠沛流离的经历，以及错综复杂的人物关系。\n《南渡北归》实际分为了《南渡》、《北归》和《离别》三部曲，全书100多万字，四大名著字数最多的《水浒传》也不过才96万字，本书可以算得上一步巨著了。之所以有那么多字数，其中有一部分篇幅是大师们的一些书信资料，真正属于作者的有效字数应该跟《水浒传》不相上下。\n《南渡》主要讲解了卢沟桥事变抗日战争爆发后，北平各学府的师生们纷纷南下，先是辗转到长沙成立了长沙临时联合大学。但不久长沙沦陷，又分几路辗转到云南昆明组建西南联合大学，像李政道、杨振宁在这里完成了部分学业。但在昆明好景不长，时常有敌机轰炸，师生们天天”跑警报“。后一部分又逃亡了四川宜宾长江边上的李庄镇，很多大师们在李庄镇度过了非常难忘的一段回忆。比如林徽因、梁思永，很多时候都是在病床上度过，又受限于当地的环境，无药可医。\n《北归》发生在了抗日战争结束，国民政府开始组织知识分子纷纷回到北平的旧时学府，回到一别多年的故乡。可是好景不长，国共内战爆发，北平沦陷。紧接着一大批的知识分子们又开始了新一轮的逃亡，很大一部分逃亡了台湾岛，另外一部分留在了大陆。这一不同的选择，直接决定了不同的人生命运。\n《离别》是读起来最为伤感的一部，大师们纷纷陨落。逃亡台湾的如傅斯年、胡适、梅贻琦等虽然过得并不富裕，谈不上悲惨，但也算在台湾开辟了一番事业，名扬千古。但逃亡台湾的知识分子，在大陆却成了千古罪人。如大陆掀起了一股非常大的反胡热潮，以至于连胡适的儿子胡思杜都登报公开批判远在海峡对岸的父亲。可惜的是，思想觉悟足够高的儿子仍然在被批斗至自杀身亡。留在大陆的知识分子的命运却大相径庭，政治觉悟高者如郭沫若风生水起。但大部分的知识分子在”三反“、”文革“运动中都遭到了残酷的迫害，被红卫兵逼迫自杀者也不在少数。当年的红卫兵如今也就其八十岁的样子，我特别想采访一番当时他们是在什么情况下怀着怎样的心情什么动力来做出如此伤天害理之事，而且自己还理直气壮。\n寒门难出贵子。民国时期但凡叫上名字的大师们大都出生在名门望族，而且往往一家不止出一位大师级人物，甚至有很多家族是世交。随便举几个例子，如周树人、周作人兄弟，虽然后来两兄弟完全走上了不同的人生道路。曾国藩的后人至少五代以内都有各种英才，如在民国时期活跃的曾宝荪、曾约农、曾昭燏、曾昭抡等。梁启超的两个儿子梁思成和梁思永兄弟，分别在建筑学和考古学方向有非常高的建树。不过也正是因为出生在名门望族，因为家庭出身的问题，也往往在后来的迫害中是最惨烈的。\n傅斯年在书中简直被作者推崇之至，甚至可以说是崇拜。关于傅斯年，作者描绘最多的字眼是胖子、高血压蹭蹭往上窜、活跃于政学两界、学术大鳄、气场强大、胡适的打手、思维清晰、擅长处理棘手问题。傅斯年毕业于大学，曾经领导过五四运动，出国留学后曾经担任了多年的史语所所长一职，也曾代理过北京大学校长一职。国共内战爆发后，随史语所一起逃亡台湾，在台湾期间，担任台湾大学校长，1950年突发脑淤血去世，后葬于台湾大学的”傅园“内为后世铭记。\n另外一个对外印象比较深刻的人物是陈寅恪。清末著名诗人陈三立之子，同梁启超、王国维、赵元任并称为”清华国学四大导师“。国共内战北平失手后，陈寅恪从北京辗转逃亡岭南大学，后岭南大学跟中山大学合并，任教于中山大学。书中罗列了陈寅恪的多首诗，诗文引经据典，读起来相当晦涩，没点文化底蕴根本无法领会其中奥妙，可见大师功底之深。晚年在双目失明的情况下，完成了80多万字的巨著《柳如是别传》，非常人能所及。\n本书描述的这段历史，实际上也就发生了一百年的时间，但却有非常多的历史是无法考证的。读史书其实想了解一个真实的历史，但本书的作者却带有了比较浓厚的个人感情色彩，比如对傅斯年推崇之至，对闻一多的评价却非常差。另外，作者有一丝错误的政治倾向，此书能够在大陆出版也算是一个奇迹了。读此书本着了解历史真相的态度，但却不可全信。\n民国时期的知识分子大都有记录日志的好习惯，日记确实是一种非常好的自我反省和自我成长的好方式，可惜这么好的成长方式在现代的社会中却在逐渐消失，更多的会出现在小学生的课后作业中。另一方面，日记作为了非常好的一种史学参考资料，日记不会说谎，是作者的最真实的内心流露。过去发生的事情可以被人遗忘，但文字永存。由于没有长途电话这么高科技的产品，朋友之间的往来很多都是以书信的形式，如林徽因跟美国好友费慰梅就有大量的书信往来，这些书信也成为了非常好的史学资料。书信在当代早已成为了过去时，取而代之的是更为便捷的聊天app，无论身处何地对方永远在线，想聊天几乎没有了任何代价。\n如果你有时间，此书值得一读。另外，再推荐一个关于昆明西南联合大学的纪录片《九零后》，针对超过16位超过90岁高龄的专家学者的口述，有助于了解当时那段历史。\n","date":"2022-03-01T00:48:29Z","permalink":"/post/%E5%8D%97%E6%B8%A1%E5%8C%97%E5%BD%92%E8%AF%BB%E5%90%8E%E6%84%9F/","title":"《南渡北归》读后感"},{"content":"简介 华为云开源的多云容器编排项目，目前为CNCF沙箱项目。Karmada是基于kubefed v2进行修改的一个项目，因此里面很多概念都是取自kubefed v2。\nKarmada相对于kubefed v2的最大优点：完全兼容k8s的API。karmada提供了一个一套独立的karmada-apiserver，跟kube-apiserver是兼容的，使用方在调用的时候只需要访问karmada-apiserver就可以了。\n架构 有三个组件构建：\nkarmada api server，用来给其他的组件提供rest api karmada controller manager karmada scheduler karmada会新建一个ETCD，用来存储karmada的API对象。\nkarmada controller manager内部包含了多个controller的功能，会通过karmada apiserver来watch karmada对象，并通过调用各个子集群的apiserver来创建标准的k8s对象，包含了如下的controller对象：\nCluster Controller：用来管理子集群的声明周期 Policy Controller：监听PropagationPolicy对象，通过resourceSelector找到匹配中的资源，并创建ResourceBinding对象。 Binding Controller：监听ResourceBinding对象，并创建每个集群的Work对象。 Execution Controller：监听Work对象，一旦Work对象场景后，会在子集群中创建Work关联的k8s对象。 组件 karmada-aggregated-apiserver 在karmada-apiserver上注册的api信息\netcd 用来存放karmada的元数据信息\nCRD Cluster 需要使用kamada-apiserver来查询\nunfold me to see the yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 apiVersion: cluster.karmada.io/v1alpha1 kind: Cluster metadata: finalizers: - karmada.io/cluster-controller name: member1 spec: apiEndpoint: https://172.18.0.3:6443 impersonatorSecretRef: name: member1-impersonator namespace: karmada-cluster secretRef: name: member1 namespace: karmada-cluster syncMode: Push status: # 支持的api列表 apiEnablements: - groupVersion: admissionregistration.k8s.io/v1 resources: - kind: MutatingWebhookConfiguration name: mutatingwebhookconfigurations - kind: ValidatingWebhookConfiguration name: validatingwebhookconfigurations - groupVersion: apiextensions.k8s.io/v1 resources: - kind: CustomResourceDefinition name: customresourcedefinitions - groupVersion: apiregistration.k8s.io/v1 resources: - kind: APIService name: apiservices - groupVersion: apps/v1 resources: - kind: ControllerRevision name: controllerrevisions - kind: DaemonSet name: daemonsets - kind: Deployment name: deployments - kind: ReplicaSet name: replicasets - kind: StatefulSet name: statefulsets - groupVersion: authentication.k8s.io/v1 resources: - kind: TokenReview name: tokenreviews - groupVersion: authorization.k8s.io/v1 resources: - kind: LocalSubjectAccessReview name: localsubjectaccessreviews - kind: SelfSubjectAccessReview name: selfsubjectaccessreviews - kind: SelfSubjectRulesReview name: selfsubjectrulesreviews - kind: SubjectAccessReview name: subjectaccessreviews - groupVersion: autoscaling/v1 resources: - kind: HorizontalPodAutoscaler name: horizontalpodautoscalers - groupVersion: autoscaling/v2beta1 resources: - kind: HorizontalPodAutoscaler name: horizontalpodautoscalers - groupVersion: autoscaling/v2beta2 resources: - kind: HorizontalPodAutoscaler name: horizontalpodautoscalers - groupVersion: batch/v1 resources: - kind: CronJob name: cronjobs - kind: Job name: jobs - groupVersion: batch/v1beta1 resources: - kind: CronJob name: cronjobs - groupVersion: certificates.k8s.io/v1 resources: - kind: CertificateSigningRequest name: certificatesigningrequests - groupVersion: coordination.k8s.io/v1 resources: - kind: Lease name: leases - groupVersion: discovery.k8s.io/v1 resources: - kind: EndpointSlice name: endpointslices - groupVersion: discovery.k8s.io/v1beta1 resources: - kind: EndpointSlice name: endpointslices - groupVersion: events.k8s.io/v1 resources: - kind: Event name: events - groupVersion: events.k8s.io/v1beta1 resources: - kind: Event name: events - groupVersion: flowcontrol.apiserver.k8s.io/v1beta1 resources: - kind: FlowSchema name: flowschemas - kind: PriorityLevelConfiguration name: prioritylevelconfigurations - groupVersion: networking.k8s.io/v1 resources: - kind: IngressClass name: ingressclasses - kind: Ingress name: ingresses - kind: NetworkPolicy name: networkpolicies - groupVersion: node.k8s.io/v1 resources: - kind: RuntimeClass name: runtimeclasses - groupVersion: node.k8s.io/v1beta1 resources: - kind: RuntimeClass name: runtimeclasses - groupVersion: policy/v1 resources: - kind: PodDisruptionBudget name: poddisruptionbudgets - groupVersion: policy/v1beta1 resources: - kind: PodDisruptionBudget name: poddisruptionbudgets - kind: PodSecurityPolicy name: podsecuritypolicies - groupVersion: rbac.authorization.k8s.io/v1 resources: - kind: ClusterRoleBinding name: clusterrolebindings - kind: ClusterRole name: clusterroles - kind: RoleBinding name: rolebindings - kind: Role name: roles - groupVersion: scheduling.k8s.io/v1 resources: - kind: PriorityClass name: priorityclasses - groupVersion: storage.k8s.io/v1 resources: - kind: CSIDriver name: csidrivers - kind: CSINode name: csinodes - kind: StorageClass name: storageclasses - kind: VolumeAttachment name: volumeattachments - groupVersion: storage.k8s.io/v1beta1 resources: - kind: CSIStorageCapacity name: csistoragecapacities - groupVersion: v1 resources: - kind: Binding name: bindings - kind: ComponentStatus name: componentstatuses - kind: ConfigMap name: configmaps - kind: Endpoints name: endpoints - kind: Event name: events - kind: LimitRange name: limitranges - kind: Namespace name: namespaces - kind: Node name: nodes - kind: PersistentVolumeClaim name: persistentvolumeclaims - kind: PersistentVolume name: persistentvolumes - kind: Pod name: pods - kind: PodTemplate name: podtemplates - kind: ReplicationController name: replicationcontrollers - kind: ResourceQuota name: resourcequotas - kind: Secret name: secrets - kind: ServiceAccount name: serviceaccounts - kind: Service name: services conditions: - lastTransitionTime: \u0026#34;2022-02-21T08:27:56Z\u0026#34; message: cluster is healthy and ready to accept workloads reason: ClusterReady status: \u0026#34;True\u0026#34; type: Ready kubernetesVersion: v1.22.0 nodeSummary: readyNum: 1 totalNum: 1 resourceSummary: allocatable: cpu: \u0026#34;8\u0026#34; ephemeral-storage: 41152812Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 32192720Ki pods: \u0026#34;110\u0026#34; allocated: cpu: 950m memory: 290Mi pods: \u0026#34;9\u0026#34; PropagationPolicy 应用发布策略\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation namespace: default spec: placement: clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaDivisionPreference: Weighted· replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - member1 weight: 1 - targetCluster: clusterNames: - member2 weight: 1 resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx namespace: default ClusterPropagationPolicy 用来定义Cluster级别资源的发布策略\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: policy.karmada.io/v1alpha1 kind: ClusterPropagationPolicy metadata: name: serviceexport-policy spec: resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition name: serviceexports.multicluster.x-k8s.io placement: clusterAffinity: clusterNames: - member1 - member2 OverridePolicy 用于修改不同集群内的对象\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: policy.karmada.io/v1alpha1 kind: OverridePolicy metadata: name: example spec: ... overriders: commandOverrider: - containerName: myapp operator: remove value: - --parameter1=foo 安装 karmada提供了三种安装方式：\nkubectl karmada插件的方式安装，较新的特性，v1.0版本才会提供，当前最新版本为v0.10.1 helm chart方式安装 使用源码安装 安装kubectl插件 karmada提供了一个cli的工具，可以是单独的二进制工具kubectl-karmada，也可以是kubectl的插件karmada，本文以kubectl插件的方式进行安装。\n1 kubectl krew install karmada 接下来就可以执行 kubectl karmada命令了。\nhelm chart的方式安装 使用kind插件一个k8s集群 host，此处步骤略 ​\n在源码目录下执行如下的命令\n1 helm install karmada -n karmada-system --create-namespace ./charts 会在karmada-system下部署如下管控的组件\n1 2 3 4 5 6 7 8 9 10 11 $ kubectl get deployments -n karmada-system NAME READY UP-TO-DATE AVAILABLE AGE karmada-apiserver 1/1 1 1 83s karmada-controller-manager 1/1 1 1 83s karmada-kube-controller-manager 1/1 1 1 83s karmada-scheduler 1/1 1 1 83s karmada-webhook 1/1 1 1 83s $ kubectl get statefulsets -n karmada-system NAME READY AGE etcd 1/1 2m1s 从源码安装一个本地测试集群 本方式仅用于本地测试，会自动使用kind拉起测试的k8s集群，包括了一个host集群和3个member集群。\n执行如下命令，会执行如下的任务：\n使用kind启动一个新的k8s集群host 构建karmada的控制平面，并部署控制平面到host集群 创建3个member集群并加入到karmada中 1 2 3 git clone https://github.com/karmada-io/karmada cd karmada hack/local-up-karmada.sh local-up-karmada.sh有两个细节问题值得关注。 kind创建出来的集群名一定带有“kind-”的前缀，该脚本中为了去掉“kind-”前缀，使用了kubectl config rename-context 命令来rename context。https://github.com/karmada-io/karmada/blob/master/hack/util.sh#L375 另外，使用kind创建出来的集群，context中的apiserver地址为类似https://127.0.0.1:45195这样的本地随机端口，只能在宿主机网络中访问。如果要想在两个k8s集群之间访问是不可行的。脚本中使用kubectl config set-cluster 命令将集群apiserver的地址替换为了docker网段的ip地址，以便于多个k8s集群之间的互访。https://github.com/karmada-io/karmada/blob/master/hack/util.sh#L389 ​\n可以看到创建了如下的4个k8s cluster\n1 2 3 4 5 $ kind get clusters karmada-host member1 member2 member3 但这四个集群会分为两个kubeconfig文件 ~/.kube/karmada.config 和 ~/.kube/members.config。karmada又分为了两个context karmada-apiserver和karmada-host，两者连接同一个k8s集群。karmada-apiserver为跟karmada控制平台交互使用的context，为容器中的karmada-apiserver。karmada-host连接容器的kube-apiserver。 ​\n如果要连接host集群设置环境变量：export KUBECONFIG=\u0026quot;$HOME/.kube/karmada.config\u0026quot; 如果要连接member集群设置环境变量：export KUBECONFIG=\u0026quot;$HOME/.kube/members.config\u0026quot;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ export KUBECONFIG=\u0026#34;$HOME/.kube/karmada.config\u0026#34; # 切换到karmada-host $ kctx karmada-host # 可以看到karmada部署的组件 $ k get deploy -n karmada-system NAME READY UP-TO-DATE AVAILABLE AGE karmada-aggregated-apiserver 2/2 2 2 17m karmada-apiserver 1/1 1 1 18m karmada-controller-manager 2/2 2 2 17m karmada-kube-controller-manager 1/1 1 1 17m karmada-scheduler 2/2 2 2 17m karmada-scheduler-estimator-member1 2/2 2 2 17m karmada-scheduler-estimator-member2 2/2 2 2 17m karmada-scheduler-estimator-member3 2/2 2 2 17m karmada-webhook 2/2 2 2 17m 在host集群中可以看到会创建出如下的pod\n1 2 3 4 5 6 7 8 9 10 11 12 # k get pod -n karmada-system NAME READY STATUS RESTARTS AGE etcd-0 1/1 Running 0 18h karmada-aggregated-apiserver-7b88b8df99-95twq 1/1 Running 0 18h karmada-apiserver-5746cf97bb-pspfg 1/1 Running 0 18h karmada-controller-manager-7d66968445-h4xsc 1/1 Running 0 18h karmada-kube-controller-manager-869d9df85-f4bqj 1/1 Running 0 18h karmada-scheduler-8677cdf96d-psnlw 1/1 Running 0 18h karmada-scheduler-estimator-member1-696b54fd56-jjg6b 1/1 Running 0 18h karmada-scheduler-estimator-member2-774fb84c5d-fldhd 1/1 Running 0 18h karmada-scheduler-estimator-member3-5c7d87f4b4-fk4lj 1/1 Running 0 18h karmada-webhook-79b87f7c5f-lt8ps 1/1 Running 0 18h 在karmada-apiserver集群会创建出如下的Cluster，同时可以看到有两个Push模式一个Pull模式的集群。\n1 2 3 4 5 6 7 $ kctx karmada-apiserver Switched to context \u0026#34;karmada-apiserver\u0026#34;. $ kubectl get clusters NAME VERSION MODE READY AGE member1 v1.22.0 Push True 61m member2 v1.22.0 Push True 61m member3 v1.22.0 Pull True 61m 应用发布 将context切换到karmada-host，在host集群部署应用nginx\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 $ export KUBECONFIG=\u0026#34;$HOME/.kube/karmada.config\u0026#34; $ kctx karmada-apiserver $ kubectl create -f samples/nginx/propagationpolicy.yaml $ cat samples/nginx/propagationpolicy.yaml apiVersion: policy.karmada.io/v1alpha1 kind: PropagationPolicy metadata: name: nginx-propagation spec: resourceSelectors: - apiVersion: apps/v1 kind: Deployment name: nginx placement: # 要提交到子集群1和2 clusterAffinity: clusterNames: - member1 - member2 replicaScheduling: replicaDivisionPreference: Weighted replicaSchedulingType: Divided weightPreference: staticWeightList: - targetCluster: clusterNames: - member1 weight: 1 - targetCluster: clusterNames: - member2 weight: 1 在karmada-apiserver中提交deployment。deployment提交后实际上仅为一个模板，只有PropagationPolicy跟deployment关联后，才会真正部署。deployment中指定的副本数为所有子集群的副本数综合。\n1 2 3 4 5 6 7 $ kubectl create -f samples/nginx/deployment.yaml $ k get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx 2/2 2 2 7m37s # 可以看到虽然deployment的副本数为2，但在karmada-apiserver集群中实际上并没有pod创建出来 $ k get pod No resources found in default namespace. 通过karmadactl命令可以查询环境中的所有子集群的pod状态\n1 2 3 4 5 6 $ karmadactl get pod The karmadactl get command now only supports Push mode, [ member3 ] are not push mode NAME CLUSTER READY STATUS RESTARTS AGE nginx-6799fc88d8-4w2gc member1 1/1 Running 0 4m16s nginx-6799fc88d8-j77f5 member2 1/1 Running 0 4m16s 元集群访问子集群 可以看到在karmada-apiserver上注册了AA服务，group为cluster.karmada.io\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver get apiservice v1alpha1.cluster.karmada.io -o yaml apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: labels: apiserver: \u0026#34;true\u0026#34; app: karmada-aggregated-apiserver name: v1alpha1.cluster.karmada.io spec: group: cluster.karmada.io groupPriorityMinimum: 2000 insecureSkipTLSVerify: true service: name: karmada-aggregated-apiserver namespace: karmada-system port: 443 version: v1alpha1 versionPriority: 10 status: conditions: - lastTransitionTime: \u0026#34;2022-02-21T08:27:47Z\u0026#34; message: all checks passed reason: Passed status: \u0026#34;True\u0026#34; type: Available 如果直接使用kubectl访问karmada-apiserver服务，会提示存在权限问题\n1 2 ￥ kubectl --kubeconfig ~/.kube/karmada-apiserver.config --context karmada-apiserver get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes Error from server (Forbidden): users \u0026#34;system:admin\u0026#34; is forbidden: User \u0026#34;system:serviceaccount:karmada-cluster:karmada-impersonator\u0026#34; cannot impersonate resource \u0026#34;users\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope 给karmada-apiserver的Account system:admin授权，创建文件cluster-proxy-rbac.yaml，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-clusterrole rules: - apiGroups: - \u0026#39;cluster.karmada.io\u0026#39; resources: - clusters/proxy resourceNames: - member1 - member2 - member3 verbs: - \u0026#39;*\u0026#39; apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-clusterrole subjects: - kind: User name: \u0026#34;system:admin\u0026#34; 执行如下命令即可给karmada-apiserver的Account system:admin 授权可访问AA服务的member1-3\n1 kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver apply -f cluster-proxy-rbac.yaml 通过url的形式访问AA服务，返回数据格式为json\n1 kubectl --kubeconfig ~/.kube/karmada-apiserver.config --context karmada-apiserver get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes 如果要想使用 kubectl get node 的形式来访问，则需要在kubeconfig文件中server字段的url地址后追加url /apis/cluster.karmada.io/v1alpha1/clusters/{clustername}/proxy\n给特定的账号授权 在karmada-apiserver中创建账号 tom\n1 kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver create serviceaccount tom 在karmada-apiserver中提交如下的yaml文件，用来给tom账号增加访问member1集群的权限\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-proxy-clusterrole rules: - apiGroups: - \u0026#39;cluster.karmada.io\u0026#39; resources: - clusters/proxy resourceNames: - member1 verbs: - \u0026#39;*\u0026#39; apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-proxy-clusterrolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-proxy-clusterrole subjects: - kind: ServiceAccount name: tom namespace: default # The token generated by the serviceaccount can parse the group information. Therefore, you need to specify the group information below. - kind: Group name: \u0026#34;system:serviceaccounts\u0026#34; - kind: Group name: \u0026#34;system:serviceaccounts:default\u0026#34; 执行如下命令提交\n1 kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver apply -f cluster-proxy-rbac.yaml 获取karmada-apiserver集群的tom账号的token信息\n1 kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver get secret `kubectl --kubeconfig /root/.kube/karmada.config --context karmada-apiserver get sa tom -oyaml | grep token | awk \u0026#39;{print $3}\u0026#39;` -oyaml | grep token: | awk \u0026#39;{print $2}\u0026#39; | base64 -d 创建~/.kube/tom.config文件，其中token信息为上一步获取的token，server的地址可以查看 ~/.kube/karmada-apiserver.config文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 clusters: - cluster: insecure-skip-tls-verify: true server: {karmada-apiserver-address} # Replace {karmada-apiserver-address} with karmada-apiserver-address. You can find it in /root/.kube/karmada.config file. name: tom contexts: - context: cluster: tom user: tom name: tom current-context: tom kind: Config users: - name: tom user: token: {token} # Replace {token} with the token obtain above. 通过karmada-apiserver的tom用户访问member1集群\n1 2 3 4 5 6 # 预期可以正常访问 $ kubectl --kubeconfig ~/.kube/tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/apis # 预期不可以访问，因为tom在member1集群没有权限 $ kubectl --kubeconfig ~/.kube/tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes Error from server (Forbidden): nodes is forbidden: User \u0026#34;system:serviceaccount:default:tom\u0026#34; cannot list resource \u0026#34;nodes\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope 在member1集群将tom账号绑定权限，创建member1-rbac.yaml文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: tom rules: - apiGroups: - \u0026#39;*\u0026#39; resources: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tom roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: tom subjects: - kind: ServiceAccount name: tom namespace: default 权限在member1集群生效\n1 kubectl --kubeconfig /root/.kube/members.config --context member1 apply -f member1-rbac.yaml 重新执行命令即可以访问子集群中的数据\n1 kubectl --kubeconfig ~/.kube/tom.config get --raw /apis/cluster.karmada.io/v1alpha1/clusters/member1/proxy/api/v1/nodes 集群注册 支持Push和Pull两种模式。 ​\npush模式karmada会直接访问成员集群的kuba-apiserver。\npull模式针对的场景是中心集群无法直接子集群的场景。每个子集群运行karmada-agent组件，一旦karmada-agent部署完成后就会自动向host集群注册，karmada-agent会watch host集群的karmada-es-下的cr，并在本集群部署。 ​\n","date":"2022-02-22T20:16:25Z","permalink":"/post/k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E6%96%B9%E6%A1%88-karmada/","title":"k8s多集群管理方案 - karmada"},{"content":"apiserver-builder-alpha k8s提供了aggregated apiserver的方式来扩容api，该项目提供了代码生成器、基础library来供开发AA使用。\n该项目的定位跟kubebuilder比较类似，kubebuilder用来生成CRD的框架，该项目用来生成AA的框架。\n相关资料：\n[Set up an Extension API Server](Set up an Extension API Server) cluster-api-provider-nested 在同一个k8s集群内提供多租户的特性，每个租户具有独立的api-server、controller-manager和scheduler。\ncluster-proportional-autoscaler k8s默认提供了hpa机制，可以根据pod的负载情况来对workload进行自动的扩缩容。同时以单独的autoscaler项目提供了vpa功能的支持。\n该项目提供提供了类似pod水平扩容的机制，跟hpa不同的是，pod的数量由集群中的节点规模来自动扩缩容pod。特别适合负载跟集群规模的变化成正比的服务，比如coredns、nginx ingress等服务。\nhpa功能k8s提供了CRD来作为hpa的配置，本项目没有单独的CRD来定义配置，而是通过在启动的时候指定参数，或者配置放到ConfigMap的方式。而且一个cluster-proportional-autoscaler实例仅能针对一个workload。\nkube-batch k8s的调度器扩展，实现了Gang Scheduling特性（一组pod必须同时被调度成功，或者处于pending状态），适用于批处理系统。\n由于该组件是以单独调度器的形式存在，会跟k8s默认的kube-scheduler并存，因为两个调度器之间并不能相互感知，在两个调度器并存的情况下会存在一定的冲突。\nprometheus-adapter k8s要实现hpa（水平自动扩容）的功能，需要监控指标。k8s的监控指标分为核心指标和自定义指标两大类。其中核心指标由metrics-server组件从kubelet、cadvisor等组件来获取，并通过aggregated apiserver的形式暴露给k8s，aggregated api的group信息为metrics.k8s.io。\n自定义监控指标则由group custom.metrics.k8s.io对通过aggregated api暴露。本项目即为自定义监控指标的aggregated apiserver的实现。\n相关参考：Horizontal Pod Autoscaling\nscheduler-plugins k8s从1.16版本开始提供了新的调度框架Kubernetes Schduling Framework机制，用户可以基于此项目来开发自己的插件。该项目可以直接构建出kube-scheduler的新镜像。\n相关参考：进击的Kubernetes调度系统（一）：Scheduling Framework\nsig-storage-local-static-provisioner k8s提供了local pv功能可以用来给pod挂载本地的数据盘，具体的local pv的定义如下所示，pv中包含了要亲和的节点信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 kind: PersistentVolume metadata: name: example-pv spec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/ssd1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - example-node 但要使用local pv功能，必须要事先创建出pv才可以，k8s本身并没有提供动态创建pv的功能。\n该工具可以根据配置的规则，自动将机器上符合条件的磁盘创建出local pv以供后续创建出的pod使用。\n另外，Rancher提供了一个类似的项目，local-path-provisioner，该项目已经被拉起k8s开发环境的开源项目kind使用。\n相关参考：LocalVolume数据卷\n","date":"2022-02-14T23:21:37Z","permalink":"/post/github-kubernetes-sigs%E7%BB%84%E7%BB%87%E4%B8%8B%E7%9A%84%E9%A1%B9%E7%9B%AE%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/","title":"Github Kubernetes SIGs组织下的项目（持续更新）"},{"content":"在k8s官方的Github kubernetes group下除了k8s的核心组件外，还有很多开源项目，本文用来简要分析这些开源项目的用途。\napimachinery 关于k8s的scheme等的sdk项目，代码跟kubernetes项目的如下路径保持同步： https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery\nautoscaler 跟k8s的弹性扩缩容相关的组件，有如下两个功能：\nvpa（pod垂直扩缩容） k8s的kube-controller-manager默认支持了hpa功能，即水平扩缩容。同时k8s还提供了vpa功能，即垂直扩缩容，会根据pod历史的资源占用，修改pod的request值，并不会修改pod的limit值。之所以k8s没有默认提供vpa功能，原因是因为vpa实现要复杂很多，需要通过webhook的技术来在pod创建的时候修改pod的request值。vpa的功能通常用于大型单体应用。\nautoscaler的功能之一即提供了vpa的实现。\nCluster Autoscaler（node节点自动扩缩容） 该功能是为了重新利用k8s node的节点资源，在节点资源不足的时候可以动态创建资源，在节点资源空闲的时候可以自动回收资源。k8s node的创建和释放需要公有云平台的支持，该功能对接了多个公有云厂商的api。\ncloud-provider k8s的cloud provider功能用来跟云厂商对接以实现对k8s的node、LoadBalancer Service管理，比如LoadBalancer Service申请vip需要跟云厂商的LB API进行对接。\n在k8s 1.6版本之前，cloud provider的代码完全是耦合在k8s的kube-apiserver、kubelet、kube-contorller-manager的代码中的，如果要支持更多的云厂商，只能修改k8s的代码，扩展性比较差。\n从k8s 1.6版本之后，cloud provider作为单独的二进制组件来提供服务，并提供了接口定义。本项目为非二进制项目，仅包含k8s的cloud-provider机制的接口定义，为了让第三方的cloud-provider的实现项目引用接口定义方便，将kubernetes项目中的定义为单独的项目，且代码跟kubernetes项目中的保持同步。\n参考资料：\n从 K8S 的 Cloud Provider 到 CCM 的演进之路 Descheduler 项目地址：https://github.com/kubernetes-sigs/descheduler\nk8s的pod调度完全动态的，kube-scheduler组件在调度pod的时候会根据当时k8s集群的运行时环境来决定pod可以调度的节点，可以保证pod的调度在当时是最优的。但是随着的推移，比如环境中增加了新的node、创建了一批亲和节点的pod，都有可能会导致如果相同的pod再重新调度会到其他的节点上。但k8s的设计是，一旦pod调度完成后，就不会再重新调度。\nDescheduler组件用来解决pod的重新调度问题，可以根据配置的一系列的规则来触发驱逐pod，让pod可以重新调度，从而使k8s集群内的pod尽可能达到最优状态，有点类似于计算机在运行了一段时间后的磁盘脆片整理功能。Descheduler组件可以以job、cronjob或者deployment的方式运行在k8s集群中。\nkubernetes-template-project 用来存放了k8s项目的标准模板，里面包含了一些必要的文件，新建的项目可以fork该项目。\nnode-problem-detector(npd) 项目地址：https://github.com/kubernetes/node-problem-detector\nk8s的管控组件对于iaas层的node运行状态是完全不感知的，比如节点出现了ntp服务挂掉、硬件告警（cpu、内存、磁盘故障）、内核死锁。node-problem-detector旨在将node节点的问题转换k8s node event，以DaemonSet的方式部署在所有的k8s节点上。\n上报故障的方式支持如下两种方式：\n对于永久性故障，通过修改node status中的condition上报给apiserver 对于临时性故障，通过Event的方式上报 node-problem-detector在将节点的故障信息上报给k8s后，通常会配合一些自愈系统搭配使用，比如Draino和Descheduler 。\nsample-apiserver k8s提供了aggregated apiserver的方式来扩展api，该项目为一个简单的aaggregated apiserver的例子，可以直接编译出二进制文件。要想开发一个AA类型的服务，只需要frok一下该项目，并在项目的基础上进行修改即可完成。\n相关资料：\n[Set up an Extension API Server](Set up an Extension API Server) ","date":"2022-02-09T20:12:00Z","permalink":"/post/github-kubernetes%E7%BB%84%E7%BB%87%E4%B8%8B%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/","title":"Github Kubernetes组织下开源项目（持续更新）"},{"content":"\n题图为一点资讯近期推出的一款定位线上社交的App啫喱，每个人可以给自己订制一个卡通形象，是国内厂商对于线上社交的一次新的尝试。\n距离上期技术分享已经约有一年半的时间，这次不定期更新的时间有些久。2022年期望在博客方面增加时间投入，多关注开源技术，预计会大幅度提升更新的频率。\n资源 1. Katacoda 可以一键创建一个k8s集群的工具，甚至无需登录，比上期推荐的工具Play with Kubernetes更为方便。\n2. OperatorHub k8s推出了CRD的机制后，大大增强了k8s的扩展能力，可以好不夸张的说，k8s之所以如此成功，跟CRD的扩展机制有很大关系。OperatorHub类似于DockerHub，收集了各种各样的operator实现。\n3. lazykube 一款可以通过命令行终端来展示和管理k8s资源的工具。\n4. LazyDocker 同上面的lazykube工具，LazyDocker是一个可以在命令行上查看本机docker的工具，可以查看docker上容器以及运行状态、本地的镜像以及分层信息、volume信息。\n5. httpbin 一个非常简单的http服务，可以用来在测试服务的连通性，尤其是可以用curl测试api层面的连通性，再也不用访问curl http://www.baidu.com了。比如执行curl http://httpbin.org/headers可以以json的形式返回request的http header信息，执行curl http://httpbin.org/status/200可以返回状态码为200。\n不过，在国内访问该网站连通性并不是太好。还提供了镜像版本，可以直接在本地以docker的方式运行该服务。\n6. Flux 一款基于k8s的GitOps工具，采用k8s的声明式api基于operator实现。\n7. OpenTelemetry 云原生领域的可观测性工具，用来制定规范、提供sdk和采集agent实现，实现了可观察性中的tracing和metric，并没有实现logging部分。不负责底层的后端存储，可以跟负责metric的prometheus集成，负责tracing的jager集成。\n8. kspan 该工具可以将k8s的event使用OpenTelemetry转换成tracing的形式，并将其存储在支持tracing的后端，比如jaeger中。\n9. skopeo skopeo是一款镜像操作工具，用来解决常用的镜像操作，但这些功能docker命令却不太具备，或者需要调用docker registry的api才可以完成操作。比如镜像从一个镜像仓库迁移到另外一个镜像仓库，从镜像仓库中删除镜像等。\n10. KubeOperator KubeOperator一个开源的轻量级 Kubernetes 发行版，提供可视化的 Web UI，可以用来在IaaS 平台上自动创建主机，通过 Ansible 完成自动化部署和变更操作。\n另外，还提供了商业版本，支持更多的企业级特性，这种模式也是类似Redhat的最为常见的开源软件的商业变现方式。\n11. Lens Kubernetes的客户端工具，非web端工具，可以通过本地访问远程的k8s集群，并且可以获取k8s集群的node、pod等信息，提供了mac、linux、windows三种客户端版本。\n12. Caddy 一款使用Golang编写的七层负载均衡软件，Github上有3万+的star数量，相比与nginx有两个明显的优势：提供了默认的https服务，支持证书的自签发；使用Golang开发，只需要一个二进制配合caddyfile配置文件即可运行，更轻量。\n13. LVScare sealyun开源的一个轻量级的lvs管理工具，可以作为轻量级的负载均衡，基于Golang实现。输入这样一条指令 lvscare care --run-once --vs 10.103.97.12:6443 --rs 192.168.0.2:6443 --rs 192.168.0.3:6443 --rs 192.168.0.4:6443 即可在宿主机上创建对应的ipvs规则。自带了健康检查功能，支持四层和七层的健康检查，是该工具的最大优势。一般lvs会配合着keepalived来进行检查检查，一旦健康检查失败后会将rs的权重调整为0，但keepalived的配置相对复杂，该工具更轻量。\n14. HUGO 非常火爆的使用Golang开发的开源博客系统，目前Github上的Star数量已经超过5万+，更老牌的开源博客系统Hexo的Star数量才3万+，基于Vue开发的VuePress Star数量还不到2万。\n15. sealer 阿里巴巴开源的一款云原生的应用发布工具。该工具的设计思路非常先进，提出了集群镜像的概念，可以将k8s集群和应用build成为一个集群镜像，类似于docker镜像，一旦集群镜像构建完成后即可像容器一样运行在不同的硬件上。\n","date":"2022-02-09T19:35:26Z","permalink":"/post/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E7%AC%AC14%E6%9C%9F/","title":"技术分享第14期"},{"content":"ackdistro 阿里云的k8s发行版，跟阿里云的ack采用了相同的源码。该项目采用sealer来部署k8s集群，并通过sealer支持k8s集群的扩容、缩容节点等运维操作。该项目并没有将k8s相关的源码开源，而主要维护了安装k8s集群需要的yaml文件、helm chart。\nack的k8s发行版比较简洁，并没有公有云ack的丰富的组件。除了k8s原生的几个组件外，网络插件集成了hybridnet，存储插件集成了open-local。\nhybridnet 容器网络插件，支持underlay网络和overlay网络，且可以支持一个k8s集群内的网络同时支持overlay网络和underlay网络。underlay网络模式下，可以支持vlan网络，也可以支持bgp网络。\nimage-syncer 镜像仓库同步工具，用于两个镜像仓库之间的数据同步。在公有云的产品ack one中有所应用。\nkube-eventer k8s的Event会存放在etcd中，并跟对象关联。Event有一定的有效时常，默认为1小时。业界通常做法是将event存放在单独的etcd集群中，并将event的生命周期设长。对于异常类型的Event，是一种非常理想的告警机制。\nkube-eventer组件可以将k8s的event对象收集起来，可以发送到钉钉、kafka等数据sink端。\nkubernetes-cronhpa-controller k8s提供了hpa机制，可以针对pod的监控信息对pod进行扩缩容。该组件提供了定期扩缩容的机制，可以定期对pod的数量进行设置。\n相关资料：容器定时伸缩（CronHPA）\nlog-pilot 针对容器类型的服务研发的日志收集agent，可以在k8s pod上通过简单配置环境变量的方式即可采集日志，使用起来非常简洁。\nopen-local k8s对于本地磁盘设备的使用相对较弱，提供了emptyDir、hostPath和local pv的能力来使用本地磁盘设备，但这些功能并没有使用到k8s的动态创建pv的功能，即在pod在使用pvc之前，pv必须实现要创建出来。\n该项目提供了本地存储设备的动态供给能力，可以将本地的一块完整磁盘作为一个pv来动态创建。也可以将本地的磁盘切分成多块，通过lvm的方式来分配本地的不同pod来使用，以满足磁盘的共享，同时又有完整的磁盘quota能力。\n相关资料：LVM数据卷\nsealer 当前的应用发布经历了三个阶段：\n阶段一 裸部署在物理机或者vm上。直接裸部署在机器上的进程，存在操作系统、软件包的依赖问题，比如要部署一个python应用，那么需要机器上必须要包含对应版本的python运行环境以及应用依赖的所有包。 阶段二 通过镜像的方式部署在宿主机上。docker通过镜像的方式将应用以及依赖打包到一起，解决了单个应用的依赖问题。 阶段三 通过k8s的方式来标准化部署。在k8s时代，可以将应用直接部署到k8s集群上，将应用的发布标准化，实现应用的跨机器部署。 在阶段三中，应用发布到k8s集群后，应用会对k8s集群有依赖，比如k8s管控组件的配置、使用的网络插件、应用的部署yaml文件，对镜像仓库和dockerd的配置也有所依赖。当前绝大多数应用发布是跟k8s集群部署完全独立的，即先部署k8s集群，然后再部署应用，跟阶段一的发布单机应用模式比较类似，先安装python运行环境，然后再启动应用。\nsealer项目是个非常有意思的开源项目，旨在解决k8s集群连同应用发布的自动化问题，可以实现类似docker镜像的方式将整个k8s集群连同应用一起打包成集群镜像，有了集群镜像后既可以标准化的发布到应用到各个地方。sealer深受docker的启发，提出的很多概念跟docker非常类似，支持docker常见的子命令run、inspect、save、load、build、login、push、pull等。\nKubefile概念跟Dockerfile非常类似，且可以执行sealer build命令打包成集群镜像，语法也类似于Dockerfile。 CloudImage：集群镜像，将Kubefile打包后的产物，类比与dockerimage。基础集群镜像通常为裸k8s集群，跟docker基础镜像为裸操作系统一致。 Clusterfile：要想运行CloudImage，需要配合Clusterfile文件来启动，类似于Docker Compose。Clusterfile跟Docker Compose一致，并不是必须的，也可以通过sealer run的方式来启动集群镜像。 sealer要实现上述功能需要实现将k8s集群中的所有用到镜像全部打包到一个集群镜像中。\n相关资料：集群镜像：实现高效的分布式应用交付\n","date":"2022-02-08T21:33:00Z","permalink":"/post/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%BC%80%E6%BA%90%E4%BA%91%E5%8E%9F%E7%94%9F%E9%A1%B9%E7%9B%AE%E5%88%86%E6%9E%90%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0/","title":"阿里巴巴开源云原生项目分析（持续更新）"},{"content":"k8s虽然已经发展了多个版本，但在多region和多zone的场景下支持还是相对比较弱的，且很多的特性在alpha版本就已经废弃，说明k8s官方对于region和zone方面的支持情况有很大的不确定性。业界支持多reigon和容灾的特性更多是从上层的应用层来解决。本文主要是介绍k8s自身以及社区在region和zone方面的支持情况。\nk8s标签 https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone 官方推荐的跟region和zone有关的标签：\nfailure-domain.beta.kubernetes.io/region：1.17版本已经废弃，被topology.kubernetes.io/region取代 failure-domain.beta.kubernetes.io/zone：1.17版本已经废弃，被topology.kubernetes.io/zone取代 topology.kubernetes.io/region：1.17版本开始支持 topology.kubernetes.io/zone：1.17版本开始支持 服务拓扑ServiceTopology 支持版本：在1.17版本引入，在1.21版本废弃\n该特性在Service对象上增加了spec.topologyKeys字段，表示访问该Service的流量优先选用的拓扑域列表。访问Service流量的具体过程如下：\n当访问该Service时，一定是从某个k8s的node节点上发起，会查看当前node的label topology.kubernetes.io/zone对应的value，如果发现有endpoint所在的node的lable topology.kubernetes.io/zone对应的value相同，那么会将流量转发到该拓扑域的endpoint上。 如果没有找到topology.kubernetes.io/zone相同拓扑域的endpoint，则尝试找topology.kubernetes.io/region相同拓扑域的endpoint。 如果没有找到任何拓扑域的endpoint，那么该service会转发失败。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: Name: my-app-web spec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 topologyKeys: - \u0026#34;topology.kubernetes.io/zone\u0026#34; - \u0026#34;topology.kubernetes.io/region\u0026#34; 在底层实现上，使用了1.16版本新引入的alpha版本特性 EndpointSlice，该特性主要是用来解决Endpoints对象过多时带来的性能问题，从而将Endpoints拆分为多个组，Service Topology特性恰好可以借助该特性来实现，本质上也是为了将Endpoints进行拆分。k8s会根据Service的topologyKeys来将Service拆分为多个EndpointSlice对象，kube-proxy根据EndpointSlice会将Service流量进行拆分。\n拓扑感知提示Topology Aware Hints 该特性在1.21版本引入，在1.23版本变为beta版本，用来取代Topology Key功能。 该特性会启用两个组件：EndpointSlice控制器和kube-proxy。\n在Service Topology功能中，需要给Service来指定topologyKeys字段。该特性会更自动化一些，只需要在Service上增加annotation service.kubernetes.io/topology-aware-hints:auto，EndpointSlice控制器会watch到Service，发现开启了拓扑感知功能，会自动向endpoints的hints中增加forZones字段，该字段的value会根据endpoint所在node的topology.kubernetes.io/zone来决定。\n值得一提的是，当前的hints中并没有包含forRegions的字段。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: discovery.k8s.io/v1 kind: EndpointSlice metadata: name: example-hints labels: kubernetes.io/service-name: example-svc addressType: IPv4 ports: - name: http protocol: TCP port: 80 endpoints: - addresses: - \u0026#34;10.1.2.3\u0026#34; conditions: ready: true hostname: pod-1 zone: zone-a hints: forZones: - name: \u0026#34;zone-a\u0026#34; 参考 Well-Known Labels, Annotations and Taints Google Ingress for Anthos：https://cloud.google.com/kubernetes-engine/docs/concepts/ingress-for-anthos#architecture 阿里云ccm支持多个k8s集群的场景（手工指定lb id和server group方案）：https://help.aliyun.com/document_detail/335878.html#title-4wt-hc5-p2p https://tencentcloudcontainerteam.github.io/2019/11/26/service-topology/ https://kubernetes.io/zh/docs/concepts/services-networking/endpoint-slices/ 使用 EndpointSlice 扩展 Kubernetes 网络 Running in multiple zones ","date":"2022-01-25T12:33:00Z","permalink":"/post/k8s%E5%9C%A8region%E5%92%8Czone%E6%96%B9%E9%9D%A2%E7%9A%84%E6%94%AF%E6%8C%81%E6%83%85%E5%86%B5/","title":"k8s在region和zone方面的支持情况"},{"content":"查看磁盘是否为ssd 如果其中的rota值为1，说明为hdd磁盘；如果rota值为0，说明为ssd。\n1 2 3 4 5 # lsblk -o name,size,type,rota,mountpoint NAME SIZE TYPE ROTA MOUNTPOINT vdc 20G disk 1 /var/lib/kubelet/pods/eea3a54c-b211-4ee3-bcbe-70ba3fe84c05/volumes/kubernetes.io~csi/d-t4n36xdqey47v9e0ej8r/mount vda 120G disk 1 └─vda1 120G part 1 / 但该规则在很多虚拟机的场景下并不成立，即使虚拟机的磁盘为ssd，但rota值仍然为1。可以通过修改rota的值的方式来标记磁盘的类型：echo \u0026lsquo;0\u0026rsquo;\u0026gt; /sys/block/vdd/queue/rotational\niostat 只能观察磁盘整体性能，不能看到进程级别的\niops = r/s+w/s 吞吐量 = rkB/s + wkB/s 响应时间 = r_await + w_await iostat -xm $interval: interval为秒\n1 2 3 4 5 6 7 8 9 $ iostat -xm 1 Linux 3.10.0-327.10.1.el7.x86_64 (103-17-52-sh-100-I03.yidian.com) 12/11/2016 _x86_64_ (32 CPU) avg-cpu: %user %nice %system %iowait %steal %idle 4.99 0.00 1.46 0.01 0.00 93.54 Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.00 0.26 0.02 1.62 0.00 0.02 29.31 0.00 0.89 3.37 0.87 0.05 0.01 sdb 0.00 0.81 10.23 67.59 1.19 29.07 796.57 0.01 0.10 0.85 2.85 0.19 1.46 r/s: 每秒发送给磁盘的读请求数 w/s: 每秒发送给磁盘的写请求数 rMB/s: 每秒从磁盘读取的数据量 wMB/s: 每秒向磁盘写入的数据量 %iowait：cpu等待磁盘时间 %util：表示磁盘使用率，该值较大说明磁盘性能存在问题，io队列不为空的时间。由于存在并行io，100%不代表磁盘io饱和 r_await：读请求处理完成时间，包括队列中的等待时间和设备实际处理时间，单位为毫秒 w_await：写请求处理完成时间，包括队列中的等待时间和设备实际处理时间，单位为毫秒 svctm: 瓶颈每次io操作的时间，单位为毫秒，可以反映出磁盘的瓶颈 avgrq-sz：平均每次请求的数据大小 avgqu-sz：平均请求io队列长度 svctm：处理io请求所需要的平均时间，不包括等待时间，单位为毫秒 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ iostat -d -x 1 Linux 3.10.0-327.10.1.el7.x86_64 (103-17-42-sh-100-i08.yidian.com) 01/19/2019 _x86_64_ (24 CPU) Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util sda 0.00 7.58 0.31 266.07 9.54 888.01 6.74 0.03 0.12 8.84 0.11 0.03 0.90 sdb 0.00 0.67 4.39 40.68 386.20 9277.38 428.79 0.03 0.64 3.10 0.37 0.30 1.33 sdd 0.00 0.82 5.08 66.78 451.99 15587.89 446.42 0.02 0.30 3.50 0.05 0.28 2.01 sde 0.00 0.67 4.14 53.77 312.74 12121.84 429.43 0.00 0.03 4.00 1.44 0.30 1.72 sdc 0.00 0.50 0.05 27.47 2.74 6098.56 443.28 0.03 0.95 12.42 0.93 0.21 0.58 sdf 0.00 0.47 1.07 34.58 57.05 7504.68 424.24 0.00 0.05 2.60 2.63 0.25 0.88 sdg 0.00 0.72 0.14 63.87 13.79 14876.74 465.22 0.03 0.42 10.57 0.40 0.19 1.24 sdj 0.00 0.79 1.85 37.81 206.01 7881.89 407.90 0.03 0.67 2.27 0.59 0.36 1.42 sdi 0.00 0.65 0.29 59.54 22.01 13501.71 452.05 0.01 0.25 5.27 0.22 0.19 1.14 sdk 0.00 0.50 0.30 29.46 16.74 6330.68 426.57 0.05 1.58 4.03 1.55 0.24 0.70 sdm 0.00 0.63 0.73 55.70 63.73 13008.16 463.34 0.02 0.35 5.98 0.28 0.23 1.29 sdh 0.00 0.47 0.06 15.68 5.16 2970.09 378.01 0.03 1.82 7.22 1.80 0.27 0.42 sdl 0.00 0.56 1.32 38.14 81.04 8945.16 457.50 0.09 2.22 4.30 2.15 0.19 0.76 wrqm: 每秒合并的写请求数 rrqm：每秒合并的读请求数 pidstat 如果要想查看进程级别的磁盘 io 使用情况，可以使用 pidstat -d 指令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ pidstat -d 1 Linux 3.10.0-1160.88.1.el7.x86_64 (izbp1feliilr5yri84y6saz) 05/08/2023 _x86_64_ (16 CPU) 09:44:24 AM UID PID kB_rd/s kB_wr/s kB_ccwr/s Command 09:44:25 AM 0 371 0.00 184.91 0.00 jbd2/vda1-8 09:44:25 AM 0 1395 0.00 22.64 7.55 rsyslogd 09:44:25 AM 0 19053 0.00 3.77 0.00 jbd2/dm-0-8 09:44:25 AM 1000 25479 0.00 3.77 0.00 org_start 09:44:25 AM 1000 25754 0.00 3.77 0.00 org_start 09:44:25 AM 0 38784 0.00 505.66 0.00 systemd-journal 09:44:25 AM 0 40474 0.00 143.40 0.00 jbd2/dm-1-8 09:44:25 AM 0 41130 0.00 11.32 0.00 jbd2/dm-2-8 09:44:25 AM 1000 41675 0.00 3.77 0.00 start 09:44:25 AM 0 41730 0.00 3.77 0.00 org_start iotop 该命令可以查看所有进程读写 io 情况，该命令通常 os 不会默认安装。在 CentOS 下可以执行 yum install iotop 命令安装。\n","date":"2022-01-18T21:27:07Z","permalink":"/post/linux%E4%B8%8B%E7%A3%81%E7%9B%98%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"Linux下磁盘常用命令"},{"content":"本文记录常用的kubectl命令，不定期更新。\n0.1 统计k8s node上的污点信息 1 kubectl get nodes -o=custom-columns=NodeName:.metadata.name,TaintKey:.spec.taints[*].key,TaintValue:.spec.taints[*].value,TaintEffect:.spec.taints[*].effect 0.2 查看不ready的pod 1 kubectl get pod --all-namespaces -o wide -w | grep -vE \u0026#34;Com|NAME|Running|1/1|2/2|3/3|4/4\u0026#34; 0.3 pod按照重启次数排序 1 2 3 kubectl get pods -A --sort-by=\u0026#39;.status.containerStatuses[0].restartCount\u0026#39; | tail kubectl get pod -A --no-headers | sort -k5 -nr | head 0.4 kubectl proxy命令代理k8s apiserver 该命令经常用在开发的场景下，用来测试k8s api的结果。执行如下命令即可在本地127.0.0.1开启10999端口。\n1 kubectl proxy --port=10999 在本地即可通过curl的方式来访问k8s的apiserver，而无需考虑鉴权问题。例如，curl http://127.0.0.1:10999/apis/batch/v1，即可直接返回结果。\n0.5 --raw命令 该命令经常用在开发的场景下，用来测试k8s api的结果。执行如下的命令，即可返回json格式的结果。\n1 kubectl get --raw /apis/batch/v1 0.6 查看集群中的pod的request和limit情况 1 kubectl get pod -n kube-system -o=custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,PHASE:.status.phase,Request-cpu:.spec.containers\\[0\\].resources.requests.cpu,Request-memory:.spec.containers\\[0\\].resources.requests.memory,Limit-cpu:.spec.containers\\[0\\].resources.limits.cpu,Limit-memory:.spec.containers\\[0\\].resources.limits.memory 得到的效果如下：\n1 2 3 4 5 NAME NAMESPACE PHASE Request-cpu Request-memory Limit-cpu Limit-memory cleanup-for-np-processor-9pjkm kube-system Succeeded \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; coredns-6c6664b94-7rnm8 kube-system Running 100m 70Mi \u0026lt;none\u0026gt; 170Mi coredns-6c6664b94-djxch kube-system Failed 100m 70Mi \u0026lt;none\u0026gt; 170Mi coredns-6c6664b94-khvrb kube-system Running 100m 70Mi \u0026lt;none\u0026gt; 170Mi 0.7 修改对象的status 因为status资源实际上为对象的subResource资源，实际上无法通过kubectl来完成，但该操作还是放到了该文档中。\n下述命令的需求为修改service的status\n1 2 3 4 APISERVER=https://kube-cn-nb-nbsjzxpoc-d01-a.intra.nbsjzx.ali.com:6443 TOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1jazdkciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjhkZWE4MWQ4LTU2YTgtMTFlYy05MDMyLTgwNjE1ZjA4NDI0YSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.O25G_MSmKRU_pIPO_9tFqDvbZm9SOM_Mix7jCJeFiZHzLiSc7n5RanP3QoEldR1IcFN4AZXzlzI1Rb0GyFQH7XmS1eLESMbKnrTR3N5s3wlRp-D5QT0c_RAVAiLKrP7KsSKNcLjQkIO8-Csf_kmizIh6fP0-b7Mp90cw0J8oSlM-ScEeUEksQyXvyisVN9qvvTIkmbsgh7pX5IFcB4mGbvV9loOUC3-LdiiaCkMJzMisEeSJRaajmLlwpWXtb2BSi9HmBMktUE9IVB8ryZ06wbPRianbjoBwtAhcXuRyj1LaEog3aJHsyMA_DOZJtvjYis60AIRZ1iBnc-gZtEFCxw obj=nginx-ingress-lb curl -XPATCH -H \u0026#34;Accept: application/json\u0026#34; -H \u0026#34;Content-Type: application/merge-patch+json\u0026#34; --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --insecure -d \u0026#39;{\u0026#34;status\u0026#34;: {\u0026#34;loadBalancer\u0026#34;: {\u0026#34;ingress\u0026#34;: [{\u0026#34;ip\u0026#34;: \u0026#34;10.209.97.170\u0026#34;}]}}}\u0026#39; $APISERVER/api/v1/namespaces/acs-system/services/nginx-ingress-lb/status APISERVER变量可以通过kubeconfig文件获取到。\nTOKEN变量可以直接使用kube-system下的admin账号。\n执行 kubectl get secrets -n kube-system | grep admin 获取到sa对应的secret 执行 kubectl get secrets -n kube-system xxxx -o yaml 获取到base64之后的token 执行 echo \u0026quot;xxxx\u0026quot; | base64 -d 即可获取到对应的token obj变量为要修改的service对象名称。\n0.8 查看 secret 内容 1 kubectl get secret -n ark-system ark.cmdb.https.origin.tls -o jsonpath=\u0026#39;{.data.ca\\.pem}\u0026#39; | base64 -d 0.9 修改 secret 或 cm 的内容 很多场景下使用 kubectl edit 修改不能完全满足需求，比如某个 key 对应的 value 非常长且包含空格，很难直接编辑。可以通过导出 key 对应的 value 到文件，然后再重新 apply 的方式合入。\n导出 configmap 中特定的 key：\n1 kubectl get cm -n kube-system networkpolicy-config -o jsonpath=\u0026#39;{.data.config\\.yaml}\u0026#39; 修改完成后，将文件重新 apply cm\n1 kubectl create --save-config cm networkpolicy-config -n kube-system --from-file /tmp/config.yaml -o yaml --dry-run | kubectl apply -f - 0.10 删除所有 pod（或特定状态 pod） 1 kubectl get pods --all-namespaces -o wide --no-headers | grep -v Running | awk \u0026#39;{print $1 \u0026#34; \u0026#34; $2}\u0026#39; | while read AA BB; do kubectl get pod -n $AA $BB --no-headers; done 0.11 kubectl debug 常用于网络问题排查，其中镜像 netshoot 中包含了丰富的网络命令。\n1 kubectl exec -it nginx-statefulset-0 bash 进入到 k8s node 网络，其中 ${node} 为 k8s 的 node 名字。\n1 kubectl debug node/${node} -it --image=nicolaka/netshoot 0.12 kubectl patch 1 kubectl patch -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;filed1\u0026#34;: \u0026#34;value1\u0026#34;}}\u0026#39; --type=merge xxx -n yyy 0.13 查询对象的 labels 1 2 3 4 5 6 7 # 查看所有 k8s 节点及其 label kubectl get nodes -o=jsonpath=\u0026#39;{range .items[*]}{.metadata.name}{\u0026#34;\\t\u0026#34;}{.metadata.labels.topology\\.kubernetes\\.io/zone}{\u0026#34;\\n\u0026#34;}{end}\u0026#39; # 查看单个 k8s 节点的 label kubectl get node c25e04016.cloud.e04.amtest130 -o jsonpath=\u0026#34;{.metadata.labels[\u0026#39;topology\\.kubernetes\\.io/zone\u0026#39;]}\u0026#34; # 获取没有污点的节点对应的 label machine-type kubectl get nodes -o json | jq -r \u0026#39;.items[] | select(.spec.taints | not) | [.metadata.name, .metadata.labels[\u0026#34;machine-type\u0026#34;]] | @tsv\u0026#39; ","date":"2022-01-18T00:00:00Z","permalink":"/post/kubectl%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","title":"kubectl常用命令"},{"content":"ipv6在k8s中的支持情况 ipv6特性在k8s中作为一个特定的feature IPv6DualStack来管理。在1.15版本到1.20版本该feature为alpha版本，默认不开启。从1.21版本开始，该feature为beta版本，默认开启，支持pod和service网络的双栈。1.23版本变为稳定版本。\n配置双栈 alpha版本需要通过kube-apiserver、kube-controller-manager、kubelet和kube-proxy的\u0026ndash;feature-gates=\u0026ldquo;IPv6DualStack=true\u0026quot;命令来开启该特性，beta版本该特性默认开启。\n​\nkube-apiserver：\n--service-cluster-ip-range=\u0026lt;IPv4 CIDR\u0026gt;,\u0026lt;IPv6 CIDR\u0026gt; kube-controller-manager:\n--cluster-cidr=\u0026lt;IPv4 CIDR\u0026gt;,\u0026lt;IPv6 CIDR\u0026gt; --service-cluster-ip-range=\u0026lt;IPv4 CIDR\u0026gt;,\u0026lt;IPv6 CIDR\u0026gt; --node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6 对于 IPv4 默认为 /24，对于 IPv6 默认为 /64 kube-proxy:\n--cluster-cidr=\u0026lt;IPv4 CIDR\u0026gt;,\u0026lt;IPv6 CIDR\u0026gt; 使用kind创建k8s集群 由于ipv6的环境并不容易找到，可以使用kind快速在本地拉起一个k8s的单集群环境，同时kind支持ipv6的配置。\n检查当前系统是否支持ipv6，如果输出结果为0，说明当前系统支持ipv6.\n1 2 $ sysctl -a | grep net.ipv6.conf.all.disable_ipv6 net.ipv6.conf.all.disable_ipv6 = 0 创建ipv6单栈k8s集群执行如下命令：\n1 2 3 4 5 6 7 8 9 10 11 cat \u0026gt; kind-ipv6.conf \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: ipv6-only networking: ipFamily: ipv6 # networking: # apiServerAddress: \u0026#34;127.0.0.1\u0026#34; # apiServerPort: 6443 EOF kind create cluster --config kind-ipv6.conf 创建ipv4/ipv6双栈集群执行如下命令：\n1 2 3 4 5 6 7 8 9 10 11 cat \u0026gt; kind-dual-stack.conf \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: dual-stack networking: ipFamily: dual # networking: # apiServerAddress: \u0026#34;127.0.0.1\u0026#34; # apiServerPort: 6443 EOF kind create cluster --config kind-dual-stack.conf service网络 spec定义 k8s的service网络为了支持双栈，新增加了几个字段。\n.spec.ipFamilies用来设置地址族，service一旦场景后该值不可认为修改，但可以通过修改.spec.ipFamilyPolicy来简介修改该字段的值。为数组格式，支持如下值：\n[\u0026ldquo;IPv4\u0026rdquo;] [\u0026ldquo;IPv6\u0026rdquo;] [\u0026ldquo;IPv4\u0026rdquo;,\u0026ldquo;IPv6\u0026rdquo;] （双栈） [\u0026ldquo;IPv6\u0026rdquo;,\u0026ldquo;IPv4\u0026rdquo;] （双栈） 上述数组中的第一个元素会决定.spec.ClusterIP中的字段。\n​\n.spec.ClusterIPs：由于.spec.ClusterIP的value为字符串，不支持同时设置ipv4和ipv6两个ip地址。因此又扩展出来了一个.spec.ClusterIPs字段，该字段的value为宿主元祖。在Headless类型的Service情况下，该字段的值为None。\n​\n.spec.ClusterIP：该字段的值跟.spec.ClusterIPs中的第一个元素的值保持一致。\n​\n.spec.ipFamilyPolicy支持如下值：\nSingleStack：默认值。会使用.spec.ipFamilies数组中配置的第一个协议来分配cluster ip。如果没有指定.spec.ipFamilies，会使用service-cluster-ip-range配置中第一个cidr中来配置地址。 PreferDualStack：如果 .spec.ipFamilies 没有设置，使用 k8s 集群默认的 ipFamily。 RequireDualStack：同时分配ipv4和ipv6地址。.spec.ClusterIP的值会从.spec.ClusterIPs选择第一个元素。 service配置场景 先创建如下的deployment，以便于后面试验的service可以关联到pod。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: MyApp spec: replicas: 3 selector: matchLabels: app: MyApp template: metadata: labels: app: MyApp spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 未指定任何协议栈信息 在没有指定协议栈信息的Service，创建出来的service .spec.ipFamilyPolicy为SingleStack。同时会使用service-cluster-ip-range配置中第一个cidr中来配置地址。如果第一个cidr为ipv6，则service分配的clusterip为ipv6地址。创建如下的service\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: Service metadata: name: my-service labels: app: MyApp spec: selector: app: MyApp ports: - protocol: TCP port: 80 会生成如下的service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: v1 kind: Service metadata: labels: app: MyApp name: my-service namespace: default spec: clusterIP: 10.96.80.114 clusterIPs: - 10.96.80.114 ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: MyApp sessionAffinity: None type: ClusterIP status: loadBalancer: {} 指定.spec.ipFamilyPolicy为PreferDualStack 创建如下的service\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: my-service-2 labels: app: MyApp spec: ipFamilyPolicy: PreferDualStack selector: app: MyApp ports: - protocol: TCP port: 80 提交到环境后会生成如下的service，可以看到.spec.clusterIPs中的ip地址跟ipFamilies中的顺序一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: v1 kind: Service metadata: labels: app: MyApp name: my-service-2 namespace: default spec: clusterIP: 10.96.221.70 clusterIPs: - 10.96.221.70 - fd00:10:96::7d1 ipFamilies: - IPv4 - IPv6 ipFamilyPolicy: PreferDualStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: MyApp sessionAffinity: None type: ClusterIP status: loadBalancer: {} 查看Endpoints，可以看到subsets中的地址为pod的ipv4协议地址。Endpoints中的地址跟service的.spec.ipFamilies数组中的第一个协议的值保持一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion: v1 kind: Endpoints metadata: labels: app: MyApp name: my-service-2 namespace: default subsets: - addresses: - ip: 10.244.0.5 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-vrfps namespace: default resourceVersion: \u0026#34;16875\u0026#34; uid: 30a1d787-f799-4250-8c56-c96564ca9239 - ip: 10.244.0.6 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-wgz72 namespace: default resourceVersion: \u0026#34;16917\u0026#34; uid: 8166d43e-2702-45c6-839e-b3005f44f647 - ip: 10.244.0.7 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-x4lt5 namespace: default resourceVersion: \u0026#34;16896\u0026#34; uid: f9c2968f-ca59-4ba9-a69f-358c202a964b ports: - port: 80 protocol: TCP 接下来指定.spec.ipFamilies的顺序再看一下执行的结果，创建如下的service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Service metadata: name: my-service-3 labels: app: MyApp spec: ipFamilyPolicy: PreferDualStack ipFamilies: - IPv6 - IPv4 selector: app: MyApp ports: - protocol: TCP port: 80 在环境中生成的service如下，可以看到.spec.clusterIPs中的顺序第一个为ipv6地址，.spec.clusterIP同样为ipv6地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: v1 kind: Service metadata: labels: app: MyApp name: my-service-3 namespace: default spec: clusterIP: fd00:10:96::c306 clusterIPs: - fd00:10:96::c306 - 10.96.147.82 ipFamilies: - IPv6 - IPv4 ipFamilyPolicy: PreferDualStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: MyApp sessionAffinity: None type: ClusterIP status: loadBalancer: {} 查看Endpoints，可以看到subsets中的地址为pod的ipv6协议地址。Endpoints中的地址跟service的.spec.ipFamilies数组中的第一个协议的值保持一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 apiVersion: v1 kind: Endpoints metadata: labels: app: MyApp name: my-service-3 namespace: default subsets: - addresses: - ip: fd00:10:244::5 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-vrfps namespace: default resourceVersion: \u0026#34;16875\u0026#34; uid: 30a1d787-f799-4250-8c56-c96564ca9239 - ip: fd00:10:244::6 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-wgz72 namespace: default resourceVersion: \u0026#34;16917\u0026#34; uid: 8166d43e-2702-45c6-839e-b3005f44f647 - ip: fd00:10:244::7 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-x4lt5 namespace: default resourceVersion: \u0026#34;16896\u0026#34; uid: f9c2968f-ca59-4ba9-a69f-358c202a964b ports: - port: 80 protocol: TCP 单栈和双栈之间的切换 虽然.spec.ipFamilies字段不允许直接修改，但.spec.ipFamilyPolicy字段允许修改，但并不影响单栈和双栈之间的切换。\n单栈变双栈只需要修改.spec.ipFamilyPolicy从SingleStack变为PreferDualStack或者RequireDualStack即可。\n双栈同样可以变为单栈，只需要修改.spec.ipFamilyPolicy从PreferDualStack或者RequireDualStack变为SingleStack即可。此时.spec.ipFamilies会自动变更为一个元素，.spec.clusterIPs同样会变更为一个元素。\nLoadBalancer类型的Service 对于LoadBalancer类型的Service，单栈的情况下，会在.status.loadBalancer.ingress设置vip地址。如果是ingress中的ip地址为双栈，此时应该是将双栈的vip地址同时写到.status.loadBalancer.ingress中，并且要保证其顺序跟serivce的.spec.ipFamilies中的顺序一致。\npod网络 pod网络要支持ipv6，需要容器网络插件的支持。为了支持ipv6特性，新增加了.status.podIPs字段，用来展示pod上分配的ipv4和ipv6的信息。.status.podIP字段的值跟.status.podIPs数组的第一个元素的值保持一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 apiVersion: v1 kind: Pod metadata: labels: k8s-app: kube-dns pod-template-hash: 558bd4d5db name: coredns-558bd4d5db-b2zbj namespace: kube-system spec: containers: - args: - -conf - /etc/coredns/Corefile image: k8s.gcr.io/coredns/coredns:v1.8.0 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /ready port: 8181 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/coredns name: config-volume readOnly: true - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-hgxnc readOnly: true dnsPolicy: Default enableServiceLinks: true nodeName: dual-stack-control-plane nodeSelector: kubernetes.io/os: linux preemptionPolicy: PreemptLowerPriority priority: 2000000000 priorityClassName: system-cluster-critical restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: coredns serviceAccountName: coredns terminationGracePeriodSeconds: 30 tolerations: - key: CriticalAddonsOnly operator: Exists ... volumes: - configMap: defaultMode: 420 items: - key: Corefile path: Corefile name: coredns name: config-volume - name: kube-api-access-hgxnc projected: defaultMode: 420 sources: - serviceAccountToken: expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespace status: conditions: - lastProbeTime: null lastTransitionTime: \u0026#34;2022-01-16T12:51:24Z\u0026#34; status: \u0026#34;True\u0026#34; type: Initialized ... containerStatuses: - containerID: containerd://6da36ab908291ca1b4141a86d70f8c2bb150a933336d852bcabe2118aa1a3439 image: k8s.gcr.io/coredns/coredns:v1.8.0 imageID: sha256:296a6d5035e2d6919249e02709a488d680ddca91357602bd65e605eac967b899 lastState: {} name: coredns ready: true restartCount: 0 started: true state: running: startedAt: \u0026#34;2022-01-16T12:51:27Z\u0026#34; hostIP: 172.18.0.2 phase: Running podIP: 10.244.0.3 podIPs: - ip: 10.244.0.3 - ip: fd00:10:244::3 qosClass: Burstable startTime: \u0026#34;2022-01-16T12:51:24Z\u0026#34; pod中要想获取到ipv4、ipv6地址，可以通过downward api的形式将.status.podIPs以环境变量的形式传递到容器中，在pod中通过环境变量获取到的格式为: 10.244.1.4,a00:100::4。\nk8s node 在宿主机上可以通过ip addr show eth0的方式来查看网卡上的ip地址。\n1 2 3 4 5 6 7 8 9 # ip addr show eth0 23: eth0@if24: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fc00:f853:ccd:e793::2/64 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:2/64 scope link valid_lft forever preferred_lft forever 宿主机的网卡上有ipv6地址，k8s node上的.status.addresses中有所体现，type为InternalIP即包含了ipv4地址，又包含了ipv6地址，但此处并没有字段标识当前地址为ipv4，还是ipv6。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 apiVersion: v1 kind: Node metadata: annotations: kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock node.alpha.kubernetes.io/ttl: \u0026#34;0\u0026#34; volumes.kubernetes.io/controller-managed-attach-detach: \u0026#34;true\u0026#34; labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: dual-stack-control-plane kubernetes.io/os: linux node-role.kubernetes.io/control-plane: \u0026#34;\u0026#34; node-role.kubernetes.io/master: \u0026#34;\u0026#34; node.kubernetes.io/exclude-from-external-load-balancers: \u0026#34;\u0026#34; name: dual-stack-control-plane spec: podCIDR: 10.244.0.0/24 podCIDRs: - 10.244.0.0/24 - fd00:10:244::/64 providerID: kind://docker/dual-stack/dual-stack-control-plane status: addresses: - address: 172.18.0.2 type: InternalIP - address: fc00:f853:ccd:e793::2 type: InternalIP - address: dual-stack-control-plane type: Hostname allocatable: cpu: \u0026#34;4\u0026#34; ephemeral-storage: 41152812Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 15936568Ki pods: \u0026#34;110\u0026#34; capacity: cpu: \u0026#34;4\u0026#34; ephemeral-storage: 41152812Ki hugepages-1Gi: \u0026#34;0\u0026#34; hugepages-2Mi: \u0026#34;0\u0026#34; memory: 15936568Ki pods: \u0026#34;110\u0026#34; conditions: - lastHeartbeatTime: \u0026#34;2022-01-16T15:01:48Z\u0026#34; lastTransitionTime: \u0026#34;2022-01-16T12:50:54Z\u0026#34; message: kubelet has sufficient memory available reason: KubeletHasSufficientMemory status: \u0026#34;False\u0026#34; type: MemoryPressure ... daemonEndpoints: kubeletEndpoint: Port: 10250 images: - names: - k8s.gcr.io/kube-proxy:v1.21.1 sizeBytes: 132714699 ... nodeInfo: architecture: amd64 bootID: 7f95abb9-7731-4a8c-9258-4a91cdcfb2ca containerRuntimeVersion: containerd://1.5.2 kernelVersion: 4.18.0-305.an8.x86_64 kubeProxyVersion: v1.21.1 kubeletVersion: v1.21.1 machineID: 8f6a98bffc184893ab6bc260e705421b operatingSystem: linux osImage: Ubuntu 21.04 systemUUID: f7928fdb-32be-4b6e-8dfd-260b6820f067 Ingress Ingress 可以通过开关 disable-ipv6 来控制是否开启 ipv6，默认 ipv6 开启。\n在开启 ipv6 的情况下，如果 nginx ingress 的 pod 本身没有ipv6 的 ip地址，则在 nginx 的配置文件中并不会监听 ipv6 的端口号。\n1 2 listen 80 ; listen 443 ssl http2 ; 如果 nginx ingress 的 pod 本身包含 ipv6 地址，则 nginx 的配置文件如下：\n1 2 3 4 listen 80 ; listen [::]:80 ; listen 443 ssl http2 ; listen [::]:443 ssl http2 ; 参考资料：\n阿里云ACK的Nginx Ingress支持ipv6特性：https://help.aliyun.com/document_detail/378167.html [Nginx Ingress 支持 ipv6](ConfigMap - NGINX Ingress Controller (kubernetes.github.io)) 引用 https://kubernetes.io/docs/concepts/services-networking/dual-stack/ https://kubernetes.io/docs/tasks/network/validate-dual-stack/ https://kind.sigs.k8s.io/docs/user/configuration/ ","date":"2022-01-16T23:41:33Z","permalink":"/post/k8s-ipv4/ipv6%E5%8F%8C%E6%A0%88/","title":"k8s ipv4/ipv6双栈"},{"content":"背景 Federation V1 Federation v1 在 K8s v1.3 左右就已经着手设计（Design Proposal），并在后面几个版本中发布了相关的组件与命令行工具（kubefed），用于帮助使用者快速建立联邦集群，并在 v1.6 时，进入了 Beta 阶段；但 Federation v1 在进入 Beta 后，就没有更进一步的发展，由于灵活性和 API 成熟度的问题，在 K8s v1.11 左右正式被弃用。\nFederation V2架构 v2 版本利用 CRD 实现了整体功能，通过定义多种自定义资源（CR），从而省掉了 v1 中的 API Server；v2 版本由两个组件构成：\nadmission-webhook 提供了准入控制 controller-manager 处理自定义资源以及协调不同集群间的状态 代码行数在2万行左右。\n术语 Host Cluster：主集群/父集群 Member Cluster：子集群/成员集群 CRD扩展 KubeFedCluster 子集群抽象，通过kubefedctl join命令创建。后续controller会使用该信息来访问子k8s集群。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 kind: KubeFedCluster metadata: name: kind-cluster1 namespace: kube-federation-system spec: apiEndpoint: https://172.21.115.165:6443 caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1USXlOREUyTlRNME9Wb1hEVE14TVRJeU1qRTJOVE0wT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTmxjCnF3UHd5cHNFc2M0aXB3QnFuRndDN044RXo2Slhqajh3ZzRybnlXTDNSdlZ0cmVua1Nha1VyYlRjZWVIck9lQTUKWGlNUVo2T1FBY25tUGU0Q2NWSkFoL2ZLQzBkeU9uL0ZZeXgyQXppRjBCK1ZNaUFhK2dvME1VMmhMZ1R5eVFGdQpDbWFmUGtsNmJxZUFJNCtCajZJUWRqY3dVMHBjY3lrNGhSTUxnQmhnTUh4NWkzVkpQckQ2Y284dHcwVnllWncyCkdDUlh2ZzlSd0QweUs5eitOVS9LVS83QjBiMTBvekpNRlVJMktPZmI4N1RkQ0h2NmlBdlVRYVdKc1BqQ0M3MzQKcnBma1ZGZXB2S2liKy9lUVBFaDE4VE5qaitveEVGMnl0Vmo2ZWVKeFc3VVZrbit0T3BxdGpEeUtKVDllNnlwdAp6U1VDTnRZWTAzckdhNTlLczBVQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZOTnRQU3hsMlMxUldOb1BCeXNIcHFoRXJONlVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDME0vaktCL2dqcWRSNlRLNXJQVktSSFU2cy8xTTh2eTY2WFBiVEx4M0srazdicUFWdAoxVUFzcUZDZmNrVk91WHY3eFQ0ZHIzVzdtMHE1bDYzRGg3ZDJRZDNiQ00zY2FuclpNd01OM0lSMlpmYzR0VlBGCnRTMFFONElTa0hsYnBudXQxb0F3cy9CaXNwaXNRQ0VtbHF3Zy9xbmdPMStlWWRoWm5vRW40SEFDamF4Slc5MS8KNXlOR1pKRXdia2dYQTVCbSs3VEZRL2FiYnp5a1JvOWtTMnl5c29pMnVzcUg0ZnlVS0ZWK2RETnp3Ujh0ck16cgpjWkRBNHpaVFB1WGlYRkVsWlNRa2NJSGIyV0VsYmZNRGpKNjlyVG5wakJCOWNPQ25jaHVmK0xiOXQwN1lJQ01wCmNlK0prNVp2RElRUFlKK3hTeGdRaVJxMTFraWlKcFE4Wm1FWgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== proxyURL: \u0026#34;\u0026#34; secretRef: name: kind-cluster1-dbxns status: conditions: - lastProbeTime: \u0026#34;2021-12-24T17:00:47Z\u0026#34; lastTransitionTime: \u0026#34;2021-12-24T17:00:07Z\u0026#34; message: /healthz responded with ok reason: ClusterReady status: \u0026#34;True\u0026#34; type: Ready Fedrated 发布应用到子集群时手工创建。每一个要被联邦管理的资源都会对应一个Fedrated类型的资源，比如ConfigMap对应的是FederatedConfigMap。FederatedConfigMap包含了三部分的信息：\ntemplate：联邦的资源详细spec信息，需要特别注意的是并没有包含metadata部分的信息。 placement：template中的spec信息要部署的k8s子集群信息 overrides：允许对部分k8s的部分资源进行自定义修改 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: types.kubefed.io/v1beta1 kind: FederatedConfigMap metadata: name: test-configmap namespace: test-namespace spec: template: data: A: ala ma kota placement: clusters: - name: cluster2 - name: cluster1 overrides: - clusterName: cluster2 clusterOverrides: - path: /data value: foo: bar FederatedTypeConfig 通过kubefedctl enable 创建。定义了哪些k8s api资源需要联邦，下面的例子描述了k8s ConfigMap要被联邦资源FederatedConfigMap所管理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: core.kubefed.io/v1beta1 kind: FederatedTypeConfig metadata: annotations: meta.helm.sh/release-name: kubefed meta.helm.sh/release-namespace: kube-federation-system finalizers: - core.kubefed.io/federated-type-config labels: app.kubernetes.io/managed-by: Helm name: configmaps namespace: kube-federation-system spec: federatedType: group: types.kubefed.io kind: FederatedConfigMap pluralName: federatedconfigmaps scope: Namespaced version: v1beta1 propagation: Enabled # 是否启用该联邦对象 targetType: kind: ConfigMap pluralName: configmaps scope: Namespaced version: v1 status: observedGeneration: 1 propagationController: Running statusController: NotRunning ReplicaSchedulingPreference 用来管理相同名字的FederatedDeployment或FederatedReplicaset资源，保证所有子集群的副本数为spec.totalReplicas。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: scheduling.kubefed.io/v1alpha1 kind: ReplicaSchedulingPreference metadata: name: test-deployment namespace: test-ns spec: targetKind: FederatedDeployment totalReplicas: 9 clusters: A: minReplicas: 4 maxReplicas: 6 weight: 1 B: minReplicas: 4 maxReplicas: 8 weight: 2 安装 使用kind来在本机创建多个k8s集群的方式测试。通过kind创建两个k8s集群 kind-cluster1和kind-cluster2。 ​\n安装kubefedctl\n1 2 3 4 wget https://github.com/kubernetes-sigs/kubefed/releases/download/v0.9.0/kubefedctl-0.9.0-linux-amd64.tgz tar zvxf kubefedctl-0.9.0-linux-amd64.tgz chmod u+x kubefedctl mv kubefedctl /usr/local/bin/ 在第一个集群安装KubeFed，在第一个集群执行如下的命令\n1 2 3 4 5 6 7 8 9 $ helm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts $ helm repo list $ helm --namespace kube-federation-system upgrade -i kubefed kubefed-charts/kubefed --version=0.9.0 --create-namespace # 会安装如下两个deployment，其中一个是controller，另外一个是webhook $ kubectl get deploy -n kube-federation-system NAME READY UP-TO-DATE AVAILABLE AGE kubefed-admission-webhook 1/1 1 1 7m40s kubefed-controller-manager 2/2 2 2 7m40s 子集群注册 将cluster1和cluster2集群加入到cluster1中\n1 2 kubefedctl join kind-cluster1 --cluster-context kind-cluster1 --host-cluster-context kind-cluster1 --v=2 kubefedctl join kind-cluster2 --cluster-context kind-cluster2 --host-cluster-context kind-cluster1 --v=2 在第一个集群可以看到在kube-federation-system中创建出了两个新的kubefedcluster对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ kubectl -n kube-federation-system get kubefedcluster NAME AGE READY kind-cluster1 17s True kind-cluster2 16s True $ apiVersion: core.kubefed.io/v1beta1 kind: KubeFedCluster metadata: name: kind-cluster1 namespace: kube-federation-system spec: apiEndpoint: https://172.21.115.165:6443 caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1USXlOREUyTlRNME9Wb1hEVE14TVRJeU1qRTJOVE0wT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTmxjCnF3UHd5cHNFc2M0aXB3QnFuRndDN044RXo2Slhqajh3ZzRybnlXTDNSdlZ0cmVua1Nha1VyYlRjZWVIck9lQTUKWGlNUVo2T1FBY25tUGU0Q2NWSkFoL2ZLQzBkeU9uL0ZZeXgyQXppRjBCK1ZNaUFhK2dvME1VMmhMZ1R5eVFGdQpDbWFmUGtsNmJxZUFJNCtCajZJUWRqY3dVMHBjY3lrNGhSTUxnQmhnTUh4NWkzVkpQckQ2Y284dHcwVnllWncyCkdDUlh2ZzlSd0QweUs5eitOVS9LVS83QjBiMTBvekpNRlVJMktPZmI4N1RkQ0h2NmlBdlVRYVdKc1BqQ0M3MzQKcnBma1ZGZXB2S2liKy9lUVBFaDE4VE5qaitveEVGMnl0Vmo2ZWVKeFc3VVZrbit0T3BxdGpEeUtKVDllNnlwdAp6U1VDTnRZWTAzckdhNTlLczBVQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZOTnRQU3hsMlMxUldOb1BCeXNIcHFoRXJONlVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDME0vaktCL2dqcWRSNlRLNXJQVktSSFU2cy8xTTh2eTY2WFBiVEx4M0srazdicUFWdAoxVUFzcUZDZmNrVk91WHY3eFQ0ZHIzVzdtMHE1bDYzRGg3ZDJRZDNiQ00zY2FuclpNd01OM0lSMlpmYzR0VlBGCnRTMFFONElTa0hsYnBudXQxb0F3cy9CaXNwaXNRQ0VtbHF3Zy9xbmdPMStlWWRoWm5vRW40SEFDamF4Slc5MS8KNXlOR1pKRXdia2dYQTVCbSs3VEZRL2FiYnp5a1JvOWtTMnl5c29pMnVzcUg0ZnlVS0ZWK2RETnp3Ujh0ck16cgpjWkRBNHpaVFB1WGlYRkVsWlNRa2NJSGIyV0VsYmZNRGpKNjlyVG5wakJCOWNPQ25jaHVmK0xiOXQwN1lJQ01wCmNlK0prNVp2RElRUFlKK3hTeGdRaVJxMTFraWlKcFE4Wm1FWgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== proxyURL: \u0026#34;\u0026#34; secretRef: name: kind-cluster1-dbxns status: conditions: - lastProbeTime: \u0026#34;2021-12-24T17:00:47Z\u0026#34; lastTransitionTime: \u0026#34;2021-12-24T17:00:07Z\u0026#34; message: /healthz responded with ok reason: ClusterReady status: \u0026#34;True\u0026#34; type: Ready 要想将cluster2从主集群中移除，可以执行 kubefedctl unjoin kind-cluster2 \u0026ndash;cluster-context kind-cluster2 \u0026ndash;host-cluster-context kind-cluster1 \u0026ndash;v=2\n应用发布 集群联邦API kubefed将资源分为普通k8s资源类型和联邦的资源类型。在默认的场景下，kubefed已经内置将很多资源类型做了集群联邦。\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ k get FederatedTypeConfig -n kube-federation-system NAME AGE clusterrolebindings.rbac.authorization.k8s.io 19h clusterroles.rbac.authorization.k8s.io 19h configmaps 19h deployments.apps 19h ingresses.extensions 19h jobs.batch 19h namespaces 19h replicasets.apps 19h secrets 19h serviceaccounts 19h services 19h 将crd资源类型集群联邦，执行 kubefedctl enable 命令\n1 2 3 4 $ kubefedctl enable customresourcedefinitions I1224 20:32:54.537112 687543 util.go:141] Api resource found. customresourcedefinition.apiextensions.k8s.io/federatedcustomresourcedefinitions.types.kubefed.io created federatedtypeconfig.core.kubefed.io/customresourcedefinitions.apiextensions.k8s.io created in namespace kube-federation-system 可以看到会多出一个名字为federatedcustomresourcedefinitions.types.kubefed.io 的CRD 资源，同时会新创建一个FederatedTypeConfig类型的资源。当创建了FederatedTypeConfig后，就可以通过创建federatedcustomresourcedefinition类型的实例来向各个子集群发布CRD资源了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ k get crd federatedcustomresourcedefinitions.types.kubefed.io NAME CREATED AT federatedcustomresourcedefinitions.types.kubefed.io 2021-12-24T12:32:54Z $ k get FederatedTypeConfig -n kube-federation-system customresourcedefinitions.apiextensions.k8s.io -o yaml apiVersion: core.kubefed.io/v1beta1 kind: FederatedTypeConfig metadata: finalizers: - core.kubefed.io/federated-type-config name: customresourcedefinitions.apiextensions.k8s.io namespace: kube-federation-system spec: federatedType: group: types.kubefed.io kind: FederatedCustomResourceDefinition pluralName: federatedcustomresourcedefinitions scope: Cluster version: v1beta1 propagation: Enabled targetType: group: apiextensions.k8s.io kind: CustomResourceDefinition pluralName: customresourcedefinitions scope: Cluster version: v1 status: observedGeneration: 1 propagationController: Running statusController: NotRunning 在example/sample1下包含了很多例子，可以直接参考。接下来使用sample1中的例子进行试验。 ​\n在父集群创建namespace test-namespace\n1 2 $ kubectl create -f namespace.yaml namespace/test-namespace created 在主集群中创建federatednamespace资源\n1 2 $ k apply -f federatednamespace.yaml federatednamespace.types.kubefed.io/test-namespace created 在子集群中即可查询到新创建的namespace资源\n1 2 3 $ k get ns test-namespace NAME STATUS AGE test-namespace Active 4m49s 其他的对象均可以按照跟上述namespace同样的方式来创建，比较特殊的对象为clusterrulebinding，该对象在默认情况下没有联邦api FederatedClusterRoleBinding，因此需要手工先创建FederatedClusterRoleBinding联邦api类型。\n1 2 3 4 $ kubefedctl enable clusterrolebinding I1225 01:46:42.779166 818254 util.go:141] Api resource found. customresourcedefinition.apiextensions.k8s.io/federatedclusterrolebindings.types.kubefed.io created federatedtypeconfig.core.kubefed.io/clusterrolebindings.rbac.authorization.k8s.io created in namespace kube-federation-system 在创建完成FederatedClusterRoleBinding联邦api类型后，即可以创建出FederatedClusterRoleBinding类型的对象。\n子集群的个性化配置 通过Fedrated中的spec.overrides来完成，可以覆盖spec.template中的内容\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 apiVersion: types.kubefed.io/v1beta1 kind: FederatedDeployment metadata: name: test-deployment namespace: test-namespace spec: template: metadata: labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx placement: clusters: - name: cluster2 - name: cluster1 overrides: - clusterName: cluster2 clusterOverrides: - path: \u0026#34;/spec/replicas\u0026#34; value: 5 - path: \u0026#34;/spec/template/spec/containers/0/image\u0026#34; value: \u0026#34;nginx:1.17.0-alpine\u0026#34; - path: \u0026#34;/metadata/annotations\u0026#34; op: \u0026#34;add\u0026#34; value: foo: bar - path: \u0026#34;/metadata/annotations/foo\u0026#34; op: \u0026#34;remove\u0026#34; 多集群应用调度 发布应用到特定的子集群 在Fedrated的spec.placement.clusters中，定义了要发布到哪个子集群的信息。\n1 2 3 4 placement: clusters: - name: cluster2 - name: cluster1 不过上述方法需要指定特定的集群，为了有更丰富的灵活性，kubefed还提供了label selector的机制，可以提供spec.placement.clusterSelector来指定一组集群。\n1 2 3 4 5 6 spec: placement: clusters: [] clusterSelector: matchLabels: foo: bar 标签选择器通过跟kubefedclusters对象的label来进行匹配，可以执行下述命令给kubefedclusters来打标签。\n1 kubectl label kubefedclusters -n kube-federation-system kind-cluster1 foo=bar 子集群差异化调度 上述调度功能被选择的子集群为平等关系，如果要想子集群能够有所差异，可以使用ReplicaSchedulingPreference来完成，目前仅支持deployment和replicaset两种类型的资源，还不支持statefulset。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: scheduling.kubefed.io/v1alpha1 kind: ReplicaSchedulingPreference metadata: name: test-deployment namespace: test-ns spec: targetKind: FederatedDeployment totalReplicas: 9 clusters: A: minReplicas: 4 maxReplicas: 6 weight: 1 B: minReplicas: 4 maxReplicas: 8 weight: 2 spec.targetKind用来指定管理的类型，仅支持FederatedDeployment和FederatedReplicaset。 spec.totalReplicas用来指定所有子集群的副本数总和。优先级要高于FederatedDeployment和FederatedReplicaset的spec.template中指定的副本数。 spec.clusters用来指定每个子集群的最大、最小副本数和权重。 spec.rebalance自动调整整个集群中的副本数，比如一个集群中的pod挂掉后，可以将pod迁移到另外的集群中。\n子集群之间的交互 无\n缺点 父集群充当了所有集群的单一控制面 通过联邦CRD来管理资源，无法直接使用k8s原生的资源，集群间维护CRD版本和API版本一致性导致升级比较复杂。 参考资料 官方文档：https://github.com/kubernetes-sigs/kubefed/blob/master/docs/userguide.md#using-cluster-selector ","date":"2022-01-09T11:33:15Z","permalink":"/post/k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E6%96%B9%E6%A1%88-kubefed-v2/","title":"k8s多集群管理方案 - KubeFed V2"},{"content":"使用openssl创建自签名证书 创建CA\n1 2 3 openssl genrsa -out root.key 4096 openssl req -new -x509 -days 1000 -key root.key -out root.crt openssl x509 -text -in root.crt -noout 创建私钥和公钥文件\n1 2 3 4 # 用来产生私钥文件server.key openssl genrsa -out server.key 2048 # 产生公钥文件 openssl rsa -in server.key -pubout -out server.pem 创建签名请求\n1 2 3 4 openssl req -new -key server.key -out server.csr # 查看创建的签名请求 openssl req -noout -text -in server.csr 创建自签名证书 新建自签名证书的附加信息server.ext，内容如下\n1 2 3 4 5 6 7 8 9 authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment subjectAltName = @alt_names [alt_names] DNS.1 = localhost IP.2 = 127.0.0.1 IP.3 = 10.66.3.6 使用ca签发ssl证书，此时会产生server.crt文件，即为证书文件\n1 2 3 openssl x509 -req -in server.csr -out server.crt -days 3650 \\ -CAcreateserial -CA root.crt -CAkey root.key \\ -CAserial serial -extfile server.ext 查看证书文件内容\n1 openssl x509 -in server.crt -noout -text 使用ca校验证书是否通过\n1 openssl verify -CAfile root.crt server.crt 使用cfssl签发证书 cfssl是CloudFlare开源的一款tls工具，使用go语言编写，地址：https://github.com/cloudflare/cfssl。\n安装 mac用户：brew install cfssl\nlinux用户可以直接下载二进制文件：https://github.com/cloudflare/cfssl/releases\n签发证书 待补充\n证书格式 证书按照格式可以分为二进制和文本文件两种格式。\n二进制格式分为：\n.der或者.cer：用来存放证书信息，不包含私钥。 文本格式分为：\n.pem：存放证书或者私钥。一般是.key文件存放私钥信息。对于pem或者key文件，如果存在**——BEGIN CERTIFICATE——，则说明这是一个证书文件。如果存在—–BEGIN RSA PRIVATE KEY—–**，则说明这是一个私钥文件。 *.key：用来存放私钥文件。 *.crt：证书请求文件，格式的开头为：\u0026mdash;\u0026ndash;BEGIN CERTIFICATE REQUEST\u0026mdash;\u0026ndash; 证书格式的转换 将cert证书转换为pem格式\n1 2 openssl rsa -in server.key -text \u0026gt; server-key.pem openssl x509 -in server.crt -out server.pem 将pem格式转换为cert格式\n证书查看 查看特定域名的证书内容：echo | openssl s_client -connect www.baidu.com:443 -servername www.baidu.com 2\u0026gt;/dev/null | openssl x509 -noout -text\n获取特定域名的证书内容：openssl s_client -servername www.baidu.com -connect www.baidu.com:443 \u0026lt;/dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p'\n在 k8s 中，证书通常存放在 secret 中，可以使用 kubeclt + openssl 命令查看证书内容：kubectl get secret -n test-cluster sharer -o jsonpath='{.data.tls\\.crt}' | base64 --decode | openssl x509 -text -noout\n证书的使用 curl命令关于证书的用法：\n\u0026ndash;cacert：指定ca来校验server端的证书合法性 \u0026ndash;cert：指定客户端的证书文件，用在双向认证mTLS中 \u0026ndash;key：私钥文件名，用在双向认证mTLS中 参考链接 https://gist.github.com/liuguangw/4d4b87b750be8edb700ff94c783b1dd4 https://coolshell.cn/articles/21708.html https://help.aliyun.com/document_detail/160093.html ","date":"2022-01-07T10:34:15Z","permalink":"/post/%E8%AF%81%E4%B9%A6%E6%8A%80%E6%9C%AF/","title":"证书技术"},{"content":"简介 腾讯云开源的k8s多集群管理方案，可发布应用到多个k8s集群。 https://github.com/clusternet/clusternet GitHub Star：891\n架构 包含clusternet-hub和clusternet-agent两个组件。\nclusternet-hub部署在父集群，用途：\n接收子集群的注册请求，并为每个自己群创建namespace、serviceaccount等资源 提供父集群跟各个子集群的长连接 提供restful api来访问各个子集群，同时支持子集群的互访 clusternet-agent部署在子集群，用途：\n自动注册子集群到父集群 心跳报告到中心集群，包括k8s版本、运行平台、健康状态等 通过websocket协议跟父集群通讯 CRD抽象 ClusterRegistrationRequest clusternet-agent在父集群中创建的CR，一个k8s集群一个\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: clusters.clusternet.io/v1beta1 kind: ClusterRegistrationRequest metadata: labels: clusters.clusternet.io/cluster-id: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusters.clusternet.io/cluster-name: clusternet-cluster-pmlbp clusters.clusternet.io/registered-by: clusternet-agent name: clusternet-2bcd48d2-0ead-45f7-938b-5af6af7da5fd spec: clusterId: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusterName: clusternet-cluster-pmlbp # 集群名称 clusterType: EdgeCluster # 集群类型 syncMode: Dual status: caCertificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1USXhNekV3TWpjeE5Wb1hEVE14TVRJeE1URXdNamN4TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHVCCmJwdXNaZXM5RXFXaEw1WUVGb2c4eHRJai9jbFh2S1lTMnZYbGR4cUt1MUR3VHV3aUcxZUN4VGlHa2dLQXVXd1IKVFFheEh5aGY3U29lc0hqMG43NnFBKzhQT05lS1VGdERJOWVzejF2WTF5bXFoUHR0QVo0cGhkWmhmbXJjZTJLRQpuMS84MzNWbWlXd0pSZmNWcEJtTU52MjFYMVVwNWp6RGtncS9tY0JOOGN0ZU5PMEpKNkVVeTE2RXZLbjhyWG90ClErTW5PUHE4anFNMzJjRFFqYWVEdGxvM2kveXlRd1NMa2wzNFo4aElwZDZNWWVSTWpXcmhrazF4L0RYZjNJK3IKOGs5S1FBbEsvNWZRMXk5NHYwT25TK0FKZTliczZMT2srYVFWYm5SbExab2E2aVZWbUJNam03UjBjQ2w0Y1hpRwpyekRnN1ZLc3lsY1o3aGFRRTNFQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZPaGwxMHUwNnJvenZJUm9XVmpNYlVPMzFDbERNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBQytqMXpPQVZYVXpNUFJ0U29Kd3JhMU1FSkJOUTNMWitmWnZRYjdIbk53b00zaCthMgoyc25yUitSWTYrOFFDbXVKeis1eU5yYStEZDlnNzQ1Q0hDaFpVZzlsY3RjQTRzZVR5OXZqcUVQNVBNSzEvbi9PCnFEcDQyMUpqTjYvUXJmamIvbVM0elUvZXNGZGowQXRYQ0FLMWJsNmF0ai9jZXVBbzh1bTRPaUVlNnJhanBYTHUKWFlmQ1FVZFV5TWpQdEZHTDMwNytna0RpdlVsdXk3RkQ4aUpURTh2QWpxOVBXOW40SmxFMjdQWXR5QnNocy9XSApCZ2czQjZpTG10SjZlNzJiWnA3ZmptdmJWTC9VdkxzYXZqRXltZDhrMnN1bFFwQUpzeVJrMkEzM1g3NWJpS0RGCk1CU29DaHc3U2JMSkJrN0FNckRzTjd1Q2U3WWVIWGdZemdhRAotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== dedicatedNamespace: clusternet-sdqrh managedClusterName: clusternet-cluster-pmlbp result: Approved token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkluZFpWMUV6UmxCbFdUUldWa1Y0UmxBeWMzaDJOV3RuUzBabVJsaG9jWG94TkhKM2JtUkxiRTlrYUZVaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpqYkhWemRHVnlibVYwTFhOa2NYSm9JaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbU5zZFhOMFpYSnVaWFF0YmpneWFHc3RkRzlyWlc0dGN6SnpjRFVpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWTJ4MWMzUmxjbTVsZEMxdU9ESm9heUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJamN3TWpRMk1tTTBMV0prTlRFdE5HVXpNeTA1WXpnekxUWmpPV1F6TUdGall6bG1aU0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwamJIVnpkR1Z5Ym1WMExYTmtjWEpvT21Oc2RYTjBaWEp1WlhRdGJqZ3lhR3NpZlEuUjJCdEQ1YzQxRm1seC1hTEFKSWQzbGxvOThSY25TakVUak5pSnVuTXg1US1jYXhwc3VkamUxekVtUF9mTHR6TjU4d21Dd3RXcHpoaXhSMWFTUHE5LXJTRlBIYzk3aDlTT3daZGdicC10alFBSjA4dllfYWdiVFRKLU1WN1dpN0xQVzRIcmt1U3RlemUzVHh2RW11NWwySlpzbG5UTXdkR3NGRVlVZzVfeWFoLUQwQ2pnTkZlR1ljUjJ1TlJBWjdkNW00Q1c5VmRkdkNsanNqRE5WX1k0RkFEbGo2cHgzSlh0SDQ4U1ZUd254TS1sNHl0eXBENjZFbG1PYXpUMmRwSWd4eWNSZ0tJSUlacWNQNnZVc0ZOM1Zka21ZZ29ydy1NSUcwc25YdzFaZ1lwRk9SUDJRa3hjd2NOUVVOTjh6VUZqSUZSSmdSSFdjNlo4aXd2eURnZTVn ClusterRole和Role ClusterRegistrationRequest被接收后会创建ClusterRole\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: clusternet.io/autoupdate: \u0026#34;true\u0026#34; labels: clusternet.io/created-by: clusternet-hub clusters.clusternet.io/bootstrapping: rbac-defaults clusters.clusternet.io/cluster-id: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd name: clusternet-2bcd48d2-0ead-45f7-938b-5af6af7da5fd rules: - apiGroups: - clusters.clusternet.io resources: - clusterregistrationrequests verbs: - create - get - apiGroups: - proxies.clusternet.io resourceNames: - 2bcd48d2-0ead-45f7-938b-5af6af7da5fd resources: - sockets verbs: - \u0026#39;*\u0026#39; 并会在cluster对应的namespace下创建Role\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: annotations: clusternet.io/autoupdate: \u0026#34;true\u0026#34; labels: clusternet.io/created-by: clusternet-hub clusters.clusternet.io/bootstrapping: rbac-defaults name: clusternet-managedcluster-role namespace: clusternet-sdqrh rules: - apiGroups: - \u0026#39;*\u0026#39; resources: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; ManagedCluster clusternet-hub在接受ClusterRegistrationRequest后，会创建一个ManagedCluster。一个k8s集群一个namespace\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 apiVersion: clusters.clusternet.io/v1beta1 kind: ManagedCluster metadata: labels: clusternet.io/created-by: clusternet-agent clusters.clusternet.io/cluster-id: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusters.clusternet.io/cluster-name: clusternet-cluster-pmlbp name: clusternet-cluster-pmlbp namespace: clusternet-sdqrh spec: clusterId: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusterType: EdgeCluster syncMode: Dual status: allocatable: cpu: 7600m memory: \u0026#34;30792789992\u0026#34; apiserverURL: https://10.233.0.1:443 # default/kubernetes的service clusterip appPusher: true capacity: cpu: \u0026#34;8\u0026#34; memory: 32192720Ki clusterCIDR: 10.233.0.0/18 conditions: - lastTransitionTime: \u0026#34;2021-12-15T07:47:10Z\u0026#34; message: managed cluster is ready. reason: ManagedClusterReady status: \u0026#34;True\u0026#34; type: Ready healthz: true heartbeatFrequencySeconds: 180 k8sVersion: v1.21.5 lastObservedTime: \u0026#34;2021-12-15T07:47:10Z\u0026#34; livez: true nodeStatistics: readyNodes: 1 parentAPIServerURL: https://172.21.115.160:6443 # 父集群地址 platform: linux/amd64 readyz: true serviceCIDR: 10.233.64.0/18 useSocket: true Subscription 应用发布的抽象，人工提交到环境中。要发布的资源即可以是helm chart，也可以是原生的k8s对象。clusternet在部署的时候会查看部署的优先级，并且支持weight，优先部署cluster级别的资源，这点跟helm部署的逻辑一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: apps.clusternet.io/v1alpha1 kind: Subscription metadata: name: app-demo namespace: default spec: # 定义应用要发布到哪个k8s集群 subscribers: # defines the clusters to be distributed to - clusterAffinity: matchLabels: clusters.clusternet.io/cluster-id: dc91021d-2361-4f6d-a404-7c33b9e01118 # PLEASE UPDATE THIS CLUSTER-ID TO YOURS!!! # 定义了要发布哪些资源 feeds: # defines all the resources to be deployed with - apiVersion: apps.clusternet.io/v1alpha1 kind: HelmChart name: mysql namespace: default - apiVersion: v1 kind: Namespace name: foo - apiVersion: v1 kind: Service name: my-nginx-svc namespace: foo - apiVersion: apps/v1 kind: Deployment name: my-nginx namespace: foo HelmChart Subscriber发布的一个子资源之一，对应一个helm chart的完整描述\n1 2 3 4 5 6 7 8 9 10 apiVersion: apps.clusternet.io/v1alpha1 kind: HelmChart metadata: name: mysql namespace: default spec: repo: https://charts.bitnami.com/bitnami chart: mysql version: 8.6.2 targetNamespace: abc Localization 差异化不同于其他集群的配置，用来描述namespace级别的差异化配置。\nGlobalization 用来描述不同集群的cluster级别的差异化配置。\nBase Description 跟Base对象根据Localization和Globalization渲染得到的最终要发布到集群中的最终对象。\n安装 本文使用kind进行测试，使用kind创建两个k8s集群host和member1，两个集群的apiserver均监听在宿主机的端口，确保从一个集群可以访问到另外一个集群的apiserver。\n在父集群执行如下命令安装clusternet管控组件：\n1 2 3 helm repo add clusternet https://clusternet.github.io/charts helm install clusternet-hub -n clusternet-system --create-namespace clusternet/clusternet-hub kubectl apply -f https://raw.githubusercontent.com/clusternet/clusternet/main/manifests/samples/cluster_bootstrap_token.yaml 会在clusternet-system namespace下创建deployment clusternet-hub。\n最后一条命令，会创建一个secret，其中的token信息为07401b.f395accd246ae52d，在子集群注册的时候需要用到。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: v1 kind: Secret metadata: # Name MUST be of form \u0026#34;bootstrap-token-\u0026lt;token id\u0026gt;\u0026#34; name: bootstrap-token-07401b namespace: kube-system # Type MUST be \u0026#39;bootstrap.kubernetes.io/token\u0026#39; type: bootstrap.kubernetes.io/token stringData: # Human readable description. Optional. description: \u0026#34;The bootstrap token used by clusternet cluster registration.\u0026#34; # Token ID and secret. Required. token-id: 07401b token-secret: f395accd246ae52d # Expiration. Optional. expiration: 2025-05-10T03:22:11Z # Allowed usages. usage-bootstrap-authentication: \u0026#34;true\u0026#34; usage-bootstrap-signing: \u0026#34;true\u0026#34; # Extra groups to authenticate the token as. Must start with \u0026#34;system:bootstrappers:\u0026#34; auth-extra-groups: system:bootstrappers:clusternet:register-cluster-token 在子集群执行如下命令，其中parentURL要修改为父集群的apiserver地址\n1 2 3 4 5 helm repo add clusternet https://clusternet.github.io/charts helm install clusternet-agent -n clusternet-system --create-namespace \\ --set parentURL=https://10.0.248.96:8443 \\ --set registrationToken=07401b.f395accd246ae52d \\ clusternet/clusternet-agent 其中parentURL为父集群的apiserver地址，registrationToken为父集群注册的token信息。\n访问子集群 kubectl-clusternet命令行方式访问 安装kubectl krew插件，参考 https://krew.sigs.k8s.io/docs/user-guide/setup/install/，执行如下命令：\n1 2 3 4 5 6 7 8 9 10 11 12 yum install git -y ( set -x; cd \u0026#34;$(mktemp -d)\u0026#34; \u0026amp;\u0026amp; OS=\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)\u0026#34; \u0026amp;\u0026amp; ARCH=\u0026#34;$(uname -m | sed -e \u0026#39;s/x86_64/amd64/\u0026#39; -e \u0026#39;s/\\(arm\\)\\(64\\)\\?.*/\\1\\2/\u0026#39; -e \u0026#39;s/aarch64$/arm64/\u0026#39;)\u0026#34; \u0026amp;\u0026amp; KREW=\u0026#34;krew-${OS}_${ARCH}\u0026#34; \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; tar zxvf \u0026#34;${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; ./\u0026#34;${KREW}\u0026#34; install krew ) echo \u0026#39;export PATH=\u0026#34;${PATH}:${HOME}/.krew/bin\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 安装kubectl插件clusternet：kubectl krew install clusternet\n使用kubectl clusternet get可以看到发布的应用，非发布的应用看不到。\n代码层面访问子集群 可以参照例子：https://github.com/clusternet/clusternet/blob/main/examples/clientgo/demo.go#L42-L45\n改动代码非常小，仅需要获取到对应集群的config信息即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 config, err := rest.InClusterConfig() if err != nil { klog.Error(err) os.Exit(1) } // This is the ONLY place you need to wrap for Clusternet config.Wrap(func(rt http.RoundTripper) http.RoundTripper { return clientgo.NewClusternetTransport(config.Host, rt) }) // now we could create and visit all the resources client := kubernetes.NewForConfigOrDie(config) _, err = client.CoreV1().Namespaces().Create(context.TODO(), \u0026amp;corev1.Namespace{ ObjectMeta: metav1.ObjectMeta{ Name: \u0026#34;demo\u0026#34;, }, }, metav1.CreateOptions{}) 应用发布 在主集群提交如下的yaml文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: apps.clusternet.io/v1alpha1 kind: Subscription metadata: name: app-demo namespace: default spec: subscribers: # defines the clusters to be distributed to - clusterAffinity: matchLabels: clusters.clusternet.io/cluster-id: dc91021d-2361-4f6d-a404-7c33b9e01118 # PLEASE UPDATE THIS CLUSTER-ID TO YOURS!!! feeds: # defines all the resources to be deployed with - apiVersion: apps.clusternet.io/v1alpha1 kind: HelmChart name: mysql namespace: default - apiVersion: v1 kind: Namespace name: foo - apiVersion: apps/v1 kind: Service name: my-nginx-svc namespace: foo - apiVersion: apps/v1 kind: Deployment name: my-nginx namespace: foo 实现分析 在主集群实现了两个aggregated apiservice\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: annotations: meta.helm.sh/release-name: clusternet-hub meta.helm.sh/release-namespace: clusternet-system labels: app.kubernetes.io/instance: clusternet-hub app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: clusternet-hub helm.sh/chart: clusternet-hub-0.2.1 name: v1alpha1.proxies.clusternet.io spec: group: proxies.clusternet.io groupPriorityMinimum: 1000 insecureSkipTLSVerify: true service: name: clusternet-hub namespace: clusternet-system port: 443 version: v1alpha1 versionPriority: 100 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: apiregistration.k8s.io/v1 kind: APIService metadata: annotations: meta.helm.sh/release-name: clusternet-hub meta.helm.sh/release-namespace: clusternet-system labels: app.kubernetes.io/instance: clusternet-hub app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: clusternet-hub helm.sh/chart: clusternet-hub-0.2.1 name: v1alpha1.shadow spec: group: shadow groupPriorityMinimum: 1 insecureSkipTLSVerify: true service: name: clusternet-hub namespace: clusternet-system port: 443 version: v1alpha1 versionPriority: 1 相关链接 https://mp.weixin.qq.com/s/BSgb2uoAuHbxQOUvn8fEsA ","date":"2021-12-26T21:44:17Z","permalink":"/post/k8s%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E6%96%B9%E6%A1%88-clusternet/","title":"k8s多集群管理方案 - clusternet"},{"content":"经常有快速创建一个测试k8s集群的场景，为了能够快速完成，整理了如下的命令，即可在主机上快速启动一个k8s集群。部分命令需要外网访问，推荐直接使用海外的主机。\n常用工具安装 1 2 3 4 5 6 7 8 9 yum install vim git make -y # neovim curl -LO https://github.com/neovim/neovim/releases/latest/download/nvim-linux64.tar.gz sudo rm -rf /opt/nvim sudo tar -C /opt -xzf nvim-linux64.tar.gz echo \u0026#39;export PATH=\u0026#34;$PATH:/opt/nvim-linux64/bin\u0026#34;\u0026#39; \u0026gt;\u0026gt;~/.bash_profile source ~/.bash_profile git clone https://github.com/LazyVim/starter ~/.config/nvim 安装docker 下面命令可以安装最新版本的docker-ce\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine yum install -y yum-utils yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum install docker-ce docker-ce-cli containerd.io -y systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 安装特定版本的docker 如果要安装特定版本的docker-ce，可以使用如下方法。\n使用如下命令查询yum源中的docker-ce版本\n1 2 3 4 5 6 yum list docker-ce --showduplicates | sort -r Last metadata expiration check: 0:00:27 ago on Sat 09 Apr 2022 12:39:09 AM CST. docker-ce.x86_64 3:20.10.9-3.el8 docker-ce-stable docker-ce.x86_64 3:20.10.8-3.el8 docker-ce-stable docker-ce.x86_64 3:20.10.7-3.el8 docker-ce-stable docker-ce.x86_64 3:20.10.6-3.el8 docker-ce-stable 选择特定版本的docker-ce和docker-ce-cli，执行如下命令\n1 yum install docker-ce-\u0026lt;VERSION_STRING\u0026gt; docker-ce-cli-\u0026lt;VERSION_STRING\u0026gt; containerd.io 安装kubectl kind helm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 安装kubectl curl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; chmod +x kubectl mv kubectl /usr/bin/ # 如果找不到rpm包，可以通过网络安全 rpm -ivh http://mirror.centos.org/centos/7/os/x86_64/Packages/bash-completion-2.1-8.el7.noarch.rpm yum install -y bash-completion echo -e \u0026#39;\\n# kubectl\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;source \u0026lt;(kubectl completion bash)\u0026#39; \u0026gt;\u0026gt;~/.bash_profile echo \u0026#39;alias k=kubectl\u0026#39; \u0026gt;\u0026gt;~/.bash_profile echo \u0026#39;complete -F __start_kubectl k\u0026#39; \u0026gt;\u0026gt;~/.bash_profile source ~/.bash_profile # 安装helm wget https://get.helm.sh/helm-v3.14.2-linux-amd64.tar.gz tar zvxf helm-v3.14.2-linux-amd64.tar.gz mv linux-amd64/helm /usr/bin/ rm -rf linux-amd64 rm -f helm-v3.14.2-linux-amd64.tar.gz # 安装kubectx kubens git clone https://github.com/ahmetb/kubectx /tmp/kubectx cp /tmp/kubectx/kubens /usr/bin/kns cp /tmp/kubectx/kubectx /usr/bin/kctx wget https://github.com/junegunn/fzf/releases/download/0.29.0/fzf-0.29.0-linux_amd64.tar.gz -P /tmp tar zvxf /tmp/fzf-0.29.0-llinux_amd64.tar.gz -C /tmp/ mv /tmp/fzf /usr/local/bin/ # 安装kind curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.18.0/kind-linux-amd64 chmod +x ./kind mv ./kind /usr/local/bin/ echo -e \u0026#34;\\n# kind\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;source \u0026lt;(kind completion bash)\u0026#39; \u0026gt;\u0026gt;~/.bash_profile 创建集群 其中将apiServerAddress指定为了本机，即创建出来的k8s集群仅允许本集群内访问。如果要是需要多个k8s集群之间的互访场景，由于kind拉起的k8s运行在docker容器中，而docker容器使用的是容器网络，此时如果设置apiserver地址为127.0.0.1，那么集群之间就没法直接通讯了，此时需要指定一个可以在docker容器中访问的宿主机ip地址。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 if=eth0 ip=`ifconfig $if|grep inet|grep -v 127.0.0.1|grep -v inet6|awk \u0026#39;{print $2}\u0026#39;|tr -d \u0026#34;addr:\u0026#34;` cat \u0026gt; kind.conf \u0026lt;\u0026lt;EOF kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 name: kind nodes: - role: control-plane # 如果需要 ingress，则需要指定该参数 kubeadmConfigPatches: - | kind: InitConfiguration nodeRegistration: kubeletExtraArgs: node-labels: \u0026#34;ingress-ready=true\u0026#34; extraPortMappings: - containerPort: 80 hostPort: 80 protocol: TCP - containerPort: 443 hostPort: 443 protocol: TCP # 指定 k8s 版本，默认不指定 # image: kindest/node:v1.23.17 - role: worker - role: worker - role: worker networking: apiServerAddress: \u0026#34;$ip\u0026#34; apiServerPort: 6443 EOF kind create cluster --config kind.conf 如果使用 nginx ingress，额外执行命令 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\n其他周边工具 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 安装kustomize curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash mv kustomize /usr/local/bin/ # 安装golang wget https://go.dev/dl/go1.21.8.linux-amd64.tar.gz -P /opt tar zvxf /opt/go1.21.8.linux-amd64.tar.gz -C /opt/ mkdir /opt/gopath echo -e \u0026#39;\\n# golang\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;export GOROOT=/opt/go\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;export GOPATH=/opt/gopath\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;export PATH=$PATH:$GOPATH/bin:$GOROOT/bin\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile source ~/.bash_profile # 安装controller-gen，会将controller-gen命令安装到GOPATH/bin目录下 go install sigs.k8s.io/controller-tools/cmd/controller-gen@latest # 安装dlv工具 go install github.com/go-delve/delve/cmd/dlv@latest 安装krew 1 2 3 4 5 6 7 8 9 10 11 12 13 ( set -x; cd \u0026#34;$(mktemp -d)\u0026#34; \u0026amp;\u0026amp; OS=\u0026#34;$(uname | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)\u0026#34; \u0026amp;\u0026amp; ARCH=\u0026#34;$(uname -m | sed -e \u0026#39;s/x86_64/amd64/\u0026#39; -e \u0026#39;s/\\(arm\\)\\(64\\)\\?.*/\\1\\2/\u0026#39; -e \u0026#39;s/aarch64$/arm64/\u0026#39;)\u0026#34; \u0026amp;\u0026amp; KREW=\u0026#34;krew-${OS}_${ARCH}\u0026#34; \u0026amp;\u0026amp; curl -fsSLO \u0026#34;https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; tar zxvf \u0026#34;${KREW}.tar.gz\u0026#34; \u0026amp;\u0026amp; ./\u0026#34;${KREW}\u0026#34; install krew ) echo -e \u0026#39;\\n# kubectl krew\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile echo \u0026#39;export PATH=\u0026#34;${KREW_ROOT:-$HOME/.krew}/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bash_profile source ~/.bash_profile 资料 kind ","date":"2021-12-15T21:32:00Z","permalink":"/post/ecs%E7%9A%84linux%E4%B8%BB%E6%9C%BA%E4%B8%8A%E5%BF%AB%E9%80%9F%E5%88%9B%E5%BB%BA%E6%B5%8B%E8%AF%95k8s%E9%9B%86%E7%BE%A4/","title":"ecs的Linux主机上快速创建测试k8s集群"},{"content":"概念 物理卷PV(Physical Volume)：可以是整块物理磁盘或者物理磁盘上的分区 卷组VG(Volume Group)：由一个或多个物理卷PV组成，可在卷组上创建一个或者多个LV，可以动态增加pv到卷组 逻辑卷LV(Logical Volume)：类似于磁盘分区，建立在VG之上，在LV上可以创建文件系统 逻辑卷建立后可以动态的增加或缩小空间 PE(Physical Extent): PV可被划分为PE的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可以配置的，默认为4MB。 LE(Logical Extent): LV可被划分为LE的基本单元，LE跟PE是一对一的关系。 基本操作 lvm相关的目录如果没有按照，在centos下使用yum install lvm2进行安装。\n初始磁盘状态如下，/dev/sda上有40g磁盘空间未分配：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 40G 2.8G 38G 7% / devtmpfs 236M 0 236M 0% /dev tmpfs 244M 0 244M 0% /dev/shm tmpfs 244M 4.5M 240M 2% /run tmpfs 244M 0 244M 0% /sys/fs/cgroup tmpfs 49M 0 49M 0% /run/user/1000 # fdisk -l Disk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000a05f8 Device Boot Start End Blocks Id System /dev/sda1 * 2048 83886079 41942016 83 Linux 对磁盘的操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 # fdisk /dev/sda Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. # 创建主分区2，空间为1g，磁盘格式为lvm Command (m for help): n Partition type: p primary (1 primary, 0 extended, 3 free) e extended Select (default p): p Partition number (2-4, default 2): 2 First sector (83886080-167772159, default 83886080): Using default value 83886080 Last sector, +sectors or +size{K,M,G} (83886080-167772159, default 167772159): +1G Partition 2 of type Linux and of size 1 GiB is set Command (m for help): t Partition number (1,2, default 2): 2 Hex code (type L to list all codes): 8e Changed type of partition \u0026#39;Linux\u0026#39; to \u0026#39;Linux LVM\u0026#39; # 创建主分区3，空间为5g，磁盘格式为lvm Command (m for help): n Partition type: p primary (2 primary, 0 extended, 2 free) e extended Select (default p): p Partition number (3,4, default 3): 3 First sector (85983232-167772159, default 85983232): Using default value 85983232 Last sector, +sectors or +size{K,M,G} (85983232-167772159, default 167772159): +5G Partition 3 of type Linux and of size 5 GiB is set Command (m for help): t Partition number (1-3, default 3): 3 Hex code (type L to list all codes): 8e Changed type of partition \u0026#39;Linux\u0026#39; to \u0026#39;Linux LVM\u0026#39; # 保存上述操作 Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: Re-reading the partition table failed with error 16: Device or resource busy. The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8) Syncing disks. 当前磁盘空间状态如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # fdisk -l Disk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x000a05f8 Device Boot Start End Blocks Id System /dev/sda1 * 2048 83886079 41942016 83 Linux /dev/sda2 83886080 85983231 1048576 8e Linux LVM /dev/sda3 85983232 96468991 5242880 8e Linux LVM 将上述两个lvm磁盘分区创建pv，使用pvremove /dev/sda2可以删除pv\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # pvcreate /dev/sda2 /dev/sda3 Physical volume \u0026#34;/dev/sda2\u0026#34; successfully created. Physical volume \u0026#34;/dev/sda3\u0026#34; successfully created. [root@localhost vagrant]# pvdisplay \u0026#34;/dev/sda2\u0026#34; is a new physical volume of \u0026#34;1.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/sda2 VG Name PV Size 1.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID MgarF2-Ka6D-blDi-8ecd-1SEU-y2GD-JtiK2c \u0026#34;/dev/sda3\u0026#34; is a new physical volume of \u0026#34;5.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/sda3 VG Name PV Size 5.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID ToKp2T-30mS-0P8c-ZrcB-2lO4-ayo9-62fuDx 接下来创建vg，vg的名字可以随便定义，并将创建的两个pv都添加到vg中，可以看到vg的空间为两个pv之和\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # vgcreate vg1 /dev/sda1 # vgdisplay -v --- Volume group --- VG Name vg1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 5.99 GiB PE Size 4.00 MiB Total PE 1534 Alloc PE / Size 0 / 0 Free PE / Size 1534 / 5.99 GiB VG UUID XI8Biv-JtUv-tsur-wuvm-IJQz-HLZu-6a2u5G --- Physical volumes --- PV Name /dev/sda2 PV UUID MgarF2-Ka6D-blDi-8ecd-1SEU-y2GD-JtiK2c PV Status allocatable Total PE / Free PE 255 / 255 PV Name /dev/sda3 PV UUID ToKp2T-30mS-0P8c-ZrcB-2lO4-ayo9-62fuDx PV Status allocatable Total PE / Free PE 1279 / 1279 接下来创建lv，并从vg1中分配空间2g\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # lvcreate -L 2G -n lv1 vg1 Logical volume \u0026#34;lv1\u0026#34; created. # lvdisplay --- Logical volume --- LV Path /dev/vg1/lv1 LV Name lv1 VG Name vg1 LV UUID EesY4i-lSqY-ef1R-599C-XTrZ-IcVL-P7W46Q LV Write Access read/write LV Creation host, time localhost.localdomain, 2019-06-16 07:29:38 +0000 LV Status available # open 0 LV Size 2.00 GiB Current LE 512 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:0 接下来给lv1格式化磁盘格式为ext4，并将磁盘挂载到/tmp/lvm目录下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # mkfs.ext4 /dev/vg1/lv1 mke2fs 1.42.9 (28-Dec-2013) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 131072 inodes, 524288 blocks 26214 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=536870912 16 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done # mkdir /tmp/lvm [root@localhost vagrant]# mount /dev/vg1/lv1 /tmp/lvm [root@localhost vagrant]# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 40G 2.9G 38G 8% / devtmpfs 236M 0 236M 0% /dev tmpfs 244M 0 244M 0% /dev/shm tmpfs 244M 4.5M 240M 2% /run tmpfs 244M 0 244M 0% /sys/fs/cgroup tmpfs 49M 0 49M 0% /run/user/1000 /dev/mapper/vg1-lv1 2.0G 6.0M 1.8G 1% /tmp/lvm 接下来对lv的空间从2G扩展到3G，此时通过df查看分区空间大小仍然为2g，需要执行resize2fs命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # lvextend -L +1G /dev/vg1/lv1 Size of logical volume vg1/lv1 changed from 2.00 GiB (512 extents) to 3.00 GiB (768 extents). Logical volume vg1/lv1 successfully resized. # resize2fs /dev/vg1/lv1 resize2fs 1.42.9 (28-Dec-2013) Filesystem at /dev/vg1/lv1 is mounted on /tmp/lvm; on-line resizing required old_desc_blocks = 1, new_desc_blocks = 1 The filesystem on /dev/vg1/lv1 is now 786432 blocks long. # df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 40G 2.9G 38G 8% / devtmpfs 236M 0 236M 0% /dev tmpfs 244M 0 244M 0% /dev/shm tmpfs 244M 4.5M 240M 2% /run tmpfs 244M 0 244M 0% /sys/fs/cgroup tmpfs 49M 0 49M 0% /run/user/1000 /dev/mapper/vg1-lv1 2.9G 6.0M 2.8G 1% /tmp/lvm 问题 试验时操作失误，出现了先格式化磁盘，后发现pv找不到对应设备。vg删除不成功。\n正常删除vg的方式，此时lv会自动消失\n1 2 vgreduce --removemissing vg1 vgremove vg1 lvremove操作执行的时候经常会出现提示“Logical volume xx contains a filesystem in use.”的情况，该问题一般是由于有其他进程在使用该文件系统导致的。网络上经常看到的是通过fuser或者lsof命令来查找使用方，但偶尔该命令会失效，尤其在本机上有容器的场景下。另外一个可行的办法是通过 grep -nr \u0026quot;/data\u0026quot; /proc/*/mount 命令，可以找到挂载该目录的所有进程，简单有效。\n三种Logic Volume LVM的机制可以类比于RAID，RAID一个核心的机制是性能和数据冗余，并提供了多种数据的冗余模块可供配置。lvm在性能和数据冗余方面支持如下三种Logic Volume: 线性逻辑卷、条带化逻辑卷和镜像逻辑卷。上述几种模式是在vg已经创建完成后创建lv的时候指定的模式，会影响到lv中的pe分配。\n下面使用如下的机器配置来进行各个模式的介绍和功能测试。\n1 2 3 4 5 6 7 8 $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 253:0 0 40G 0 disk └─vda1 253:1 0 40G 0 part / vdb 253:16 0 100G 0 disk vdc 253:32 0 200G 0 disk vdd 253:48 0 300G 0 disk vde 253:64 0 400G 0 disk 线性逻辑卷 Linear Logic Volume 当一个VG中有两个或者多个磁盘的时候，LV分配磁盘容量的时候是按照VG中的PV安装顺序分配的，即一个PV用完后才会分配下一块PV。也可以在创建LV的通过指定PV中的PE段来将数据分散到多个PV上。该模式也是LVM的默认模式。\n使用/dev/vdb和/dev/vdc两块磁盘来进行测试，先创建对应的PV。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 $ pvcreate /dev/vdb /dev/vdc Physical volume \u0026#34;/dev/vdb\u0026#34; successfully created. Physical volume \u0026#34;/dev/vdc\u0026#34; successfully created. $ pvdisplay \u0026#34;/dev/vdb\u0026#34; is a new physical volume of \u0026#34;100.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/vdb VG Name PV Size 100.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID 8NnlYc-4f3f-fkeW-a3l3-LXoC-9UEH-fvpb5V \u0026#34;/dev/vdc\u0026#34; is a new physical volume of \u0026#34;200.00 GiB\u0026#34; --- NEW Physical volume --- PV Name /dev/vdc VG Name PV Size 200.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID I9ffpN-c1vc-PQOB-yKyd-MdzO-Ngff-6e116t 创建VG vg1，该容量为上面两个磁盘空间之和\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ vgcreate vg1 /dev/vdb /dev/vdc Volume group \u0026#34;vg1\u0026#34; successfully created $ vgdisplay --- Volume group --- VG Name vg1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 299.99 GiB PE Size 4.00 MiB Total PE 76798 Alloc PE / Size 0 / 0 Free PE / Size 76798 / 299.99 GiB VG UUID Gi0bJx-jqY8-YpSo-kB0l-9wdk-ZfCT-GpgFZY 从vg1中创建LV lv1，其大小为vg的全部大小\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ lvcreate --type linear -l 100%VG -n lv1 vg1 Logical volume \u0026#34;lv1\u0026#34; created. $ lvdisplay --- Logical volume --- LV Path /dev/vg1/lv1 LV Name lv1 VG Name vg1 LV UUID JQZ193-dz6A-I0Ue-rTKC-6XrQ-gb1F-Qy9kDl LV Write Access read/write LV Creation host, time iZt4nd6wiprf8foracovwqZ, 2022-01-08 23:28:45 +0800 LV Status available # open 0 LV Size 299.99 GiB Current LE 76798 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:0 $ lvs -o lv_name,lv_attr,lv_size,seg_pe_ranges LV Attr LSize PE Ranges lv1 -wi-a----- 299.99g /dev/vdc:0-51198 lv1 -wi-a----- 299.99g /dev/vdb:0-25598 条带化逻辑卷 Striped Logic Volume 类似于raid0模式，在该模式下，多块磁盘均会分配PE给LV。可以通过-i参数来指定可以使用VG中多少个PV。\n优点：将数据的读写压力分散到了多个磁盘，可以提升读写性能。 缺点：一个磁盘损坏后会导致数据丢失。\n但在使用striped模式时，需要注意：\n如果PV来自于同一个磁盘的不同分区，会导致更多的随机读写，不仅不能提升磁盘性能，反而会导致性能下降。 如果VG中某一个PV过小，则无法将所有的PV平均使用起来，存在木桶效应。 使用--type striped来指定为striped模式，--stripes来指定需要使用的pv数量，--stripesize指定写足够数量的数据后再更换为另外一个pv。在下面的创建命令中，可以看到创建出来了12800个LE，PE是平均分配到了两个pv上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ lvcreate --type striped --stripes 2 --stripesize 32k -L 50G -n lv1 vg1 Logical volume \u0026#34;lv1\u0026#34; created. $ lvs -o lv_name,lv_attr,lv_size,seg_pe_ranges LV Attr LSize PE Ranges lv1 -wi-a----- 50.00g /dev/vdb:0-6399 /dev/vdc:0-6399 $ lvdisplay --- Logical volume --- LV Path /dev/vg1/lv1 LV Name lv1 VG Name vg1 LV UUID FnOmCM-IkuE-3Rvv-fQqk-Cv1Q-lb8S-l5X7O3 LV Write Access read/write LV Creation host, time iZt4nd6wiprf8foracovwqZ, 2022-01-09 00:07:08 +0800 LV Status available # open 0 LV Size 50.00 GiB Current LE 12800 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:0 如果指定的lv大小为250G，由于分配到两块磁盘上，由于最小的磁盘只有100G，100G * 2无法满足250G的磁盘大小需求，此时会报错。\n1 2 lvcreate --type striped --stripes 2 --stripesize 32k -L 250G -n lv1 vg1 Insufficient suitable allocatable extents for logical volume lv1: 12802 more required 镜像逻辑卷 Mirror Logic Volume 类似于raid1。可以解决磁盘的单点问题，一块磁盘挂掉后不至于丢失数据。\n通过使用--type mirror来指定为镜像模式，-m参数来指定冗余的数量。\n常见问题 报错 \u0026ldquo;Device /dev/sdd excluded by a filter.\u0026rdquo; 当执行 pvcreate /dev/sdd 命令是报错信息如下：\n1 Device /dev/sdd excluded by a filter. 检查下 /etc/lvm/lvm.conf 文件中的 filter 字段是否将磁盘过滤掉了。\n执行 pvcreate /dev/sdd -vvv可以查看更详细的报错信息。\n可以通过执行wipefs -a /dev/sdd 命令后再执行 pvcreate。\nref Linux LVM简明教程 初识LVM及ECS上LVM分区扩容 Linux LVM\u0026ndash;三种Logic Volume ","date":"2021-11-28T02:13:14Z","permalink":"/post/lvm/","title":"lvm"},{"content":"\n《金字塔原理》是一本教你如何清晰的思考问题，表达观点的畅销书籍，但其并非万能的，其层层架构建立在因果关系基础之上。但现实世界中的很多问题并非简单的因果关系，此时金字塔原理就用不上了。\n什么是金字塔原理 定义：任何一件事情都可以归纳出一个中心论点，而这个中心论点由3到7个论据进行支撑。每一个论据它本身又可以拆分成一个论点，该论点同样也可以拆分成3到7个论据，如此重复形状就像金字塔一样。\n三个注意点：\n结论先行 每层结论下面的论据不要超过7个 每一个论点要言之有物，有明确的思想 组织思想的方法 在有了论点之后，论据该怎么拆分呢？可以按照下面四个原则来组织思想：\n时间顺序 空间顺序 重要性顺序，比如按照老弱优先原则 逻辑演绎顺序。所以的逻辑演绎，就是三大段：大前提、小前提和结论。比如，凡人皆有一死，苏格拉底是人，所以苏格拉底也会死。但不推荐用此方法，因为对于听众的要求比较高，必须能够集中注意力才能听明白。 在思考的过程中，如果还没有论点，也可以使用上述方法先得出论据，然后推理出论点。 梳理出的论据必须符合MECE法则：每一个论点下的论据，都应该相互独立，但又可以完全穷尽，即论据要做到不遗漏不重叠。\n如何让别人对自己的观点感兴趣 SCQ法则：Situation（设置背景）、Complication（冲突）和Question（疑问）。背景即先介绍大家都认同的背景信息从而引入话题。在SCQ都讲完后，就可以引入自己的论点了。\n参考 得到 - 《金字塔原理》成甲解读 ","date":"2021-05-18T00:07:17Z","permalink":"/post/%E9%87%91%E5%AD%97%E5%A1%94%E5%8E%9F%E7%90%86%E6%80%BB%E7%BB%93/","title":"《金字塔原理》总结"},{"content":"最近因为《致阿里》的缘故，读了不少阿里内网的热帖，很多都是洋洋洒洒几千字，而且说的有理有据。如果换做是我，很多帖子哪怕我憋上一天都是写不出来的。我一直在思考，我到底比别人差在了哪里。思来想去，其中一个原因是因为平时的思考总结不够，缺少积累。\n恰巧看了张一鸣在字节跳动9周年演讲，三观跟我特别合，想以贴为例，一来分享一下我个人的内心想法加深对演讲内容的认识，二来可以依次来锻炼自己的逻辑归纳能力。后续如有特别值得学习的演讲，也会分享一下自己的学习和思考，比如2020年张小龙的微信公开课的演讲就特别值得学习。\n文中提到最多的词莫过于“平常心”了，“平常心”是一个佛源词，看来张一鸣没少研究佛学，整篇演讲显得也比较佛系，对于平常心的最直白的解释就是：\n| 吃饭的时候好好吃饭，睡觉的时候好好睡觉\n要想做到“好好吃饭，好好睡觉”对我来说是挺难的，我举一个简单的例子。平常周末来说，特别想睡一个懒觉，如果在睡觉前自己明确知道因为工作没有完成，第二天早上会有人找我，那么第二天早上一定会醒的比较早，想睡个懒觉都很难，即使第二天上午并不一定有人在我醒之前找到我。说明因为有事情的缘故，已经在无形中影响了睡眠质量。再举个例子，春节假期的睡眠质量明显会高于平常周末的睡眠质量，原因是心里总有各种工作的事情不能完全放下。如果将春节假期的心态为平常心，那么一年中的其他时间对我而言都不是平常心。\n每家公司在年初的时候总会定义一些目标，比如全年营收目标为1个亿。一旦有了营收目标后，那么大家的工作重心一定会围绕的着目标展开。因为毕竟公司的资源是有限的，但是为了达成营收的目标，并不能保持一颗“平常心”来工作，焦虑的员工很难打磨出最好的产品，往往其他的地方就不会做的太好，比如用户体验、代码质量等等，一些本来很好的点子因为有营收目标的缘故，也很难展开实施，从而导致一些创新项目的流失。\n接下来就是一些平常心的工作原则，总结下来有如下几点：\n平常心对待自己，平常人做非常事 平常心对待预期，没有预期和标签的束缚会发挥的更好 平常心对待过去和未来，关注当下 平常心对待竞争对手 平常心对待业务 平常心对待成功和失败 文中特意提到了互联网八股文的一段话，这里就不再摘出来，我平时也没少见到类似的话术，看完后的感觉就是真牛逼，打死我都写不出来，但转头就忘记讲啥了，也许就是当时压根就看不懂，因为这些话太抽象了。互联网行业本身是一个特别务实接地气的行业，现在也渐渐在内卷严重，抽象的词汇也越来越多，期望下一个风口的到来，或许会将这股邪气吹走一些。\n引用 张一鸣演讲全文：外部波澜起伏，内心平静如常\n","date":"2021-04-05T18:50:11Z","permalink":"/post/%E6%BC%94%E8%AE%B2%E7%9A%84%E6%80%9D%E8%80%83%E7%B3%BB%E5%88%97%E6%96%87%E7%AB%A0-%E5%BC%A0%E4%B8%80%E9%B8%A3%E5%AD%97%E8%8A%82%E8%B7%B3%E5%8A%A89%E5%91%A8%E5%B9%B4/","title":"演讲的思考系列文章 - 张一鸣字节跳动9周年"},{"content":"异常处理分为error和defer和recover两类，其中error用来处理可预期的异常，recover用来处理意外的异常。\nerror 支持多个返回值，可以将业务的返回值和错误的返回值分开，很多都会返回两个值。如果不使用error返回值，可以用_变量来忽略。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 // parseConfig returns a parsed configuration for an Azure cloudprovider config file func parseConfig(configReader io.Reader) (*Config, error) { var config Config if configReader == nil { return \u0026amp;config, nil } configContents, err := ioutil.ReadAll(configReader) if err != nil { return nil, err } err = yaml.Unmarshal(configContents, \u0026amp;config) if err != nil { return nil, err } // The resource group name may be in different cases from different Azure APIs, hence it is converted to lower here. // See more context at https://github.com/kubernetes/kubernetes/issues/71994. config.ResourceGroup = strings.ToLower(config.ResourceGroup) return \u0026amp;config, nil } error的几种使用方式：\n使用error的方式 说明 举例 errors.New 简单静态字符串的错误，没有额外的信息 errors.New(\u0026ldquo;shell not specified\u0026rdquo;) fmt.Errorf 用于格式化的错误字符串 fmt.Errorf(\u0026ldquo;failed to start kubernetes.io/kube-apiserver-client-kubelet certificate controller: %v\u0026rdquo;, err) 实现Error()方法的自定义类型 客户段需要检测并处理该错误时使用该方式 见下文自定义error Error wrapping Go 1.13支持的特性 errors.New 原则：\n不要在客户端判断error中的包含字符串信息。 BadGood ```go // package foo func Open() error { return errors.New(\u0026ldquo;could not open\u0026rdquo;) }\n// package bar\nfunc use() { if err := foo.Open(); err != nil { if err.Error() == \u0026ldquo;could not open\u0026rdquo; { // handle } else { panic(\u0026ldquo;unknown error\u0026rdquo;) } } }\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 \u0026lt;/td\u0026gt;\u0026lt;td\u0026gt; ```go // package foo var ErrCouldNotOpen = errors.New(\u0026#34;could not open\u0026#34;) func Open() error { return ErrCouldNotOpen } // package bar if err := foo.Open(); err != nil { if errors.Is(err, foo.ErrCouldNotOpen) { // handle } else { panic(\u0026#34;unknown error\u0026#34;) } } 当然也可以使用自定义error类型，但此时由于要实现自定义error类型，代码量会增加。\n自定义error error是个接口，可以用来扩展自定义的错误处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 // file: k8s.io/kubernetes/pkg/volume/util/nestedpendingoperations/nestedpendingoperations.go // NewAlreadyExistsError returns a new instance of AlreadyExists error. func NewAlreadyExistsError(operationName string) error { return alreadyExistsError{operationName} } // IsAlreadyExists returns true if an error returned from // NestedPendingOperations indicates a new operation can not be started because // an operation with the same operation name is already executing. func IsAlreadyExists(err error) bool { switch err.(type) { case alreadyExistsError: return true default: return false } } type alreadyExistsError struct { operationName string } var _ error = alreadyExistsError{} func (err alreadyExistsError) Error() string { return fmt.Sprintf( \u0026#34;Failed to create operation with name %q. An operation with that name is already executing.\u0026#34;, err.operationName) } 还可以延伸出更复杂一些的树形error体系：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // package net type Error interface { error Timeout() bool // Is the error a timeout? Temporary() bool // Is the error temporary? } type UnknownNetworkError string func (e UnknownNetworkError) Error() string func (e UnknownNetworkError) Temporary() bool func (e UnknownNetworkError) Timeout() bool Error Wrapping error类型仅包含一个字符串类型的信息，如果函数的调用栈信息为A -\u0026gt; B -\u0026gt; C，如果函数C返回err，在函数A处打印err信息，那么很难判断出err的真正出错位置，不利于快速定位问题。我们期望的效果是在函数A出打印err，能够精确的找到err的源头。\n为了解决上述问题，需要error类型在函数调用栈之间传递，有如下解决方法：\n使用fmt.Errorf()函数来增加额外的信息 使用Error Wrapping golang 1.13支持%w https://github.com/pkg/errors 使用fmt.Errorf()来封装error信息，基于已经存在的error再产生一个新的error类型，需要避免error中包含冗余信息。\nBadGood ```go // err: failed to call api: connection refused s, err := store.New() if err != nil { return fmt.Errorf( \"failed to create new store: %s\", err) } ``` ```go // err: call api: connection refused s, err := store.New() if err != nil { return fmt.Errorf( \"new store: %s\", err) } ``` ``` failed to create new store: failed to call api: connection refused error中会有很多的冗余信息 ``` ``` new store: call api: connection refused error中没有冗余信息，同时包含了调用栈信息 ``` 但使用fmt.Errorf()来全新封装的error信息的缺点也非常明显，丢失了最初的err信息，已经在中间转换为了全新的err。\n类型断言 类型转换如果类型不正确，会导致程序crash，必须使用类型判断来判断类型的正确性。\nBadGood ```go t := i.(string) ``` ```go t, ok := i.(string) if !ok { // handle the error gracefully } ``` panic 用于处理运行时的异常情况。\n使用原则\n不要使用panic，在kubernetes项目中几乎没有使用panic的场景 即使使用panic后，一定要使用recover会捕获异常 在测试用例中可以使用panic BadGood 1 2 3 4 5 6 7 8 9 10 func run(args []string) { if len(args) == 0 { panic(\u0026#34;an argument is required\u0026#34;) } // ... } func main() { run(os.Args[1:]) } ```go func run(args []string) error { if len(args) == 0 { return errors.New(\"an argument is required\") } // ... return nil } func main() { if err := run(os.Args[1:]); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } }\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;/td\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt;\u0026lt;/table\u0026gt; ## client-go client-go利用队列来进行重试\u0026lt;br /\u0026gt; \u0026lt;br /\u0026gt;[https://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go#L93](https://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go#L93) ## kube-builder kube-builder为client-go的更上次封装，本质上跟client-go利用队列来进行重试的机制完全一致。 # 发生了错误后该如何处理 - 打印错误日志 - 根据业务场景选择忽略或者自动重试 - 程序自己crash # 如何避免 - 在编写代码时增加防御式编程意识，不能靠契约式编程。一个比较简单的判断错误处理情况的方法，看下代码中if语句占用的比例。[https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet_volumes.go](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet_volumes.go) - 需求的评估周期中，不仅要考虑到软件开发完成的时间，同时要考虑到单元测试（单元测试用例的编写需要较长的时间）和集成测试的时间 - 单元测试覆盖率提升，测试场景要考虑到各种异常场景 ","date":"2021-03-17T00:24:44Z","permalink":"/post/golang%E4%B8%AD%E7%9A%84%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6/","title":"Golang中的异常处理机制"},{"content":"之前购买的网件R6400V2路由器刷到了梅林系统，但一直以来信号都特别差，甚至都比不过最便宜的水星路由器，想重新刷回官方系统看下是否是梅林系统的问题。本文记录下重新刷回梅林系统的操作步骤。\n刷机之路 从如下的地址下载固件。\n1 链接：https://pan.baidu.com/s/1EBvUBlXozo_4zkXaeUMQng 密码：pqqv 在梅林系统的界面上找到固件升级的地方，将下载的固件上传。\n结果悲剧的事情发生了，路由器出现了不断重启的状态，并且没有无线信号。猜测可能是因为固件用的是R6400，而非R6400V2导致的。或者是因为是用无线网络升级的原因，导致升级到一半网断掉了，从而导致失败了。\n救砖之路 按照网件通用救砖，超详细教程文档中的网件通用救砖方法，将路由器的一个LAN口跟电脑的网卡用网线直接相连，并设置网卡的ip地址为192.168.1.3，网关为255.255.255.0。\n在路由器启动的状态下，在命令行执行ping -t 192.168.1.1来验证路由器是否可以ping通，如果不通，可能是因为路由器的LAN段不是192.168.1.0，可能是192.168.50.0的。\n在windows电脑上下载对应的文件，并保存到本地的F盘下。\n手工安装winpcap，主要是给nmrpflash来使用。\n以管理员身份运行命令行工具，执行nmrpflash.exe -L后找到本机的网卡net1。 执行nmrpflash命令后，立即重启路由器，如果第一次提示Timeout，可以立即执行该命令，如果出现下图的提示，说明命令执行成功。 参考文档 万能网件R6400刷回最近官方固件的方法（不适用于R6400V2） 网件通用救砖，超详细教程 Netgear 网件系列路由器救砖工具 ","date":"2021-02-16T02:59:38Z","permalink":"/post/%E7%BD%91%E4%BB%B6r6400v2%E9%87%8D%E6%96%B0%E5%88%B7%E5%9B%9E%E5%AE%98%E6%96%B9%E7%B3%BB%E7%BB%9F%E6%95%99%E7%A8%8B/","title":"网件R6400V2重新刷回官方系统教程"},{"content":"MTU MTU是指一个以太网帧能够携带的最大数据部分的大小，并不包含以太网的头部部分。一般情况下MTU的值为1500字节。\n当指定的数据包大小超过MTU值时，ip层会根据当前的mtu值对超过数据包进行分片，并会设置ip层头部的More Fragments标志位，并会设置Fragment offset属性，即分片的第二个以及后续的数据包会增加offset，第一个数据包的offset值为0。接收方会根据ip头部的More Fragment标志位和Fragment offset属性来进行切片的重组。\n如果手工将发送方的MTU值设置为较大值，比如9000（巨型帧），如果发送方设置了不分片（ip头部的Don\u0026rsquo;t fragment），此时如果发送的链路上有地方不支持该MTU，报文就会被丢弃。\noffload特性 执行 ethtool -k ${device} 可以看到很多跟网络接口相关的特性，这些特性的目的是为了提升网络的收发性能。TSO、UFO和GSO是对应网络发送，LRO、GRO对应网络接收。\n执行ethtool -K ${device} gro off/on 来开启或者关闭相关的特性。\nLRO(Large Receive Offload) 通过将接收的多个tcp segment聚合为一个大的tcp包，然后传送给网络协议栈处理，以减少上层网络协议栈的处理开销。\n但由于tcp segment并不是在同一时刻到达网卡，因此组装起来就会变得比较困难。\n由于LRO的一些局限性，在最新的网络上，该功能已经删除。\nGRO(Generic Receive Offload) GRO是LRO的升级版，正在逐渐取代LRO。运行与内核态，不再依赖于硬件。\nRSS hash 特性 网卡可以根据数据包放到不同的网卡队列来处理，并可以根据不同的数据协议来设置不同的值。\n注意：该特性并非所有的网卡都支持\n下面命令为查询 udp 协议的设置，可以看到 hash 的策略为根据源 ip 地址和目的 ip 地址。\n1 2 3 4 $ ethtool -n eth0 rx-flow-hash udp4 UDP over IPV4 flows use these fields for computing Hash flow key: IP SA IP DA 可以使用 ethtool -N eth0 rx-flow-hash udp4 sdfn来修改hash 策略，sdfn对应的含义如下：\n1 2 3 4 5 6 7 8 9 m Hash on the Layer 2 destination address of the rx packet. v Hash on the VLAN tag of the rx packet. t Hash on the Layer 3 protocol field of the rx packet. s Hash on the IP source address of the rx packet. d Hash on the IP destination address of the rx packet. f Hash on bytes 0 and 1 of the Layer 4 header of the rx packet. n Hash on bytes 2 and 3 of the Layer 4 header of the rx packet. r Discard all packets of this flow type. When this option is set, all other options are ignored. 修改完成后再查看网卡的 hash 策略如下：\n1 2 3 4 5 6 $ ethtool -n eth0 rx-flow-hash udp4 UDP over IPV4 flows use these fields for computing Hash flow key: IP SA IP DA L4 bytes 0 \u0026amp; 1 [TCP/UDP src port] L4 bytes 2 \u0026amp; 3 [TCP/UDP dst port] 参考文章 关于MTU，这里也许有你不知道的地方 常见网络加速技术浅谈 ","date":"2020-08-04T00:48:45Z","permalink":"/post/linux%E7%BD%91%E7%BB%9C%E6%8E%A5%E5%8F%A3%E7%89%B9%E6%80%A7/","title":"Linux网络接口特性"},{"content":"\n又是很长的一段时间没有更新，果然又是不定期更新，文章的有些内容也是很久以前积累的，并不是因为太懒，而是确实没有太多的精力。\n题图为雨中的望京SOHO，今年全国的雨水特别多，北京亦是如此。南方的鱼米之乡地区出现了严重的洪灾，不知道今年的粮食产量会受多大影响。我们的地球在人类翻天覆地的变更后实在经受不了太多的hack，愿雨季早日过去。\n资源 1.bocker\nbocker=bash + docker，其利用100多行bash代码实现的简易版的docker，使用到的底层技术跟docker是一致的，包括chroot、namespace、cgroup。\n2.kubectx\n天天操作k8s的工程师一定少不了使用kubectl命令，而用kubectl命令的工程师一定会特别烦天天输入-n ${namespace}这样的操作，该工具可以省去输入namespace的操作。刚开始的时候不是太习惯该工具，直到近期才感知到该工具的价值。🤦‍♂️\n3.KubeOperator\nk8s集群的安装操作基本上都是黑屏来完成的，同时集群规模较大时，还需要一些自动化的手段来解决安装和运维物理机的问题。KubeOperator提供了界面化的操作来完成k8s集群的配置、安装、升级等的操作，底层也是调用了ansible来作为自动化的工具。该项目已经加入CNCF，期望后面可以做的功能更加强大，给k8s集群的运维带来便利。\n4.awesome-operators\nk8s生态的operator非常火爆，作为k8s扩展能力的一个重要组成部分，该项目汇总了常见的operator项目。\n5.chaos-mesh\npingcap开源的Kubernetes的混沌工程项目，可以使用CRD的方式来注入故障到Kubernetes集群中。\n6.devops-exercises\nDevOps相关的一些面试题，涉及到的方面还是比较全的。\n7.shell2http\n可以将shell脚本放到业务页面上执行的工具，在web页面上点击按钮后，会执行shell脚本，shell脚本的输出会在web页面上显示。\n8.Google Shell 风格指南\nGoogle编程规范还是比较有权威性的，此为Shell的编码规范。\n9.shellcheck\nShell作为弱类型的编程语言，稍有不慎还是非常容易写错语法的，至少很多的语法我是记不住的，每次都是边查语法边写🤦‍。该项目为Shell的静态检查工具，用来检查可能的语法错误，在Github上的start数量还是非常高的。\n不仅支持命令行工具检查，而且还可以跟常用的编辑器集成（比如vim、vscode），用来实现边写边检查的效果。还提供了web界面，可以将shell脚本输入到web界面上来在线检查。\n10.teambition\n阿里的一款的远程协作工具，类似于国外slack+trello的结合版，在产品设计上能看到太多地方借鉴了trello，非常像是trello的本土化版本，更贴近国人的使用习惯，可用于管理团队和个人的任务。\n11.IcePanel\nIcePanel为vscode的一款插件，提供了k8s一些基础对象的编辑生成器，通过ui的界面即可生成k8s的ConfigMap、Deployment、Service等对象。\nPlay with Kubernetes 一个提供在线的kubernetes集群的工具，在界面上点一下按钮就可以创建一个k8s集群，不需要注册，非常方便，但创建的集群只有四个小时的使用时间。可以用来熟悉k8s的基本操作，或者试验一些功能。\n精彩文章 1.腾讯自研业务上云：优化Kubernetes集群负载的技术方案探讨\nk8s虽然在服务器的资源利用率上比起传统的物理机或虚拟机部署服务方式有了非常大的提升，本文结合实践经验，从pod、node、hpa等多个维护来优化以便进一步的压榨服务器的资源。\n书籍 1.[Linux开源网络全栈详解：从DPDK到OpenFlow])(http://product.china-pub.com/8061094)\n该书可以作为全面了解开源软件网络的相关技术，涉及到Linux虚拟网络、DPDK、OpenStack、容器相关网络等知识。\n2.Kubernetes 网络权威指南：基础、原理与实践\n该书可以作为全面了解k8s相关的容器网络的相关技术，如果对k8s周边的虚拟网络知识有所全面了解，该书籍还是比较适合的。\n","date":"2020-07-28T01:35:26Z","permalink":"/post/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E7%AC%AC13%E6%9C%9F/","title":"技术分享第13期"},{"content":"在k8s中的内置资源很多都有status部分，比如deployment，用来标识当前资源的状态信息。同样在CRD的体系中，也都有status部分。这些status部分信息，是由operator来负责维护的。\n如果直接采用kubectl edit的方式来修改status部分信息，会发现是无法直接修改status部分的，因为status是无法修改成功的，因为status部分是CR的一个子资源。\n可以通过如下的方式来完成修改\n首先要准备一个完整的yaml文件，包含了status部分信息 这个的格式必须为json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \u0026#34;apiVersion\u0026#34;: \u0026#34;kuring.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Certificate\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;test\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34; }, \u0026#34;spec\u0026#34;: { \u0026#34;secretName\u0026#34;: \u0026#34;test\u0026#34; }, \u0026#34;status\u0026#34;: { \u0026#34;phase\u0026#34;: \u0026#34;pending\u0026#34; } } 获取系统的TOKEN信息 通常在kube-system下会有admin的ServiceAccount，会有一个对应的Secret来存放该ServiceAccount的token信息。执行kubectl get secret -n kube-system admin-token-r2bvt -o yaml获取到token信息，并其中的token部分进行base64解码。\n执行如下的脚本 需要将其中的变量信息修改一下\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/bash obj_file=$1 kind=certificates APISERVER=https://10.0.0.100:6443 namespace=default TOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1yMmJ2dCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImRiY2IyNzUzLWE5OGMtMTFlYS04NGVjLTAwMTYzZTAwOGU3MCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.tX3jyNh-GEuZQg-hmy7igqh9vpTAz8Jh9uEv-diZ5XWjX9JYhxwD9nxTQvCcvzY7iPIbvxQfW2GHDZISPoopX0vQy9mQ7npVitrOvFovk06plefI5Gxjdft6vdpt-ArsGTpm7-s9G-3aBg5x41h3Cdgyv-W-ypFlCr9dKu9K7BcRIXSq_GQlq5TBmd-LKFXoer4QGwkn7geq5-ziMk_lY21jIGVdIkq9IRiH8NWuCl7l8i6nQESQDUUpMyKDCqkJqUFV8UkrQL7TfqurFP36_TUAQTh2ZAE8nFnrKRoa09BnjT-FoPO6Jnq6COQjk3PGDHV8LKNDAjCCrs0A53IYGw obj=nginx-test echo \u0026#34;begin to patch $obj the file \u0026#34;${obj_file} curl -XPATCH -H \u0026#34;Accept: application/json\u0026#34; -H \u0026#34;Content-Type: application/merge-patch+json\u0026#34; --header \u0026#34;Authorization: Bearer $TOKEN\u0026#34; --insecure -d @${obj_file} $APISERVER/apis/kuring.github.io/v1alpha1/namespaces/${namespace}/${kind}/$obj/status ","date":"2020-06-21T21:27:09Z","permalink":"/post/%E6%89%8B%E5%8A%A8%E6%9B%B4%E6%96%B0custom-resource%E4%B8%AD%E7%9A%84status%E9%83%A8%E5%88%86/","title":"手动更新Custom Resource中的status部分"},{"content":"查看网桥设备以及端口 使用brctl show可以查看本地上的所有的网桥设备以及接到网桥设备上的所有网络设备。\n查看网桥设备的mac地址表 执行brctl showmacs ${dev}，常用来排查一些包丢在网桥上的场景。 其中port no为网桥通过mac地址学习到的某个mac地址所在的网桥端口号。\n1 2 3 4 5 6 7 8 9 10 $ brctl showmacs br0 port no mac addr is local? ageing timer 1 02:50:89:59:ac:4b no 3.96 69 02:e2:14:78:d7:92 no 0.57 1 0a:1e:01:dc:67:87 no 10.23 1 0a:60:3c:ca:a8:85 no 6.04 1 0e:01:ce:d6:fc:66 no 8.36 1 0e:0c:f8:6c:08:75 no 56.73 58 0e:49:85:f6:a1:40 no 1.30 22 0e:c0:99:b0:d9:f9 no 0.85 查看网桥设备的某个端口的挂载设备 在上文中中可以获取到某个mac地址对应的网桥设备的端口号，要想知道某个网桥设备的端口号对应的设备可以使用brctl showstp ${dev}命令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 brctl showstp br0 br0 bridge id 8000.ae90501b5b47 designated root 8000.ae90501b5b47 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 300.00 hello timer 0.03 tcn timer 0.00 topology change timer 0.00 gc timer 62.37 flags bond0.11 (1) port id 8001 state forwarding designated root 8000.ae90501b5b47 path cost 100 designated bridge 8000.ae90501b5b47 message age timer 0.00 designated port 8001 forward delay timer 0.00 designated cost 0 hold timer 0.00 flags veth02b41ce8 (20) port id 8014 state forwarding designated root 8000.ae90501b5b47 path cost 2 designated bridge 8000.ae90501b5b47 message age timer 0.00 designated port 8014 forward delay timer 0.00 designated cost 0 hold timer 0.00 flags hairpin mode 1 ","date":"2020-06-21T17:01:40Z","permalink":"/post/linux-bridge-brctl%E5%91%BD%E4%BB%A4/","title":"Linux Bridge brctl命令"},{"content":"在日常开发的过程中，经常会需要在本地开发的程序需要在k8s中调试的场景，比如，写了一个operator。如果此时，本地又没有可以直接可达的k8s集群，比如k8s是在公有云的vpc环境内，外面无法直接访问。此时，ssh又是可以直接通过公网vip访问的vpc的网络内的。为了满足此类需要，可以采用ssh tunnel的方式来打通本地跟远程的k8s集群。\n1. 本地建立ssh tunnel到远程集群网络 mac用户可以通过SSH TUNNEL这个软件来在界面上自动化配置，具体的配置方式如下：\n需要增加一个ssh的动态代理，监听本地的9909端口号。\n2. 将远程集群的kubeconfig文件复制到本地 将k8s集群中~/.kube/config文件复制到本地的~/.kube/config目录下\n3. 在命令行中执行kubectl命令 在命令行中配置http和https代理\n1 2 export http_proxy=127.0.0.1:9909 export https_proxy=127.0.0.1:9909 然后至此就可以通过kubectl命令来访问远程的k8s集群了。\n","date":"2020-05-11T19:41:53Z","permalink":"/post/%E6%9C%AC%E5%9C%B0%E8%BF%9E%E6%8E%A5%E8%BF%9C%E7%A8%8B%E7%9A%84%E5%86%85%E7%BD%91k8s%E9%9B%86%E7%BE%A4/","title":"本地连接远程的内网k8s集群"},{"content":" conntrack是netfilter提供的连接跟踪机制，允许内核识别出数据包属于哪个连接。是iptables实现状态匹配(-m state)以及nat的基础，由单独的内核模块nf_conntrack实现。\nconntrack在图中有两处，一处位于prerouting，一处位于output。主机自身进程产生的数据包会经过output链的conntrack，主机的网络设备接收到的数据包会通过prerouting链的conntrack。\n每个通过conntrack的数据包，内核会判断是否为新的连接。如果是新的连接，则在连接跟踪表中插入一条记录。如果是已有连接，会更新连接跟踪表中的记录。\n需要特别注意的是，conntrack并不会修改数据包，如dnat、snat，而仅仅是维护连接跟踪表。\n连接跟踪表的内容 /proc/net/nf_conntrack可以看到连接跟踪表的所有内容，通过hash表来实现。\n1 ipv4 2 tcp 6 117 TIME_WAIT src=10.45.4.124 dst=10.45.8.10 sport=36903 dport=8080 src=10.45.8.10 dst=10.45.4.124 sport=8080 dport=36903 [ASSURED] mark=0 zone=0 use=2 117: 该连接的生存时间，每个连接都有一个timeout值，可以通过内核参数进行设置，如果超过该时间还没有报文到达，该连接将会删除。如果有新的数据到达，该计数会被重置。 TIME_WAIT: 当前该连接的最新状态 涉及到的内核参数 net.netfilter.nf_conntrack_buckets: 用来设置hash表的大小 net.netfilter.nf_conntrack_max: 用来设置连接跟踪表的数据条数上限 iptables与conntrack的关系 iptables使用-m state模块来从连接跟踪表查找数据包的状态，上面例子中的TIME_WAIT即为连接跟踪表中的状态，但这些状态对应到iptable中就只有五种状态。特别需要注意的是，这五种状态是跟具体的协议是tcp、udp无关的。\n状态 含义 NEW 匹配连接的第一个包 ESTABLISHED NEW状态后，如果对端有回复包，此时连接状态为NEW RELATED 不是太好理解，当已经有一个状态为ESTABLISHED连接后，如果又产生了一个新的连接并且跟此时关联的，那么该连接就是RELATED状态的。对于ftp协议而言，有控制连接和数据连接，控制连接要先建立为ESTABLISHED，数据连接就变为控制连接的RELATED。那么conntrack怎么能够识别到两个连接是有关联的呢，即能够识别出协议相关的内容，这就需要通过扩展模块来完成了，比如ftp就需要nf_conntrack_ftp INVALID 无法识别的或者有状态的数据包 UNTRACKED 匹配带有NOTRACK标签的数据包，raw表可以将数据包标记为NOTRACK，这种数据包的连接状态为NOTRACK conntrack-tools 1 2 # 查看连接跟踪表 conntrack -L reference 云计算底层技术-netfilter框架研究 ","date":"2020-02-13T23:51:16Z","permalink":"/post/conntrack%E4%BB%8B%E7%BB%8D/","title":"conntrack介绍"},{"content":"控制器向上提供接口，用来供应用程序调用，此接口成为北向接口；控制器向下调用接口，控制网络设备，此接口成为南向接口。\nOpenFlow是控制器和网络设备之间互通的南向协议，OpenvSwitch 用于创建软件的虚拟交换机。\n原理 用户态进程\novsdb：本地数据库，存储ovs的配置信息 vswitchd：ovs-ofctl用来跟该命令通讯，下发流表规则 install OpenvSwitch on CentOS 7 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 安装依赖 yum install openssl-devel python-sphinx gcc make python-devel openssl-devel kernel-devel graphviz kernel-debug-devel autoconf automake rpm-build redhat-rpm-config libtool python-twisted-core python-zope-interface PyQt4 desktop-file-utils libcap-ng-devel groff checkpolicy selinux-policy-devel gcc-c++ python-six unbound unbound-devel -y mkdir -p ~/rpmbuild/SOURCES \u0026amp;\u0026amp; cd ~/rpmbuild/SOURCES # 下载ovs源码 wget https://www.openvswitch.org/releases/openvswitch-2.12.0.tar.gz tar zvxf openvswitch-2.12.0.tar.gz # 构建rpm包 rpmbuild -bb --nocheck openvswitch-2.12.0/rhel/openvswitch-fedora.spec # 安装rpm包 yum localinstall /root/rpmbuild/RPMS/x86_64/openvswitch-2.12.0-1.el7.x86_64.rpm systemctl start openvswitch.service 在编译的时候有如下报错：\n1 2 3 File \u0026#34;/usr/lib64/python2.7/site-packages/jinja2/sandbox.py\u0026#34;, line 22, in \u0026lt;module\u0026gt; from markupsafe import EscapeFormatter ImportError: cannot import name EscapeFormatter 是因为markupsafe的版本不对导致的，解决方法为安装合适的版本：\n1 2 pip uninstall markupsafe pip install markupsafe==0.23 vlan实验 1 2 3 # 创建虚拟交换机ovs_br ovs-vsctl add-br ovs_br ovs-vsctl add-port ovs_br first_port flow table试验 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 创建namespace ip netns add ns1 ip netns add ns2 # 创建veth pair设备 ip link add veth1 type veth peer name veth1_br ip link add veth2 type veth peer name veth2_br # 设置veth pair设备的namespace ip link set veth1 netns ns1 ip link set veth2 netns ns2 # 创建OVS网桥 ovs-vsctl add-br ovs1 # 将veth pair设备另一端绑到网桥ovs1 ovs-vsctl add-port ovs1 veth1_br ovs-vsctl add-port ovs1 veth2_br # 启动veth pair ip netns exec ns1 ip link set veth1 up ip netns exec ns2 ip link set veth2 up ip link set veth1_br up ip link set veth2_br up # 设置veth1和veth2的ip地址 ip netns exec ns1 ip addr add 192.168.1.100 dev veth1 ip netns exec ns2 ip addr add 192.168.1.200 dev veth2 # 配置路由 ip netns exec ns1 route add -net 192.168.1.0 netmask 255.255.255.0 dev veth1 ip netns exec ns2 route add -net 192.168.1.0 netmask 255.255.255.0 dev veth2 可以使用如下命令来查看刚才的操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 可以看到刚才创建的网桥 $ ovs-vsctl list-br ovs1 # 查看网桥的端口 $ ovs-vsctl list-ports ovs1 veth1_br veth2_br # 查看网桥的状态 $ ovs-vsctl show 4ec35070-a763-4748-878a-c3784b5938a4 Bridge \u0026#34;ovs1\u0026#34; Port \u0026#34;veth1_br\u0026#34; Interface \u0026#34;veth1_br\u0026#34; Port \u0026#34;ovs1\u0026#34; Interface \u0026#34;ovs1\u0026#34; type: internal Port \u0026#34;veth2_br\u0026#34; Interface \u0026#34;veth2_br\u0026#34; ovs_version: \u0026#34;2.12.0\u0026#34; # 查看interface的状态，跟port是一一对应的 $ ovs-vsctl list interface veth1_br ... mac : [] mac_in_use : \u0026#34;32:13:6b:99:91:2b\u0026#34; mtu : 1500 mtu_request : [] name : \u0026#34;veth1_br\u0026#34; ofport : 1 # ovs port编号 ... 接下来测试一下网络的连通性是没问题的。\n1 ip netns exec ns1 ping 192.168.1.200 查看当前流表，可以看到有一条默认的规则，该条规则用来实现交换机的基本动作。\n1 2 $ ovs-ofctl dump-flows ovs1 cookie=0x0, duration=1428.955s, table=0, n_packets=22, n_bytes=1676, priority=0 actions=NORMAL 将上述规则删除，再执行ping命令发现已经不通。说明该默认规则会将流量在端口之间进行转发。\n1 ovs-ofctl del-flows ovs1 新增加如下两条规则，用来表示将port 1的流量转发到port 3，将port 3的流量转发到port 1。其中的1和3分别为port编号，使用ovs-vsctl list interface veth1_br命令中的ofport可以看到。\n1 2 3 4 5 6 ovs-ofctl add-flow ovs1 \u0026#34;priority=1,in_port=1,actions=output:3\u0026#34; ovs-ofctl add-flow ovs1 \u0026#34;priority=2,in_port=3,actions=output:1\u0026#34; $ ovs-ofctl dump-flows ovs1 cookie=0x0, duration=69.378s, table=0, n_packets=4, n_bytes=280, priority=1,in_port=\u0026#34;veth1_br\u0026#34; actions=output:\u0026#34;veth2_br\u0026#34; cookie=0x0, duration=69.063s, table=0, n_packets=4, n_bytes=280, priority=2,in_port=\u0026#34;veth2_br\u0026#34; actions=output:\u0026#34;veth1_br\u0026#34; 再执行ping命令，发现可以ping通了。\n重新增加一条优先级更高的规则，将port 1的数据drop掉。此时再ping发现已经不通了。\n1 ovs-ofctl add-flow ovs1 \u0026#34;priority=3,in_port=1,actions=drop\u0026#34; 多table 接下来清理掉规则，并将规则重新写入到table1中，默认规则是写入到table0中的\n1 2 3 ovs-ofctl del-flows ovs1 ovs-ofctl add-flow ovs1 \u0026#34;table=1,priority=1,in_port=1,actions=output:3\u0026#34; ovs-ofctl add-flow ovs1 \u0026#34;table=1,priority=2,in_port=3,actions=output:1\u0026#34; 此时再执行ping命令，发现网络是不通的。因为table0中没有匹配成功，包被drop掉了。\n再增加如下规则，即将table 0的规则发送到table 1处理，此时可以ping通。\n1 ovs-ofctl add-flow ovs1 \u0026#34;table=0,actions=goto_table=1\u0026#34; group table 执行ovs-ofctl del-flows ovs1重新清理掉规则，执行下面命令查看group table内容，可以看到内容为空。\n1 2 # ovs-ofctl -O OpenFlow13 dump-groups ovs1 OFPST_GROUP_DESC reply (OF1.3) (xid=0x2): 执行如下命令，完成数据包从table0 -\u0026gt; group table -\u0026gt; table1的过程，真正数据处理在table1中。\n1 2 3 4 5 6 7 8 9 10 # 创建一个group table，其作用为将数据包发送到table 1 ovs-ofctl add-group ovs1 \u0026#34;group_id=1,type=select,bucket=resubmit(,1)\u0026#34; # 将port 1和3 的数据发往group table 1 ovs-ofctl add-flow ovs1 \u0026#34;table=0,in_port=1,actions=group:1\u0026#34; ovs-ofctl add-flow ovs1 \u0026#34;table=0,in_port=3,actions=group:1\u0026#34; # table 1为真正要处理数据的逻辑 ovs-ofctl add-flow ovs1 \u0026#34;table=1,priority=1,in_port=1,actions=output:3\u0026#34; ovs-ofctl add-flow ovs1 \u0026#34;table=1,priority=2,in_port=3,actions=output:1\u0026#34; 此时再执行ping命令，发现是可以ping通的。\n清理操作 1 2 3 4 5 6 # 删除网桥 ovs-vsctl del-br ovs1 ip link delete veth1_br ip link delete veth2_br ip netns del ns1 ip netns del ns2 常用操作 ovs-appctl fdb/show ovs1: 查看mac地址表 ovs-ofctl show ovs1: 可以查看网桥的端口号 ovs-vsctl set bridge ovs1 stp_enable=false: 开启网桥的生成树协议 ovs-appctl ofproto/trace ovs1 in_port=1,dl_dst=7a:42:0a:ca:04:65: 可用来验证一个包到达网桥后的处理流程 reference CentOS 7 安装 Open vSwitch OpenvSwitch初探 - FLOW篇 Open vSwitch (OVS) commands for troubleshooting ","date":"2020-02-10T23:52:22Z","permalink":"/post/ovs%E5%85%A5%E9%97%A8/","title":"ovs入门"},{"content":"kube-proxy默认使用iptables规则来做k8s集群内部的负载均衡，本文通过例子来分析创建的iptabels规则。\n主要的自定义链涉及到：\nKUBE-SERVICES： 访问集群内服务的CLusterIP数据包入口，根据匹配到的目标ip+port将数据包分发到相应的KUBE-SVC-xxx链上。一个Service对应一条规则。由OUTPUT链调用。 KUBE-NODEPORTS: 用来匹配nodeport端口号，并将规则转发到KUBE-SVC-xxx。一个NodePort类型的Service一条。在KUBE-SERVICES链的最后被调用 KUBE-SVC-xxx：相当于是负载均衡，将流量利用random模块均分到KUBE-SEP-xxx链上。 KUBE-SEP-xxx：通过dnat规则将连接的目的地址和端口号做dnat，从Service的ClusterIP或者NodePort转换为后端的pod ip KUBE-MARK-MASQ: 使用mark命令，对数据包设置标记0x4000/0x4000。在KUBE-POSTROUTING链上有MARK标记的数据包进行一次MASQUERADE，即SNAT，会用节点ip替换源ip地址。 环境准备 创建nginx deployment\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-svc version: nginx name: nginx namespace: default spec: replicas: 2 selector: matchLabels: app: nginx-svc template: metadata: labels: app: nginx-svc version: nginx spec: containers: - image: \u0026#39;nginx:1.9.0\u0026#39; name: nginx ports: - containerPort: 443 protocol: TCP - containerPort: 80 protocol: TCP 创建service对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Service metadata: name: nginx-svc namespace: default spec: ports: - name: \u0026#39;80\u0026#39; port: 8000 protocol: TCP targetPort: 80 nodePort: 30080 selector: app: nginx-svc sessionAffinity: None type: NodePort 环境信息如下：\n容器网段：172.20.0.0/16 Service ClusterIP cidr: 192.168.0.0/20 k8s版本： 提交后创建出来的信息如下：\nService ClusterIP：192.168.103.148 nginx pod的两个ip地址：172.16.3.3 172.16.4.4 从宿主机上访问ClusterIP 从本机请求ClusterIP的数据包会经过iptables的链：OUTPUT -\u0026gt; POSTROUTING\n要想详细知道iptabels的执行情况，可以通过iptables的trace功能。如何开启trace功能可以参考：/post/iptables/。\n执行 iptables -nvL OUTPUT -t nat 可以看到如下的iptables规则命令\n1 2 pkts bytes target prot opt in out source destination 17M 1150M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ 执行 iptables -nvL KUBE-SERVICES -t nat 可以查看自定义链的具体内容，里面包含了多条规则，其中跟当前Service相关的规则如下。\n1 2 pkts bytes target prot opt in out source destination 1 60 KUBE-SVC-Y5VDFIEGM3DY2PZE tcp -- * * 0.0.0.0/0 192.168.103.148 /* default/nginx-svc:80 cluster IP */ tcp dpt:8000 执行 iptables -nvL KUBE-SVC-Y5VDFIEGM3DY2PZE -t nat 查看自定义链的具体规则\n1 2 3 pkts bytes target prot opt in out source destination 0 0 KUBE-SEP-IFV44I3EMZAL3LH3 all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ statistic mode random probability 0.50000000000 1 60 KUBE-SEP-6PNQETFAD2JPG53P all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ 上述规则会按照特定的概率将流量均等的执行自定义链的规则，两个自定义的链的规则跟endpoint相关，执行 iptables -nvL KUBE-SEP-IFV44I3EMZAL3LH3 -t nat可查看endpoint级别的iptabels规则。dnat操作会修改数据包的目的地址和端口，从clusterip+service port修改为访问pod ip+pod端口。\n1 2 3 pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 172.16.3.3 0.0.0.0/0 /* default/nginx-svc:80 */ 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp to:172.16.3.3:80 会在dnat操作之前为对数据包执行打标签操作。KUBE-MARK-MASQ 自定义链为对数据包打标记的自定义规则，执行 iptables -nvL KUBE-MARK-MASQ -t nat\n1 2 pkts bytes target prot opt in out source destination 1 60 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK or 0x4000 接下来看一下POSTROUTING链上的规则，iptables -nvL POSTROUTING -t nat。\n1 2 pkts bytes target prot opt in out source destination 205K 13M KUBE-POSTROUTING all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes postrouting rules */ 继续看一下KUBE-POSTROUTING链的内容，iptables -nvL KUBE-POSTROUTING -t nat，其中最后一条的MASQUERADE指令的操作实际上为SNAT操作。\n1 2 3 4 5 Chain KUBE-POSTROUTING (1 references) pkts bytes target prot opt in out source destination 6499 398K RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 mark match ! 0x4000/0x4000 1 60 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK xor 0x4000 1 60 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service traffic requiring SNAT */ 即从本机访问service clusterip的数据包，在output链上经过了dnat操作，在postrouting链上经过了snat操作后，最终会发往目标pod。pod在处理完请求后，回的数据包最终会经过nat的逆过程返回到本机。\n外部访问nodeport 从外部访问本机的nodeport数据包会经过iptables的链：PREROUTING -\u0026gt; FORWARD -\u0026gt; POSTROUTING\nnodeport都是被外部访问的情况，入口位于PREROUTING链上。执行 iptables -nvL PREROUTING -t nat：\n1 2 pkts bytes target prot opt in out source destination 349K 21M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ 在KUBE-SERVICES链的最后一条规则为跳转到KUBE-NODEPORTS链\n1 4079 246K KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL 执行iptables -nvL KUBE-NODEPORTS -t nat， 查看KUBE-NODEPORTS链\n1 2 3 pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp dpt:30080 0 0 KUBE-SVC-Y5VDFIEGM3DY2PZE tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp dpt:30080 其中KUBE-MARK-MASQ链只有一条规则，即打上0x4000的标签。\n1 2 pkts bytes target prot opt in out source destination 0 0 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK or 0x4000 自定义链KUBE-SVC-Y5VDFIEGM3DY2PZE的内容如下，跟clusterip的规则是重叠的：\n1 2 3 pkts bytes target prot opt in out source destination 0 0 KUBE-SEP-IFV44I3EMZAL3LH3 all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ statistic mode random probability 0.50000000000 0 0 KUBE-SEP-6PNQETFAD2JPG53P all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ KUBE-SEP-IFV44I3EMZAL3LH3的内容为，会经过一次DNAT操作:\n1 2 3 pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 172.16.3.3 0.0.0.0/0 /* default/nginx-svc:80 */ 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp to:172.16.3.3:80 在经过了PREROUTING链后，接下来会判断目的ip地址不是本机的ip地址，接下来会经过FORWARD链。在FORWARD链上，仅做了一件事情，就是将前面大了0x4000的数据包允许转发。\n1 2 3 4 pkts bytes target prot opt in out source destination 0 0 KUBE-FORWARD all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding rules */ 0 0 KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate NEW /* kubernetes service portals */ 0 0 KUBE-EXTERNAL-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate NEW /* kubernetes externally-visible service portals */ KUBE-FORWARD的内容如下：\n1 2 3 4 5 pkts bytes target prot opt in out source destination 0 0 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate INVALID 0 0 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding rules */ mark match 0x4000/0x4000 0 0 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding conntrack pod source rule */ ctstate RELATED,ESTABLISHED 0 0 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding conntrack pod destination rule */ ctstate RELATED,ESTABLISHED 跟clusterip一样，会在POSTROUTING阶段匹配mark为0x4000/0x4000的数据包，并进行一次MASQUERADE转换，将ip包替换为宿主上的ip地址。\n加入这里不做MASQUERADE，流量发到目的的pod后，pod回包时目的地址为发起端的源地址，而发起端的源地址很可能是在k8s集群外部的，此时pod发回的包是不能回到发起端的。NodePort跟ClusterIP的最大不同就是NodePort的发起端很可能是在集群外部的，从而这里必须做一层SNAT转换。\n在上述分析中，访问NodePort类型的Service会经过snat，从而服务端的pod不能获取到正确的客户端ip。可以设置Service的spec.externalTrafficPolicy为Local，此时iptables规则只会将ip包转发给运行在这台宿主机上的pod，而不需要经过snat。pod回包时，直接回复源ip地址即可，此时源ip地址是可达的，因为源ip地址跟宿主机是可达的。如果所在的宿主机上没有pod，那么此时流量就不可以转发，此为限制。\n使用LoadBalancer类型访问的情况 externalTrafficPolicy为local 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -A KUBE-SERVICES -d 10.149.30.186/32 -p tcp -m comment --comment \u0026#34;acs-system/nginx-ingress-lb-cloudbiz:http loadbalancer IP\u0026#34; -m tcp --dport 80 -j KUBE-FW-76HLDRT5IPNSMPF5 -A KUBE-FW-76HLDRT5IPNSMPF5 -m comment --comment \u0026#34;acs-system/nginx-ingress-lb-cloudbiz:http loadbalancer IP\u0026#34; -j KUBE-XLB-76HLDRT5IPNSMPF5 -A KUBE-FW-76HLDRT5IPNSMPF5 -m comment --comment \u0026#34;acs-system/nginx-ingress-lb-cloudbiz:http loadbalancer IP\u0026#34; -j KUBE-MARK-DROP # 10.149.112.0/23为pod网段 -A KUBE-XLB-76HLDRT5IPNSMPF5 -s 10.149.112.0/23 -m comment --comment \u0026#34;Redirect pods trying to reach external loadbalancer VIP to clusterIP\u0026#34; -j KUBE-SVC-76HLDRT5IPNSMPF5 -A KUBE-XLB-76HLDRT5IPNSMPF5 -m comment --comment \u0026#34;Balancing rule 0 for acs-system/nginx-ingress-lb-cloudbiz:http\u0026#34; -j KUBE-SEP-XZXLBWOKJBSJBGVU -A KUBE-SVC-76HLDRT5IPNSMPF5 -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-XZXLBWOKJBSJBGVU -A KUBE-SVC-76HLDRT5IPNSMPF5 -j KUBE-SEP-GP4UCOZEF3X7PGLR -A KUBE-SEP-XZXLBWOKJBSJBGVU -s 10.149.112.45/32 -j KUBE-MARK-MASQ -A KUBE-SEP-XZXLBWOKJBSJBGVU -p tcp -m tcp -j DNAT --to-destination 10.149.112.45:80 -A KUBE-SEP-GP4UCOZEF3X7PGLR -s 10.149.112.46/32 -j KUBE-MARK-MASQ -A KUBE-SEP-GP4UCOZEF3X7PGLR -p tcp -m tcp -j DNAT --to-destination 10.149.112.46:80 缺点 iptables规则特别乱，一旦出现问题非常难以排查 由于iptables规则是串行执行，算法复杂度为O(n)，一旦iptables规则多了后，性能将非常差。 iptables规则提供的负载均衡功能非常有限，不支持较为复杂的负载均衡算法。 ","date":"2020-02-08T12:10:33Z","permalink":"/post/kube-proxy-iptables%E8%A7%84%E5%88%99%E5%88%86%E6%9E%90/","title":"kube-proxy iptables规则分析"},{"content":"docker bridge是默认的网络模式\n容器访问外网 默认情况下，容器即可访问外网。\n启动一个容器：docker run -d nginx\n容器中访问外网的请求如www.baidu.com，内核协议栈根据路由信息，会选择默认路由，将请求发送到容器中的eth0网卡，目的mac地址为网关172.17.0.1的mac地址。\n1 2 3 4 5 $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 eth0网卡接收到数据包后，会将数据包转发到veth pair的另外一端，即宿主机网络中的veth6b173fd设备。\nveth6b173fd设备是挂在网桥上的，会将数据包转发到网桥br0，br0即为网关172.17.0.1。\nbr0接收到数据包后，会将数据包转发给内核协议栈。\n宿主机上的/proc/sys/net/ipv4/ip_forward为1，表示转发功能开启，即目的ip不是本机的会根据路由规则进行转发，而不是丢弃。\n仅在宿主机上开启了ip_forward后，包即使转发了，还是无法回来的，因为包中的源ip地址为172.17.0.1，是私有网段的ip地址。需要做一次SNAT才可以，docker会在iptabels的nat表中的postrouting链中增加SNAT规则，下面规则的意思是源地址为172.17.0.0/16的会做一次地址伪装，即SNAT。\n1 2 3 4 5 # iptables -nL -t nat Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 在eth0网卡上抓包，可以发现源ip已经是eth0的网卡ip地址。\n端口映射 命令格式：-p ${host_port}:${container_port}\n启动一个容器：docker run -d -p 8080:80 nginx\n查看本地的iptables规则\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ iptables -nL -t nat Chain PREROUTING (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL Chain INPUT (policy ACCEPT) target prot opt source destination Chain OUTPUT (policy ACCEPT) target prot opt source destination DOCKER all -- 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCAL Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 MASQUERADE tcp -- 172.17.0.2 172.17.0.2 tcp dpt:80 Chain DOCKER (2 references) target prot opt source destination RETURN all -- 0.0.0.0/0 0.0.0.0/0 ## prerouting链引用，外面发往本机的8080端口的数据包，会dnat为172.17.0.2:80 DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.17.0.2:80 丢包问题 SNAT在并发比较高的情况下，会存在少量的丢包现象，具体原因跟conntrack模式的实现有关。conntrack在SNAT端口的分配和插入conntrack表之间有个延时，如果在这中间存在冲突的话会导致插入失败，从而出现丢包的问题。\n该问题没有根治的解决办法，能大大缓解的解决办法为使用iptabels的\u0026ndash;random-fully选项，SNAT选择端口为随机，大大降低出现冲突的概率。\n","date":"2020-02-01T21:34:53Z","permalink":"/post/docker-bridge-network/","title":"docker bridge network"},{"content":"ipip协议为在ip协议报文的基础上继续封装ip报文，基于tun设备实现，是一种点对点的通讯技术。\ninstall ipip需要内核模块ipip的支持\n1 2 3 4 5 $ modprobe ipip $ lsmod | grep ipip ipip 13465 0 tunnel4 13252 1 ipip ip_tunnel 25163 1 ipip 实战 两台主机：172.16.5.126(host1)和172.16.5.127(host2)\n在host1上创建tun1设备，执行如下命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 用来创建tun1设备，并ipip协议的外层ip，目的ip为172.16.5.127， 源ip为172.16.5.126 ip tunnel add tun1 mode ipip remote 172.16.5.127 local 172.16.5.126 # 给tun1设备增加ip地址，并设置tun1设备的对端ip地址为10.10.200.10 ip addr add 10.10.100.10 peer 10.10.200.10 dev tun1 ip link set tun1 up $ ifconfig tun1 tun1: flags=209\u0026lt;UP,POINTOPOINT,RUNNING,NOARP\u0026gt; mtu 1480 inet 10.10.100.10 netmask 255.255.255.255 destination 10.10.200.10 tunnel txqueuelen 1000 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 增加一条路由，所有到达10.10.200.10的请求会经过设备tun1 $ route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.16.7.253 0.0.0.0 UG 0 0 0 eth0 10.10.200.10 0.0.0.0 255.255.255.255 UH 0 0 0 tun1 172.16.0.0 0.0.0.0 255.255.248.0 U 0 0 0 eth0 同样在host2上创建tun1设备：\n1 2 3 ip tunnel add tun1 mode ipip remote 172.16.5.126 local 172.16.5.127 ip addr add 10.10.200.10 peer 10.10.100.10 dev tun1 ip link set tun1 up 并分别在host1和host2上打开ip_forward功能\n1 echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward 然后在host1上ping 10.10.200.10，可以ping通。\n在host1的tun1上抓包，可以看到正常的ping包。\n在host1的eth1上抓包，可以看到已经是ipip的数据包了。\ntun1.pcap\n清理现场分别在两台主机上执行\n1 ip link delete tun0 ref 什么是 IP 隧道，Linux 怎么实现隧道通信？ ","date":"2020-02-01T00:00:15Z","permalink":"/post/linux-ipip%E9%9A%A7%E9%81%93%E5%8D%8F%E8%AE%AE/","title":"Linux IPIP隧道协议"},{"content":"Linux虚拟网络设备tap/tun tap/tun常用于隧道通讯，通过一个字符设备来实现用户态和内核态的通讯，字符设备一端连接着用户空间，一端连接着内核空间。\n与物理网卡的最大不同是，tap/tun的数据源来自于用户态的程序，而物理网卡的数据源来自于物理链路。\n对应的字符设备文件位置：\ntap: /dev/tap0 tun: /dev/net/tun 当应用程序打开字符设备文件时，驱动程序会创建并注册相应的虚拟设备接口，以tunX或tapX命名。应用程序关闭设备文件时，驱动程序会删除tunX和tapX网络虚拟设备，并删除建立起来的路由信息。\n两个设备的不同点：\ntap是一个二层网络设备，只能处理二层的以太网帧，可以与物理网卡做桥接 tun是一个点对点的三层网络设备，只能处理处理三层的IP数据包，无法与物理网卡做桥接，可以通过三层交换方式与物理网卡连通。Linux下的隧道协议基于该tun设备实现，如ipip、gre。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ┌──────────────┐ │ │ │ APP │ │ │ └───────┬──────┘ │ │ │ │ │ │ ┌────────────▼──────────┐ │ │ ─ ─ ─ ─ ─ ─│ /dev/net/tun ├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ └────────────┬──────────┘ │ │ │ │ │ ┌───────▼──────┐ ┌──────────────┐ │ │ │ │ │ tunX ├────────────────▶│Network Stack │ │ │ │ │ └──────────────┘ └──────────────┘ tun设备应用举例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ┌──────────────┐ ┌──────────────┐ │ │ │ │ │ APP A │ │ APP B │◀┐ │ │ │ │ │ └───────┬──────┘ └───────┬──────┘ │ │ │ │ │ │ │ 1│ │ │ │ 5│ │ │ │ │ ─ ─ ─ ─ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─│─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ │ │ │ │ │ 4│ │ │ │ ┌────────────▼────────────────────────▼─────┐ │ │ │ │ │ Network Stack │ │ │ │ │ └────────────┬───────────────────────┬──────┘ │ │ │ │ 6│ 3│ │ │ │ │ ┌───────▼──────┐ ┌─▼─────────┴──┐ │ │ │ │ 10.1.1.11 │ eth0 │ │ tun0 │ 192.168.1.11 │ │ │ │ └───────┬──────┘ └──────────────┘ │ 7│ │ ▼ 10.1.1.100 / 192.168.1.100 应用程序A要发送数据到其他物理机192.168.1.100，由于物理网络环境下只有10.1.1.11和10.1.1.100是相互连通的，192.168.1.11和192.168.1.100是不通的，为了192.168.1.11和192.168.1.100能够进行通讯，需要将数据包进行一次封装。\n应用程序B是通过打开字符设备文件/dev/net/tun0的方式来打开网络设备\n流程如下：\nA构造数据包，目的ip为192.168.1.100，并发送给协议栈 协议栈根据数据包中的ip地址，匹配路由规则，要从tun0出去 内核协议栈将数据包发送给tun0网络设备 tun0发送应用程序B打开，于是将数据发送给应用程序B B收到数据包后，在用户态构造一个新的数据包，源IP为eth0的IP 10.1.1.11，目的IP为配置的对端10.1.1.100，并封装原来的数据包 协议栈根据当前数据包的IP地址选择路由，将数据包发送给eth0 reference 详解云计算网络底层技术——虚拟网络设备 tap/tun 原理解析 Linux 网络工具详解之 ip tuntap 和 tunctl 创建 tap/tun 设备 Linux虚拟网络设备之tun/tap ","date":"2020-01-31T00:55:34Z","permalink":"/post/linux%E4%B8%AD%E7%9A%84tap/tun%E8%AE%BE%E5%A4%87/","title":"Linux中的tap/tun设备"},{"content":"网卡的混杂模式是指网卡将其接收的所有流量都交给cpu。非混杂模式下，网卡仅接收目的mac地址是自己mac地址的单播，以及多播和广播包，可以看出混杂模式是工作在二层的。\n通过ifconfig eth0的方式，如果输出中包含了PROMISC字段，说明网卡处于混杂模式。但是如果ifconfig 命令的输出中未包含PROMISC字段，并不能说明网卡处于非混杂模式下。\n可以通过查看 cat /sys/class/net/bond0/flags 的输出得知，如果置位了0x100，说明处于混杂模式。\n","date":"2020-01-31T00:19:09Z","permalink":"/post/linux%E4%B8%8B%E7%BD%91%E5%8D%A1%E6%B7%B7%E6%9D%82%E6%A8%A1%E5%BC%8F/","title":"Linux下网卡混杂模式"},{"content":"veth pair是一对虚拟的网络设备，两个网络设备彼此连接。常用于两个network namespace之间的连接，如果在同一个命名空间下有很多的限制。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ┌──────────────────────────────────────────────────────────────────────────────┐ │ │ │ │ │ network protocol │ │ │ │ │ └────────────────────▲─────────────────────────▲──────────────────────▲────────┘ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ┌─────▼────┐ ┌─────▼────┐ ┌─────▼────┐ │ │ │ │ │ │ │ eth0 │ │ veth0 ◀───────────▶ veth1 │ │ │ │ │ │ │ └─────▲────┘ └──────────┘ └──────────┘ │ │ │ │ │ ▼ physical network 实战 veth设备的ping测试 1. 只给一个veth设备配置ip的情况测试 给veth0配置ip 192.168.100.10，可以看到主机的路由表中增加了目的地为192.168.100.0的记录\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 [root@localhost vagrant]# ip link add veth0 type veth peer name veth1 [root@localhost vagrant]# ip addr add 192.168.100.10/24 dev veth0 [root@localhost vagrant]# ip addr add 192.168.100.11/24 dev veth1 ## 因为veth创建完后默认不启用，此时还没有路由 [root@localhost vagrant]# route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default gateway 0.0.0.0 UG 100 0 0 eth0 10.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 192.168.33.0 0.0.0.0 255.255.255.0 U 101 0 0 eth1 ## 启用veth0后增加路由 [root@localhost vagrant]# ip link set veth0 up [root@localhost vagrant]# route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default gateway 0.0.0.0 UG 100 0 0 eth0 10.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 192.168.33.0 0.0.0.0 255.255.255.0 U 101 0 0 eth1 192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0 ## 启用veth1后居然又增加了一条路由信息 [root@localhost vagrant]# ip link set veth1 up [root@localhost vagrant]# ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:26:10:60 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0 valid_lft 86214sec preferred_lft 86214sec inet6 fe80::5054:ff:fe26:1060/64 scope link valid_lft forever preferred_lft forever 3: eth1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:98:06:20 brd ff:ff:ff:ff:ff:ff inet 192.168.33.11/24 brd 192.168.33.255 scope global noprefixroute eth1 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe98:620/64 scope link valid_lft forever preferred_lft forever 4: veth1@veth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether e2:15:95:0a:1f:da brd ff:ff:ff:ff:ff:ff inet 192.168.100.11/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::e015:95ff:fe0a:1fda/64 scope link valid_lft forever preferred_lft forever 5: veth0@veth1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b2:2c:f6:e4:74:c5 brd ff:ff:ff:ff:ff:ff inet 192.168.100.10/24 scope global veth0 valid_lft forever preferred_lft forever inet6 fe80::b02c:f6ff:fee4:74c5/64 scope link valid_lft forever preferred_lft forever [root@localhost vagrant]# route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default gateway 0.0.0.0 UG 100 0 0 eth0 10.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 192.168.33.0 0.0.0.0 255.255.255.0 U 101 0 0 eth1 192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0 192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 veth1 默认情况下arp表如下：\n1 2 3 4 5 6 # arp Address HWtype HWaddress Flags Mask Iface localhost.localdomain (incomplete) veth0 192.168.33.1 ether 0a:00:27:00:00:00 C eth1 gateway ether 52:54:00:12:35:02 C eth0 10.0.2.3 ether 52:54:00:12:35:03 C eth0 使用ping命令ping -I veth0 192.168.100.11 -c 2，默认情况下veth1和veth0会接收到arp报文，但并没有arp的响应报文。这是因为默认情况下有些arp内核参数的限制。执行如下命令解决arp的限制。\n1 2 3 4 5 echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth1/accept_local echo 1 \u0026gt; /proc/sys/net/ipv4/conf/veth0/accept_local echo 0 \u0026gt; /proc/sys/net/ipv4/conf/all/rp_filter echo 0 \u0026gt; /proc/sys/net/ipv4/conf/veth0/rp_filter echo 0 \u0026gt; /proc/sys/net/ipv4/conf/veth1/rp_filter veth pair设备的删除 1 2 # 删除veth0后会自动删除veth1 $ ip link delete veth0 container与host veth pair的关系 veth pair的其中一个设备位于container中备位于container中，另外一个设备位于host network namespace中，如何知道container中的eth0和host network namesapce中的veth设备的对应关系呢？\n原理为veth pair设备都有一个ifindex和iflink值，，容器中的eth0设备的ifindex值跟host network namespace中的对应veth pair设备的iflink值相等，反之亦然。\n在容器中找到eth0的iflink 方法一\n获取iflink值：cat /sys/class/net/eth0/iflink\n也可用此方法获取ifindex值：cat /sys/class/net/eth0/ifindex\n方法二\n1 2 3 $ ip link show eth0 3: eth0@if18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u0026gt; mtu 1500 qdisc noqueue state UP link/ether 96:5f:80:a3:a3:01 brd ff:ff:ff:ff:ff:ff 其中的3为eth0的ifindex。18为eth0的iflink，即对应的veth pair的另外一个设备的ifindex。\nhost network namespace中找到对应ifindex值的veth pair设备 1 2 3 4 5 $ ip addr 18: veth0e09999e@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master br0 state UP group default link/ether de:b0:74:89:e8:3e brd ff:ff:ff:ff:ff:ff link-netnsid 4 inet6 fe80::dcb0:74ff:fe89:e83e/64 scope link valid_lft forever preferred_lft forever 其中的18为ifindex，3为对应的veth pair的ifindex。\nreference Linux 虚拟网络设备 veth-pair 详解，看这一篇就够了 Linux虚拟网络设备之veth ","date":"2020-01-29T21:21:06Z","permalink":"/post/linux%E4%B8%AD%E7%9A%84veth-pair%E8%AE%BE%E5%A4%87/","title":"Linux中的veth pair设备"},{"content":"\n距离上次的知识分享系列已经过去了半年之久，难以想象。在该系列的开始我就说过该系列是不定期的，果不食言，只是这次的不定期有些久😓。该系列会继续下去，但节奏仍然是不定期的，但应该不会间隔半年之巨了。\n题图来自首钢工业遗址公园，首钢于2010年完成了搬迁到唐山市的工作，位于石景山的工厂被废弃。2019年国庆节前夕以公园的形式部分对外开放，跟普通公园不同的地方在于保留了很多之前的工厂建筑及大型生产机械，足够硬核，非常原汁原味。\n2022年要举办的冬奥会也非常明智的将场地选择在了该公园，预计将来会有很多的场馆位于该公园内且对外开放。\n国内很多的地市都有一些类似的建筑，比如90年代下岗潮之前的一些国企工厂，我知道的济南的机床厂就有很多个，但很多这些倒闭关门的工厂后来的建筑及地皮都给卖掉了，建筑物也直接给拆掉了，殊不知其衍生价值也是有不少的。位于北京酒仙桥的798就是个非常好的改造案例，798园区的建筑稍加改造，给很多艺术工作室提供了非常好的办公场地，也是城市中的一个亮眼的名片。\n资源 1.MessagePack\njson作为一种常见的数据序列化方案，存在占用空间过多、反序列化过于消耗cpu的问题。MessagePack是一种基于二进制的高效轻量的数据序列化方案，支持数据的压缩，支持丰富的编程语言。在上图中，可以看出原27字节的json数据转换为MessagePack后仅占用了18字节。\n另外，据今日头条的压测，要比Thrift的二进制序列化方案更高效一些。\n2.GoAst Viewer\nGolang中的ast、parser、token包可用来对golang的源码进行语法分析，并构建出AST树。GoAst Viewer支持在线输入Golang源码来构建AST树。\n3.KubeEdge\nIoT目前正在大力发展，边缘设备的计算能力在逐渐增强，同时处理的数据量的需求正在快速增加，而数据中心的数据处理能力、网络带宽、扩展能力并没有太多的增强，未来势必会将部分计算能力下放到边缘设备，以降低数据中心的成本。\n华为开源的KubeEdge为基于Kubernetes的边缘计算平台，支持边缘集群的编排和管理。\n4.Mycat\n国内开源的关系型数据库中间件，支持MySQL、Oracle等常见的关系型数据库。关系型数据库单表过大导致性能下降后的解决思路往往是分库分表，分库分表后需要增加中间件层来解决多个数据库多张表的数据增删改查问题，而Mycat是一个不错的解决方案。\n当然Mycat的功能不仅限于此，比如支持跨库的两张表join、Mycat-eye可以来监控Mycat等，更多功能请参照官方网站。\n5.WeChat Format\n一款转换markdown格式的文档为微信公众号排版的工具，排版比较精美，推荐一试。\n6.GitNote\n一款基于git的笔记管理软件，可以将笔记存放到Github中，支持多种图床插件，支持多个平台（还没有移动端），支持富文本编辑和Markdown编辑。由于笔记是可以同步Github上的，可以做到永久保存和版本控制，而且笔记的存放目录和格式不受该工具的影响，可以说是完全没有侵入性，脱离了该工具仍然可以通过直接编辑git项目的方式来发布笔记。\n7.Vlang开源啦\nV语言宣布开源，从V语言的特性上看到了很多Golang的影子，并未看到耳目一新的特性。\n8.krew\n一款kubectl的plunin管理工具，mac平台下有brew包管理工具，随着kubectl的plugin机制的成熟，plugin管理工具应运而生。\n9.cert-manager\n运行在k8s上的证书管理工具，可以签发证书，基于CRD实现。\nLet\u0026rsquo;s Encrypt提供了免费的tls证书，但证书有有效期限制，过期后需要手工重新申请证书，cert-manager可以做到从Let\u0026rsquo;s Encrypt自动申请证书，并过期后重新申请证书。\n10.cmatrix\n一款黑客帝国效果的命令行工具，除了炫酷也没啥其他用途了。\n11.boxes\nboxes为一款有趣的命令行工具，可以显示很多神奇效果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 __ _,--=\u0026#34;=--,_ __ / \\.\u0026#34; .-. \u0026#34;./ \\ / ,/ _ : : _ \\/` \\ \\ `| /o\\ :_: /o\\ |\\__/ `-\u0026#39;| :=\u0026#34;~` _ `~\u0026#34;=: | \\` (_) `/ .-\u0026#34;-. \\ | / .-\u0026#34;-. .---{ }--| /,.-\u0026#39;-.,\\ |--{ }---. ) (_)_)_) \\_/`~-===-~`\\_/ (_(_(_) ( ( Different all twisty a ) ) of in maze are you, ( ( passages little. ) ) ( \u0026#39;---------------------------------------\u0026#39; 12.rally\n一款elasticsearch的压测工具。\n13.tekton\nGoogle开源的一款基于Kubernetes的应用发布框架，Google在云原生生态中出品一般质量都比较高，主要用来做CI/CD。\n14.kubectl-debug\n一款基于kubectl插件的debug工具，基础镜像使用nicolaka/netshoot(内置了大量的网络排查工具)，可用于kubernetes集群中快速定位问题。值得一提的是，该工具的初版是作者在参加pingcap面试时的小作业。\n15.Monocular\nRancher出品的一款基于管理helm chart的ui工具。\n16.gitmoji\ngithub上的开源项目中经常会看到一些git commit message中包含了moji表情，而且有越来越多的趋势，这些moji表情不紧紧是好玩，而且还非常生动形象的表达了commit message的含义，并且非常醒目，但这些moji表情可不应该是滥用的。该网站记录了一些常用的moji表情在git commit中的含义。\n精彩文章 1.Monitoring and Tuning the Linux Networking Stack: Receiving Data\n本文讲解了一个数据包到达网卡后是怎么一步步从网卡 -\u0026gt; 操作系统 -\u0026gt; 应用程序，并讲解了Linux中的实现方式。\n绝大多数的工程师对于这一块的知识是较为模糊的，建议一读。\n视频 https://mp.weixin.qq.com/s?__biz=MzU3OTc1Njk4MQ==\u0026mid=2247486851\u0026idx=1\u0026sn=d0322f6d1a59c21e977488d9701d0476\u0026chksm=fd607b59ca17f24fb07f60b8f488e602ac7e75302c999c206938682387e1d3cac44feb67b208\u0026mpshare=1\u0026scene=1\u0026srcid=%23rd\n半年多前比较火的视频，但我还是经常会想起来，给大家重温一下。\n视频中为杭州一小伙深夜骑车逆行被交警拦下后情绪崩溃，失声痛哭。小伙每晚加班到十一二点，一方面女朋友在催着给送药匙，另一方面公司还在催着赶回公司，再加上被交警拦下，最终来自三方面的催促导致积压在小伙内心长久以来的压力爆发而情绪失控。隔着屏幕都能感受到小伙长期以来的压力，我猜想如果给他一些自由的时间，他一定会选择独自一人到一个安静的地方过上一段时间与世隔绝的生活。\n生活本不易，在觉大多数的成年人生活中没有简单二字，祝愿各位生活如意！\n","date":"2019-10-09T01:35:26Z","permalink":"/post/%E6%8A%80%E6%9C%AF%E5%88%86%E4%BA%AB%E7%AC%AC12%E6%9C%9F/","title":"技术分享第12期"},{"content":"Linux内核会存在一些严重的bug，导致内核crash，会在/var/crash目录下产生类似”127.0.0.1-2019-09-30-21:33:38“这种的文件夹，里面包含了vmcore文件，该文件对于debug 内核crash的原因非常有帮助。\n本文在CentOS 7下操作。\n执行yum install crash来安装crash\n另外还需要两个rpm包：kernel-debuginfo-3.10.0-957.el7.x86_64.rpm 和 kernel-debuginfo-common-x86_64-3.10.0-957.el7.x86_64.rpm，需要关注下操作系统的内核版本，这两个rpm包可以通过搜索引擎找到。\n下到包后即可执行rpm -ivh *.rpm的方式来安装rpm包。\n在机器上执行crash /usr/lib/debug/lib/modules/3.10.0-957.el7.x86_64/vmlinux /var/crash/xx/vmcore进行debug，可以输入bt命令来查看栈信息。\n","date":"2019-10-02T02:09:32Z","permalink":"/post/linux%E4%B8%8Bdebug%E5%86%85%E6%A0%B8coredump/","title":"Linux下debug内核coredump"},{"content":"go mod从1.11开始已经成为了go的默认包管理工具，本文记录go mod的一些使用经验。\n要想使用go mod，需要将go升级到1.11或者更高版本。\n在没有go mod之前，项目源码必须是放在GOPATH目录下的，有了go mod之后项目即可以放在GOPATH目录下，也可以放在非GOPATH的目录下，在GOPATH目录下在执行时需要指定环境变量GO111MODULE=on,具体的写法可以是GO111MODULE=on go mod init\n由于众所周知的原因，go的包相对还是比较难下载的，很多情况下还是需要vendor目录存在的，并将vendor目录中的包一并提交到代码库中。可以使用go mod vendor命令来完成，执行该命令后会将本地下载的包copy到vendor目录下。\n坑1 提示unknown revision 1 2 3 4 # GO111MODULE=on go get gitlab.aa-inc.com/bb@v2 go: finding gitlab.aa-inc.com/bb v2 go: finding gitlab.aa-inc.com v2 go get gitlab.aa-inc.com/bb@v2: unknown revision v2 在获取单个包的时候提示unknown revision错误，后发现是go get默认是使用的https协议，而不是git协议，而git仓库的https协议不支持导致的，解决办法为:\n1 git config --global url.\u0026#34;git@gitlab.aa-inc.com:\u0026#34;.insteadOf \u0026#34;https://gitlab.aa-inc.com/\u0026#34; 参考文档 Go 每日漫谈——Go Module 的一些坑 ","date":"2019-09-18T11:39:13Z","permalink":"/post/go-mod%E4%BD%BF%E7%94%A8/","title":"go mod使用"},{"content":"Linux虚拟网络设备 - tap/tun tap/tun常用于隧道通讯，通过一个字符设备来实现用户态和内核态的通讯，字符设备一端连接着用户空间，一端连接着内核空间。\n对应的字符设备文件位置：\ntap: /dev/tap0 tun: /dev/net/tun 当应用程序打开字符设备文件时，驱动程序会创建并注册相应的虚拟设备接口，以tunX或tapX命名。应用程序关闭设备文件时，驱动程序会删除tunX和tapX网络虚拟设备，并删除建立起来的路由信息。\n两个设备的不同点：\ntap是一个二层网络设备，只能处理二层的以太网帧 tun是一个点对点的三层网络设备，只能处理处理三层的IP数据包 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ┌──────────────┐ │ │ │ APP │ │ │ └───────┬──────┘ │ │ │ │ │ │ ┌────────────▼──────────┐ │ │ ─ ─ ─ ─ ─ ─│ /dev/net/tun ├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ └────────────┬──────────┘ │ │ │ │ │ ┌───────▼──────┐ ┌──────────────┐ │ │ │ │ │ tunX ├────────────────▶│Network Stack │ │ │ │ │ └──────────────┘ └──────────────┘ tun设备应用举例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ┌──────────────┐ ┌──────────────┐ │ │ │ │ │ APP A │ │ APP B │◀┐ │ │ │ │ │ └───────┬──────┘ └───────┬──────┘ │ │ │ │ │ │ │ 1│ │ │ │ 5│ │ │ │ │ ─ ─ ─ ─ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─│─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ │ │ │ │ │ 4│ │ │ │ ┌────────────▼────────────────────────▼─────┐ │ │ │ │ │ Network Stack │ │ │ │ │ └────────────┬───────────────────────┬──────┘ │ │ │ │ 6│ 3│ │ │ │ │ ┌───────▼──────┐ ┌─▼─────────┴──┐ │ │ │ │ 10.1.1.11 │ eth0 │ │ tun0 │ 192.168.1.11 │ │ │ │ └───────┬──────┘ └──────────────┘ │ 7│ │ ▼ 10.1.1.100 / 192.168.1.100 应用程序A要发送数据到其他物理机192.168.1.100，由于物理网络环境下只有10.1.1.11和10.1.1.100是相互连通的，192.168.1.11和192.168.1.100是不通的，为了192.168.1.11和192.168.1.100能够进行通讯，需要将数据包进行一次封装。\n应用程序B是通过打开字符设备文件/dev/net/tun0的方式来打开网络设备\n流程如下：\nA构造数据包，目的ip为192.168.1.100，并发送给协议栈 协议栈根据数据包中的ip地址，匹配路由规则，要从tun0出去 内核协议栈将数据包发送给tun0网络设备 tun0发送应用程序B打开，于是将数据发送给应用程序B B收到数据包后，在用户态构造一个新的数据包，源IP为eth0的IP 10.1.1.11，目的IP为配置的对端10.1.1.100，并封装原来的数据包 协议栈根据当前数据包的IP地址选择路由，将数据包发送给eth0 reference 详解云计算网络底层技术——虚拟网络设备 tap/tun 原理解析 Linux 网络工具详解之 ip tuntap 和 tunctl 创建 tap/tun 设备 Linux虚拟网络设备之tun/tap ","date":"2019-07-06T16:48:41Z","permalink":"/post/linux%E8%99%9A%E6%8B%9F%E7%BD%91%E7%BB%9C%E8%AE%BE%E5%A4%87-tap/tun/","title":"Linux虚拟网络设备 - tap/tun"},{"content":"\n行程：海南藏族自治州黑马河乡（9:30） -\u0026gt; 茶卡盐湖（11:30 - 15:00）-\u0026gt; 柴达木盆地 -\u0026gt; 可鲁克湖（18:00 - 18:30） -\u0026gt; 大柴旦镇（21:00）\n路程：500+公里\n日期：2019.4.22\n今天是整个行程的第五天，第四天晚上到达了青海湖西端的黑马河乡，今天的主要行程是茶卡盐湖，以及横穿几乎整个柴达木盆地。\n黑马河日出 黑马河地处青海湖最西端，往东边看去是平静的湖水，完全没有任何遮挡，加上高原的空气通透性特别好，况且地处干旱地带，以晴天为主，是个看日出的绝佳地点。\n从黑马河赶往茶卡镇需要经过橡皮山，橡皮山口的海拔在3800米+，是整个行程中的海拔最高点。此时的橡皮山还完全是一座雪山，是整个行程中的第二次到达雪山。同时也是由于这些山脉的原因，才阻隔了青海湖和茶卡盐湖，使其分别成为了只进不出的咸水湖。\n茶卡盐湖 茶卡盐湖几乎是青海旅游的毕竟之地，无论是环青海湖小环线，还是青海甘肃大环线，同时也是网红景点，网络上有太多穿着红色裙子的漂亮的小姐姐的照片和视频。\n茶卡盐湖地处柴达木盆地，周边地势较低，在离盐湖十几公里的地方湖泊就清晰可见，天上的白云倒影在湖中，宛如一面镜子，因此素有“天空之境”之称。远处的昆仑山脉和祁连山脉的雪山清晰可见，由于天空太过透彻，使常年生活在华北地区的我很难判断出雪山的距离。\n(图为采盐船)\n茶卡盐湖的开发利用相当充分，一方面是用来开发采盐，并建有盐厂，湖面上可以看到开采盐矿的船只。另一方面，通过旅游来进一步最大化其价值，其实整个柴达木盆地有很多的盐湖，但最为大众所熟知的仅此一家。而且旅游跟采盐是完全分离的，互不相干，毕竟大部分游客都是来拍照的，旅游占用的盐湖面积比重相对较小。\n茶卡盐湖的含盐量实在太高，大部分地方湖面的水都较浅，仅有薄薄的一层，下面全是沉积的盐矿，我此行并没有看到有数米深的湖水。很多地方是可以穿着靴子进入到里面的，但要小心盐坑，看似平坦的盐矿其实有很多的暗洞，很容易就陷进去，好在游客能去的地方盐洞都给堵住了，没啥安全问题。上图中黄色的网状结构为防止游客陷入的盖子。\n死海的含盐量肯定没有盐湖的含量高的，尚且人体可以漂浮在表面，我想茶卡盐湖如果有较深的地方，人体一定是可以漂浮在表面的，可惜盐湖并未提供该项旅游体验服务，要不还真想一试。\n茶卡盐湖的最佳拍照时机是清晨和傍晚，中午烈日当头的时候晒得人很不舒服，紫外线相当强烈，尤其是对于我这种没有墨镜装备的近视眼游客。在烈日下，去拍人像也很难拍的漂亮，眼睛是很难睁开的，手机在拍人像时，由于背景实在太亮，人像总是会偏黑一些。对于我而言，是很难做到早起的，也就赶不上清晨的茶卡盐湖，实际上到达盐湖景区的时候已经是中午了。\n(盐湖的路都是用盐结晶铺成的，好不奢侈)\n(在湖中的沉淀物捞出来随手一攥都能成为一个大疙瘩)\n柴达木盆地 游览完茶卡盐湖已经是下午三点钟了，接下来还有400多公里的路程要走，好在高原的天黑时间要到晚上八点半。离开茶卡镇后，就进入了漫长的柴达木盆地。\n我原以为柴达木盆地是属于新疆地区的，后来一看地图才发现彻头彻尾的属于青海省，夹在昆仑山、祁连山和阿尔金山三大山脉之间，而今天的行程几乎就是横穿整个柴达木盆地。\n刚离开茶卡镇，还能看到一些牧民养的成群的牛羊，地上也长有一些枯草，但后来地上随着枯草的减少，牛羊也就看不到了。到后面几乎就是戈壁，看不到任何的动物，完全的荒原，让我想起了《北方的空地》中描述的羌塘无人区中的画面，我感觉无人区中也不过如此罢了。地理课本上说柴达木盆地是“聚宝盆”，矿产资源异常丰富，抛开这些埋藏在地底下的矿产不说，但就感官而言，是极其的荒凉。好在一路上，总有雪山陪伴，也不至于太孤单。一路上有很多动物出没的指示牌，好想看到一个活物，但都以失望告终。\n由于地广人稀，这段高速公路的管理也相对简陋，曾看到了一辆当地的三轮车在高速的快车道上逆行，吓得我一脸懵，估计是要按正常的路线行驶要多走出好多路。\n可鲁克湖 可鲁克湖位于高速公路旁的大约两公里处，甚至都不用下高速就可以到达，属于3A级景区。到达可鲁克湖时已经是下午6点钟，对于进入景区本不报太大的希望，经询问景区居然营业到晚上八点钟，高原地区果然天黑的晚。景区主要是一个偌大的可鲁克湖，可供游玩的地方特别少，简单拍照走人。如果在这里拍张照，拿给别人看，说是青海湖拍的，没有人能够看得出破绽。如果时间紧迫，该景区完全可以不去参观。\n可鲁克湖景区旁边还有个比其更大的托素湖，托素湖是可鲁克湖的三倍大小，可鲁克湖的湖水最终会流入托素湖中。比较有意思的是，可鲁克湖的湖水属于淡水湖，而托素湖却属于典型的内陆咸水湖。咸水湖的形成离不开封闭的环境，托素湖由于没有排水渠道，只能靠蒸发来消耗水分，水分被蒸发后，水分中的盐分却不会被蒸发，日积月累就会形成咸水湖。而可鲁克湖由于会流向托素湖，湖水是流动的，因此没有形成咸水湖。\n托素湖离高速较远，要开车过去需要沿着可鲁克湖的湖边，来回至少还得要一个半小时的时间，时间不太允许，且可玩性较差，就直接忽略了该景区。\n托素湖旁边还有个外星人遗址，单看景区的名字是我特别感兴趣的类型，网上一搜，景区居然暂未开放!\n当天终点 - 大柴旦 又是在晚上的时候到达了当天的终点站大柴旦县。\n1 2 3 4 5 6 7 8 天空之镜湖 白雪皑皑山 漫漫柴达木 渺渺无人烟 盆地全是宝 可惜只见草 戈壁连成片 夜宿大柴旦 未完待续\u0026hellip;\n","date":"2019-05-29T02:22:36Z","permalink":"/post/%E5%8C%97%E4%BA%AC%E8%87%AA%E9%A9%BE%E9%9D%92%E6%B5%B7%E7%94%98%E8%82%83%E5%A4%A7%E7%8E%AF%E7%BA%BF%E7%B3%BB%E5%88%97-5/","title":"北京自驾青海甘肃大环线系列 - 5"},{"content":"第四天 行程：西宁市 -\u0026gt; 海南藏族自治州黑马河镇\n路程：250公里\n日期：2019.4.21\n经过了前三天的长途跋涉，今天开始了正式的旅行。早上在西宁市的华润万家采购了一些食物，因为还不太清楚接下来的大环线旅行中沿途的住宿饮食条件是什么样子的，也不太清楚很多情况下是否有足够的时间来饱餐一顿，毕竟每天都要赶很多的路。\n第一站 塔尔寺 塔尔寺通常都是大家青海旅游的第一站，因为距离西宁市区比较近，大概有几十公里的路程，开车一个小时即可到达。\n塔尔寺属于典型的藏传佛教圣地，藏传佛教是受印度佛教、青藏高原本土的苯波教和汉传佛教影响的共同产物，因此有很多区别于其他佛教分支的特点。唐朝时期的文成公主是信奉佛教的，远嫁松赞干布后，为汉族的佛教向青藏地区的渗透起到了非常大的作用。\n塔尔寺在藏传佛教中的地位应该跟五台山差不多，但同时也吸收了一些中原道教的文化，比如寺内能看到有西王母的塑像。寺内建筑宏伟，院落也较多，比起五台山而言，除了整体面积小点，其它方面并不逊色。\n未能领悟佛教的我，草草的参观塔尔寺后，明显发现了藏传佛教的很多特色，也带给了我不少的震撼。\n好几个大殿的院内或者周围都有虔诚的佛教徒在做周而复始的朝拜，口中还振振有词，木质结构的地板早已磨的光滑无比。那种虔诚、执着非常值得敬佩，虽然以我现在的阅历还很难理解这种宗教行为。\n第二站 日月山 从塔尔寺出来后，已经是下午两点了，接下来就要往青海湖进发了。日月山是此行的必经之路，属于祁连山脉的支脉，也是此行中看到的第一个雪山，沿途海拔在3000米以上。\n进入日月山之前，阳光普照，高原的阳光相当强烈，隔着车玻璃都能感觉到。一开进日月山，雪山清晰可见，突然下起了冰雹，紧接着又下起了雨夹雪，原本没有积雪覆盖的山上也披上了一层薄薄的银装。这一切仿佛在预示着什么，仿佛在迎接着远道而来的客人，又仿佛是在告诫我不要打扰到了雪山上的神灵。\n因为日月山的风景太美，全然没有了赶路的想法，就想驻足体验一下日月山的神奇。主干道是京藏高速，有条公路可以通往日月山风景区，索性直接开往日月山风景区。到了景区门口，雨夹雪渐小，一下车发现空气凉了好多，原本仅穿了单件衣服，感觉立马要穿羽绒服的节奏，穿了件外套仍冻得直打哆嗦。高原的天气就是这般神奇，有阳光和没有阳光完全就是两个世界。\n日月山风景区并没有必要进入参观，因为景观在景区外一览无余。\n景区旁边能看到很多高原鼠兔，发出吱吱的响声，这玩意特别爱打洞，鼠兔的旁边可以看到几个洞。\n之后的行程中在公路边上看到了一只死去的马匹，马的头颅已不知所踪。我想放到东部地区，这种现象是肯定不会出现的，死去的马儿肯定还有它的利用价值。\n第三站 青海湖 从日月山出来沿着G109国道继续前行，中间经过了倒淌河风景区，倒淌河水源自日月山，自东向西注入青海湖，故名倒淌河。倒淌河的水量不大，加上本来也很难分清东南西北，就很难体会到倒淌河的精妙。看评价景点比较坑，并没有进入参观。\n过不了多久，青海湖就会出现在了眼前，而且接下来的两个多小时会一直沿着青海湖南沿的公路前进，足足有一百公里的距离，不愧为中国最大的湖泊。青海湖有个游客去的较多的二郎剑景区，并没有进入，据说性价比依旧很低，无非就是看湖，青海湖的美从哪个角度都能感受到。\nG109国道跟青海湖稍微有一段距离，最近处差不多有一里路的样子，中间都是草原，被牧民给围成了一块一块，山羊绵羊在悠闲的吃着草。找了一处牧民的区域，开车过去，一人10元的门票，即可到青海湖边拍照。\n身为游客一定觉得湖边的牧民是真正的逍遥快活，每天在自己的牧场上骑马放牧，住在青海湖畔，远处有雪山作伴，呼吸着几乎没有污染的空气。可是一旦天天都是这样的生活，又有几个游客可以放下尘世间的所有诱惑来到这近乎纯净的地方呢？牧民们也有自己的烦恼，牧民的孩子上学怎么办，牧民们生病了怎么办，各种生活的不便利。虽呼吸着最新鲜的空气，却也承受着杀伤力很强的紫外线。冬天的高原夜晚想想都会瑟瑟发抖，但牧民们也要忍受。\n我天真的以为，青海湖最初是真正的大海，后来由于地壳运动，地壳上升，而青海湖变成了一个内陆湖。哈哈，不过这个来源忽悠人还是蛮不错的。后来经查阅资料，青海湖的最初是一个内陆淡水湖，并且注入黄河。后来由于地壳运动，青海湖流入黄河的入口被切断，从而青海湖没有了任何出水口。再加上天气较为干旱，湖水的蒸发量大于湖水的注入量，导致湖水的盐分浓度逐渐增加，从而变成了一个咸水湖。青海境内的咸水湖成因都是因为蒸发量大于注入量，包括比较出名的茶卡盐湖。青海湖的湖水浓度并没有海水的浓度高，湖水帮大家尝过了，确实挺咸的。\n青海湖的湖水非常清澈，虽湖水很宽，但空气透彻，一眼可以望到河对面远处的雪山。远处的云朵看起来非常低，跟湖水连在一起，微风吹过，湖水上一层层的波澜。\n湖边的小水坑里已经有了青蛙，晚上的青海湖畔温度还是非常低的，不知道这些青蛙夜晚是怎么度过的。当然这些小水坑的水是纯淡水，亲尝无误。\n终点站 黑马河镇 黑马河位于青海湖的西端，因为是去往茶卡盐湖的必经之地，且可以看到青海湖，因此顺其自然靠着旅游而生存。早晨起来看日出的是个不错的地方，可以想象一下朝霞倒映在湖水中的场景，很多游客都会选择在黑马河观日出。\n到达黑马河已经是晚上八点多了，温度较低，索性换上羽绒服，一点也不觉得热。住宿条件比较一般，但价格却不便宜，是整个行程住的性价比最低的宾馆了。找了一家餐馆就餐，全都是组团的游客，非常不错的食材却做的难吃的不行，上等的牦牛肉放到餐桌上后却嚼也嚼不烂，看着放弃的牦牛肉甚是可惜。这大概就是国内很多因旅游而起的餐馆的真实写照，反正能坑一个是一个，不要回头客，回回都是新客人。做的烂对自己的生意影响不大，但游客在心中对于当地的评价确实难以磨灭的。\n未完待续\u0026hellip;\n","date":"2019-05-18T08:56:57Z","permalink":"/post/%E5%8C%97%E4%BA%AC%E8%87%AA%E9%A9%BE%E9%9D%92%E6%B5%B7%E7%94%98%E8%82%83%E5%A4%A7%E7%8E%AF%E7%BA%BF%E7%B3%BB%E5%88%97-4/","title":"北京自驾青海甘肃大环线系列 - 4"},{"content":"第三天 行程：宁夏回族自治区中卫市中宁县 -\u0026gt; 青海省西宁市\n路程：500公里\n日期：2019.4.20\n昨晚到酒店比较晚，早上拉开窗帘，一所学校的操场映入眼帘，令我眼前一亮。离开校园已经太久了，看到同学们身穿运动装在操场上洋溢着青春即亲切又陌生。\n第一站 黄河畔 从地图上看中宁县城离黄河比较近，自然要去黄河边上走一走。但与保德县和府谷县不同的是，中宁县并不傍黄河而建，主城区而是离黄河有一公里的距离。我猜测保德县和府谷县边的黄河在地势低洼处，即使河水泛滥也不会殃及县城。但中宁县地势却是非常平坦，不能依靠地势优势阻止泛滥的河水，因此县城的选址离黄河稍有距离。\n远处的黄河边上有一个多层的塔，是中宁县枸杞博物馆，中宁县有“天下枸杞出宁夏,中宁枸杞甲天下”的说法，因此建个枸杞博物馆还是比较合适的。由于到枸杞博物馆的路并不好走，而且不能判断此刻博物馆是对外开放，并未前往参观。\n黄河水比其下游的府谷县的黄河水要变黄了很多，这也是我一直比较奇怪的地方。河道非常宽，目测有至少1公里的样子，现在是枯水期，河水并不太多。河床上有各式各样的鹅卵石，不知经过了多少次的河水冲刷才形成了现在的形状，我们个体的生命相对鹅卵石的生命实在是太短，挑选了几个像模像样的留作纪念。\n在路上 宁夏境内，一路上偶尔会看到种植了一些铺着地膜的农作物，我猜测是土豆的可能性比较大，原因是山东的种植土豆方法就是类似思路。但看下田地中的土质，大部分都是小石块，哪有多少土壤成分呀，倒是拿着石块压着地膜来防止大风非常的方便。\n在甘肃和青海接壤地方，道路两侧多了很多树木，，山上的草有很多都绿了起来，生机勃勃的。两天之内满眼都是枯黄色的画面，突然出现鲜绿色还有点不适应。\n查看青海省的地图会发现，有好几个地级市都是围绕着青海湖命名的，海东市、海南藏族自治州、海西蒙古族藏族自治州、海北藏族自治州。提到海南，不熟悉的人还以为是海南岛呢。\n地图中还有一个比较有意思的地方，海西蒙古族藏族自治州居然有两块区域，中间被玉树给分割开了，完全不接壤。让我立马想到的是在中国版图中另外比较有意思的是河北省的三河市，三河市被北京市和天津市包围，像是河北省的一个孤岛。\n青海省有藏族和回族比较容易理解，居然还有蒙古族？要不也就不会叫海西蒙古族藏族自治州了。实际上内蒙古的最西端跟青海省仅隔了甘肃省一段狭长的河西走廊，直线距离上还是特别近的。蒙古族迁徙到青海要从元朝说起，整个中国的版图都是元朝的天下，自然青海省有就了蒙古族，后续又陆陆续续的有蒙古族迁徙都青海省。但青海省的蒙古族还吸收了部分当地藏族的习俗，很难说是蒙古族入侵藏族，还是藏族同化了蒙古族。\n青海境内的青藏高速施工，走了很长一段的非高速，跟青海的村庄和县城有了第一次近距离接触的机会。青海省的海东市相对于甘肃而言，植被还是比较茂盛的，树木挺多的，沿路很多梨树，开着洁白的梨花。经过村庄的房屋看起来也不差，很多都是二层的小楼，而且村庄还是比较密集的，大概青海人口都聚集在了东部地区。总体给我外观感受，海东地区要比甘肃东部地区富裕很多。\n第二站 西宁市 西宁市属于中国的西部地区，如果放眼到中国的地图中，相同纬度上，西宁算是比较靠中间的城市了，如果不是西部人烟稀少，更像是个中部城市。\n晚上八点钟到达西宁市，事先知道青海省的人口只有区区600万，省会应该不咋地，结果还挺另我刮目相看的，城区面积感官上还是挺大的，颇具省会城市的规模，各种品牌应有尽有，夜景也还不错。\n晚上吃了一碗当地特色羊肠面，面相不咋地，吃起来味道还不错。\n收个尾 1 2 3 4 5 6 人人都说宁夏美 砂石大漠看到尾 甘东地区很荒凉 似乎水源很紧张 夏都街头走一走 车水马龙大高楼 未完待续\u0026hellip;\n","date":"2019-05-12T01:31:00Z","permalink":"/post/%E5%8C%97%E4%BA%AC%E8%87%AA%E9%A9%BE%E9%9D%92%E6%B5%B7%E7%94%98%E8%82%83%E5%A4%A7%E7%8E%AF%E7%BA%BF%E7%B3%BB%E5%88%97-3/","title":"北京自驾青海甘肃大环线系列 - 3"},{"content":"第二天 行程：山西省保德县 -\u0026gt; 陕西省榆林市府谷县 -\u0026gt; 宁夏回族自治区中宁县\n路程：600公里\n第一站 - 府谷县 第一天留宿的保德县临近黄河，早上醒来后本想去黄河边看下景色，其实最关心的是中段黄河的河水是什么颜色的，可惜黄河边是一条公路沿着黄河蜿蜒，而且局部在修，因为黄河的地势较低，视线受阻严重，连黄河都没看到。\n索性直接导航到了黄河对岸的府谷县，山西省保德县跟陕西省榆林市府谷县隔黄河相望，仅一河之遥，可以在跨河大桥上非常方便的在两个省之间通行。跨河大桥并不是特别宽，两车会车是没任何问题的，桥上还有一些行人在通行。上图为连接两省的大桥，在府谷县可以看到“秦晋之好“几个大字。\n保德县虽紧挨着黄河，却没有依靠上黄河的任何优势，反而黄河成为了一大劣势。相反，府谷县却非常明智的，楼房离着黄河稍有一段距离，选择在黄河边修建了滨河公园供休闲娱乐。\n在府谷县的滨河公园可以清晰看到和对面的山西省保德县城，建筑物的朝向大多都是面向黄河，或者面向对面的府谷县城的。\n由于现在是枯水期，人行道离河水还有一段距离，但仍可以看出黄河水还是特别清澈的，跟黄河的淤泥还是有比较鲜明的区别。跟更下游那夹杂的泥沙的黄河水一对比，让人完全联想不到是一条河。后面的行程中在更上游看到了几次黄河，但河水都要比此段的黄河水要黄，至少兰州的黄河水夹杂的泥沙量已经非常大，也就是说此段黄河水是我见到最清澈的。不知道是否我的判断有误，从理论上讲应该越下游黄河水越黄才对的。\n在路上 下午一点从府谷县出发，直奔西宁方向而去，最终到达了宁夏回族自治州中卫市中宁县城，最初地貌还是黄土高坡，大概从榆林市之后逐渐沙漠化，地上草还没长出来，在高速上很难判断远处是草原，还是完全的荒漠。\n上图为在高速上的简陋厕所，与其说是黄土高坡地貌，我感觉更像是荒漠。\n本以为到达宁夏后，地貌会变成接近于草原，因为提到宁夏我最先想到的是黄河冲积而成的河套平原。一路上一直在期盼，但一直也没遇到梦中的草肥水美的河套平原。事后一查资料才发现，即使河套平原最南的“西套”地区也在银川市周边，而我走的高速在银川市的南边大约100多公里的距离，可以说是跟河套平原擦肩而过。从卫星地图上可以清晰的看出河套地区的植被还是比较茂密的。上图中绿色的线为我经过的路线。\n终点站 - 中宁县 最终到达了宁夏回族自治州中卫市中宁县城，到达中宁县后，恰巧赶上了一次沙尘暴，停车吃饭的时候，车上已经积了厚厚的一层土。本以为沙尘暴在当地是家常便饭，问下了当地人，说是沙尘暴也不常有，沙尘是从北部的腾格里沙漠吹过来的。从上图中可以看出，中宁县离得腾格里沙漠还是特别近的，刮个北风，沙尘天气还是非常容易出现的。\n比较巧合的是，中宁县也恰巧在黄河边上，一天之内，驱车600公里，从黄河“几”字形的右边到达了左边位置。\n却道是\n1 2 3 4 5 6 7 8 9 10 11 12 日行千余里 胜似的卢骋 朝食保德县 夕塌宁安镇 黄土变沙漠 高山转草原 早在黄河畔 夕亦黄河边 自东向西行 作何现此况 若君心有疑 一览地图觅 未完待续\u0026hellip;\n","date":"2019-05-04T23:31:20Z","permalink":"/post/%E5%8C%97%E4%BA%AC%E8%87%AA%E9%A9%BE%E9%9D%92%E6%B5%B7%E7%94%98%E8%82%83%E5%A4%A7%E7%8E%AF%E7%BA%BF%E7%B3%BB%E5%88%97-2/","title":"北京自驾青海甘肃大环线系列 - 2"},{"content":"近期有一段较为充裕的假期，出去旅游对我而言是再合适不过的选择，带着对远方田野的那份向往。特别有意向的方向有两个：南下和西进。\n南方的很多地方都未曾去过， 比如张家界、桂林等，可以从北京出发一路到海南三亚地区，而且当下正巧是烟花三月下江南的好季节。\n对于西部地区向往已久，青海、西藏和新疆地区都未曾去过，当下四月份不是西部地区的最佳旅游季节，山基本上还都是枯黄色，草儿还在沉睡中。\n但心想南方交通较为便利，后续会去的可能性比较大，而且景区之间跨度较大，可以分开旅游。而西部往往需要拿出一大把的时间来玩耍才能尽兴，很多地方的交通不是特别方便，往往都是需要多个景点一起才最划算。\n因此，最终的决定是北京自驾甘肃-青海大环线。\n此刻的我已经结束了整个行程，行程的时间段为2019.4.18 - 2019.4.30，共计13天时间。起点为北京市，终点为山东省，整个的行程达6000公里。\n事先计划的时间为12天，留了一天的buffer，最终用时为13天，没有太大的偏差。由于一直是我一人在开车，相对而言还是比较累的，基本上一天得开500公里，尤其是前面和后面赶路的时间，基本上一天在600公里以上。大部分时间是上午在10点之后出发，下午开车的时间比较多，很多时候需要晚上开一段时间才能满足一天的行程目标。如果时间比较充裕，把整个行程放慢为20天，感觉会比较轻松，会对旅行有更深的体会，只是我时间不太允许。\n一路上经过北京、河北、山西、陕西、宁夏、甘肃、青海、河南、山东这些省，地貌包括平原、山脉、黄土高原、戈壁、沙漠、黄河、湖泊、雪山、盆地等地貌。\n这一路上遇到的很多地方，之前在历史和地理课本上曾经了解过，很多地方有比较深厚的历史和文化底蕴，自己的认识却是很浅薄，而旅行可以加深对一个地方的地理和历史的认识，正所谓：“纸上得来终觉浅，绝知此事要躬行”。\n本系列文章并不打算写成单纯游记的形式，而是更多会将我个人的体会和相关的历史地理知识融合在其中，在这其中查阅相关的资料对我而言也是一种成长。由于才疏学浅，所查资料有限，很多观点不见得是正确的，欢迎指正。\n第一天 行程：北京市 -\u0026gt; 山西省忻州市保德县\n上午11点从北京出发，驱车700公里，晚上八点半到达山西省忻州市保德县城。\n先后经过了华北平原、太行山脉、黄土高原，北京和河北境内大部分属于华北平原，河北和山西省的交界处为太行山脉，期间穿过了多个隧道，基本上是整个的行程中走过的最长的一段隧道了，后续除了祁连山脉、六盘山外隧道就比较少了。\n到了山西，以黄土高坡地貌为主，黄土高原也是整个行程中看到最多的地貌了。黄土高原东起太行山脉，西至青海省日月山，南抵秦岭，北达长城，包括了山西、陕西、宁夏、甘肃、青海、河南、内蒙古7个省。比我印象中的要大得多，我印象中主要是在山西境内和陕西的北部地区，一下子刷新了我的认知。\n大家都知道，黄土高原是水土流失导致现在的千沟万壑的地貌，也是因此而导致黄河水特别的黄，那么是什么时间段发生的水土流失呢？历史上的黄土高原又是什么样的呢？\n黄土高原最初是被植被茂密的森林所覆盖，而且土壤在有充足水源的情况下还是特别肥沃的，并非大家印象中的植被少的可怜，要不然也不可能孕育出华夏文明。相反，正是由于黄土土质疏松易开垦的特点，才容易诞生人类文明，因为最早的人类文明生产工具是特别简陋的，对大自然的改造能力特别有限。黄土高原被破坏的历史时期基本上伴随着华夏文明的发展，到明清时期达到顶峰，建国之后开始逐渐恢复。\n很难想象，偌大的黄土高原，竟然因为农耕时代的人类影响变成了如今这幅模样。假如再给现代的人类一次利用自然的机会，我想当下的人类一定会善加利用这片广袤的土地，黄河也许会改成另外一个名字。也就是说，人类不同的文明程度决定了对自然的破坏程度。\n但今天的我们殊不知也在对自然进行着更深层次的破坏，如果放到未来来审视我们人类今天的作为，我们今天对这个星球的破坏要大大高于农耕时代对地球的破坏。比如，人类在贪婪的消耗着地球上的石油、煤炭资源，也许在不久的未来，人类会发现在这些燃料资源中还蕴藏着无穷尽的价值，但是这些资源已经被当下愚蠢的人类当做燃料给烧掉了。\n一路上雾霾还是比较严重的，尤其是河北保定境内。保定的空气质量是早有心理预期，印象中山西的整体空气还是非常不错的。去年去过一次五台山，穿过几个长长的隧道进入山西境内后，空气会一下子好转，山西省忻州市的空气跟河北省保定的空气质量有着强烈的反差。但这次结果却比较失望，在整个山西境内并未出现晴空万里，而且看上去更像是雾霾，而不是阴天。\n山西省境内有那么几段高速，连一辆车都看不到，让我一度怀疑已经抵达大西北的感觉。\n晚上到达保德县，县城傍黄河而建，位于河道东侧。离黄河稍远的地方是黄土高坡，大部分楼房都离建在离黄河较劲的地势平坦处，而黄土高坡上由于地势的原因房屋逐渐变少。一进入县城，道路上到处都是开着远光灯的，仿佛对面一颗颗行进中的太阳，闪的我实在有些难受。有点匪夷所思，明明有路灯，实在想不明白远光灯的用途。真期望每个人作为社会中的一份子，在做事情的时候要多为他人考虑一分，而不是仅顾及自己的感受。\n保德县城到处打着扫黑除恶的标语，吓得我开车都格外小心。甚至连小区门口都打着口号，让市民天天回家都可以看到。我深知越是提倡什么，说明越是缺少什么。后来走过好多地方后，发现到处都打着该口号，并不是当地特色。\n却道是：\n1 2 3 4 5 6 驱车青甘大环线 夜晚留宿保德县 县城建在黄河畔 黄土高坡把城建 黄河对岸府谷县 一桥之隔秦晋缘 未完待续\u0026hellip;\n","date":"2019-05-04T13:30:16Z","permalink":"/post/%E5%8C%97%E4%BA%AC%E8%87%AA%E9%A9%BE%E9%9D%92%E6%B5%B7%E7%94%98%E8%82%83%E5%A4%A7%E7%8E%AF%E7%BA%BF%E7%B3%BB%E5%88%97-1/","title":"北京自驾青海甘肃大环线系列 - 1"},{"content":"最长公共子串及公共子序列问题属于一类问题，都可以使用动态规划的算法来解析，且动态规划方式比较类似，比较容易混淆。\n定义 最长公共子串：两个字符串中，相同的最长子串，字符必须是连续的\n最长公共子序列：两个字符串中，相同的最长序列，字符不一定是连续的\n比如：a[] = \u0026ldquo;abcde\u0026rdquo; b[] = \u0026ldquo;bce\u0026rdquo;\n那么： 最长子串：\u0026ldquo;bc\u0026rdquo; 最长子序列：\u0026ldquo;bce\u0026rdquo;\n解法 假设A、B分别表示两个字符串\n最长公共子串 dp[i][j]表示子串A[:i]、B[:j]必须以A[i]、B[j]为结尾的两个字符串的最大子串长度\n1 2 3 4 5 if A[i] == B[j] { dp[i][j] = dp[i-1][j-1] } else { dp[i][j] = 0 } 最终dp二维数组中的最大值即为结果\n最长公共子序列 dp[i][j]表示子串A[:i]、B[:j]的两个字符串的最大子序列\n1 2 3 4 5 if A[i] == B[j] { dp[i][j] = dp[i-1][j-1] } else { dp[i][j] = max(dp[i-1][j], dp[i][j-1]) } 最终dp[i-1][j-1]为结果\nleetcode Delete Operation for Two Strings （最长公共子序列） Maximum Length of Repeated Subarray （最长公共子串） ","date":"2019-04-05T22:18:05Z","permalink":"/post/%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97%E5%92%8C%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E4%B8%B2%E9%97%AE%E9%A2%98/","title":"最长公共子序列和最长公共子串问题"},{"content":"今天发现vagrant的其中一个虚拟机磁盘空间不够了，需要对其进行磁盘扩容，但不期望是通过增加新硬盘的方式，而是直接增加原磁盘容量的方式来无缝扩容。\n修改磁盘文件 进入到vm磁盘文件所在的目录~/VirtualBox VMs/dev_default_1531796361866_92956下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 % vboxmanage showhdinfo centos-vm-disk1.vmdk UUID: acbb4ffc-0580-40d6-8627-3ed24cd0beff Parent UUID: base State: created Type: normal (base) Location: /Users/lvkai/VirtualBox VMs/dev_default_1531796361866_92956/centos-vm-disk1.vmdk Storage format: VMDK Format variant: dynamic default Capacity: 10000 MBytes Size on disk: 9634 MBytes Encryption: disabled In use by VMs: dev_default_1531796361866_92956 (UUID: a153957c-e43f-4dd2-8512-f51d42dee3d3) # 将之前存储的vmdk格式的文件复制一份vdi格式的文件，由于需要复制文件，该命令需要执行一段时间 % vboxmanage clonehd centos-vm-disk1.vmdk new-centos-vm-disk1.vdi --format vdi # 将vdi格式的文件修改磁盘空间上限大小为80g，但实际占用磁盘空间仍然为之前的大小 % vboxmanage modifyhd new-centos-vm-disk1.vdi --resize 81920 # 将vdi格式的文件重新转换为vmdk格式，会产生一个新的uuid % vboxmanage clonehd new-centos-vm-disk1.vdi resized.vmdk --format vmdk 0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100% Clone medium created in format \u0026#39;vmdk\u0026#39;. UUID: 7e454b50-0681-494b-b9ca-81700d217c0a 新的硬盘创建完成后，在virtualbox的界面上将对应虚拟机的硬盘更换为resized.vmdk，并将之前旧的centos-vm-disk1.vmdk给删除掉。\n使用fdisk创建新的磁盘分区 以上命令执行完成后，开启虚拟机，进入系统，可以看到磁盘空间大小变更为85.9GB，但挂载的磁盘空间大小仍然为8.3G，新增加的磁盘空间仍然处于未分配状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # fdisk -l Disk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x0000ca5e Device Boot Start End Blocks Id System /dev/sda1 * 2048 1026047 512000 83 Linux /dev/sda2 1026048 20479999 9726976 8e Linux LVM Disk /dev/mapper/centos-root: 8866 MB, 8866758656 bytes, 17317888 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/centos-swap: 1048 MB, 1048576000 bytes, 2048000 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes # df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 8.3G 7.9G 386M 96% / devtmpfs 296M 0 296M 0% /dev tmpfs 307M 0 307M 0% /dev/shm tmpfs 307M 4.5M 303M 2% /run tmpfs 307M 0 307M 0% /sys/fs/cgroup /dev/sda1 497M 195M 303M 40% /boot vagrant 466G 390G 77G 84% /vagrant vagrant_data 466G 390G 77G 84% /vagrant_data tmpfs 62M 0 62M 0% /run/user/1000 接下来需要将未分配的磁盘空间\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # fdisk /dev/sda # 依次输入可创建新的分区 n p 回车 回车 # 继续输入p，可以看到磁盘的情况，多出了/dev/sda3 # /dev/sda3的System为Linux，而/dev/sda2的System为Linux LVM Command (m for help): p Disk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x0000ca5e Device Boot Start End Blocks Id System /dev/sda1 * 2048 1026047 512000 83 Linux /dev/sda2 1026048 20479999 9726976 8e Linux LVM /dev/sda3 20480000 167772159 73646080 83 Linux # 依次输入将/dev/sda3更改为LVM格式 t 3 8e p Disk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk label type: dos Disk identifier: 0x0000ca5e Device Boot Start End Blocks Id System /dev/sda1 * 2048 1026047 512000 83 Linux /dev/sda2 1026048 20479999 9726976 8e Linux LVM /dev/sda3 20480000 167772159 73646080 8e Linux LVM # 输入w后进行保存操作 Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: Re-reading the partition table failed with error 16: Device or resource busy. The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8) Syncing disks. 将新创建的磁盘分区添加到LVM分区中 将机器重启后，继续执行如下命令\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 # vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size 9.27 GiB PE Size 4.00 MiB Total PE 2374 Alloc PE / Size 2364 / 9.23 GiB Free PE / Size 10 / 40.00 MiB VG UUID cpEmYK-XFew-6ZWT-GEeY-yEou-0vLq-OJiD08 # lvscan ACTIVE \u0026#39;/dev/centos/swap\u0026#39; [1000.00 MiB] inherit ACTIVE \u0026#39;/dev/centos/root\u0026#39; [\u0026lt;8.26 GiB] inherit # pvcreate /dev/sda3 Physical volume \u0026#34;/dev/sda3\u0026#34; successfully created. # vgextend centos /dev/sda3 Volume group \u0026#34;centos\u0026#34; successfully extended # lvextend /dev/centos/root /dev/sda3 Size of logical volume centos/root changed from \u0026lt;8.26 GiB (2114 extents) to \u0026lt;78.49 GiB (20093 extents). Logical volume centos/root successfully resized. # xfs_growfs /dev/centos/root meta-data=/dev/mapper/centos-root isize=256 agcount=4, agsize=541184 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0 spinodes=0 data = bsize=4096 blocks=2164736, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=0 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 2164736 to 20575232 # 最后执行命令可以看到磁盘空间已经增加 # df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/centos-root 79G 7.9G 71G 11% / devtmpfs 296M 0 296M 0% /dev tmpfs 307M 0 307M 0% /dev/shm tmpfs 307M 4.5M 303M 2% /run tmpfs 307M 0 307M 0% /sys/fs/cgroup /dev/sda1 497M 195M 303M 40% /boot vagrant 466G 390G 77G 84% /vagrant vagrant_data 466G 390G 77G 84% /vagrant_data tmpfs 62M 0 62M 0% /run/user/1000 ","date":"2019-04-04T20:15:44Z","permalink":"/post/virtualbox%E7%A3%81%E7%9B%98%E6%89%A9%E5%AE%B9/","title":"VirtualBox磁盘扩容"},{"content":"常用垃圾回收方法 1.引用计数 类似于C++中的智能指针，每个对象维护一个引用计数，当对象被销毁时引用计数减一，当引用计数为0时立即回收对象。\n在PHP、Python中使用，适用于内存比较紧张和实时性比较高的系统。\n缺点：\n由于频繁更新引用计数，降低了性能。 循环引用问题。解决办法为避免循环引用。 2.标记(mark)-清除(sweep) 分为标记和清除两个阶段，算法在70年代就提出了，非常古老的算法。\n该算法有一个标记初始的root区域，和一个受控堆区。root区域主要是程序当前的栈和全局数据区域。\n从root区域开始遍历所有被引用的对象，所有被访问到的对象被标记为“被引用”，标记完成后对未被引用的对象进行内存回收。\n缺点：\n标记阶段，都会Stop The World，会大大降低性能。\n3.分代回收 将堆划分为两个或者多个代空间，新创建的对象存放于新生代，随着垃圾回收的重复执行，生命周期较长的对象会被放到老年代中。新生代和老年代采用不同的垃圾回收策略。\nGo垃圾回收机制 Go中采用三色标记算法，本质上还是标记清除算法，但是对标记阶段有改进。\n步骤如下：\n开始时，所有的对象均为白色 从root开始扫描所有可达对象，标记为灰色，放入待处理队列。root包括了全局指针和goroutine栈上的指针。 从队列取出所有灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。 重复3，直到灰色队列为空。 将白色对象进行回收 优点：用户程序和标记操作可以并行执行。\n详细过程如下图：\n通过上图可以看出，STW有两个过程：\ngc开始的时候，需要一些准备工作，如开启write barrier re-scan的过程 每个对象需要一个标记位，go中并未将标记位存放于对象的内存区域中，而是采用非侵入式的标记位。go单独使用了标记位图区域来对应内存中的堆区域。\nwrite barrier: 在运行垃圾回收算法的同时，应用程序在一直执行，上文中提到了写屏障write barrier用于将这些内存的操作记录下来。\nGC日志 GC日志对于定位问题还是比较方便的\n1.开启GC日志 可以增加环境变量GODEBUG=gctrace=1来开启gc日志，gc日志会打印到标准错误中。例如有如下的程序:\n1 2 3 4 5 6 7 8 9 10 11 12 13 package main import \u0026#34;time\u0026#34; func main() { for { s := make([]int, 10) for i := 0; i \u0026lt; 10000; i++ { s = append(s, i) } time.Sleep(time.Nanosecond) } } 执行GODEBUG=gctrace=1 go run gc.go即可打印gc日志到标准错误中。\n2.GC日志的含义 可以参照runtime\n1 2 3 4 5 6 7 8 9 gc # @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #-\u0026gt;#-\u0026gt;# MB, # MB goal, # P where the fields are as follows: gc # the GC number, incremented at each GC @#s time in seconds since program start #% percentage of time spent in GC since program start #+...+# wall-clock/CPU times for the phases of the GC #-\u0026gt;#-\u0026gt;# MB heap size at GC start, at GC end, and live heap # MB goal goal heap size # P number of processors used 例子:\ngc 11557 @179.565s 2%: 0.018+1.2+0.049 ms clock, 0.14+1.1/2.3/0.90+0.39 ms cpu, 13-\u0026gt;14-\u0026gt;7 MB, 14 MB goal, 8 P\n3.GC的监控 对于线上GC的监控，基本上读取runtime.MemStats结构中的内容，然后存储到时序数据库中。具体有如下两种获取方式：\n1 2 3 4 5 6 // 方式1 memStats := \u0026amp;runtime.MemStats{} runtime.ReadMemStats(memStats) // 方式2 json格式 expvar.Get(\u0026#34;memstats\u0026#34;).String() references Getting to Go: The Journey of Go\u0026rsquo;s Garbage Collector Go: runtime ","date":"2019-04-01T00:56:25Z","permalink":"/post/golang-gc/","title":"Golang GC"},{"content":"\n古语有云，一年之计在于春，这句话对于很多植物而言再合适不过。在经历了寒冬之后，很多植物选择将最美好的一面在春天里绽放。\n资源 1.PagerDuty\nPagerDuty是一家Sass平台厂商，其产品为一款告警处理平台，提供了On-Call管理、告警收敛分组、告警时间报表，并集成了多种告警方式。\n2.Fathom\n一款开源的简易网站数据分析工具，类似于Google Analytics或者百度分析。\n3.Data Structure Visualizations\n该网站将常见的计算机数据结构以可视化的形式展示在了界面上，可以在界面上点击按钮完成插入元素、删除元素等操作，对应的数据结构展现会实时发生变化，非常直观。\n4.snowflake\nTwitter开源的分布式算法，常用于分布式id的生成，使用毫秒时间戳、机器id、毫秒内的流水号来生成随机id。采用此种方法生成的id可以保证单机递增，但不能保证是全局递增的。\n5.Netlify\n很多人喜欢将自己的博客系统利用Github的Pages功能托管到Github上，但Github Pages并不支持对静态页面的构建，只能将构建完成后的页面推动到Github上。Netlify支持对静态网站的持续集成和持续部署，代码可以托管于Github上，Netlify会自动构建和发布，支持免费的https协议和CDN。\n但可惜的是，实际测试下来，网站的访问速度在国内不是很理想。\n6.996.ICU\n全世界最大的同性交友社区Github上异常火爆的声讨996工作制度的项目，两天的时间内已经突破6万多Star，issue的数量也已经破万（截止到2019-03-28 19:55），要知道Github上Star数最多的项目freeCodeCamp也才接近30万Star，全世界运行设备最多的操作系统linux也才7万多Star，这简直创造了Github上Star数增长的奇迹。\n很多人在issue中提到了加群交友吐槽、倾诉加班不满，甚至还有过来找男盆友的，活脱脱把issue玩成了贴吧，留言中清一色的汉字，说明基本是中国人在玩。\n我个人对于强制996加班的事情不是很赞成，虽然过去三年中我的工作强度应该大于996，但更多的是出于个人自愿和对健康的无视，公司层面并没有强制要求。人生确实有非常多美好的事情可以参与和享受，对于程序员这个群体而言，电脑屏幕之外的世界还很大，还有太多的领域值得去探索和挖掘。但如果确实是因为个人的爱好，在工作中能够获得很好的成就感和满足感，996或者更高强度的加班，我个人觉得都是值得的。\n说起ICU，程序员这个群的职业病是颈椎、腰椎、视力等，失眠多梦也是大有人在。我个人也确实身体出过一些问题而住过院，人往往都是在身体好的时候不懂得去重视自己的身体，当身体一旦出毛病的时候才懂得去珍惜。我曾经生病的时候也是鼓励自己要多锻炼和注意身体，但当身体好了之后，当时的愿望又抛到了脑后。\n身体出现问题往往不是一朝一夕造成的，而是长期积累的结果，尤其是刚工作的前几年，趁着年轻确实能多加班熬夜，但30岁之后往往体力就跟不上了。还是奉劝各位，在工作的时候多注意休息和加强锻炼，无论是996，还是朝九晚五，都要多注意。\n精彩文章 1.互联网运维工作\n滴滴运维总监来伟在2017年对运维工作范围的思考，公司处在不同的阶段，运维所能干的事情也有所不同。\n2.早点懂这几个道理，就不害怕被裁员了\nIT行业中的一些职场规则，程序员在中年时期如果不做好职场转型，会逐步被更有活力更有体力的年轻人给取代。\n","date":"2019-03-28T20:32:42Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC11%E6%9C%9F/","title":"知识分享第11期"},{"content":"golang中的panic用于异常处理，个人感觉没有try catch finally方式直观和易用。\nfunc panic(v interface{})函数的作用为抛出一个错误信息，同时函数的执行流程会结束，但panic之前的defer语句会执行，之后该goroutine会立即停止执行，进而当前进程会退出执行。\nfunc recover() interface{}定义在panic之前的defer语句中，用于将panic()进行捕获，这样触发panic时，当前gotoutine不会被退出。\nrecover所返回的内容为panic的函数参数，如果没有捕获到panic，则返回nil。\n注意：recover仅能定义在defer中使用，在普通语句中无法捕获recover异常。recover可以不跟panic定义在同一个函数中使用。\nexample 1 1 2 3 4 5 6 7 8 9 import \u0026#34;fmt\u0026#34; func main() { defer fmt.Println(1) fmt.Println(2) panic(3) fmt.Println(4) defer fmt.Println(5) } panic()执行后，会先调用defer函数，然后打印panic: 3，当前goroutine退出，后续语句不再执行，程序输出：\n1 2 3 4 5 6 7 8 2 1 panic: 3 goroutine 1 [running]: main.main() /Users/lvkai/src_test/go/panic/panic.go:8 +0xd5 exit status 2 example 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 package main import \u0026#34;fmt\u0026#34; func main() { func() { defer func() { if r := recover(); r != nil { fmt.Println(\u0026#34;recover: \u0026#34;, r) } }() defer fmt.Println(1) fmt.Println(2) panic(3) fmt.Println(4) defer fmt.Println(5) }() fmt.Println(6) } 在执行panic后，触发当前函数中的defer中的recover函数，此时panic后的当前函数中的语句同样是不再执行，但当前goroutine不会退出。也就是说panic被recover后，会影响到当前函数中的后续语句的执行，但不影响当前goroutine的继续执行，输出内容如下：\n1 2 3 4 2 1 recover: 3 6 example 3 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; func main() { func() { defer func() { if r := recover(); r != nil { fmt.Println(\u0026#34;recover: \u0026#34;, r) } }() func() { defer fmt.Println(1) panic(2) }() fmt.Println(3) defer fmt.Println(4) }() fmt.Println(5) } recover跟panic定义在不同的函数中，仍然可以发挥作用。\n1 2 3 1 recover: 2 5 ","date":"2019-03-19T23:52:04Z","permalink":"/post/golang%E4%B8%AD%E7%9A%84panic%E5%92%8Crecover%E7%94%A8%E6%B3%95/","title":"Golang中的panic和recover用法"},{"content":"\n春回大地，题图为即将融化的河面以及还在冰面上行走的路人。\n资源 1.fastThread\n在线的JVM线程栈分析工具，通过上传JVM Dump文件，在线查看线程分析结果。\n2.全球空气质量地图\n可以在线查看全球的PM2.5情况，很多国家的PM2.5都超过了200，但并不包含中国的数据，不知道是不是怕数据把其他国家吓死的缘故。\n3.Walle\n使用Python3开发的CI/CD平台，有相对友好的界面，目前Github Star数在6000+。\n4.Træfɪk\n为微服务而生的HTTP协议反向代理，自带API接口、dashboard，并支持Kubernetes Ingress、Mesos等，可动态加载配置文件等诸多nginx不具备的特性。\n5.kcptun\n基于KCP协议的UDP隧道，KCP协议能以比 TCP浪费10%-20%的带宽的代价，换取平均延迟降低 30%-40%。\n6.k3s - 5 less than k8s\n有人搞了个k3s项目，作为轻量级的k8s，整个二进制包只有40M大小。项目定位为边缘计算、IoT、CI等，支持多种硬件平台，裁剪了k8s的很多功能，比如云依赖，存储插件等，甚至连k8s依赖的etcd存储都默认更换为了sqlite3。\n7.Drone\n基于Golang的Container Native的CD平台，Github上star 17000+，插件的安装也是基于容器的。\n8.kaniko\n通过Dockerfile来构建docker镜像，需要dockerd进程的支持，这在物理机上操作没有任何问题。而如果要想在容器中通过Dockerfile构建docker镜像却变得困难起来，因为dockerd的运行需要root权限，而容器为了安全是不建议开启root权限的。\n该工具可以在容器中不运行dockerd的情况下通过Dockerfile构建出docker镜像。\n9.Nacos\n阿里巴巴开源的微服务框架，支持配置中心、动态服务发现、动态DNS。\n10.PowerDNS\nLinux下除了bind外的另一个可选择的DNS服务器，数据存储在mysql中，还有一个可选择的漂亮UI。\n精彩书籍 《激荡十年，水大鱼大》 要想回顾一下过去的十年中都发生了哪些大事，中国发生了哪些变化，经济领域里有哪些大起大落，本书可以拿来一读。\n","date":"2019-03-01T19:41:57Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC10%E6%9C%9F/","title":"知识分享第10期"},{"content":"最近突发奇想，想折腾一下路由器。经过研究半天后，锁定了网件R6400这款路由器，原因是可选择的系统较多，跟华硕的路由器架构一致，且支持较为强大的梅林系统。\n整个刷机的过程还是非常简单的，虽然花了我不少时间，本文简单记录一下。\n拿到手后，R6400比我印象中的要大不少，可以通过下图中的苹果手机进行对比。\nR6400有v1和v2两个版本，其中v1版本的CPU频率为800MHz，v2版本的CPU频率为1GHz。v1版本的刷梅林系统和v2版本刷系统有所区别，v2版本需要先刷DD-WRT固件作为过渡固件，然后再刷梅林固件。\n整个的过程最好用有线连接路由器操作，用无线会频繁掉线。\n.chk结尾的文件为过渡固件，.trx为最终固件。\n注意：下载的固件文件最好检验一下md5，确保固件的正确性。\n刷dd-wrt固件 连接上路由器后，在chrome浏览器中输入http://www.routerlogin.net可跳转到网件管理系统，通过一堆路由器设置后会重启路由器，然后重新登录。\n选择“是”后会下载新网件固件，其实该步骤可以选择“否”即可。固件下载完成后，会升级固件，路由器会自动重启。\n下载DD-WRT固件文件“DD固件.chk”，并在路由器的管理界面“高级 -\u0026gt; 管理 -\u0026gt; 路由器升级”中上传固件。\n这里选择是，然后开始升级固件，升级完成后路由器会重启。\n重启完成后，Wifi信号变成dd-wrt，没有密码，可直接连接。\n在浏览器中输入192.168.1.1，会出现dd-wrt的界面。用户名和密码可以直接输入admin，因为该系统仅为中间过度系统。在dd-wrt这个偏工程师化的系统中有非常详细的信息，包括路由器的CPU和Memory等的硬件信息，甚至还有load average，多么熟悉的指标。\n升级梅林固件 在dd-wrt的固件升级中选择“R6400_380.70_0-X7.9.1-koolshare.trx”，刷入梅林固件。待路由器重启完成后，即完成梅林固件的刷入。此时路由器的Wifi SSID变为“NETGEAR”。\n访问192.168.1.1，会出现梅林系统的管理界面，依次设置即可。\n题外话：无线的密码修改完后，悲剧的事情发生了，路由器重启后居然连不上wifi，提示密码错误。不得不找来一台带有网口的笔记本用有线连接。在梅林管理系统中查看，未发现密码输入错误，明明输入的密码是对的，但SSID换一个密码居然奇迹般的可以无线连接了，怀疑是一个bug。\n设置完成后路由器会重启，此时管理系统地址变更为192.168.50.1。\n在“高级设置 -\u0026gt; 无线网络 -\u0026gt; 专业设置”中，调整区域为“United States”，据说可以加快速度。\n要想使用软件中心，需要在系统设置中开启下图选项，并重启路由器。重启后，Format JFFS partition at next boot会自动设置为false。\nASUS Router 下载app “ASUS Router”，可以直接连接到路由器，这是因为网件的路由器架构跟华硕完全一致。\nref 网件Netgear R6400 v2 开箱 刷梅林固件 梅林系统官网 梅林固件下载地址 ","date":"2019-02-21T21:17:32Z","permalink":"/post/%E7%BD%91%E4%BB%B6r6400v2%E5%88%B7%E6%A2%85%E6%9E%97%E6%95%99%E7%A8%8B/","title":"网件R6400V2刷梅林教程"},{"content":"现象 1 2 3 4 5 6 7 8 $ cat /etc/redhat-release CentOS Linux release 7.2.1511 (Core) $ uname -a Linux c3-a05-136-45-10.com 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux $ docker info | grep \u0026#34;Storage Driver\u0026#34; Storage Driver: devicemapper 在CentOS7.2的系统上，发现有一部分pod在delete后一直处于Terminating状态\n1 2 3 4 5 6 7 8 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE httpserver-prod-1-6cb97dfbcc-25dsh 0/1 Terminating 0 55d \u0026lt;none\u0026gt; 10.136.45.6 \u0026lt;none\u0026gt; httpserver-prod-1-6cb97dfbcc-f9flb 0/1 Terminating 0 54d \u0026lt;none\u0026gt; 10.136.45.4 \u0026lt;none\u0026gt; httpserver-prod-1-6cb97dfbcc-m7sl4 0/1 Terminating 0 55d \u0026lt;none\u0026gt; 10.136.45.6 \u0026lt;none\u0026gt; httpserver-prod-1-6cb97dfbcc-pqpht 0/1 Terminating 0 55d \u0026lt;none\u0026gt; 10.136.45.6 \u0026lt;none\u0026gt; httpserver-prod-1-6cb97dfbcc-r987g 0/1 Terminating 0 55d \u0026lt;none\u0026gt; 10.136.45.4 \u0026lt;none\u0026gt; httpserver-prod-1-6cb97dfbcc-zghhr 0/1 Terminating 0 54d \u0026lt;none\u0026gt; 10.136.45.6 \u0026lt;none\u0026gt; 查看docker的日志发现有如下报错信息如下，含义为在删除pod时由于/var/lib/docker/overlay/*/merged目录被其他应用占用，从而导致容器无法清除。\n1 2 3 4 Jan 30 14:57:47 c3-a05-136-45-4.com dockerd[1510]: time=\u0026#34;2019-01-30T14:57:47.704641914+08:00\u0026#34; level=error msg=\u0026#34;Error removing mounted layer e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9: remove /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged: device or resource busy\u0026#34; Jan 30 14:57:47 c3-a05-136-45-4.com dockerd[1510]: time=\u0026#34;2019-01-30T14:57:47.704772288+08:00\u0026#34; level=error msg=\u0026#34;Handler for DELETE /v1.31/containers/e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9 returned error: driver \\\u0026#34;overlay\\\u0026#34; failed to remove root filesystem for e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9: remove /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged: device or resource busy\u0026#34; Jan 30 14:57:48 c3-a05-136-45-4.com dockerd[1510]: time=\u0026#34;2019-01-30T14:57:48.228837657+08:00\u0026#34; level=error msg=\u0026#34;Error removing mounted layer 2851b80d5c45d1cac3e7384116da0ad022af21701f9aa0d9ba3598efd5723030: remove /var/lib/docker/overlay/0ff0f98e1abf43c10711f2804cae3cf37efd597016d38b4753e2af19c2e27eb9/merged: device or resource busy\u0026#34; Jan 30 14:57:48 c3-a05-136-45-4.com dockerd[1510]: time=\u0026#34;2019-01-30T14:57:48.228953497+08:00\u0026#34; level=error msg=\u0026#34;Handler for DELETE /v1.31/containers/2851b80d5c45d1cac3e7384116da0ad022af21701f9aa0d9ba3598efd5723030 returned error: driver \\\u0026#34;overlay\\\u0026#34; failed to remove root filesystem for 2851b80d5c45d1cac3e7384116da0ad022af21701f9aa0d9ba3598efd5723030: remove /var/lib/docker/overlay/0ff0f98e1abf43c10711f2804cae3cf37efd597016d38b4753e2af19c2e27eb9/merged: device or resource busy\u0026#34; 通过docker ps -a看到容器的状态为\u0026quot;Removal In Progress\u0026quot;。通过docker inspect可以看到容器的进程已经退出了。\n1 2 3 4 5 6 # docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e6b7378c58a3 golang-httpserver \u0026#34;/bin/sh -c \u0026#39;go ru...\u0026#34; 7 weeks ago Removal In Progress k8s_golang-httpserver_httpserver-prod-1-6cb97dfbcc-f9flb_default_9e3d2cbb-f9d4-11e8-b61c-f01fafd10a1b_0 # docker inspect e6b7378c58a3 --format \u0026#39;{{.State.Pid}}\u0026#39; 0 使用docker rm命令删除容器会报错\n1 2 # docker rm e6b7378c58a3 Error response from daemon: driver \u0026#34;overlay\u0026#34; failed to remove root filesystem for e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9: remove /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged: device or resource busy 通过kubectl delete pods命令虽然可以强制删除pod，但在宿主机上仍然能看到容器的状态为\u0026quot;Removal In Progress\u0026quot;。\n1 2 3 # kubectl delete pods httpserver-prod-1-6cb97dfbcc-f9flb --grace-period=0 --force warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. pod \u0026#34;httpserver-prod-1-6cb97dfbcc-f9flb\u0026#34; force deleted 通过搜索挂载目录的信息，可以找到是哪个进程挂载了该目录。可以看到是ntpd服务挂载了该目录。\n1 2 3 4 5 6 7 8 9 10 # grep -nr 98a56 /proc/*/mountinfo /proc/2725007/mountinfo:48:296 183 0:183 / /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged rw,relatime shared:88 - overlay overlay rw,lowerdir=/var/lib/docker/overlay/5e2a5f7af24e555a5afacd6a8faa406b42c51d7f2bb4cde22adcea22e0153583/root,upperdir=/var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/upper,workdir=/var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/work # ps -ef | grep 2725007 ntp 2725007 1 0 Jan07 ? 00:00:02 /usr/sbin/ntpd -u ntp:ntp -g # ntpd进程的启动时间在容器启动之后 # ps -ef | grep ntpd root 1179644 18205 0 19:52 pts/1 00:00:00 grep --color=auto -d skip -i ntpd ntp 3853149 1 0 Jan07 ? 00:00:02 /usr/sbin/ntpd -u ntp:ntp -g 查看ntpd.service文件内容如下，其中PrivateTmp=true，该选项用于控制服务是否使用单独的tmp目录：\n1 2 3 4 5 6 7 8 9 10 11 12 [Unit] Description=Network Time Service After=syslog.target ntpdate.service sntp.service [Service] Type=forking EnvironmentFile=-/etc/sysconfig/ntpd ExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONS PrivateTmp=true [Install] WantedBy=multi-user.target 问题复现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 在系统上启动一个容器，此时ntpd必须处于running状态 $ docker run -d httpserver:1 /bin/sh -c \u0026#34;while : ; do sleep 1000 ; done\u0026#34; # 启动容器 $ docker run -d httpserver:1 /bin/sh -c \u0026#34;while : ; do sleep 1000 ; done\u0026#34; 200222b438aac43bbe32a6c54e31ced0848482b9dec3e519d2f847c70c1ce801 # 重启ntpd $ systemctl restart ntpd $ docker stop 200222b438aa # 此时容器的相关信息还存在 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 200222b438aa httpserver:1 \u0026#34;/bin/sh -c \u0026#39;while...\u0026#34; About a minute ago Exited (137) 7 seconds ago hardcore_yalow # 强制删除容器失败 $ docker rm -f 200222b438aa Error response from daemon: driver \u0026#34;devicemapper\u0026#34; failed to remove root filesystem for 200222b438aac43bbe32a6c54e31ced0848482b9dec3e519d2f847c70c1ce801: remove /var/lib/docker/devicemapper/mnt/e53342aa9cf5f43e73b6596f88939b8d3fdefaf1ca03ee95a24d867e1de6c522: device or resource busy # 此时容器处于Removal In Progress状态 $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 200222b438aa httpserver:1 \u0026#34;/bin/sh -c \u0026#39;while...\u0026#34; 2 minutes ago Removal In Progress hardcore_yalow # 再次重启ntpd进程 $ systemctl restart ntpd # 强制删除成功 $ docker rm 200222b438aa 200222b438aa 经在如下版本的CentOS7系统实验，该问题不存在。\n1 2 3 4 5 6 7 8 $ uname -a Linux localhost.localdomain 3.10.0-862.9.1.el7.x86_64 #1 SMP Mon Jul 16 16:29:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux $ cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) $ docker info | grep \u0026#34;Storage Driver\u0026#34; Storage Driver: overlay2 问题产生原因 此问题为Systemd启用PrivateTmp选项后，导致mount namespace的一处内核bug。\n处理方式 在/usr/lib/systemd/system/docker.service的[Service]中增加MountFlags=slave，并重新启动docker服务，注意重启docker后，容器会重启。\n当然也可以通过重启ntpd服务的方式来临时解决问题，但当下次删除容器时还需要重启ntpd。\n还有一种办法是修改ntpd.service中的PrivateTmp=true，然后重启ntpd服务。\nref Docker 故障（device or resource busy）\n","date":"2019-01-30T20:51:02Z","permalink":"/post/kubernetes%E4%B8%ADpod%E6%97%A0%E6%B3%95%E5%88%A0%E9%99%A4%E7%9A%84%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","title":"kubernetes中pod无法删除的问题排查"},{"content":"Linux中的Buffer与Cache的含义通常非常容易混淆，两者翻译成中文都可以叫做缓存，都是数据在内存中的临时存储，而且网络上很多文章都是错误的。\n1 2 3 4 $ free -h total used free shared buff/cache available Mem: 125G 12G 347M 9.3M 113G 113G Swap: 0B 0B 0B free命令直接将buff和cache写到了一块，说明两者有很多共同点。\n1 2 3 4 5 $ vmstat 1 procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 7 1 0 364076 18664 118624552 0 0 214 11198 106 118 6 4 89 1 0 13 1 0 349096 18664 118638192 0 0 0 1012404 171031 270124 20 13 66 2 0 而通过vmstat命令可以分别看到buffer和cache的大小，单位为KB。\n使用man free命令看到的解释如下：\n1 2 3 buffers: Memory used by kernel buffers (Buffers in /proc/meminfo) cache: Memory used by the page cache and slabs (Cached and Slab in /proc/meminfo) 查看proc的man手册结果如下：\n1 2 3 4 5 6 7 8 9 10 11 Buffers %lu Relatively temporary storage for raw disk blocks that shouldn\u0026#39;t get tremendously large (20MB or so). Cached %lu In-memory cache for files read from the disk (the pagecache). Doesn\u0026#39;t include SwapCached. SReclaimable %lu (since Linux 2.6.19) Part of Slab, that might be reclaimed, such as caches. SUnreclaim %lu (since Linux 2.6.19) Part of Slab, that cannot be reclaimed on memory pressure. 上述信息，文档写的并不是非常明确。\n可以看出buffers是磁盘数据的缓存，通常不会特别大，缓存的数据包括磁盘的写请求和读请求。内核用于将分散的写磁盘操作集中起来，批量写入磁盘。\nCached是文件数据的缓存，同样可以缓存读请求和写请求。\nSlab包括了SReclaimalbe和Sunreclaim两部分信息，其中SReclaimable是可回收部分，SUnreclaim是不可回收部分。\n关于文件和磁盘的区别如下：\n磁盘是一个块设备，可以划分为多个分区，每个分区上可以构建不同的文件系统，文件系统挂载到目录上后，就可以对该文件系统进行读写文件操作了。\n读写普通文件系统中的文件时，会经过文件系统，由文件系统跟磁盘进行交互，而文件系统的缓存为cache。读写磁盘或者分区时，会跳过文件系统，直接对磁盘进行操作，而操作系统对磁盘的缓存称之为buffer。\nref Linux Programmer\u0026rsquo;s Manual PROC[5] ","date":"2019-01-29T00:27:45Z","permalink":"/post/linux-buffer%E4%B8%8Ecache%E7%9A%84%E5%90%AB%E4%B9%89/","title":"Linux Buffer与Cache的含义"},{"content":"\n富士山\u0026amp;富士吉田市，富士山的海拔高达3776米，远在80公里外的东京都能够看到。令人称奇的是，富士山海拔3360米以上的土地并不是归日本政府所有，而是归富士山上的浅间寺所有，日本政府每年都要支付大量的租金给浅间寺。在富士山周边游览后，突然萌生了登顶富士山的想法，不知是否有志同道合的驴友，可以相约在某年的夏季去一起实现梦想。\n资源 1.CRIU\nLinux下的一款实现checkpoint/restore功能的软件，该软件可以冻结某个正在运行的应用程序，并将应用程序的当前状态作为checkpoint存放在磁盘上的文件中，此后正在运行的应用程序会被kill。\n此后，可以通过读取磁盘上的文件，恢复之前冻结的应用程序继续执行，而不是从main函数开始执行。\n2.bindfs\n将一个目录mount到另外一个目录的工具，利用该命令可以将docker中的路径挂载到宿主机上。具体操作命令类似如下：\n1 2 3 4 PID=$(docker inspect b991b7ad105f --format {{.State.Pid}}) bindfs /proc/$PID/root /tmp/root # 别忘了卸载目录 umount /tmp/root 3.微软亚洲研究院-对联电脑\n微软亚洲研究院的自动对对联系统，给出上联后，可以自动给出多个下联，最终生成横批。\n4.bcc\n基于Linux eBPF的一系列的性能分析工具，包括IO、网络等多个方面。\n5.pcstat\n基于golang开发的linux下的文件缓存统计工具。\n6.Electron\n利用前端技术（JavaScript、HTML、CSS）来构建桌面程序的框架，当前很多流行的桌面应用都是使用该技术来开发的，比如VSCode、Slack、Atom等技术。\n得益于ES6、V8引擎和Node.js，JavaScript技术已经横跨前端、后端、桌面端的技术栈。\n7.Reading-and-comprehense-linux-Kernel-network-protocol-stack\n该项目包含了对Linux网络协议栈的源码中文注释，对阅读Linux网络协议栈的代码有一些帮助。\n精彩文章 Kubernetes API 与 Operator：不为人知的开发者战争（一） Kubernetes API 与 Operator：不为人知的开发者战争（二） 精彩语句 “不能用”“不好用”“需要定制开发”，这才是落地开源基础设施项目的三大常态。\n\u0026ndash; 张磊《深入剖析Kubernetes》\n开源项目在落地到公司内部实际使用时，会发现有这样或者那样的问题。开源项目往往是个通用项目，公司在落地时，总有其特殊需求之处，开源软件无法面面俱到，往往只能覆盖一些通用的需求。再加上靠社区来驱动，在bug方面、功能方面跟商业软件也还有较大差距。\n娱乐 1.《塞尔达传说-旷野之息》\n任天堂Switch上的游戏神作，历时四年时间，300人的团队开发，最近一直在玩，已经深深被游戏设计的海拉鲁大陆所折服，完全开放的世界，不同于传统的闯关类游戏，该游戏的自由度非常高，有时候就单纯的在地图中瞎逛都是一种享受，随时都会有惊喜发生。\n曾天真的以为，一个单机游戏能好玩到哪里去，但在玩游戏的每一刻都能体会到制作团队的用心，心里总是念到这才是我想要玩的游戏。自从玩了该游戏后，手机上的游戏再也没有打开过。我甚至一度感叹，在国内快糙猛的环境下是产生不了如此细腻良心作品的。如果大家有机会，可以尝试下这款游戏，或许会发现单机游戏还可以做得如此出彩。\n2.ZELDA MAPS\n同样是跟《塞尔达传说-旷野之息》相关的，由于塞尔达传说的地图实在过于庞大，包含了神庙、驿站、村庄、回忆（没错主人公Link失忆了）、各种支线任务、装备、呀哈哈、各类大小boss、迷宫等等，有玩家制作了一款在线的地图，可以在线查询地图中的各类元素，使用体验类似Google Map。还包含了账号体系，可以在地图上标记自己已经完成的任务。\n","date":"2019-01-19T20:18:13Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC9%E6%9C%9F/","title":"知识分享第9期"},{"content":"在Dockerfile中ENTRYPOINT与CMD的功能类似，同时再加上docker run后面追加的容器启动参数，是极其容易混淆的。而且又掺杂着exec模式和shell模式。\n这里先说几个结论，有了结论再跟进下面的例子来理解会更容易一些：\n实际上docker容器进程的完整启动参数为ENTRYPOINT CMD，如果没有指定ENTRYPOINT，docker会提供一个隐式的值/bin/sh -c。 docker run后面跟的容器启动参数仅会覆盖CMD部分。 exec模式与shell模式 CMD和ENTRYPOINT两个命令均支持exec模式和shell模式。\nexec模式格式类似CMD [ \u0026quot;top\u0026quot; ]，当容器启动时，top命令的进程号为1。\n为了能够获取到环境变量，通常的写法为CMD [ \u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo $HOME\u0026quot; ]，此时1号进程为sh。\nshell模式的写法为CMD top，docker会以/bin/sh -c top的方式来执行命令，此时容器的1号进程为sh。\n如果需要容器进程处理外部信号的情况下，shell模式下信号实际上时发送给了sh，而不是容器中的应用进程。\n因此比较推荐使用exec模式，shell模式实际使用较少。\nCMD CMD [\u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] 为ENTRYPOINT提供默认参数，需要指定ENTRYPOINT CMD [\u0026ldquo;executable\u0026rdquo;,\u0026ldquo;param1\u0026rdquo;,\u0026ldquo;param2\u0026rdquo;] exec模式 CMD command param1 param2 shell模式 CMD为容器提供默认的启动命令，如果在启动容器时通过命令行指定了的启动参数，则该启动参数会覆盖CMD默认的启动参数。\nENTRYPOINT 不能被docker run增加的参数覆盖，启动时要执行ENTRYPOINT的参数。\nENTRYPOINT [\u0026ldquo;executable\u0026rdquo;, \u0026ldquo;param1\u0026rdquo;, \u0026ldquo;param2\u0026rdquo;] exec模式 ENTRYPOINT command param1 param2 shell模式 exec模式 当为exec模式时，容器启动时，在命令行上添加的参数会被追加到ENTRYPOINT的参数列表中。\n例如：\n1 2 FROM ubuntu:latest ENTRYPOINT [ \u0026#34;echo\u0026#34;, \u0026#34;hello\u0026#34; ] 执行docker run --rm 0d89e8d4425a world，会输出hello world\nshell模式 当ENTRYPOINT为shell模式时，docker run启动后追加的参数会被忽略。\n例如：\n1 2 3 FROM ubuntu:latest ENTRYPOINT echo hello 执行docker run --rm 0841e19b4d2e world仅输出hello。\nENTRYPOINT命令的覆盖 ENTRYPOINT的命令可以通过docker run中增加--entrypoint选项来使用命令行中指定的参数覆盖ENTRYPOINT的参数。\nENTRYPOINT与CMD的组合使用 当同时指定CMD和ENTRYPOINT模式时，实际上为ENTRYPOINT CMD\n1 2 3 4 FROM ubuntu:latest ENTRYPOINT [ \u0026#34;echo\u0026#34;, \u0026#34;hello\u0026#34; ] CMD [ \u0026#34;world\u0026#34; ] docker run --rm 7edf658370d9会输出hello world，而docker run --rm 7edf658370d9 kitty会输出hello kitty。\n更复杂的情况可以参照下图：\n如何查看ENTRYPOINT和CMD 可以通过docker history ${image} --no-trunc来查生成镜像的所有Dockerfile命令\nref Dockerfile reference Dockerfile 中的 CMD 与 ENTRYPOINT ","date":"2019-01-04T22:00:56Z","permalink":"/post/dockerfile%E4%B8%AD%E7%9A%84entrypoint%E4%B8%8Ecmd/","title":"Dockerfile中的ENTRYPOINT与CMD"},{"content":"\n题图为中国铁道博物馆东郊馆中的毛泽东号列车\n资源 1.Hawkular\nHawkular为RedHat开源的监控解决方案，实现语言为java，监控数据的底层存储引擎使用Cassandra，包含了告警功能。目前Github上的Star还较少。RedHat的OpenShift就使用了该监控方案。\n2.Kong\n基于Nginx OpenResty的API网关，支持自定义插件，支持比原生nginx更多的功能。\n3.NuoDB\n弹性可伸缩的关系型数据库，兼容SQL标准。将数据库中的事务和存储进行了分离，存储层支持多种存储系统，比如文件系统、Amazon S3和HDFS。因为存储层可以是外部的存储，意味着NuoDB的扩展性会大大增强，使其部署到Kubernetes成为了比较容易的事情。\n4.Linux命令hping3\nhping3是一个用于生成和解析tcp/ip协议的工具，能够对数据包进行定制，可用于端口扫描、DDOS攻击等，是一个比较常见的黑客工具。\n5.Firecracker\nAmazon开源的轻量级的虚拟机软件，使用KVM来创建和管理虚拟机，整体架构类似Kata Container。容器采用cgroup和namespace来做资源隔离，但是在安全性方面却比较差，轻量级的虚拟机在做到隔离性的同时，又提供了不错的启动速度，是容器领域的一个发展方向。\n6.NginxConfig.io\nNginxConfig.io是一款在线生成nginx配置文件的工具，可以通过点点鼠标，在文本框中内容的方式轻松生成nginx的配置文件。\n7.Caddy\n一款实用Go语言编写的负载均衡工具，默认启用HTTPS服务，可以使用Let\u0026rsquo;s Encrypt来自动签发证书。配置文件的写法也比nginx要简洁。\n8.loki\nGrafana团队最新发布的基于Go语言开发的日志聚合系统，loki不会对日志进行全文索引，而是以压缩聚合的方式进行存储，可以对日志流通过打标签的方式进行分组，页面的展示直接使用grafana。对Kubernetes Pod中的log做了特别的支持，比较适合抓取和存储Kubernetes Pod中的log。\n个人感觉该工具未来会很火爆，尤其是跟Grafana有着无缝的整合。很多公司会使用ES来作为日志中心的底层存储，但不见得所有的服务都有按照关键字进行匹配搜索的需求，ES作为日志中心就显得不够高效和经济。\n9.JSON-RPC\njson-rpc是rpc通讯中的一种json格式标准，该协议要求request和response的内容必须为json格式，且json有固定的格式。\n10.KSQL\nApache Kafka的开源SQL引擎，可以使用SQL的形式查询kafka中的消息，该产品跟Kafka一样，同样为Confluent出品。\n精彩文章 1.北京五环外的真实中国\n朋友圈刷屏文章，文章以gif动画的形式描述了社会底层人士的艰辛生活，他们背上扛起的不仅是压得直不起腰来的砖头，而是面对困难努力生活的勇气，有些时候为了生计确实没得选择。\n当我们在抱怨生活的同时，可以想想比我们更苦更累却默默承受生活之重的人们，或许心里会好受些。\n书籍 1.《深入解析Go》\n从底层角度分析go语言实现，推荐所有golang开发者一看。\n2.深入浅出Serverless：技术原理与应用实践\n要想能够对Serverless技术的概念和现状有所了解，该书还是挺合适的。\n该书介绍了公有云上的Serverless产品AWS Lambda、Azure Functions，开源项目OpenWhisk、Kubeless、Fission和OpenFasS，提供对这些技术的一站式了解。\n","date":"2018-12-20T21:51:10Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC8%E6%9C%9F/","title":"知识分享第8期"},{"content":"\ntime_wait状态 客户端在收到服务器端发送的FIN报文后发送ACK报文，并进入TIME_WAIT状态，等待2MSL（最大报文生存时间）后才断开连接，MSL在Linux中值为30s。\n之所以设计time_wait主要用来解决以下异常场景：\n确保对端处于关闭状态。主动断开连接一段发送最后一个ack报文，如果丢失，被动断开连接一端会重新发送fin报文。如果主动断开连接一方直接关闭，被动方会一直处于last-ack状态。 防止上一个连接中的包影响新的连接，上一个连接中的包在2MSL中一定可以到达对端。 过多的危害：在客户端占用过多的端口号\ntime_wait过多的解决思路 将net.ipv4.tcp_max_tw_buckets值调小，当TIME_WAIT的数量到达该值后，TIME_WAIT状态会被清除，相当于没有遵守tcp协议 修改TCP_TIMEWAIT_LEN的值，但需要重新编译内核，非常不建议修改 打开tcp_tw_recycle和tcp_timestamps 打开tcp_tw_reuse和tcp_timestamps 采用长连接 tcp有个tcp时间戳选项，第一个是发送方的当前时钟时间戳（4个字节），第二个4字节为从远程主机接收到的最新时间戳\n相关内核参数 net.ipv4.tcp_max_tw_buckets (integer; default: see below; since Linux 2.4) The maximum number of sockets in TIME_WAIT state allowed in the system. This limit exists only to prevent simple denial-of-service attacks. The default value of NR_FILE*2 is adjusted depending on the memory in the system. If this number is exceeded, the socket is closed and a warning is printed.\n系统中允许的 time_wait 数量的最大值，当达到最大值后，新的连接会被拒绝。\nnet.ipv4.tcp_tw_timeout time_wait 的超时时间，默认为 2MSL，即 60s，在大部分的 linux 系统下该值没法修改，仅在某些 OS 系统下可用。比如：Alibaba Cloud Linux 修改TCP TIME-WAIT超时时间。\ntcp_timestamp 用来控制tcp option字段，发送方在发送报文时会将当前时钟的时间值放入到时间戳字段。\nnet.ipv4.tcp_tw_reuse (Boolean; default: disabled; since Linux 2.4.19/2.6) Allow to reuse TIME_WAIT sockets for new connections when it is safe from protocol viewpoint. It should not be changed without advice/request of technical experts.\ntcp_tw_reuse意思为主动关闭连接的一方可以复用之前的time_wait状态的连接。\n复用连接后，这条连接的时间更改为当前时间，延迟数据到达时，延迟数据时间小于新连接时间。\n需要连接双方都打开timestamp选项。\n该选项适用的范围为作为客户端主动断开连接，复用客户端的time_wait的状态，对服务端无影响。\nnet.ipv4.tcp_tw_recycle 内核会在一个RTO的时间内快速销毁掉time_wait状态，RTO时间为数据包重传的超时时间，该时间通过RTT动态计算，远小于2MSL。\n需要连接双方都打开timestamp选项。\n适用场景为服务端主动断开连接，time_wait状态位于服务端，服务端适用该选项快速回收time_wait状态的连接。\n弊端：如果客户端在NAT网络中，如果配置了tcp_tw_recycle，可能会出现在一个RTO的时间内，只有一个客户端和自己连接成功的情况。\n4.10之后，Linux内核修改了时间戳生成机制，该选项已经抛弃。\nIn Action 解决time_wait状态过多的比较好的思路为采用http的keepalive功能。\nnginx nginx对于upstream，默认是使用http1.0协议的，要想启用keepalive，需要在location中增加\n1 2 proxy_http_version 1.1; proxy_set_header Connection \u0026#34;\u0026#34;; 在upstream中增加keepalive参数，这里的参数含义为每个nginx worker连接所有后端的最大连接数。\n1 keepalive 200; 如果keepalive连接过少，此时由于使用的是http1.1的协议，upstream端不会主动断开连接，nginx会主动断开连接，此时nginx端的time_wait就会过多，会占用端口号，导致nginx端没有端口号可以使用。\n引用 关于 Nginx upstream keepalive 的说明 HAProxy and HTTP errors 408 in Chrome 被抛弃的tcp_recycle ","date":"2018-12-17T23:02:09Z","permalink":"/post/tcp-time_wait/","title":"TCP TIME_WAIT"},{"content":"限流的方式有多种，每种都有其应用场景。\n限制请求的方式包括：\n丢弃请求 放在队列中，等有令牌后再请求 走降级逻辑 计数器 我之前设计的流控系统，以每秒为单位，如果一秒内超过固定的QPS，则将请求进行降级处理。该算法已经在生产环境中平稳运行了很久，也确实满足了业务的需求。\n计数器流控算法简单粗暴，有一个缺点，即流控的单位为秒，但一秒的请求很可能是不均匀的，不能进行更细粒度的控制，也不允许流量存在某种程度的突发。\n漏桶算法 请求先进入漏桶中，漏桶以一定的速度出水，当水流的速度过大时会直接溢出。\n漏桶大小：起到缓冲的作用\n漏桶的出水速度：该值固定\n令牌桶算法 令牌桶算法相比漏桶算法而言，允许请求存在某种程度的突发，常用于网络流量整形和速率限制。\n系统会恒定的速度往令牌桶中注入令牌，如果令牌桶中的令牌满后就不再增加。新请求来临时，会拿走一个令牌，如果没有令牌就会限制该请求。\n这里的请求可以代表一个网络请求，或者网络的一个字节。\n涉及到的变量：\n网络请求平均速率r：每隔1/r秒向令牌桶中放入一个令牌，1秒共放入r个令牌 令牌桶的最大大小：令牌桶慢后，再放入的令牌会直接丢弃 令牌相当于操作系统中信号量机制。\n业界较为出名的流控工具当属Guava中的RateLimiter，基于令牌桶算法实现。\n在实际的代码实现中，并不一定需要一个固定的线程来定期往令牌桶中放入令牌，而是在请求到来时，直接计算得出当前是否还有令牌。比如下面的python代码实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import time class TokenBucket(object): # rate是令牌发放速度，capacity是桶的大小 def __init__(self, rate, capacity): self._rate = rate self._capacity = capacity self._current_amount = 0 self._last_consume_time = int(time.time()) # token_amount是发送数据需要的令牌数 def consume(self, token_amount): increment = (int(time.time()) - self._last_consume_time) * self._rate # 计算从上次发送到这次发送，新发放的令牌数量 self._current_amount = min( increment + self._current_amount, self._capacity) # 令牌数量不能超过桶的容量 if token_amount \u0026gt; self._current_amount: # 如果没有足够的令牌，则不能发送数据 return False self._last_consume_time = int(time.time()) self._current_amount -= token_amount return True ref 15行Python代码，帮你理解令牌桶算法\n","date":"2018-12-16T01:25:50Z","permalink":"/post/%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%E7%AE%97%E6%B3%95/","title":"流量控制算法"},{"content":"iowait和load一样，都是非常容易让人产生误解的系统指标。\niowait表示cpu空闲且有未完成的io请求的时间，iowait高并不能反映出磁盘是系统的性能瓶颈。iowait高的时候cpu正处于空闲状态，没有任务可以执行。此时存在已经发出的磁盘io，此时的cpu空闲状态称之为iowait。本质上，iowait是一种特殊的cpu空闲状态。\niowait状态的cpu是运行在pid为0的idle线程上。\ncpu此时之所以进入睡眠状态，是因为进程处于睡眠状态，在等待某个特定的事件（比如网络数据，io操作完成等）。\niowait仅能反应磁盘io的指标，并不能反应其他io设备的指标，比如网络丢包。\n在io wait的进程处于不可中断状态，通过top命令可以看到进程状态为\n由此可见，iowait包含的信息量非常少，仅凭iowait升高不能判断出系统io有问题。要想判断系统io有问题，还需要使用iostat等命令来查看系统的svctm、util、avgqu-sz等指标。\ncase 1 仅cpu的繁忙程度变化的情况下，会影响到iowait的值。\ncase 2 在cpu繁忙程序不变的情况下，发起io请求的时间不同也会影响到iowait的值。\n","date":"2018-12-08T22:58:53Z","permalink":"/post/linux-iowait/","title":"linux iowait"},{"content":"seccomp是secure computing mode的缩写，是Linux内核中的一个安全计算工具，机制用于限制应用程序可以使用的系统调用，增加系统的安全性。可以理解为系统调用的防火墙，利用BPF来规律系统调用。\n在/proc/${pid}/status文件中的Seccomp字段可以看到进程的Seccomp。\nprctl 下面程序使用prctl来设置程序的seccomp为strict模式，仅允许read、write、_exit和sigreturn四个系统调用。当调用未在seccomp白名单中的系统调用后，应用程序会被kill。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 #include \u0026lt;stdio.h\u0026gt; /* printf */ #include \u0026lt;sys/prctl.h\u0026gt; /* prctl */ #include \u0026lt;linux/seccomp.h\u0026gt; /* seccomp\u0026#39;s constants */ #include \u0026lt;unistd.h\u0026gt; /* dup2: just for test */ int main() { printf(\u0026#34;step 1: unrestricted\\n\u0026#34;); // Enable filtering prctl(PR_SET_SECCOMP, SECCOMP_MODE_STRICT); printf(\u0026#34;step 2: only \u0026#39;read\u0026#39;, \u0026#39;write\u0026#39;, \u0026#39;_exit\u0026#39; and \u0026#39;sigreturn\u0026#39; syscalls\\n\u0026#34;); // Redirect stderr to stdout dup2(1, 2); printf(\u0026#34;step 3: !! YOU SHOULD NOT SEE ME !!\\n\u0026#34;); // Success (well, not so in this case...) return 0; } 执行上述程序后会输出如下内容：\n1 2 3 step 1: unrestricted step 2: only \u0026#39;read\u0026#39;, \u0026#39;write\u0026#39;, \u0026#39;_exit\u0026#39; and \u0026#39;sigreturn\u0026#39; syscalls Killed 基于BPF的seccomp 上述基于prctl系统调用的seccomp机制不够灵活，在linux 3.5之后引入了基于BPF的可定制的系统调用过滤功能。\n需要先安装依赖包：yum install libseccomp-dev\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \u0026lt;stdio.h\u0026gt; /* printf */ #include \u0026lt;unistd.h\u0026gt; /* dup2: just for test */ #include \u0026lt;seccomp.h\u0026gt; /* libseccomp */ int main() { printf(\u0026#34;step 1: unrestricted\\n\u0026#34;); // Init the filter scmp_filter_ctx ctx; ctx = seccomp_init(SCMP_ACT_KILL); // default action: kill // setup basic whitelist seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigreturn), 0); seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit), 0); seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(read), 0); seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 0); // setup our rule seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(dup2), 2, SCMP_A0(SCMP_CMP_EQ, 1), SCMP_A1(SCMP_CMP_EQ, 2)); // build and load the filter seccomp_load(ctx); printf(\u0026#34;step 2: only \u0026#39;write\u0026#39; and dup2(1, 2) syscalls\\n\u0026#34;); // Redirect stderr to stdout dup2(1, 2); printf(\u0026#34;step 3: stderr redirected to stdout\\n\u0026#34;); // Duplicate stderr to arbitrary fd dup2(2, 42); printf(\u0026#34;step 4: !! YOU SHOULD NOT SEE ME !!\\n\u0026#34;); // Success (well, not so in this case...) return 0; } 输入如下内容：\n1 2 3 4 step 1: unrestricted step 2: only \u0026#39;write\u0026#39; and dup2(1, 2) syscalls step 3: stderr redirected to stdout Bad system call docker中的应用 通过如下方式可以查看docker是否启用seccomp：\n1 2 # docker info --format \u0026#34;{{ .SecurityOptions }}\u0026#34; [name=seccomp,profile=default] docker每个容器默认都设置了一个seccomp profile，启用的系统调用可以从default.json中看到。\ndocker会将seccomp传递给runc中的sepc.linux.seccomp。\n可以通过—security-opt seccomp=xxx来设置docker的seccomp策略，xxx为json格式的文件，其中定义了seccomp规则。\n也可以通过--security-opt seccomp=unconfined来关闭docker引入默认的seccomp规则的限制。\nref Introduction to seccomp: BPF linux syscall filter 如何在Docker内部使用gdb调试器 ","date":"2018-12-08T17:21:40Z","permalink":"/post/linux-seccomp/","title":"Linux Seccomp"},{"content":"\n题图为金山岭长城，明代著名抗倭名将戚继光从南方调任至此修筑，为明长城之精华，\n资源 1.GoAccess\n一款开源的实时分析nginx日志的工具，并拥有一个比较强大的dashboard。\n2.Wayne\n360开源的kubernetes的多集群管理平台。\n3.MacKey\n一个分享KeyNote模版的网站，每个KeyNote模版都带有动画和图片截图。\n4.Nomad\nHashicorp公司开源的集群调度工具，该公司另一款较为出名的产品为Vagrant。\n5.registrator\n该服务部署在宿主机上，自动将docker的容器注册到服务注册中心中，如consul、etcd等。\n6.CNI-Genie\n华为开源的容器网络解决方案，CNI（Container Network Interface）仅支持加载一个插件，该插件可以同时一次加载多个网络插件，在容器中可以同时存在多个网络解决方案的ip。\n7.stress-ng\nLinux下有一个命令行的压测测试工具stress，可以用来测试cpu、内存、io等，stress-ng提供了更丰富的选项。\n8.Resilience4j\njava版的开源熔断工具Hystrix宣布停止开发，并推荐了Resilience4j工具，该工具灵感来自于Hystrix，主要为java 8和函数式编程设计的自动熔断工具。\n9.Standard Go Project Layout\n我刚开始写go的时候，一度被golang的源码目录结构所困惑，这个项目提供了一个标准的goalng目录结构的用法，很多开源项目都是按照这个标准组织的。\n10.dive\ndocker images不是一个单独的文件存储在宿主机上，而是采用分层设计，以便于多个镜像之间复用相同的层数据。dive可以用来分析docker image的每一层的具体组成。\n11.Swoole\nphp号称是世界上最好的编程语言之一，但最为人诟病的是其网络模型是同步模型，导致其性能一直上不去。Swoole可以实现类似于Golang中的goroutine同步编程模型来实现异步的功能。\n精彩文章 1.知乎社区核心业务 Golang 化实践\n本文记录了知乎内部使用golang来重构python的实践经验，用来解决python编程语言的运行效率低和维护成本高的问题。\n2.如何在Docker内部使用gdb调试器\n本文记录了一些docker关于权限相关的技术实现。\n3.ofo剧中人：我不愿谢幕\n以记者的角度记录了OFO的发家、辉煌、衰败，曾有过彷徨与迷茫，曾有过野性与嚣张，但最终还是要倒在资本面前。\n大家都在吐槽OFO押金退不了的事情，看到一个评论中的不错的点子，可以在OFO的退押金页面增加广告位，毕竟流量就是金钱，退押金页面的流量也是流量，反正押金也退不了，不如借此来一波，至少比在公众号中卖蜂蜜要好的多。\n一个生动的细节是，有黑摩的司机不爽共享单车影响他们生意，砸ofo的车。ofo后期转化了一批相当数量的司机当修车师傅，化干戈为玉帛。\n上述操作还是非常犀利的，说白了还是利益在作怪。\n","date":"2018-12-06T23:58:09Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC7%E6%9C%9F/","title":"知识分享第7期"},{"content":"\n一个正常的tcp server在处理请求时会经过如下的系统调用：socket() bind() listen() accept() read() write() close()。一个请求在被应用程序读取之前，可能处于SYN_RCVD和ESTABLISHED两种状态。\n第一个队列：SYN_RCVD状态是server端接收到了client端的SYN包，server端会将该连接放到半连接队列中，并向客户端发送SYN+ACK包，此时连接处于半连接状态。通常该队列被称为半连接队列。\n第二个队列：ESTABLISHED状态为已经完成了三次握手，但是server端的应用程序还未调用accept系统调用的情况。通常该队列被称为全连接队列。\n这两种情况下都需要操作系统提供相应队列来保存连接状态。\nbacklog用来设置这两个队列的最大值，但在不同的操作系统中有不同的含义，下面的说明以linux操作系统为准。\n其中第一个维护SYN_RCVD状态的队列使用内核参数net.ipv4.tcp_max_syn_backlog来控制，如果队列超过这一阈值，连接会被拒绝。该值默认为1000.\n第二个维护ESTABLISHED状态的队列，该队列的长度由应用程序调用listen系统调用时指定。\n内核参数 net.ipv4.tcp_max_syn_backlog 1 2 tcp_max_syn_backlog (integer; default: see below; since Linux 2.2) The maximum number of queued connection requests which have still not received an acknowledgement from the connecting client. If this number is exceeded, the kernel will begin dropping requests. The default value of 256 is increased to 1024 when the memory present in the system is adequate or greater (\u0026gt;= 128Mb), and reduced to 128 for those systems with very low memory (\u0026lt;= 32Mb). It is recommended that if this needs to be increased above 1024, TCP_SYNQ_HSIZE in include/net/tcp.h be modified to keep TCP_SYNQ_HSIZE*16\u0026lt;=tcp_max_syn_backlog, and the kernel be recompiled. 用来设置 syn 队列的大小，通常也会称为半连接队列。该参数的默认值一般为 1024。如果 syn 队列满，此时 syn 报文会被丢弃，无法回复 syn + ack 报文。可以通过 netstat -s 命令看到 \u0026ldquo;XX SYNs to LISTEN sockets dropped\u0026rdquo;. 的报错信息。\nnet.ipv4.tcp_syncookies 1 tcp_syncookies (Boolean; since Linux 2.2) Enable TCP syncookies. The kernel must be compiled with CONFIG_SYN_COOKIES. Send out syncookies when the syn backlog queue of a socket overflows. The syncookies feature attempts to protect a socket from a SYN flood attack. This should be used as a last resort, if at all. This is a violation of the TCP protocol, and con‐ flicts with other areas of TCP such as TCP extensions. It can cause problems for clients and relays. It is not recommended as a tuning mechanism for heavily loaded servers to help with overloaded or misconfigured conditions. For recommended alternatives see tcp_max_syn_backlog, tcp_synack_retries, and tcp_abort_on_overflow. 因为 syn 队列的存在，当客户端一直在发送 syn 包，但是不回 ack 报文时，一旦服务端的队列超过 net.ipv4.tcp_max_syn_backlog 设置的大小就会存在队列溢出的问题，从而导致服务端无法响应客户端的请求，这就是 syn flood 攻击。\n为了防止 syn flood 攻击，引入了 syn cookies 机制，该机制并非 tcp 协议的一部分。原理参见：深入浅出TCP中的SYN-Cookies\n一旦开启了 syn cookies 机制后，即使 syn 队列满，仍可以对新建的连接回复 syn + ack 报文，但是不需要进入队列。\n因为 syn cookies 存在部分缺陷，只有当 syn 队列满时该特性才会生效。\nnet.ipv4.tcp_abort_on_overflow 在三次握手完成后，该连接会进入到 ESTABLISHED 状态，并将该连接放入到用户程序队列中。若该队列已满，默认会将该连接重新设置为 SYN_ACK 状态，相当于是服务端没有接收到客户端的 syn + ack 报文，后续可以利用客户端的重传机制重新接收报文。\n一旦开启了 net.ipv4.tcp_abort_on_overflow 选项后，会直接发送 RST 报文给到客户端，客户端会终止该连接，并报错 104 Connection reset by peer。\nnet.core.somaxconn 全连接队列的最大值，该配置为全局默认配置。单个 socket 的全连接队列的长度选择为 Min(backlog, somaxconn)。\n内核源码 在整个内核模块中主要涉及到 listen() 和 accept() 系统调用，listen() 系统调用的作用是申请和初始化半连接队列和全连接队列。队列位于内核代码的 include/net/inet_connection_sock.h 中的如下位置:\n1 2 3 4 5 6 struct inet_connection_sock { /* inet_sock has to be the first member! */ struct inet_sock\ticsk_inet; struct request_sock_queue icsk_accept_queue; // 队列 struct inet_bind_bucket\t*icsk_bind_hash; } 如何参看 查看 tcp 状态为 SYN_RECV 的链接即为半连接状态的请求：netstat -napt | grep SYN_RECV。也可以通过 netstat -s | grep 'SYNs to LISTEN' 查看。\n全连接队列可以使用 ss -nlt | grep 8080 的方式查看 Recv-Q 的值。 也可通过如下命令来查看全连接队列的溢出情况。\n1 2 $netstat -s | grep overflow 3255 times the listen queue of a socket overflowed ","date":"2018-12-01T21:24:07Z","permalink":"/post/linux-tcp-backlog/","title":"Linux TCP backlog"},{"content":"概念 中断由硬件产生，并发送到中断控制器，中断控制器再发送中断到CPU，CPU检测到中断信号后，会中断当前的工作，每个中断都有IRQ（中断请求），基于IRQ，CPU将中断请求分发到对应的硬件驱动上通知操作系统，操作系统会对中断进行处理。\n中断控制器 常见的中断控制器有两种：可编程中断控制器8259A和高级可编程中断控制器（APIC）。传统的8259A只适合单CPU的情况，现在都是多CPU、多核心的SMP体系，所以为了充分利用SMP体系结构，把中断传递给系统上的每个CPU以便更好实现并行和提高性能，Intel引入了高级可编程中断控制器（APIC）。\n光有高级可编程中断控制器的硬件支持还不够，Linux内核还必须能利用这些硬件的特质，所以只有kernel 2.4以后的版本才支持把不同的硬件中断请求（IRQs）分配到特定的CPU核心上，这个绑定技术被称为SMP IRQ Affinity。\n在设置网卡中断的cpu core时，有一个限制就是，IO-APIC 有两种工作模式：logic 和 physical，在 logic 模式下 IO-APIC 可以同时分布同一种 IO 中断到8颗 CPU (core) 上（受到 bitmask 寄存器的限制，因为 bitmask 只有8位长。）；在 physical 模式下不能同时分布同一中断到不同 CPU 上，比如，不能让 eth0 中断同时由 CPU0 和 CPU1 处理，这个时候只能定位 eth0 到 CPU0、eth1 到 CPU1，也就是说 eth0 中断不能像 logic 模式那样可以同时由多个 CPU 处理。\n软中断和硬中断 为了解决中断处理程序执行时间过长和中断丢失的问题，Linux系统将中断分为上半部和下半部。\n上半部在中断禁止模式下运行，用来快速处理中断，主要用来处理跟硬件密切相关的工作。\n下半部处理上半部未完成的工作，通常以内核线程的方式运行。\n以Linux接收网卡数据包为例进行说明：\n网卡接收到一个数据包后，会通过硬件中断的方式通知内核新的数据到了。内核会调用中断处理程序进行处理。\n上半部将网卡中的数据写入到内存中，并更新一下硬件寄存器的状态，最后发送一个软中断信号，通知下半部进一步的处理。\n下半部被软中断信号唤醒后，从内存中读到数据，按照网络协议栈对数据进行解析和处理，并发送给应用程序。\n上面所说的上半部即硬中断，下半部即软中断，但一些内核自定义的事件也属于软中断，比如内核调度和RCU锁等。\n硬中断：由外设产生，用来通知操作系统外设状态的变化。在处理中断的同时要关闭中断。特点为处理要尽可能的快。\n软中断：为了满足实时性要求，硬中断处理时间都比较短，将时间比较长的中断放到软中断中来完成，称为下半部。int就是软中断指令，中断向量表是中断号和中断处理程序的对应表。每个CPU对应一个软中断内核线程ksoftirqd/cpu编号。\n1 2 3 4 5 [root@120-14-29-SH-1037-B07 ~]# ps -ef | grep ksoft root 3 2 0 2016 ? 01:47:12 [ksoftirqd/0] root 21 2 0 2016 ? 00:47:51 [ksoftirqd/1] root 26 2 0 2016 ? 00:47:34 [ksoftirqd/2] root 31 2 0 2016 ? 00:47:46 [ksoftirqd/3] 中断嵌套：硬中断可以嵌套，即新的硬中断可以打断正在执行的中断，但同种中断不可以。软中断不可嵌套，但相同类型中断可在不同的cpu上执行。\n相关命令 mpstat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 显示cpu处理的中断数量 [root@103-17-164-sh-100-k07 ~]# mpstat -I SUM 1 Linux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU) 05:27:59 PM CPU intr/s 05:28:00 PM all 61274.00 05:28:01 PM all 61712.00 05:28:02 PM all 62315.00 05:28:03 PM all 59280.00 ^C Average: all 61145.25 # 显示每个核处理的中断数量 [root@103-17-164-sh-100-k07 ~]# mpstat -I SUM 1 -P ALL Linux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU) 05:30:30 PM CPU intr/s 05:30:31 PM all 61446.00 05:30:31 PM 0 40489.00 05:30:31 PM 1 6839.00 05:30:31 PM 2 6935.00 05:30:31 PM 3 7185.00 # 显示更详细的信息 [root@120-14-31-SH-1037-B07 ~]# mpstat -P ALL 1 Linux 3.10.0-327.el7.x86_64 (120-14-31-SH-1037-B07.yidian.com) 09/10/2017 _x86_64_ (4 CPU) 01:15:09 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle 01:15:10 AM all 6.35 0.00 11.64 0.00 0.00 7.67 0.00 0.00 0.00 74.34 01:15:10 AM 0 5.05 0.00 11.11 0.00 0.00 0.00 0.00 0.00 0.00 83.84 01:15:10 AM 1 6.38 0.00 12.77 0.00 0.00 9.57 0.00 0.00 0.00 71.28 01:15:10 AM 2 8.70 0.00 10.87 0.00 0.00 14.13 0.00 0.00 0.00 66.30 01:15:10 AM 3 6.32 0.00 12.63 0.00 0.00 7.37 0.00 0.00 0.00 73.68 lspci 可以来查看网卡型号，驱动等信息，内容较多\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 [root@103-17-164-sh-100-k07 ~]# lspci -vvv | more 00:00.0 Host bridge: Intel Corporation Xeon E7 v2/Xeon E5 v2/Core i7 DMI2 (rev 04) Subsystem: Super Micro Computer Inc Device 0668 Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx- Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast \u0026gt;TAbort- \u0026lt;TAbort- \u0026lt;MAbort- \u0026gt;SERR- \u0026lt;PERR- INTx- Interrupt: pin A routed to IRQ 0 Capabilities: [90] Express (v2) Root Port (Slot-), MSI 00 DevCap: MaxPayload 128 bytes, PhantFunc 0 ExtTag- RBE+ DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported- RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop- MaxPayload 128 bytes, MaxReadReq 128 bytes DevSta: CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend- LnkCap: Port #0, Speed 5GT/s, Width x4, ASPM not supported, Exit Latency L0s \u0026lt;64ns, L1 \u0026lt;16us ClockPM- Surprise+ LLActRep+ BwNot+ LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk- ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt- LnkSta: Speed unknown, Width x0, TrErr- Train- SlotClk- DLActive- BWMgmt- ABWMgmt- RootCtl: ErrCorrectable- ErrNon-Fatal- ErrFatal- PMEIntEna- CRSVisible- RootCap: CRSVisible- RootSta: PME ReqID 0000, PMEStatus- PMEPending- DevCap2: Completion Timeout: Range BCD, TimeoutDis+, LTR-, OBFF Not Supported ARIFwd- DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled ARIFwd- LnkCtl2: Target Link Speed: 2.5GT/s, EnterCompliance- SpeedDis- Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS- Compliance De-emphasis: -6dB LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete-, EqualizationPhase1- EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest- Capabilities: [e0] Power Management version 3 Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold+) Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME- Capabilities: [100 v1] Vendor Specific Information: ID=0002 Rev=0 Len=00c \u0026lt;?\u0026gt; Capabilities: [144 v1] Vendor Specific Information: ID=0004 Rev=1 Len=03c \u0026lt;?\u0026gt; Capabilities: [1d0 v1] Vendor Specific Information: ID=0003 Rev=1 Len=00a \u0026lt;?\u0026gt; Capabilities: [280 v1] Vendor Specific Information: ID=0005 Rev=3 Len=018 \u0026lt;?\u0026gt; ... ethtool 用来查看网卡信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 [root@103-17-164-sh-100-k07 ~]# ethtool eth3 Settings for eth3: Supported ports: [ FIBRE ] Supported link modes: 1000baseT/Full 10000baseT/Full Supported pause frame use: No Supports auto-negotiation: Yes Advertised link modes: 1000baseT/Full 10000baseT/Full Advertised pause frame use: No Advertised auto-negotiation: Yes Speed: 10000Mb/s Duplex: Full Port: FIBRE PHYAD: 0 Transceiver: external Auto-negotiation: on Supports Wake-on: d Wake-on: d Current message level: 0x00000007 (7) drv probe link Link detected: yes # 可查看Ring buffer的大小 [root@103-17-164-sh-100-k07 ~]# ethtool -g eth3 Ring parameters for eth3: Pre-set maximums: RX: 4096 RX Mini: 0 RX Jumbo: 0 TX: 4096 Current hardware settings: RX: 512 RX Mini: 0 RX Jumbo: 0 TX: 512 # 列出信息较多，包含网卡的统计信息，包括丢包量信息 [root@103-17-164-sh-100-k07 ~]# ethtool -S eth3 | more NIC statistics: rx_packets: 680955795162 tx_packets: 27260701850 rx_bytes: 248285654162670 tx_bytes: 195321924245892 rx_pkts_nic: 683081802539 tx_pkts_nic: 27260700665 rx_bytes_nic: 251132784690871 tx_bytes_nic: 195447730645152 lsc_int: 11 tx_busy: 0 non_eop_descs: 1811095697 rx_errors: 67381 tx_errors: 0 rx_dropped: 0 tx_dropped: 0 multicast: 1025690658 broadcast: 206937242 rx_no_buffer_count: 0 collisions: 0 rx_over_errors: 0 rx_crc_errors: 67302 rx_frame_errors: 0 hw_rsc_aggregated: 2358414213 hw_rsc_flushed: 232402327 fdir_match: 10634169417 fdir_miss: 669277016191 fdir_overflow: 3321 rx_fifo_errors: 0 rx_missed_errors: 2538 tx_aborted_errors: 0 tx_carrier_errors: 0 tx_fifo_errors: 0 tx_heartbeat_errors: 0 tx_timeout_count: 0 tx_restart_queue: 0 rx_long_length_errors: 80264 rx_short_length_errors: 0 tx_flow_control_xon: 1 rx_flow_control_xon: 0 tx_flow_control_xoff: 51 rx_flow_control_xoff: 0 rx_csum_offload_errors: 0 alloc_rx_page_failed: 0 alloc_rx_buff_failed: 0 rx_no_dma_resources: 0 os2bmc_rx_by_bmc: 0 os2bmc_tx_by_bmc: 0 os2bmc_tx_by_host: 0 os2bmc_rx_by_host: 0 # 查看网卡多队列的支持情况，当前网卡支持8个队列，使用了8个队列 [root@103-17-6-sh-100-j11 ~]# ethtool -l eth0 Channel parameters for eth0: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 8 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 8 # 设置网卡当前使用的多队列，当前使用的网卡数量不能超过最大值8，该值跟网卡的中断数量一一对应，即/proc/interrupts中看到的eth0的中断数量 [root@103-17-6-sh-100-j11 ~]# ethtool -L eth0 combined 2 [root@103-17-6-sh-100-j11 ~]# ethtool -l eth0 Channel parameters for eth0: Pre-set maximums: RX: 0 TX: 0 Other: 1 Combined: 8 Current hardware settings: RX: 0 TX: 0 Other: 1 Combined: 2 sar 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 列出网卡的接收包信息，比iftop更直观 [root@103-17-164-sh-100-k07 ~]# sar -n DEV 1 Linux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU) 05:06:12 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s 05:06:13 PM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 05:06:13 PM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 05:06:13 PM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 05:06:13 PM eth3 77893.00 4328.00 31413.92 30134.16 0.00 0.00 46.00 05:06:13 PM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 # 可列出错误包的相关信息 [root@103-17-164-sh-100-k07 ~]# sar -n EDEV 1 Linux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU) 05:07:13 PM IFACE rxerr/s txerr/s coll/s rxdrop/s txdrop/s txcarr/s rxfram/s rxfifo/s txfifo/s 05:07:14 PM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 05:07:14 PM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 05:07:14 PM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 05:07:14 PM eth3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 05:07:14 PM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 通过中断可以看到网卡包含四个中断55-58，均位于cpu0上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 [root@103-17-164-sh-100-k07 ~]# cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 0: 44 0 0 0 IR-IO-APIC-edge timer 1: 3 0 0 0 IR-IO-APIC-edge i8042 8: 42 0 0 0 IR-IO-APIC-edge rtc0 9: 1 0 0 0 IR-IO-APIC-fasteoi acpi 12: 4 0 0 0 IR-IO-APIC-edge i8042 16: 99 0 0 0 IR-IO-APIC-fasteoi ehci_hcd:usb1 18: 0 0 0 0 IR-IO-APIC-fasteoi i801_smbus 23: 83 0 0 0 IR-IO-APIC-fasteoi ehci_hcd:usb2 37: 12019917 0 0 0 IR-PCI-MSI-edge 0000:00:1f.2 48: 0 0 0 0 DMAR_MSI-edge dmar0 55: 746827548 0 0 0 IR-PCI-MSI-edge eth3-TxRx-0 56: 2674693551 0 0 0 IR-PCI-MSI-edge eth3-TxRx-1 57: 2341522223 0 0 0 IR-PCI-MSI-edge eth3-TxRx-2 58: 3587929355 0 0 0 IR-PCI-MSI-edge eth3-TxRx-3 59: 3334 0 0 0 IR-PCI-MSI-edge eth3 61: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 63: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 64: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 65: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 66: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 67: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 68: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 69: 2 0 0 0 IR-PCI-MSI-edge ioat-msix NMI: 1100069 693493 635982 615953 Non-maskable interrupts LOC: 1120358899 146726541 4134846029 168005659 Local timer interrupts SPU: 0 0 0 0 Spurious interrupts PMI: 1100069 693493 635982 615953 Performance monitoring interrupts IWI: 57255892 115292160 113458706 112987848 IRQ work interrupts RTR: 0 0 0 0 APIC ICR read retries RES: 525229423 2791640970 427214674 1986396041 Rescheduling interrupts CAL: 4294536344 4294488238 4294478163 4294552026 Function call interrupts TLB: 65239533 57937650 55104990 52690662 TLB shootdowns TRM: 0 0 0 0 Thermal event interrupts THR: 0 0 0 0 Threshold APIC interrupts MCE: 0 0 0 0 Machine check exceptions MCP: 160132 160132 160132 160132 Machine check polls ERR: 0 MIS: 0 修改中断的cpu分配 echo \u0026ldquo;2\u0026rdquo; \u0026gt; /proc/irq/49/smp_affinity\n其中2表示cpu1, 49表示中断号。\n查看软中断 1 2 3 4 5 6 7 8 9 10 11 12 [root@120-14-31-SH-1037-B07 ~]# cat /proc/softirqs CPU0 CPU1 CPU2 CPU3 HI: 1 3 0 1 TIMER: 1795378091 3617740778 2674553229 1524071492 NET_TX: 202188392 22218135 17427628 17205883 NET_RX: 3388060179 60871361 68145291 38670323 BLOCK: 4741950 2422 1309 1489 BLOCK_IOPOLL: 0 0 0 0 TASKLET: 2102131738 3720214 3104944 1942912 SCHED: 526046585 612421231 496815061 456047989 HRTIMER: 0 0 0 0 RCU: 3147020579 4237695975 3820083676 3267816268 软中断包括10个类别，NET_RX（网络接收中断）、NET_TX（网络发送中断）\n查看数据包统计 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 [root@c1-g08-120-166-30 ~]# netstat -s Ip: 7586513384 total packets received 0 forwarded 741 with unknown protocol 0 incoming packets discarded 7586512643 incoming packets delivered 7948370396 requests sent out Icmp: 23 ICMP messages received 0 input ICMP message failed. ICMP input histogram: destination unreachable: 1 echo requests: 19 echo replies: 3 46 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 9 echo request: 18 echo replies: 19 IcmpMsg: InType0: 3 InType3: 1 InType8: 19 OutType0: 19 OutType3: 9 OutType8: 18 Tcp: 561299810 active connections openings 2005002 passive connection openings 8 failed connection attempts 282644949 connection resets received 725 connections established 7585817181 segments received 13957880471 segments send out 1742807 segments retransmited 136 bad segments received. 523811266 resets sent Udp: 445457 packets received 3 packets to unknown port received. 0 packet receive errors 553840367 packets sent 0 receive buffer errors 0 send buffer errors UdpLite: InErrors: 3 TcpExt: 341304 invalid SYN cookies received 1 resets received for embryonic SYN_RECV sockets 1 ICMP packets dropped because they were out-of-window 1997421 TCP sockets finished time wait in fast timer 222787 delayed acks sent 1819 delayed acks further delayed because of locked socket Quick ack mode was activated 178186 times 13 packets directly queued to recvmsg prequeue. 3280604069 packet headers predicted 1972740996 acknowledgments not containing data payload received 798190042 predicted acknowledgments 642444 times recovered from packet loss by selective acknowledgements Detected reordering 23 times using FACK Detected reordering 1618 times using SACK 59 congestion windows fully recovered without slow start 16 congestion windows partially recovered using Hoe heuristic 17089 congestion windows recovered without slow start by DSACK 9594 congestion windows recovered without slow start after partial ack TCPLostRetransmit: 2849 4926 timeouts after SACK recovery 672451 fast retransmits 28946 forward retransmits 680 retransmits in slow start 5024 other TCP timeouts TCPLossProbes: 1390602 TCPLossProbeRecovery: 997235 4723 SACK retransmits failed 178189 DSACKs sent for old packets 979863 DSACKs received 62 DSACKs for out of order packets received 282645553 connections reset due to unexpected data 9 connections reset due to early user close TCPDSACKIgnoredNoUndo: 936061 TCPSpuriousRTOs: 5150 TCPSackShifted: 233309 TCPSackMerged: 733778 TCPSackShiftFallback: 1768422 IPReversePathFilter: 1 TCPRetransFail: 11 TCPRcvCoalesce: 1290965687 TCPOFOQueue: 1753072 TCPChallengeACK: 136 TCPSYNChallenge: 136 TCPSpuriousRtxHostQueues: 73 TCPAutoCorking: 272452106 TCPSynRetrans: 4538 TCPOrigDataSent: 9260789170 TCPHystartTrainDetect: 3721895 TCPHystartTrainCwnd: 64417412 TCPHystartDelayDetect: 80 TCPHystartDelayCwnd: 2579 TCPACKSkippedSynRecv: 4 IpExt: InMcastPkts: 249744 OutMcastPkts: 83317 InBcastPkts: 226 InOctets: 12312144006244 OutOctets: 12449967070063 InMcastOctets: 22309104 OutMcastOctets: 9664620 InBcastOctets: 104240 InNoECTPkts: 7586513474 InECT0Pkts: 47 irqbalance 查看是否运行：systemctl status irqbalance\nirqbalance根据系统中断负载的情况，自动迁移中断保持中断的平衡，同时会考虑到省电因素等等。 但是在实时系统中会导致中断自动漂移，对性能造成不稳定因素，在高性能的场合建议关闭。\nirqbalance用于优化中断分配，它会自动收集系统数据以分析使用模式，并依据系统负载状况将工作状态置于 Performance mode 或 Power-save mode。处于Performance mode 时，irqbalance 会将中断尽可能均匀地分发给各个 CPU core，以充分利用 CPU 多核，提升性能。 处于Power-save mode 时，irqbalance 会将中断集中分配给第一个 CPU，以保证其它空闲 CPU 的睡眠时间，降低能耗。\nref Linux 多核下绑定硬件中断到不同 CPU（IRQ Affinity） 深度剖析告诉你irqbalance有用吗？ 怎么理解Linux软中断？ ","date":"2018-11-30T21:09:20Z","permalink":"/post/linux%E4%B8%AD%E6%96%AD/","title":"Linux中断"},{"content":"本文为极客时间专栏从0开始学习微服务的阅读笔记。\n在了解微服务之前，先来了解一下单体应用。\n在上学那会，做过一些企业站，技术往往是基于LAMP（Linux + Apache + MySQL + PHP），这种业务较为简单的企业站，就是单体应用。所有的业务代码都是放在一个PHP程序中，代码只需要我自己来维护就可以了，测试、上线、运维全部搞定。\n但这种单体应用要是放到稍微有点规模的互联网公司中必然是行不通的。一个公司里有很多的人，不可能公司这么多技术人员共同维护一套代码，这样子团队的写作必然是个问题。系统的健壮性也会比较差，一旦单体应用中的一个模块出问题后往往会影响到其他的模块，导致整个系统不可用，比如PHP的服务业界通常使用php-fpm来管理，而php-fpm处理连接的方式一个请求一个线程，当一部分请求因为延时高时会消耗过多的线程资源，导致其他请求没有可用的线程可以处理。\n单体应用的其他缺点不再罗列，比如代码膨胀过度、发布较慢、系统高可用性差、团队协作成本高等。\n为了解决单体应用的这些缺点，方法只有一个就是将单体应用拆分为多个服务即服务化，服务之间通过RPC的方式相互调用。\n即服务化后，业界又提出了微服务的概念。\n维基百科中有如下定义：\n2014年，Martin Fowler 与 James Lewis 共同提出了微服务的概念，定义了微服务是由以单一应用程序构成的小服务，自己拥有自己的行程与轻量化处理，服务依业务功能设计，以全自动的方式部署，与其他服务使用 HTTP API 通讯。同时服务会使用最小的规模的集中管理 (例如 Docker) 能力，服务可以用不同的编程语言与数据库等元件实作。\n但看这个定义，看的我一脸懵逼，实在太过抽象。其实微服务也没有一个特别明确的定义。\n那么服务化和微服务之间有什么不同之处呢？\n服务拆分粒度更细。 每个微服务都独立部署和维护。 微服务需要服务治理。由于微服务会将服务变多，势必需要一个服务管理平台来对微服务进行管理。 将单体应用向微服务拆分的方式可以分为纵向拆分和横向拆分。这里以一点资讯app为例来解释纵向拆分和横向拆分。\n一点资讯分为信息流、正文页等模块，而这些功能都依赖于用户信息获取模块，纵向拆分即将信息流、正文页拆分为微服务，横向拆分即将信息流、正文页都依赖的用户信息获取模块拆分为单独的服务。\n微服务架构 采用微服务后会带来一系列复杂的问题，新技术在解决了一部分问题的同时，总会带来一些新的问题，比如服务怎么定义接口、服务的发布方式和服务发现、服务的监控、服务的治理（包括依赖关系梳理、熔断机制等）、故障快速定位等。\n那么一个标准的微服务架构应该长什么样子呢？\n服务提供方在服务启动时向服务注册中心注册服务，声明自己能够提供的服务及当前服务的地址等信息。\n服务调用者请求注册中心，查询所要调用服务的地址，并通过约定好的协议向服务提供者发起请求，获取到结果后按照数据协议格式将数据进行反序列化。\n在整个服务的调用过程中，服务的请求耗时、调用量、调用成功与否等信息都会作为监控记录下来，服务的调用关系会通过trace系统记录下来，以便用于后续的故障定位和追踪。如果服务调用失败，则需要服务治理的方式来保证调用方的正常运行。\n服务的接口定义 需要解决的问题是服务的接口有哪些？每个接口的输入什么？每个接口的输出是什么？\nRESTful API HTTP协议的接口定义通常使用该协议，常见Wiki或者Swagger的方式来管理。\nIDL文件 IDL（interface description language）用于描述接口，使得不同的编程语言不同的平台服务之间可以相互通讯。常见实现包括Thrift和protobuf，protobuf作为序列化方式的一种，通常会使用gRPC进行通讯。\nXML文件方式 服务提供者将接口描述信息保存在xml配置文件，服务启动后会加载配置文件，将服务暴露出去。\n服务消费者将要调用的接口信息写在xml配置文件中，进程启动后加载xml配置文件。\n坑 服务提供者通常需要提供服务的超时时间等参数，而消费者端也需要该信息，为了让消费者端能看到生产者端的配置，可以将信息放在配置中心中，但这却增大了配置中心的内容，当一旦配置变化需要同步时，同步的数据会变多。\n注册中心 要想实现服务之间的调用，通常会使用反向代理和服务注册中心的方法。\n反向代理的方法业界一般使用较多的包括nginx、haproxy以及业界新秀envoy等。\n在微服务架构中更多提及的方案为采用服务注册中心的方法，而注册中心的稳定性就显得尤其重要。\n业界注册中心采用较多的方案为zookeeper、etcd、consul、eureka。这些注册中心的存储数结构要么是树状接口，要么是key-value形式，其中k-v形式的可以变更key的值以实现类似树状结构。\n注册中心的设计 注册中心API及功能 注册中心需要提供以下api以供服务提供方和服务调用方使用。\n服务注册接口，提供服务提供方使用 服务反注册接口，提供服务提供方以便销毁服务 心跳汇报接口，服务提供方通过心跳汇报服务是存活状态的。一旦当服务出现异常时，服务注册中心应该立即将服务从注册中心剔除。 服务订阅接口，服务调用方用于获取服务提供方的实例列表 服务变更接口，服务调用方用于获取最新可用服务。一旦注册中心探测到有新的服务实例或者实例减少，应该立即通知所有订阅该服务的服务调用者更新本地的节点信息。 注册中心存储哪些服务信息 通常可以按照“服务名-分组-节点信息”三层结构来存储，其中节点信息包括：节点ip地址、节点端口号、请求失败时重试次数、请求结果是否压缩。\n分组的划分原则包括：\n按照业务的核心程度 按照机房维度 线上环境、测试环境 注册中心如何工作 服务提供者注册节点 查看注册节点是否在白名单内，即是否可以向注册中心注册 查看注册的服务名、服务分组是否存在 将节点信息添加到对应的存储位置 服务提供者反注册 查看服务名、服务分组对应的服务是否存在 将节点删除 服务消费者查询节点信息 从本机内存中查找服务信息 如果有本地快照存储可以从中查找 服务消费者订阅服务变更 消费者获取到服务信息后，在本地保留cluster的sign值 每个一段时间从注册中心获取cluster的sign值，如果不一致，就从注册中心拉取服务节点，并更新内存环境和本地快照 服务框架 完整的服务框架包括：通讯框架、通讯协议、序列化和反序列化。\n开源RPC框架 跟语言相关的框架：Dubbo、Motan（微博）、Tars（腾讯）、Spring Cloud\n跨平台的开源RPC框架：gRPC、Thrift\n服务监控 常用的开源监控软件包括：ELK、Graphite、TICK、Prometheus\n服务追踪 使用分布式会话跟踪技术，利用traceid\n开源方案包括OpenZipkin、jaeger等\n服务治理 通过一系列的手段保证在意外情况下，服务仍然能够正常运行。\n节点管理 服务调用失败可能是服务提供者自身出现了问题，也可能是网络问题导致。\n1.注册中心自动摘除机制\n服务提供者和注册中心之间保持心跳，当超时后注册中心自动摘除服务提供者。\n2.服务消费者摘除\n将服务提供者的探活机制放到消费者端，消费者在探测到服务提供者失败后自动摘除服务提供者。这种情况可以避免服务提供者和注册中心之间网络出现异常，但是服务提供者和服务消费者之间可以通讯的情况。\n负载均衡 常用的包括随机算法、轮询、加权轮询算法、最少活跃调用、一致性hash算法（可以配合着静态注册中心达到比较好的效果）\n自适应最优选择算法：在客户端维护一份每一个服务节点的性能统计快照，每隔一段时间去更新快照。在发起请求时，根据二八原则，将服务节点分成两部分，找出20%的那部分响应最慢的节点并降低权重。也可称为动态加权轮询算法。1分钟的更新时间间隔是个不错的选择。\n个人感觉自适应最优选择算是个不错的选择，但还可以针对业务场景继续优化，比如权重进行动态调整。\n服务路由 用于限定服务消费者可选择服务提供者节点的范围。\n应用场景：分组调用（组的划分可以按照机房等维度）、灰度发布、流量切换（比如机房故障后，用于机房之间的流量调度）、读写分离（读接口部署在一起，写接口部署在一起）。\n规则的写法 A.条件路由\n某个ip的消费者仅访问某个ip的服务提供者 排除某个服务节点（所有的服务消费者都不能访问某个服务提供者） 白名单和黑名单 机房级别的隔离（可以基于ip地址的规则来做） 读写分离（所有get类方法仅访问某些节点等） B.脚本路由\n使用脚本语言的形式来描述\n路由的获取方式 本地配置：存储在服务消费者本地 配置中心配置，可以修改规则后动态下发 服务容错 手段包括超时、重试、双发、熔断等。\n双发的思路为服务调用者在发起一次服务调用后，在给定的时间内（该时间要比服务超时的时间短）没有得到结果，再发起另外一个请求。\n熔断的思路为在某一个时间内如果服务调用失败次数超过一定值，则触发熔断，不再向服务提供者发起请求。\n断路器中的状态包括：Closed、Open、Half Open（半打开状态，用于探测后端服务是否已经正常）\nHystrix是最出名的熔断器，计算服务调用的失败率是通过滑动窗口来实现，滑动窗口内包含10个桶，每个桶为1秒内的服务调用情况。\nFailOver，失败自动切换。消费者调用失败后，自动从可以节点中选择下一个节点重新调用，可设置失败的次数。通常适合只读的场景。\nFailBack，失败通知。调用失败后，不能重试，而是根据失败的信息来决定后续的执行策略。通常用于写场景。\nFailCache，失败缓存。调用失败后，隔一段时间后再重试。\nFailFast，快速失败。调用失败后不再重试。\n如何识别服务节点是否存活 如果注册中心为zk，在服务节点变化的时候，注册中心会向服务调用方推送服务节点变化的通知。但当网络抖动的时候，可能会存在节点的状态频繁变化的问题，导致服务消费者频繁收到节点变更通知，或者导致注册中心获取到的服务节点过少。可通过以下手段来避免该问题。\n服务端故障时的应对策略 故障包括：集群故障、单个idc故障、单机故障\n集群故障 比如触发bug、突发流量等\n限流：限制超出接口阈值的部分请求\n降级：一种思路为通过开关来实现。具体可以分为多种等级：一级降级对业务影响比较小，可以设置为自动降级；二级降级对业务有一定影响，设置为手工降级；三级降级需要谨慎操作。\n单idc故障 基于dns的流量切换：延时稍高，比较适合入口流量。\n基于rpc的分组流量切换：将之前单个机房内访问的流量切换为多个机房访问。\n单机故障 可以通过自动重启服务的手段来解决。但要避免一次性重启服务过多的问题。\n动态注册中心的保护机制 心跳开关保护机制，给注册中心设置一个开关，当开发打开时，即使网络频繁抖动，注册中心也不会通知消费者节点变更，后者设置一定的百分比打开该开关。正常情况下该开关可以不打开。 服务节点摘除保护机制，设定一个阈值比例，在出现网络抖动的情况下，注册中心也不会将超过这个阈值的节点给下掉，防止一下子下掉过多节点。该机制正常情况下，应该开启。 静态注册中心的保护机制 在服务消费者端来判断是否服务提供者是否存活。服务消费者调用某一个节点失败超过一定次数就将节点标记为不可用。并隔一段时间后再去探测该节点是否存活。\n注册中心的服务正常情况下不改变的，只有当服务在发布的时候才去修改注册中心中的节点。注册中心中的节点变化后仍然通知服务消费者，只是在网络出现抖动的时候，不再去通知。\n个人感觉这个思路更靠谱。\n服务治理平台 1.服务管理 包括服务上下线、节点添加和删除、服务查询、服务节点查询等\n2.服务治理 限流、降级、切流量\n3.服务监控 可以包含服务的tracing监控等\n4.问题定位\n5.日志查询\n6.服务运维 发布部署和扩缩容\n配置中心 业内采用的开源配置中心包括:\nSpring Cloud 功能相对较弱，变更配置需要通过git操作 Apollo 对Spring Boot的支持比较好 进阶内容 做好容量规划 包括容量评估和调度决策两方面。\n压测服务的单机最大容量可以采用区间加权的方式来计算，比如0 ~ 10ms区间权重为1，10 ~ 50ms区间权重为2,500ms以上权重为32，通过累加的方式计算出单机的权重，从而评估出单机最大容量。(该评估容量的方式非常实用)\n调度策略可以根据水位线来做决定，一条是安全线，一条是致命线。当水位线处于致命线需要立即扩容，当水位线回到安全线以上时可以进行缩容。（水位线的方式很赞）\n缩容的思路是采用逐步缩容，每隔5分钟判断一次水位线是否在致命线以上，并按照10%、30%的比例进行缩容。\n为了防止水位线的抖动，可以一分钟采集一个点，当5个点中的3个点都满足条件时才进行缩容。\n多机房部署 1.主从架构\n所有的写请求都发给主机房，主机房更新本机房的缓存和数据库，其他机房的缓存和数据库从主机房同步。主机房出现问题后，就没法更新了。\n2.独立机房架构\n缓存层，每个机房都有写请求，每个机房的写请求通过消息同步组件将写请求同步给另外一个机房。数据库mysql只有一个主库，另外机房的mysql同步数据到该机房。\n以上缓存消息同步组件的实现大概如下：\n包括两个模块reship和collector，reship负责将本机房的写请求发一份给别的机房，collector负责从别的机房读取写请求，并发给本机房的处理服务器。\n可以通过消息队列和rpc调用来实现reship和collector的通讯。\n3.多机房的数据一致性\n可通过消息对账机制来保证一致性，原理为通过一个单独的程序后面来验证是否一个写请求是否已经被所有的机房都处理，如果没有处理，则重新发送写请求。\n混合云部署 公有云上为了安全往往不部署数据库\nDevOps实践 持续集成：确保每一次代码的merge request都通过，分为四个阶段：build（开发分支代码的编译与单元测试）、package（开发分支代码打包成docker镜像）、deploy（开发分支代码部署到测试环境）、test（集成测试）。包括了代码检查和单元测试环节。\n持续交付：代码merge request到develop分支后，develop分支的代码能够在生产环境中测试通过，并进行小流量灰度验证，可以随时上线。分为五个阶段：build（develop分支的代码编译与单元测试）、package（develop分支代码打包）、deploy、test、canary（develop分支代码的小流量灰度验证）。\n持续部署：合并develop代码到master分支，并打包成docker镜像，并可随时上线。包括：build、package、clear、production。\nService Mesh Service Mesh技术以轻量级网络代理的方式与应用代码部署在一起，主要有两个关键点技术。\nSideCar用于转发服务之间的调用，在服务消费者端SideCar用于将请求转发到服务提供者端的SideCar中，在服务提供者端的SideCar在接收到请求后转发给本机上的服务提供者。\nSideCar实现方式有基于ipstables的网络拦截和直接转发请求两种方式。\nControl plane用于基于SideCar的服务调用的治理，用来取代微服务中需要服务框架干的事情，包括服务发现、负载均衡、请求路由、故障处理、安全认证、监控上报、日志记录、配额限制等。\nIstio Pilot主要用于流量控制，包括Rules API（提供API，用于流量控制）、Envoy API（给Envoy提供API，获取服务注册信息、流量控制信息等）、抽象模型（对服务注册信息、流量控制进行抽象）、平台适配层（适配k8s、Mesos等多个平台，将平台特定的注册信息转换成平台无关的抽象模型）。\nPilot的流量控制功能包括服务发现和负载均衡、请求路由、超时重试、故障注入。\nMixer实现策略控制和监控日志收集等功能，理论上Envoy发起的每次请求都会发送到Mixer，可以异步发送。\nMixer的策略控制包括对服务的访问频率限制和访问控制。\nCitadel用于保证服务之间的安全，需要Envoy的配合。Citadel中存储了秘钥和证书，通过Pilot将授权策略和安全命名信息分发给Envoy，Envoy和Envoy之间通过双向TLS证书来进行通讯，由Mixer来管理授权和审计。\nref 微服务架构技术栈选型手册 ","date":"2018-11-25T20:27:17Z","permalink":"/post/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E5%AD%A6%E4%B9%A0%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","title":"从0开始学习微服务阅读笔记"},{"content":"Nagle算法为了避免网络中存在太多的小数据包，尽可能发送大的数据包。定义为在任意时刻，最多只有一个未被确认的小段。小段为小于MSS尺寸的数据块，未被确认是指数据发出去后未收到对端的ack。\nNagle算法是在网速较慢的时代的产物，目前的网络环境已经不太需要该机制，该算法在linux系统中默认关闭。\n延时ACK机制: 在接收到对端的报文后，并不会立即发送ack，而是等待一段时间发送ack，以便将ack和要发送的数据一块发送。当然ack不能无限延长，否则对端会认为包超时而造成报文重传。linux采用动态调节算法来确定延时的时间。\n可以举例来描述一下，client连续向server端发送两个小于MSS的数据包。client发送第一个数据包，根据Nagle算法，此时没有未确认的数据段，该数据包可以直接发送。server端接收到数据包后，由于延时ACK机制，并不会立即发送ack，而是需要等到延时ack机制超时后再发送第二个数据包。此时client端由于Nagle算法， 存在一个未被确认的数据包，不能向server端发送第二个数据包。\n在延时要求尽量小的情况下，并不适合用Nagle算法，比如SSH会话。可以通过设置TCP_NODELAY来完成。\n","date":"2018-11-23T20:31:04Z","permalink":"/post/tcp%E5%8D%8F%E8%AE%AE%E4%B8%AD%E7%9A%84nagle%E7%AE%97%E6%B3%95/","title":"TCP协议中的Nagle算法"},{"content":"java 8u131之后的版本开始支持容器特性，之前的版本中并不支持容器相关的特性。\njava基础知识 JVM默认的最大堆内存大小为系统内存的1/4，可以使用参数-XX:MaxRAMFraction=1表示将所有可用内存作为最大堆。\ncgroup的限制在docker中能够看到，通过查看/sys/fs/cgroup目录下的文件可以获取。\nJVM的用户地址空间分为JVM数据区和direct memory。JVM数据区由heap、stack等组成，GC是操作的这一片内存。direct memory是额外划分出来的一片内存空间，需要手工管理内存的申请和释放。\ndirect memory使用Unsafe.allocateMemory和Unsafe.setMemory来申请和设置内存，是直接使用了C语言中的malloc来申请内存。由jvm参数MaxDirectMemorySize来限制direct memory可使用的内存大小。\njava \u0026lt; 8u131 没有对容器的任何支持，对cpu和内存的限制需要通过jvm的参数来配置。\njava中并不能看到内存资源的限制，会存在使用内存超过限制而被OOM的问题。可通过在程序中设置-Xmx来解决该问题。\nJVM GC（垃圾对象回收）对Java程序执行性能有一定的影响。默认的JVM使用公式“ParallelGCThreads = (ncpus \u0026lt;= 8) ? ncpus : 3 + ((ncpus * 5) / 8)” 来计算做并行GC的线程数，其中ncpus是JVM发现的系统CPU个数。一旦容器中JVM发现了宿主机的CPU个数（通常比容器实际CPU限制多很多），这就会导致JVM启动过多的GC线程，直接的结果就导致GC性能下降。Java服务的感受就是延时增加，TP监控曲线突刺增加，吞吐量下降。\n显式的传递JVM启动参数-XX:ParallelGCThreads告诉JVM应该启动几个并行GC线程。它的缺点是需要业务感知，为不同配置的容器传不同的JVM参数。\njava9 and java \u0026gt;= 8u131 增加了XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap参数来检查内存限制。JVM中可以看到cgroup中的内存限制。\n可以根据容器中的cpu限制来动态设置GC线程数，不再需要单独设置-XX:ParallelGCThreads。\njava10 jvm参数XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap已经默认开启，但新增加-XX:-UseContainerSupport参数来更好支持容器，支持内存和cpu。\n在开启-XX:-UseContainerSupport的同时，XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap会被关闭。\nref Java SE support for Docker CPU and memory limits 美团容器平台架构及容器技术实践 ","date":"2018-11-17T20:16:02Z","permalink":"/post/%E9%83%A8%E7%BD%B2java%E5%BA%94%E7%94%A8%E5%88%B0%E5%AE%B9%E5%99%A8/","title":"部署java应用到容器"},{"content":"\n题图为公司楼下公园的杨树林。时光易逝弹指间，又到一年叶落时。\n资源 1.runV\n基于 hypervisor 的 OCI runtime\n2.operator-sdk\noperator机制利用CRD机制增强了kubernetes的灵活性，但operator的编写代码很多模式都是固定的，该项目提供了更高层次的抽象。\n3.orchestrator\n用来管理mysql的集群拓扑和故障自动转移的工具。\n4.Tars\n腾讯开源的RPC框架，在腾讯内部已经有多年的使用历史，目前支持多种语言。\n5.ngx_http_dyups_module\nnginx module，可以提供Restful API的形式来动态修改upstream，而不用重新reload nginx。\n6.Dragonfly\n阿里巴巴开源的基于P2P的容器镜像分发系统。\n7.SonarQube\n开源的代码检查和扫描工具，支持多种语言，并提供了友好的web界面用来查看分析结果。\n8.QUIC\nQUIC是Google开发的基于UDP的传输层协议，提供了像TCP一样的数据可靠性，但降低了数据的传输延时，并具有灵活的拥塞控制和流量控制。\n9.OpenMessaging\n阿里巴巴发起的分布式消息的应用开发标准，目前github上的star数还较少。\n10.nsenter\nnsenter是一个命令行工具，用来进入到进程的linux namespace中。\ndocker提供了exec命令可以进入到容器中，nsenter具有跟docker exec差不多的执行效果，但是更底层，特别是docker daemon进程异常的时候，nsenter的作用就显示出来了，因此可以用于排查线上的docker问题。\n精彩文章 1.为何程序员永远是高薪行业\n从记者的视角来了解阿里云的历史。\n2.Harbor传奇（1）- Harbor前世\n3.蚂蚁金服 Service Mesh 实践探索\n4.美团容器平台架构及容器技术实践\n美团内部的容器平台HULK已经从第一代的自研升级为第二代的基于kubernetes的容器管理平台。由此可以反映出kubernetes在容器管理领域的地位。\n5.Serverless：后端小程序的未来\nServerless是未来软件架构的一个演进方向，包括BasS（Backend as a Service，后端即服务）和FaaS（Functions as a Service，函数即服务）两个组成部分。\nBaaS包括对象存储、数据库、消息队列等服务，并以API的形式提供应用依赖的后端服务。\nFaaS中的运行是通过事件触发的方式，代码执行完成后即运行结束，因此代码必须是无状态的。FaaS平台负责服务的自动扩容，并可做到按照服务的使用资源付费，以节省大量开支。\nServerless给开发人员带来了非常大的便利性，但同时也软件跟云平台绑定特别紧密。\n图书 1.《奈飞文化手册:“硅谷重要文件”的深度解读》\nNetflix公司的技术文化一直非常被业界推崇，可以从Netflix OSS已经开源的软件项目，很多的开源项目在社区也有不错的影响力，本书值得每一位技术从业者一读。\n精彩句子 我们要求大家做出的任何举动，出发点都是以对客户和公司最有利为出发点，而不是试图证明自己正确。\n- 奈飞文化手册:“硅谷重要文件”的深度解读\n","date":"2018-11-17T00:01:06Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC6%E6%9C%9F/","title":"知识分享第6期"},{"content":"golang中的pprof工具可以分析系统问题，开启pprof功能非常简单，即在import中增加_ \u0026quot;net/http/pprof\u0026quot;的导入即可，然后通过http调用/debug/pprof接口即可在web界面上看到pprof的相关信息。\ngolang 1.11版本中已经自带了火焰图功能，火焰图为性能分析的利器，可以快速找到程序性能的瓶颈。不再需要使用go-torch项目。\n查看火焰图需要用到Graphviz工具，该工具需要单独安装。\nGraphviz工具运行的服务器系统为CentOS，使用下载源码包的方式进行安装。\n依次执行下面命令：\n1 2 3 ./configure make make install 例子 本文的使用环境：服务器程序为transfer，运行在linux系统中。执行go tool pprof命令运行在linux服务器10.103.17.184上。\n程序运行后调用程序的/debug/pprof/profilehttp接口，可获取到cpu profile数据文件。例如，在服务器上可以执行运行wget http://10.103.34.138:3300/debug/pprof/profile命令，将profile文件下载到另外一台分析的服务器上。\n在分析服务器上执行go tool pprof -http=\u0026quot;10.103.17.184:20000\u0026quot; transfer profile\n在浏览器中打开10.103.17.184:20000即可得到性能分析的结果。\n同样也可以在本机访问远程的程序暴露的pprof数据，使用命令如：go tool pprof -http :9090 http://10.66.161.43:10245/debug/pprof/heap\n","date":"2018-11-16T22:07:49Z","permalink":"/post/golang%E4%BD%BF%E7%94%A8pprof%E5%88%86%E6%9E%90%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/","title":"golang使用pprof分析程序性能瓶颈"},{"content":"nsenter是一个命令行工具，用来进入到进程的linux namespace中。\ndocker提供了exec命令可以进入到容器中，nsenter具有跟docker exec差不多的执行效果，但是更底层，特别是docker daemon进程异常的时候，nsenter的作用就显示出来了，因此可以用于排查线上的docker问题。\nCentOS用户可以直接使用yum install util-linux来进行安装。\n启动要进入的容器：docker run -d ubuntu /bin/bash -c \u0026quot;sleep 1000\u0026quot;\n获取容器的pid可以使用`\n要进入容器执行如下命令：\n1 2 3 4 # 获取容器的pid docker inspect 9f7f7a7f0f26 -f \u0026#39;{{.State.Pid}}\u0026#39; # 进入pid对应的namespace sudo nsenter --target $PID --mount --uts --ipc --net --pid ref nsenter man page nsenter GitHub ","date":"2018-11-15T23:51:53Z","permalink":"/post/nsenter%E7%9A%84%E7%94%A8%E6%B3%95/","title":"nsenter的用法"},{"content":"\n题图为北京城西部的潭柘寺，始建于西晋年间，有“先有潭柘寺，后有幽州城”的说法。\n明朝燕王朱棣听取了重臣姚广孝的建议后，起兵“靖难”，并成功夺取皇位。朱棣继皇帝位后，姚广孝辞官到京西的潭柘寺隐居修行。据说当年修建北京城时，设计师就是姚广孝，他从潭柘寺的建筑和布局中获得了不少灵感。\n资源 1.virtual-kubelet\n很多公有云厂商都提供了弹性容器服务实例，比如阿里云的ECI（Elastic Container Instance）、AWS Fargate、Azure Container Instances等，但这些平台都提供了私有的API，与kubernetes的API不兼容。该项目将公有云厂商的的容器组虚拟为kubernetes集群中的一个超级node，以便支持kubernetes的API，与此同时失去了很多kubernetes的特性。\n2.Kata Containers\n容器在部署服务方面有得天独厚的优势，但受限于内核特性，在隔离性和安全性方面仍然较弱。虚拟机（VM）在隔离性和安全性方面都比较好，但启动速度和占用资源方面却不如容器。Kata Containers项目作为轻量级的虚拟机，但提供了快速的启动速度。同时支持Docker容器的OCI标准和kubernetes的CRI。目前华为公有云已经将此技术用于生产环境中。\n3.Knative\n在今年的Google Cloud Next大会上，Google发布了Knative, 这是由Google、Pivotal、Redhat和IBM等云厂商共同推出的Serverless开源工具组件，它与Istio，Kubernetes一起，形成了开源Serverless服务的三驾马车。\n4.naftis\n小米信息部武汉研发中心开源的istio的dashboard。\n5.kubespy\n用来查看kubernetes中资源实时变化的命令行工具。\n6.Md2All\n如果你已经习惯了markdown写作，在微信公账号发文时，可以使用该工具渲染后，将文章复制到微信公众号后台。\n精彩文章 1.阿里云的这群疯子\n从记者的视角来了解阿里云的历史。\n2.王垠最近博客-更新一下\n曾经以天才自居桀骜不驯傲视一切的垠神，突然变得温顺了许多，开始意识到自己的缺点，开始享受生活。\n3.为何“秀恩爱，死得快”？我是认真的\n用量子物理学的知识来解释为啥“秀恩爱，死得快”。\n4.CTO、技术总监、首席架构师的区别\n5.面对云厂商插管吸血，MongoDB使出绝杀\n半年后看下MongoDB的修改开源协议的做法在国内奏效否。\n6.终于明白了 K8S 亲和性调度\n通过该文章，已经差不多可以了解kubernetes调度的亲和性、反亲和性、taint和toleration机制了。\n7.微软资深工程师详解 K8S 容器运行时\n图书 1.鸟哥的Linux私房菜：基础学习篇（第四版）\n鸟哥的linux私房菜终于出新版了，最新版本是基于CentOS7的。\n电影 1.嗝嗝老师\n电影讲述了印度贫民窟中的孩子在学校上学总是遭受歧视不爱学习各种调皮捣蛋，在一位新老师来了后，将学生们带向正轨的故事。印度电影总能将平凡的电影演绎的很魔性，单就这些故事就已经足够了。偏偏这位老师还是抽动秽语综合征患者，在受到老师和学生们的双重歧视下，给故事情节增加了许多感人和励志色彩。\n强迫症患者谨慎观看，看完电影后，总感觉得抽搐两下才舒服。\n精彩句子 货币的贬值，是永恒的趋势。明天的物价，一定比今天的贵。 你想要赚钱，就一定要把今天的钱，换成明天的物。 而且时间越紧凑越好。\n\u0026ndash; 八年之后 房价多少？\n","date":"2018-10-26T21:02:27Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC5%E6%9C%9F/","title":"知识分享第5期"},{"content":"\n今年以来，网络上一直在传言济南市要吞并莱芜市的消息，最近几天尤甚。市民们纷纷去市政府门前拍照留念，纪念莱芜市的最后一天，虽然到今天为止传言还未变成现实，但应该是迟早要到来的。\n有趣的是，莱芜市是1993才从泰安市中独立出来，很多人都感慨道：出生是泰安人，长大是莱芜人，明天变成济南人。地理位置上而言，莱芜跟济南搭界，而且莱芜市是山东省17地市中面积最小的一个。\n山东的发展策略一直是各地市全面发展，济南市作为省会，在中国城市中的存在感确实不够强，吞并莱芜后也一样不会变强太多，一个地市要想变强，要从多方面找原因。\n资源 1.k8s-deployment-strategies\nkubernetes内置的deployment和statefulset对象往往很难满足企业的部署需求，比如蓝绿发布、金丝雀发布等，Github上的项目介绍了其他部署方式在kubernetes上的具体实现方式。\n2.Let\u0026rsquo;s Encrypt\nhttps越来越普及，通常CA颁发的证书都是收费的，Let\u0026rsquo;s Encrypt是一家非盈利的CA机构，为广大的小型站点和博客博主提供了非常大的帮助。Github pages中也是采用了Let\u0026rsquo;s Encrypt来提供自定义域名的https服务支持。\n3.openmetrics\n监控领域存在多款开源软件，比如premetheus、influxdb、opentsdb等，每种软件的写入数据格式都不一致，该开源项目旨在定义监控数据的标准格式，目前支持premetheus的文本格式和protobuf两种格式。该项目目前还在起步阶段，已经加入CNCF，期待后续一统行业标准。\n4.NATS\nGo语言实现的消息队列，目前已经加入CNCF。\n5.sequel fumpt\nsql的在线格式化工具。\n6.The Linux Audit Project\nLinux下的日志审计工具，CentOS系统下默认安装，可以通过man auditd看到该工具的说明。\n7.kafkabridge\n360开源的kafka客户端库的封装，只需调用极少量的接口，就可完成消息的生产和消费。支持多种语言：c++/c、php、python、golang。\n精彩文章 1.Keyhole,Google Maps发展史\n文章为微信公众号余晟以为的系列文章，大部分素材来源于《Never Lost Again》一书，该书作者为Bill Kilday，Keyhole和Google Maps团队的核心成员。文章介绍了Google Maps的前身Keyhole的创业史，后被Google收购后，又推出了基于web的Google Maps产品，继而开发出了Google Earth产品。即使在Google内部，也存在团队之间的孤立及不信任问题。\nKeyhole，Google Maps前传 Google Maps，Keyhole后传 从Google Maps到Google Earth 2.Kubernetes 调度器介绍\n文章对kubernetes的kube-scheduler的整体流程介绍的比较清晰。\n3.A Brief History of Alibaba Founders\n阿里巴巴的18罗汉介绍。\n图片 1.电传打字机设备(Teletype)\n早期的计算机设备比较笨重，计算机放在单独的一个房间中，操作计算机的人坐在另外一个房间中，通过终端机设备来操作计算机。\n早期的终端设备为电传打字机(Teletype)，该设备价格比较低廉，通过键盘输入，并将输出内容打印出来。图中的设备为ASR-33，在YouTube上可以可以看到视频。\n有意思的是，实际上Teletype的出现要早于计算机，原本用于在电报线路上发送电报，但是后来计算机出现后直接拿来作为计算机的终端设备。\n在linux操作系统中设计了tty子系统用于支持tty设备，并将具体的硬件设备放到/dev/tty*目录下，这里的tty设备即Teletype。不过后来随着其他终端设备的引入，tty这个名字仍旧保留了下来，tty目前已基本代表终端的总称。\n","date":"2018-10-14T01:13:02Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC4%E6%9C%9F/","title":"知识分享第4期"},{"content":"\n题图为北京玉渡山风景区中的盘山公路，旁边有个观景台，在观景台上可以鸟瞰官厅水库。\n资源 1.Intel RDT\nIntel RDT(Resource Director Technology)资源调配技术框架，包括高速缓存监控技术（CMT）、高速缓存分配技术（CAT）、内存带宽监控（MBM）和代码和数据优先级（CDP），容器技术的runc项目中使用到了CAT技术来解决cgroup下的CPU的三级缓存隔离性问题。\n在linux 4.10以上内核中通过资源控制文件系统的方式来提供给用户接口，类似cgroup的管理方式。\n感兴趣的可以了解下runc项目源码。\n2.Thanos\nPrometheus作为Google内部监控系统Borgmon的开源实现版本，存在高可用和历史数据存储两个致命的缺点，Thanos利用Sidecar等技术来解决Prometheus的缺点。\n3.netshoot\n1 2 3 4 5 6 dP dP dP 88 88 88 88d888b. .d8888b. d8888P .d8888b. 88d888b. .d8888b. .d8888b. d8888P 88\u0026#39; `88 88ooood8 88 Y8ooooo. 88\u0026#39; `88 88\u0026#39; `88 88\u0026#39; `88 88 88 88 88. ... 88 88 88 88 88. .88 88. .88 88 dP dP `88888P\u0026#39; dP `88888P\u0026#39; dP dP `88888P\u0026#39; `88888P\u0026#39; dP 用于排查docker网络问题的工具，以容器的方式运行在跟要排查问题的容器同一个网络命名空间中，该容器中已经具备了较为丰富的网络命令行工具，用于排查容器中的网络问题。\n4.bat\n用来替代cat的命令行工具，支持语法高亮、自动分页。mac下可直接使用brew install bat来安装。\n5.asciiflow\n写博客的往往都比较痛恨图片的存储问题，尤其是使用markdown语法写作的，图片往往需要图床来存储，常常跟文章不在一起存储。asciiflow是较为小众的一款ascii图形工具，可以应付较为简单的图形绘制，直接以文字的形式呈现简单图形，省去了存储图片的繁琐。\n6.processon\n免费的在线图行绘制协作工具，支持流程图、思维导图等多种图形，有类似visio的使用体验，同时是web版的，支持多人协作。我目前在使用，不过免费版有使用限制。\n精彩文章 1.手把手教你打造高效的 Kubernetes 命令行终端 文中汇总了各种可以取代kubernetes的命令行kubectl的工具，以便提供更方便的操作，比如更完善的自动补全。\n2.Understand Container - Index Page\n学习容器的cgroup和namespace的系列文章。\n3.gVisor是什么？可以解决什么问题？\ndocker容器技术基于cgroup和namespace来实现，但系统调用仍是调用宿主机的系统调用，比如在其中一个容器中通过系统调用修改了当前系统时间，在其他容器中看到的时间也已经修改过了，这显然不是符合期望的，通常可以通过Seccomp来限制容器中的系统调用。\ngVisor为Google开源的容器Runtime，通过pstrace技术来截获系统调用，从而保证系统的安全。目前还不成熟，单就凭Google的开源项目，该项目还是非常值得关注的。\n4.Use multi-stage builds\nDockerfile的多阶段构建技术，对于解决编译型语言的发布非常有帮助，可以在其中一个image中编译源码，另外一个image用于将编译完成后的二进制文件复制过来后打包成单独的线上运行镜像。而这两部操作可以合并到一个Dockerfile中来完成。\n5.唯品会Noah云平台实现内幕披露\n唯品会内部云平台的实践，涉及到大量的干货，值的花时间一读。\nApp推荐 1.Nike Training\n健身类app我用过keep、火辣健身、FitTime（以收费课程居多），偶然间在AppStore上看到了Nike Training，如果厌倦了国内的健身类app，不防尝试一下。\n新奇 1.手机QQ扫一扫\n用手机QQ扫一扫100元人民币正面，可以出现浮动的凤凰图案，并会跳转到人民币鉴别真伪的视频页面，视频效果确实不错，忍不住会多扫描几遍。\n2.kubeadm\nkubernetes的组件非常多，部署起来非常复杂，因此社区就推出了kubeadm工具来简化集群的部署，将除了kubelet外的其他组件都部署在容器中。令人惊奇的是，kubeadm几乎完全是一个芬兰高中生Lucas KaIdstrom的作品，是他在17岁时利用业余时间完成的一个社区项目。\n","date":"2018-09-26T00:01:27Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC3%E6%9C%9F/","title":"知识分享第3期"},{"content":"传统的unix权限模型将进程分为root用户进程（有效用户id为0）和普通用户进程。普通用户需要root权限的某些功能，通常通过setuid系统调用实现。但普通用户并不需要root的所有权限，可能仅仅需要修改系统时间的权限而已。这种粗放的权限管理方式势必会带来一定的安全隐患。\nlinux内核中引入了capability，用于消除需要执行某些操作的程序对root权限的依赖。\ncapability用于分割root用户的权限，将root的权限分割为不同的能力，每一种能力代表一定的特权操作。例如，CAP_SYS_MODULE用于表示用户加载内核模块的特权操作。根据进程具有的能力来进行特权操作的访问控制。\n只有进程和可执行文件才有能力，每个进程拥有以下几组能力集(set)。\ncap_effective: 进程当前可用的能力集 cap_inheritable: 进程可以传递给子进程的能力集 cap_permitted: 进程可拥有的最大能力集 cap_ambient: Linux 4.3后引入的能力集， cap_bounding: 用于进一步限制能力的获取 可以通过/proc/${pid}/status文件中的CapInh CapPrm CapEff CapBnd CapAmb来表示，每个字段为8个字节即64bit，每个比特表示一种能力，这几个字段存放在进程的内核数据结构task_struct中，由此可见capability的最小单位为线程，而不是进程。\nexample 1 设置进程能力 在执行下面程序之前需要安装yum install libcap-devel\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #undef _POSIX_SOURCE #include \u0026lt;sys/capability.h\u0026gt; extern int errno; void whoami(void) { printf(\u0026#34;uid=%i euid=%i gid=%i\\n\u0026#34;, getuid(), geteuid(), getgid()); } void listCaps() { cap_t caps = cap_get_proc(); ssize_t y = 0; printf(\u0026#34;The process %d was give capabilities %s\\n\u0026#34;,(int) getpid(), cap_to_text(caps, \u0026amp;y)); fflush(0); cap_free(caps); } int main(int argc, char **argv) { int stat; whoami(); stat = setuid(geteuid()); pid_t parentPid = getpid(); if(!parentPid) return 1; cap_t caps = cap_init(); // 给进程增加5中能力 cap_value_t capList[5] ={ CAP_NET_RAW, CAP_NET_BIND_SERVICE , CAP_SETUID, CAP_SETGID,CAP_SETPCAP } ; unsigned num_caps = 5; cap_set_flag(caps, CAP_EFFECTIVE, num_caps, capList, CAP_SET); cap_set_flag(caps, CAP_INHERITABLE, num_caps, capList, CAP_SET); cap_set_flag(caps, CAP_PERMITTED, num_caps, capList, CAP_SET); if (cap_set_proc(caps)) { perror(\u0026#34;capset()\u0026#34;); return EXIT_FAILURE; } listCaps(); // 将进程的能力清除 printf(\u0026#34;dropping caps\\n\u0026#34;); cap_clear(caps); // resetting caps storage if (cap_set_proc(caps)) { perror(\u0026#34;capset()\u0026#34;); return EXIT_FAILURE; } listCaps(); cap_free(caps); return 0; } 并执行如下操作：\n1 2 3 4 5 6 7 8 9 10 gcc capability.c -lcap -o capability # 需要使用root执行，因为普通用户不能给进程设置能力 sudo ./capability # 输出如下内容 uid=0 euid=0 gid=0 The process 5044 was give capabilities = cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw+eip dropping caps The process 5044 was give capabilities = example 2 获取进程能力 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #undef _POSIX_SOURCE #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;linux/capability.h\u0026gt; #include \u0026lt;errno.h\u0026gt; int main() { struct __user_cap_header_struct cap_header_data; cap_user_header_t cap_header = \u0026amp;cap_header_data; struct __user_cap_data_struct cap_data_data; cap_user_data_t cap_data = \u0026amp;cap_data_data; cap_header-\u0026gt;pid = getpid(); cap_header-\u0026gt;version = _LINUX_CAPABILITY_VERSION_1; if (capget(cap_header, cap_data) \u0026lt; 0) { perror(\u0026#34;Failed capget\u0026#34;); exit(1); } printf(\u0026#34;Cap data 0x%x, 0x%x, 0x%x\\n\u0026#34;, cap_data-\u0026gt;effective,cap_data-\u0026gt;permitted, cap_data-\u0026gt;inheritable); } 可以通过capget命令获取进程的能力\n1 2 3 4 5 6 7 8 [vagrant@localhost tmp]$ gcc get_capability.c -lcap -o get_capability # 普通用户默认情况下没有任何能力 [vagrant@localhost tmp]$ ./get_capability Cap data 0x0, 0x0, 0x0 # root用户默认拥有所有的能力 [vagrant@localhost tmp]$ sudo ./get_capability Cap data 0xffffffff, 0xffffffff, 0x0 工具 getcap用于获取程序文件所具有的能力。 getpcaps用于获取进程所具有的能力。 setcap用于设置程序文件所具有的能力。 capsh 查看和设置程序的能力 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 将chown命令授权给普通用户也具备更改文件owner的能力 # 其中eip分别代表cap_effective(e) cap_inheritable(i) cap_permitted(p) [vagrant@localhost tmp]$ sudo setcap cap_chown=eip /usr/bin/chown [vagrant@localhost tmp]$ getcap /usr/bin/chown /usr/bin/chown = cap_chown+eip # 使用root创建测试文件 [vagrant@localhost tmp]$ sudo touch /tmp/aa # 普通用户也可以修改root用户创建文件的owner了 [vagrant@localhost tmp]$ chown vagrant:vagrant /tmp/aa # 清除chown的能力 [vagrant@localhost tmp]$ sudo setcap -r /usr/bin/chown [vagrant@localhost tmp]$ getcap /usr/bin/chown # 获取到进程的 capability [vagrant@localhost tmp]$ cat /proc/95373/status | grep Cap CapInh:\t0000000000000000 CapPrm:\t0000000000000000 CapEff:\t0000000000000000 CapBnd:\t0000003fffffffff CapAmb:\t0000000000000000 # 对上述 capability 进行解码 [vagrant@localhost tmp]$ capsh --decode=0000003fffffffff 0x0000003fffffffff=cap_chown,cap_dac_override,cap_dac_read_search,cap_fowner,cap_fsetid,cap_kill,cap_setgid,cap_setuid,cap_setpcap,cap_linux_immutable,cap_net_bind_service,cap_net_broadcast,cap_net_admin,cap_net_raw,cap_ipc_lock,cap_ipc_owner,cap_sys_module,cap_sys_rawio,cap_sys_chroot,cap_sys_ptrace,cap_sys_pacct,cap_sys_admin,cap_sys_boot,cap_sys_nice,cap_sys_resource,cap_sys_time,cap_sys_tty_config,cap_mknod,cap_lease,cap_audit_write,cap_audit_control,cap_setfcap,cap_mac_override,cap_mac_admin,cap_syslog,35,36,37 runc项目中的应用 runc的容器配置文件spec.Process.Capabilities可以定义各个能力集的能力，用来限制容器的能力。\ndocker中的应用 docker默认情况下给容器去掉了一些比较危险的capabilities，比如cap_sys_admin。\n例如在docker中使用gdb命令默认是不允许的，这是因为docker已经将SYS_PTRACE相关的能力给去掉了。\n在docker中使用--cap-add和--cap-drop命令来增加和删除capabilities，\n可以使用--privileged赋予容器所有的capabilities，该操作谨慎使用。\nref Linux的capability深入分析(1) Linux的capability深入分析(2) Linux Programmer\u0026rsquo;s Manual CAPABILITIES 如何在Docker内部使用gdb调试器 docker run ","date":"2018-09-23T21:42:36Z","permalink":"/post/linux-capability/","title":"linux capability"},{"content":"user namespace是所有namespace中实现最复杂的一个，也是最晚引入的一个，是在linux2.6版本中才引入。因为涉及到权限机制，跟capability有着比较密切的关系。\n创建user namespace 在clone或者unshare系统调用使用CLONE_NEWUSER参数后，在子进程中看到的uid和gid跟父进程中的不一样，子进程中找不到uid时，会显示最大的uid 65534（在/proc/sys/kernel/overflowuid中设置）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 vagrant@ubuntu-xenial:/tmp$ id uid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant) vagrant@ubuntu-xenial:/tmp$ readlink /proc/$$/ns/user user:[4026531837] # 使用unshare命令创建新的user namespace vagrant@ubuntu-xenial:/tmp$ unshare --user /bin/bash nobody@ubuntu-xenial:/tmp$ readlink /proc/$$/ns/user user:[4026532145] # 新的user namespace没有映射关系，默认使用/proc/sys/kernel/overflowuid中定义的user id和/proc/sys/kernel/overflowgid中的group id nobody@ubuntu-xenial:/tmp$ id uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup) #--------------------------第二个shell窗口---------------------- # 在父user namespace上创建文件夹，可以看到用户为vagrant vagrant@ubuntu-xenial:/tmp$ mkdir test vagrant@ubuntu-xenial:/tmp$ ll /tmp | grep test drwxrwxr-x 2 vagrant vagrant 4096 Sep 23 17:11 test/ #--------------------------第一个shell窗口---------------------- # 在新创建的user namespace下用户显示为nobody nobody@ubuntu-xenial:/tmp$ ll /tmp | grep test drwxrwxr-x 2 nobody nogroup 4096 Sep 23 17:11 test/ 映射user id和group id到新的user namespace 创建完新的user namespace后，通常会先映射user id和group id，方法为添加映射关系到/proc/${pid}/uid_map和/proc/${pid}/gid_map中。\nuser namespace被创建以后，第一个进程被赋予了该namespace的所有权限，但该进程并不拥有父namespace的任何权限。利用该机制可以做到一个用户在父user namespace中是普通用户，在子user namespace中是超级用户的功能。\n为了将容器中的uid和父user namespace上的uid和gid进行 关联起来，可通过/proc//uid_map和/proc//gid_map来进行映射的。这两个文件的格式为：\n1 ID-inside-ns ID-outside-ns length 第一个字段ID-inside-ns表示在容器显示的UID或GID， 第二个字段ID-outside-ns表示容器外映射的真实的UID或GID。 第三个字段表示映射的范围，一般填1，表示一一对应。\n0 1000 256这个配置的含义为父user namespace的1000-1256映射到新user namespace的0-256.\n创建子进程在没有指定CLONE_NEWUSER时文件内容如下，子进程跟父进程的用户完全一致：\n1 2 3 # 表示把namespace内部从0开始的uid映射到外部从0开始的uid，其最大范围是无符号32位整形 [root@centos7 1325]# cat uid_map 0 0 4294967295 要想实现以普通用户运行程序，在子进程中以root用户执行，仅需要将uid_map文件修改为普通用户映射到子进程中的0即可，因为uid为0表示root用户。\n那么谁拥有写入该文件的权限呢？\n/proc/${pid}/[u|g]id的拥有者为创建新user namespace的用户，拥有map文件写入权限的仅有两个用户：和该用户在同一个user namespace中的root用户，创建新的user namespace的用户。创建新的user namespace有没有写入map文件的权限，还要取决于capability中的CAP_SETUID和CAP_SETGID两个权限。\n为了方便写入/proc/${pid}/uid_map和/proc/${pid}/gid_map文件，可以使用newuidmap和newgidmap命令来完成。\n继续上述例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #--------------------------第一个shell窗口---------------------- # 在新user namespace中获取当前的进程号 nobody@ubuntu-xenial:/tmp$ echo $$ 13226 #--------------------------第二个shell窗口---------------------- vagrant@ubuntu-xenial:/tmp$ ll /proc/13226/uid_map /proc/13226/gid_map -rw-r--r-- 1 vagrant vagrant 0 Sep 23 17:31 /proc/13226/gid_map -rw-r--r-- 1 vagrant vagrant 0 Sep 23 17:31 /proc/13226/uid_map # 提示当前进程没有权限写入 vagrant@ubuntu-xenial:/tmp$ echo \u0026#39;0 1000 100\u0026#39; \u0026gt; /proc/13226/uid_map -bash: echo: write error: Operation not permitted # 查看当前bash没有任何capability vagrant@ubuntu-xenial:/tmp$ cat /proc/$$/status | egrep \u0026#39;Cap(Inh|Prm|Eff)\u0026#39; CapInh:\t0000000000000000 CapPrm:\t0000000000000000 CapEff:\t0000000000000000 # 使用root权限给/bin/bash可执行文件增加cap_setgid和cap_setuid vagrant@ubuntu-xenial:/tmp$ sudo setcap cap_setgid,cap_setuid+ep /bin/bash # 启动新的bash后capability会生效 vagrant@ubuntu-xenial:/tmp$ bash vagrant@ubuntu-xenial:/tmp$ cat /proc/$$/status | egrep \u0026#39;Cap(Inh|Prm|Eff)\u0026#39; CapInh:\t0000000000000000 CapPrm:\t00000000000000c0 CapEff:\t00000000000000c0 # 重新写入 vagrant@ubuntu-xenial:/tmp$ echo \u0026#39;0 1000 100\u0026#39; \u0026gt; /proc/13226/uid_map vagrant@ubuntu-xenial:/tmp$ echo \u0026#39;0 1000 100\u0026#39; \u0026gt; /proc/13226/gid_map # 第二次再写入会失败，仅允许写入一次 vagrant@ubuntu-xenial:/tmp$ echo \u0026#39;0 1000 100\u0026#39; \u0026gt; /proc/13226/uid_map bash: echo: write error: Operation not permitted # 将刚才设置的capability取消 vagrant@ubuntu-xenial:/tmp$ sudo setcap cap_setgid,cap_setuid-ep /bin/bash vagrant@ubuntu-xenial:/tmp$ getcap /bin/bash /bin/bash = vagrant@ubuntu-xenial:/tmp$ exit exit #--------------------------第一个shell窗口---------------------- # 第一个窗口中userid已经变更为0了 nobody@ubuntu-xenial:/tmp$ id uid=0(root) gid=0(root) groups=0(root) # 重新执行一个新的bash，会发现提示符已经变更为root了 nobody@ubuntu-xenial:/tmp$ bash root@ubuntu-xenial:/tmp# # 可以看到新的bash已经拥有的所有的capability，但也仅限于当前的user namespace中 root@ubuntu-xenial:/tmp# cat /proc/$$/status | egrep \u0026#39;Cap(Inh|Prm|Eff)\u0026#39; CapInh:\t0000000000000000 CapPrm:\t0000003fffffffff CapEff:\t0000003fffffffff 问题 user namespace在linux3.8内核版本上才实现，存在一定的安全问题。在redhat和centos系统下，user namespace作为了一个实验feature，默认情况下未开启。\n执行如下命令sudo grubby --args=\u0026quot;user_namespace.enable=1\u0026quot; --update-kernel=\u0026quot;$(grubby --default-kernel)\u0026quot;并重启系统后可以就可以打开user namespace feature了。执行grubby --remove-args=\u0026quot;user_namespace.enable=1\u0026quot; --update-kernel=\u0026quot;$(grubby --default-kernel)\u0026quot;可关闭user namespace feature。\n经实验，上述操作未生效，后续待查该问题。\nref What’s Next for Containers? User Namespaces Linux Namespace系列（07）：user namespace (CLONE_NEWUSER) (第一部分) ","date":"2018-09-23T19:13:16Z","permalink":"/post/docker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8Buser-namespace/","title":"docker基础知识之user namespace"},{"content":"Mount Namespace 是 Linux 下的一种挂载隔离机制，可以实现不同的进程拥有独立的挂载视图，各挂载点之间相互不受影响。\nmount namespace 基本用法 Linux 启动时会创建一个默认的 mount namespace，在使用 clone() 或者 unshare() 系统调用通过 CLONE_NEWNS 会创建出新的 mount namespace。因为是第一个加入到 Linux 内核的中的 namespace，CLONE_NEWNS 的取名并不是太合理。\n每个进程的挂载信息保存在了 /proc/\u0026lt;pid\u0026gt;/{mountinfo,mounts,mountstats} 中。\n使用如下命令可以看到一个进程对应的 mount namespace 名称。\n1 2 #ll /proc/$$/ns/mnt lrwxrwxrwx 1 root root 0 Aug 30 00:02 /proc/94832/ns/mnt -\u0026gt; mnt:[4026531840] 可以使用 unshare 命令来创建一个新的命名空间，执行如下命令：\n1 2 3 4 5 6 7 # 创建新的命名空间，此时会继承当前 mount namespace 的挂载点信息 unshare --mount --fork --pid --mount-proc /bin/bash # 创建新的挂载点并挂载，执行完成后可以看到新的挂载点存在 mkdir /mnt/new_mount mount -t tmpfs tmpfs /mnt/new_mount # 查看新的挂载点 df -h | grep new_mount 新打开一个终端，切换回原始的命名空间，可以看到挂载点并不存在。\nmount namespace 的 shared subtree mount namespace 实现了挂载点的隔离，但在很多创建下又希望能够在不同的 namespace 下共享挂载点。shared subtree 机制允许在 mount namespace 之间自动的或者受控的传播 mount 和 umount 事件。\n注意：该处理解起来比较绕。在创建 mount namespace 和挂载目录的时候分别可以指定参数。\n在做单个目录挂载的时候通过指定传播类型来指定的不同隔离行为，支持如下的值：\nprivate：挂载点不会传播到其他命名空间，默认类型。 shared：挂载点会传播到共享这个挂载点的其他命名空间。 slave：挂载点不会传播到另一个命名空间，但其他命名空间的传播会影响到这个挂载点。 unbindable：挂载点不能被移动。 rprivate：递归地将挂载点及其子挂载点设置为 private。 rshared：递归地将挂载点及其子挂载点设置为 shared。 rslave：递归地将挂载点及其子挂载点设置为 slave。 mount 在挂载目录的时候可以指定如下参数：\n\u0026ndash;make-rshared：将挂载方式设置为 rshared。 \u0026ndash;make-rslave：将挂载方式设置为 rslave。 \u0026ndash;make-rprivate：将挂载方式设置为 rprivate。 \u0026ndash;make-runbindable：将挂载方式设置为不可移动。 可以通过 findmnt -o TARGET,PROPAGATION 命令查看到当前 mount namespace 下的挂载点的 shared subtree 属性。但该命令无法区分出 shared 和 rshared 属性。\nunshare 命令在创建新的 mount namespace 时通过选项 --propagation来指定新创建的 mount namespace 中所有挂载点的共享方式。支持如下值：\nprivate：新创建的 mount namespace 中的所有挂载点的 shared subtrees 属性全部为 private。默认值。 shared：新创建的 mount namespace 中的挂载点的 shared subtrees 属性全部为 share。 slave：新创建的 mount namespace 中的挂载点的 shared subtrees 属性全部为 slave。 unchanged：新创建的 mount namespace 中的挂载点的 shared subtrees 属性保持不变。 在上面的例子中可以通过指定为 rshared 的模式来查看效果。\n1 2 3 4 5 6 7 8 # 创建新的命名空间 unshare --mount --fork --pid --mount-proc /bin/bash # 创建新的挂载点并挂载，执行完成后可以看到新的挂载点存在 mount --make-rshared / # 查看新的挂载点 df -h | grep new_mount mount namespace 在容器技术中的应用 在容器技术中，在容器中不应该看到宿主机上挂载的目录。在Linux中使用chroot技术来实现，在容器中将/挂载到指定目录，这样在容器中就看不到宿主机上的其他挂载项。在容器技术中，chroot所使用的目录即容器镜像的目录。容器镜像中并不包含操作系统的内核。\nDocker Volume 为了解决容器中能够访问宿主机上文件的问题，docker引入了Volume机制，将宿主机上指定的文件或者目录挂载到容器中。而整个的docker Volume机制跟mount namespace的关系不太大。\nVolume用到的技术为Linux的绑定挂载机制，该机制将一个指定的目录或者文件挂载到一个指定的目录上。\n容器启动顺序如下：\n创建新的mount namespace dockerinit根据容器镜像准备好rootfs dockerinit使用绑定挂载机制将一个指定的目录挂载到rootfs的某个目录上 dockerinit调用chroot 容器启动时需要创建新的mount namespace，根据容器镜像准备好rootfs，调用chroot。docker volume的挂载时机是在rootfs准备好之后，调用chroot之前完成。\n上文提到进入新的mount namespace后，mount namespace会继承父mount namespace的挂载, docker volume一定是在新的mount namespce中执行，否则会影响到宿主机上的mount。在调用chroot之后已经看不到宿主机上的文件系统，无法进行挂载。\n执行这一操作的进程为docker的容器进程dockerinit，该进程会负责完成根目录的准备、挂载设备和目录、配置hostname等一系列需要在容器内进行的初始化操作。在初始化完成后，会调用execv()系统调用，用容器中的ENTRYPOINT进程取代，成为容器中的1号进程。\nvolume挂载的目录是挂载在读写层，由于使用了mount namespace，在宿主机上看不到挂载目录的信息，因此docker commit操作不会将挂载的目录提交。\n下面使用例子来演示docker volume的用法\n在宿主机上使用docker run -d -v /test ubuntu sleep 10000创建新的容器，并创建docker容器中的挂载点/test，该命令会自动在容器中创建目录，并将宿主机上指定目录下的随机目录挂载到容器中的/test目录下。\n可在宿主机上通过如下命令查看到volume的情况\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 列出当前docker在使用的所有volume [root@localhost vagrant]# docker volume ls DRIVER VOLUME NAME local 4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6 # 查看volume在宿主机上的挂载点 [root@localhost vagrant]# docker volume inspect 4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6 [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2018-09-15T23:36:09+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] docker文件系统 rootfs在最下层为docker镜像的只读层。\nrootfs之上为dockerinit进程自己添加的init层，用来存放dockerinit添加或者修改的/etc/hostname等文件。\nrootfs的最上层为可读写层，以Copy-On-Write的方式存放任何对只读层的修改，容器声明的volume挂载点也出现这一层。\nmount namespace 在 k8s 中应用 在 k8s 中可以通过 volumeMounts 中的参数 mountPropagation 来指定挂载的参数。\n引用 极客时间-深入剖析Kubernetes-08 ","date":"2018-09-15T01:52:17Z","permalink":"/post/docker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8Bmount-namespace/","title":"docker基础知识之mount namespace"},{"content":"network namespace用来隔离Linux系统的网络设备、ip地址、端口号、路由表、防火墙等网络资源。用户可以随意将虚拟网络设备分配到自定义的networknamespace里，而连接真实硬件的物理设备则只能放在系统的根networknamesapce中。\n一个物理的网络设备最多存在于一个network namespace，可以通过创建veth pair在不同的network namespace之间创建通道，来达到通讯的目的。\n容器的bridge模式的实现思路为创建一个veth pair，一端放置在新的namespace，通常命名为eth0，另外一端放在原先的namespace中连接物理网络设备，以此实现网络通信。\ndocker daemon负责在宿主机上创建veth pair，把一端绑定到docker0网桥，另一端到新建的network namespace进程中。建立的过程中，docker daemon和dockerinit通过pipe进行通讯。\n一、测试例子 测试network namespace的过程比较复杂。\ndocker默认采用的为bridge模式，在容器所在的宿主机上看到的网卡情况如下：\n1 2 3 4 5 6 7 8 9 10 11 [root@localhost software]# ip link show 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff 3: enp0s8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:a5:78:ca brd ff:ff:ff:ff:ff:ff 4: docker0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a3:75:00:16 brd ff:ff:ff:ff:ff:ff 18: veth71f2650@if17: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether ca:05:f7:db:6f:4c brd ff:ff:ff:ff:ff:ff link-netnsid 0 其中的enp0s3和enp0s8可以忽略，为虚拟机使用的网卡。docker0和veth71f2650@if17是需要关注的网卡。\n1 2 3 [root@localhost software]# brctl show bridge name\tbridge id\tSTP enabled\tinterfaces docker0\t8000.0242a3750016\tno\tveth71f2650 下面的操作为在已经运行docker的虚拟机上的，以便于跟docker进行比较。\n以下命令根据coolshell中的步骤进行配置，并对执行命令的顺序进行了调整。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 # 增加network namespace ns1 [root@localhost software]# ip netns add ns1 [root@localhost software]# ip netns ns1 # 激活namespace ns1中的lo设备 [root@localhost software]# ip netns exec ns1 ip link set dev lo up # 创建veth pair [root@localhost software]# ip link add veth-ns1 type veth peer name lxcbr0.1 # 多出了lxcbr0.1@veth-ns1和veth-ns1@lxcbr0.1两个设备 # 后面的操作步骤中将lxcbr0.1位于主网络命名空间中，veth-ns1位于ns1命名空间中 [root@localhost software]# ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff 3: enp0s8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:a5:78:ca brd ff:ff:ff:ff:ff:ff 4: docker0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a3:75:00:16 brd ff:ff:ff:ff:ff:ff 18: veth71f2650@if17: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether ca:05:f7:db:6f:4c brd ff:ff:ff:ff:ff:ff link-netnsid 0 19: lxcbr0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether c6:b7:4d:7f:f8:90 brd ff:ff:ff:ff:ff:ff 20: lxcbr0.1@veth-ns1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether c6:8a:26:3d:ba:de brd ff:ff:ff:ff:ff:ff 21: veth-ns1@lxcbr0.1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f2:03:22:93:d6:f4 brd ff:ff:ff:ff:ff:ff # 将设备veth-ns1放入到ns1命名空间中 [root@localhost software]# ip link set veth-ns1 netns ns1 # 可以看到veth-ns1设备在当前命名空间消失了 [root@localhost software]# ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff 3: enp0s8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:a5:78:ca brd ff:ff:ff:ff:ff:ff 4: docker0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a3:75:00:16 brd ff:ff:ff:ff:ff:ff 18: veth71f2650@if17: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether ca:05:f7:db:6f:4c brd ff:ff:ff:ff:ff:ff link-netnsid 0 19: lxcbr0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether c6:b7:4d:7f:f8:90 brd ff:ff:ff:ff:ff:ff 20: lxcbr0.1@if21: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether c6:8a:26:3d:ba:de brd ff:ff:ff:ff:ff:ff link-netnsid 1 # 同时在命名空间ns1中看到了设备veth-ns1，同时可以看到veth-ns1设备的状态为DOWN [root@localhost software]# ip netns exec ns1 ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 21: veth-ns1@if20: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f2:03:22:93:d6:f4 brd ff:ff:ff:ff:ff:ff link-netnsid 0 # 将ns1中的veth-ns1设备更名为eth0 [root@localhost software]# ip netns exec ns1 ip link set dev veth-ns1 name eth0 [root@localhost software]# ip netns exec ns1 ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 21: eth0@if20: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f2:03:22:93:d6:f4 brd ff:ff:ff:ff:ff:ff link-netnsid 0 # 为容器中的网卡分配一个IP地址，并激活它 [root@localhost software]# ip netns exec ns1 ifconfig eth0 192.168.10.11/24 up # 可以看到eth0网卡上有ip地址 [root@localhost software]# ip netns exec ns1 ifconfig eth0: flags=4099\u0026lt;UP,BROADCAST,MULTICAST\u0026gt; mtu 1500 inet 192.168.10.11 netmask 255.255.255.0 broadcast 192.168.10.255 ether f2:03:22:93:d6:f4 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags=73\u0026lt;UP,LOOPBACK,RUNNING\u0026gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10\u0026lt;host\u0026gt; loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 添加一个网桥lxcbr0，类似于docker中的docker0 [root@localhost software]# brctl addbr lxcbr0 [root@localhost software]# brctl show bridge name\tbridge id\tSTP enabled\tinterfaces docker0\t8000.0242a3750016\tno\tveth71f2650 lxcbr0\t8000.000000000000\tno # 关闭生成树协议，默认该协议为关闭状态 [root@localhost software]# brctl stp lxcbr0 off [root@localhost software]# brctl show bridge name\tbridge id\tSTP enabled\tinterfaces docker0\t8000.0242a3750016\tno\tveth71f2650 lxcbr0\t8000.000000000000\tno # 为网桥配置ip地址 ifconfig lxcbr0 192.168.10.1/24 up [root@localhost software]# ifconfig lxcbr0 lxcbr0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500 inet 192.168.10.1 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::c4b7:4dff:fe7f:f890 prefixlen 64 scopeid 0x20\u0026lt;link\u0026gt; ether c6:b7:4d:7f:f8:90 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 648 (648.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 # 将veth设备中的其中一个lxcbr0.1添加到网桥lxcbr0上 [root@localhost software]# brctl addif lxcbr0 lxcbr0.1 # 可以看到网桥lxcbr0中已经包含了设备lxcbr0.1 [root@localhost software]# brctl show bridge name\tbridge id\tSTP enabled\tinterfaces docker0\t8000.0242a3750016\tno\tveth71f2650 lxcbr0\t8000.c68a263dbade\tno\tlxcbr0.1 # 为网络空间ns1增加默认路由规则，出口为网桥ip地址 [root@localhost software]# ip netns exec ns1 ip route add default via 192.168.10.1 [root@localhost software]# ip netns exec ns1 ip route default via 192.168.10.1 dev eth0 192.168.10.0/24 dev eth0 proto kernel scope link src 192.168.10.11 # 为ns1增加resolv.conf [root@localhost software]# mkdir -p /etc/netns/ns1 [root@localhost software]# echo \u0026#34;nameserver 8.8.8.8\u0026#34; \u0026gt; /etc/netns/ns1/resolv.conf 二、常用命令 1. 列出当前的network namespace 1.1 使用lsns命令 lsns命令通过读取/proc/${pid}/ns目录下进程所属的命名空间来实现，如果是通过ip netns add场景的命名空间，但是没有使用该命名空间的进程，该命令是看不到的。\n1 2 3 4 5 6 7 8 9 10 11 12 # lsns -t net NS TYPE NPROCS PID USER COMMAND 4026531956 net 383 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 21 4026532490 net 1 1026 rtkit /usr/libexec/rtkit-daemon 4026532762 net 2 24872 root /pause 4026532866 net 20 25817 root /pause 4026532965 net 3 30763 root /pause 4026533059 net 3 2794 root /bin/sh -c python /usr/src/app/clean.py \u0026#34;${endpoints}\u0026#34; \u0026#34;${expire}\u0026#34; 4026533163 net 2 1122 102 /docker-java-home/jre/bin/java -Xms2g -Xmx2g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupan 4026533266 net 4 13920 root /pause 4026533371 net 2 1844 root /pause 4026533559 net 3 1067 root sleep 4 1.2 通过ip netns命令 该命令仅会列出有名字的namespace，对于未命名的不能显示。\nip netns identify ${pid} 可以找到进程所属的网络命名空间 ip netns list: 显示所有有名字的namespace 2. 通过pid进入具体的network namespace 2.1 通过nsenter命令 nsenter --target $PID --net可以进入到对应的命名空间\n2.2 docker --net参数 docker提供了--net参数用于加入另一个容器的网络命名空间docker run -it --net container:7835490487c1 busybox ifconfig。\n2.3 setns系统调用 一个进程可以通过setns()系统调用来进入到另外一个namespace中。\n编写setns.c程序，该程序会进入到进程id所在的网络命令空间，并使用gcc setns.c -o setns进行编译，编译完成后执行./setns /proc/4913/ns/net ifconfig可以看到网卡的信息为容器中的网卡信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(int argc, char *argv[]) { int fd = open(argv[1], O_RDONLY); if (setns(fd, 0) == -1) { perror(\u0026#34;setns\u0026#34;); exit(-1); } execvp(argv[2], \u0026amp;argv[2]); printf(\u0026#34;execvp exit\\n\u0026#34;); } 如果执行./setns /proc/4913/ns/net /bin/bash，在宿主机上查看docker进程和/bin/bash进程的网络命名空间/proc/${pid}/ns/net，会发现都指向lrwxrwxrwx 1 root root 0 Sep 14 14:42 net -\u0026gt; net:[4026532133]同一个位置。\n3. pid的获取方式 最简单的方式上文第1点中的PID列\n3.1 /proc/[pid]/ns 可以使用如下命令查看当前容器在宿主机上的进程id。\n1 docker inspect --format \u0026#39;{{.State.Pid}}\u0026#39; a1bf0119d891 每个进程在/proc/${pid}/ns/目录下都会创建其对应的虚拟文件，并链接到一个真实的namespace文件上，如果两个进程下的链接文件链接到同一个地方，说明两个进程同属于一个namespace。\n1 2 3 4 5 6 7 8 [root@localhost runc]# ls -l /proc/4913/ns/ total 0 lrwxrwxrwx 1 root root 0 Sep 11 00:21 ipc -\u0026gt; ipc:[4026532130] lrwxrwxrwx 1 root root 0 Sep 11 00:21 mnt -\u0026gt; mnt:[4026532128] lrwxrwxrwx 1 root root 0 Sep 11 00:18 net -\u0026gt; net:[4026532133] lrwxrwxrwx 1 root root 0 Sep 11 00:21 pid -\u0026gt; pid:[4026532131] lrwxrwxrwx 1 root root 0 Sep 11 00:21 user -\u0026gt; user:[4026531837] lrwxrwxrwx 1 root root 0 Sep 11 00:21 uts -\u0026gt; uts:[4026532129] reference DOCKER基础技术：LINUX NAMESPACE（下） 极客时间-深入剖析Kubernetes 一文搞懂 Linux network namespace ","date":"2018-09-15T00:40:49Z","permalink":"/post/docker%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%B9%8Bnetwork-namespace/","title":"docker基础知识之network namespace"},{"content":"\n题图为北京灵山主峰，海拔2303米，北京最高峰。登上主峰的时候，恰巧一头牛就在山顶悠闲，拍照的时候，牛哥把我带上去的枣、葡萄、花生米全部吃光了，甚至连橘子皮都没剩下，吃完后牛哥又悠闲的去吃草了，让我见识了啥叫吃葡萄不吐葡萄皮。\n教程 《Kubernetes权威指南 企业级容器云实战》 kubernetes的书籍并不多，该书八月份刚初版，内容较新，并不是一本kubernetes的入门书籍，而是讲解kubernetes在企业落地为PASS平台时需要做的工作，建议对kubernetes有一定了解后再看。\n书的内容为HPE的多名工程师拼凑而成，有些部分的内容明显是没有经过实践验证的理论派想法，但总体来看值得一读。\n书中提到了很多kubernetes较新版本才有的特性、微服务、service mesh、lstio，对于补充自己已经掌握的知识点有一定帮助。\n书的后半部分反而显的干货少了非常多，我仅草草的过了一遍。\n《Go语言高级编程》 Golang中相对进阶的中文教程，我还没来得及看。\nGorilla facebook发表的分布式的时序数据库论文，如果英文看起来吃力，可以看一下小米运维公众号中的翻译版本。facebook并未提供开源的实现，但在github上能找到一些开源的实现。\n《深入剖析Kubernetes》 极客时间app的专栏，本来购买之前没有报特别大的预期，但读完头几篇文章后被作者的文字功底折服，将PASS、容器的来龙去脉、docker的发展讲解的很到位，超出了我的预期。期待后面更新的专栏能够保持搞水准。\nSDN手册 一本介绍SDN相关知识的开源电子书。\n资源 M3DB 监控领域还是比较缺少特别好用的分布式时间序列存储数据库，性能特别优异的数据库往往都是单机版的，缺少高可用的方案，比如rrdtool、influxdb、graphite等。OpenTSDB、KairosDB、Druid等虽为分布式的时序数据库，但使用或者运维起来总有各种不方便的地方。uber开源的m3在分布式时序数据库领域又多了一个方案，并可作为prometheus的远程存储。\ncilium 使用BPF(Berkeley Packet Filter)和XDP(eXpress Data Path)内核技术来提供网络安全控制的高性能开源网络方案。\nkubeless kubernetes平台上的Serverless项目，Faas（功能即服务）一定是云计算发展的一个趋势。目前CNCF中还没有Serverless项目，期待CNCF下能够孵化一个Serverless项目。\n工具 vagrant 还在使用virturalbox的你，是时候使用vagrant了。vagrant作为对虚拟机的管理，虽然引入了一些概念带来了更大的复杂性。但同时功能上也更强大，比如对box的管理，可以将box理解为docker image，便于将虚拟机的环境在不同的主机上分发。\n公众账号推荐 小米运维 开通时间不算特别长，但文章的质量不错，都是比较接地气的干货，看得出确实是在工作中遇到的问题或者是总结经验，值的一读。\n开柒 曾经公众号的名字为开八，江湖人称八姐，忘记为何更改为开柒了，曾经的搜狐记者。总能非常及时的爆料很多互联网的内幕，消息来源往往非常准确，可见八姐在圈内的人脉非同一般。\n毕导 打发时间非常好的公众号，用理科男的思维方式进行恶搞，是不是拿出冗长的数学公式来证明日常生活中的小尝试，语言诙谐幽默，绝对是公众号中的一股清流。可惜每篇文章都很长，我没有太多时间把每一篇文章都看一遍。\n","date":"2018-09-09T01:21:03Z","permalink":"/post/%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB%E7%AC%AC2%E6%9C%9F/","title":"知识分享第2期"},{"content":"\n晚上在工位上安静地看着技术文档，不知为何脑中突然闪现了小时候打场的场景，那段记忆并无特殊之处，但大概是那片金黄色的背景和村民脸上丰收的喜悦以及村民之间的和谐相处的画面突然浮现在了我的眼前，让我不由得想写点什么以便回忆那段近乎忘却的回忆。\n说起“打场”这个词，可能很多人都不太熟悉，其实这个词也已经很多年我都没有接触过了，我甚至都不知道现在农村老家的村民们还是否会时长提起这个词语。我甚至不知道这个词是不是我们那地域性的方言，这些都不重要了。以下内容摘自百度百科：\n打场[dǎ cháng]，指把收割下来带壳的粮食平摊在场院里，用马拉磙子，或者用小型拖拉机，碾压这些粮食，使之脱去外壳，这一系列活动就叫打场。\n我的家在山东，小的时候还没有那么多的农业经济作物，地里更多的农作物以小麦和玉米为主，一年两季。农作物的种植周期跟二十四节气是息息相关的，每年的二十四节气的芒种左右是小麦丰收的季节，时间大概在高考前后。比我大一点的应该还有麦假，就是在收麦子的时候不上学了回家帮着收麦子，我没怎么有印象我有过麦假。\n小时候的机械化程度低，甚至谈不上机械化，收麦子这种活自然完全是人工完成的。在机械化程度还可以的今天，收麦子这种活在当时还是有着稍微繁琐的工序，也比较耗费人力，一到收麦子的时候，几乎家里所有的劳力都要出来干活了。\n村里场地基本上都是挨着的，这应该是生产队时代的产物，那时候土地是公有的，场地自己要连成一片比较容易。后来土地开始分割给个人，这块场地仍然是保留的，只不过是场地被划分成了很多份，每家分上一点。\n在打场那个年代，我是几乎很少参与过收麦子的活动，那时我还小，就是在场地里捣乱和瞎跑，还时不时让在场地里干活的村民调侃一番，毕竟小孩子活泼好动，村民们调侃一下解解乏也算是为他们做的一点点微不足道的贡献了。\n收麦子的活第一步是割麦子，割麦子这个活我还正儿八经得干过几次，虽然每次干的时间都非常短。关于割麦子的画面，我脑海中浮现的是烈日当头，家人们带着帽子弓着背拿着镰刀一人一沟麦子往前赶，一会儿麦秸在怀里就抱不下了，然后将麦秸放下继续往前赶。那时候的帽子几乎全部是用草编织成的，两根绳子沿着帽檐搭下来系在下巴上，很是牢固，即使低着头帽子也不会掉下来。我干活的时候每次负责的那一沟麦子总是割的最慢的，几乎要慢一倍的样子，而且是干的最差的，背后总会留下一些麦子没有割下来，到现在我也仍然好奇，他们是怎么干的这么快的，我感觉当时已经尽最大的努力了，而这仅是他们的正常速度。\n在麦子割完后，就需要将麦子放到集中的晒干了。之所以要晒干，是因为要是麦秸不晒干，麦粒是很难从麦秸上拖不下来的。而要晒干，那么多麦秸就需要一片地方来晾晒麦秸和麦粒。那时候还没怎么有柏油路，家里也很少有水泥的房顶，为了能够有晾晒的地方，村里有一片地就是场地，在收麦子的时候专门用来收麦子的，收完麦子后再种其他作物。之所以场地是集中式的，这还要从生产队时代说起，那时候土地都是公有的，自然场地就会划到一块比较容易，后来土地慢慢私有后，这些土地仍然是作为场地，只是被划的一小块一小块的。\n割麦子的时候偶尔会碰到鹌鹑蛋，这也是一些额外的小收获，但遗憾的是从没看到过鹌鹑。\n场地要想能晾晒麦秸和麦粒，自然就需要特别的平整，而要想土地特别的平整，在石器时代，仅有一个办法，那就是用轱辘一遍一遍的撵，一般至少需要两个人用绳子拉着轱辘满地里转，要想将土地撵的特别平整且不能有缝隙，哪怕一个麦粒掉在地上能够看的见捡的起来，想想都不是一件特别容易的事情，尤其是刚开始土地特别不平的时候。\n并非所有的麦秸都是需要在场地里暴晒的，这个要看麦秸的干燥程度，有些麦秸割完后已经非常干燥了，就不需要再暴晒了。\n干燥的麦秸在干燥后就需要将麦粒从麦秸上分离出来，毕竟农民们想要的是麦粒，那才是实打实的粮食。要想麦粒从麦秸上分离，这时候仍然是重量级的轱辘上场，这时候场地相对平整了，自然拉起来会省力气很多，我也曾经拉过一次，但每次都是拉一会就不知道去哪里玩了，小孩子干这么枯燥的事情自然没有耐心。\n拖完粒的麦秸一般是就地放到的场地的地头上，为了省空间，会将麦秸垛起来，一个剁的高度差不多在两三米高的样子，差不多是农民拿着叉子往上扔麦秸扔不上去的高度，叉子的长度差不多在两米高的样子。\n白天的时间是晒麦粒的最佳时机，到了傍晚时分就需要将晒的麦粒堆起来盖住，这个时候也是场上人最多的时候，几乎每家的地里都有人在忙着。我脑中闪现的场景就是在这个时候，村民们在场地上忙来忙去，边干活边聊着天，扯东扯西的。我在场地上跑来跑去，从这个地里跑到另外一家，时不时村民们还会那我来开个玩笑。累了靠着垛儿打个滚，不一会体力就恢复了，想来那时体力真是好。那时虽然很多事情还不懂，但我确实能从中体会村民们丰收的喜悦。\n麦子收完后，用轱辘碾压了无数遍的场地也就失去了存在的价值，村民们在场地上该种点其他作物就种上其他作物了，总之场地不会空闲特别久。场地上总会落下一些麦粒，雨后空闲的场地上的缝隙处时长会生出新的绿油油的麦苗。\n现在随着机械化的进步，收麦子这种事自然不需要手工割麦子了，晒麦子的地方也开始变多了，柏油路上，自家的房顶上，自家的院子里都是及其不错的晒麦子的地方，打场这种原始的方式也自然就退出了历史的舞台，而且再也不会回来。\n但就是那幅金黄色的画面却在我的记忆中留下了一道抹不去的色彩，让我时长回忆起来那温馨的画面。纵科技的发展，村民们的亲情却在变淡，该进城的都进城了，该出去打工的都出去打工了，一年村民们都见不上几回。也许将来村落的概念会消失，也许人与人之间的关系会更淡，但我曾经经历过村落时代的美好，也许这就足够了。\n","date":"2018-09-05T21:08:20Z","permalink":"/post/%E6%89%93%E5%9C%BA%E7%9A%84%E8%AE%B0%E5%BF%86/","title":"打场的记忆"},{"content":"sync.Cond类似于pthread中的条件变量，但等待的为goroutine，而不是线程。比较难理解的为Wait函数，在调用该函数时必须L为Lock状态，调用Wait函数后，goroutine会自动解锁，并等待条件的到来，等条件到来后会重新加锁。\n代码量并不多，下面是去掉注释后的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 package sync import ( \u0026#34;sync/atomic\u0026#34; \u0026#34;unsafe\u0026#34; ) type Cond struct { noCopy noCopy // L is held while observing or changing the condition L Locker notify notifyList checker copyChecker } func NewCond(l Locker) *Cond { return \u0026amp;Cond{L: l} } func (c *Cond) Wait() { c.checker.check() t := runtime_notifyListAdd(\u0026amp;c.notify) c.L.Unlock() runtime_notifyListWait(\u0026amp;c.notify, t) c.L.Lock() } func (c *Cond) Signal() { c.checker.check() runtime_notifyListNotifyOne(\u0026amp;c.notify) } func (c *Cond) Broadcast() { c.checker.check() runtime_notifyListNotifyAll(\u0026amp;c.notify) } // copyChecker holds back pointer to itself to detect object copying. type copyChecker uintptr func (c *copyChecker) check() { if uintptr(*c) != uintptr(unsafe.Pointer(c)) \u0026amp;\u0026amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) \u0026amp;\u0026amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) { panic(\u0026#34;sync.Cond is copied\u0026#34;) } } // noCopy may be embedded into structs which must not be copied // after the first use. // // See https://github.com/golang/go/issues/8005#issuecomment-190753527 // for details. type noCopy struct{} // Lock is a no-op used by -copylocks checker from `go vet`. func (*noCopy) Lock() {} 具体的使用例子如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { mutex := \u0026amp;sync.Mutex{} cond := sync.NewCond(mutex) wg := \u0026amp;sync.WaitGroup{} wait := func(i int, c chan int) { defer wg.Done() fmt.Println(\u0026#34;start chan \u0026#34;, i) cond.L.Lock() defer cond.L.Unlock() fmt.Printf(\u0026#34;chan %d wait before\\n\u0026#34;, i) c \u0026lt;- i // Wait是理解起来稍微麻烦的点，Cond.Wait会自动释放锁等待信号的到来，当信号到来后，第一个获取到信号的Wait将继续往下执行并从新上锁 cond.Wait() fmt.Printf(\u0026#34;chan %d wait end\\n\u0026#34;, i) } signal := func(count int, c chan int) { defer wg.Done() for i := 0; i \u0026lt; count; i++ { fmt.Printf(\u0026#34;read chan %d ready\\n\u0026#34;, \u0026lt;-c) } fmt.Println(\u0026#34;call signal\u0026#34;) cond.Signal() } broadcast := func(count int, c chan int) { defer wg.Done() for i := 0; i \u0026lt; count; i++ { fmt.Printf(\u0026#34;read chan %d ready\\n\u0026#34;, \u0026lt;-c) } fmt.Println(\u0026#34;call broadcast\u0026#34;) cond.Broadcast() } c := make(chan int) wg.Add(2) go wait(0, c) go signal(1, c) wg.Wait() fmt.Println(\u0026#34;signal test finished\\n\\n\u0026#34;) count := 3 for i := 0; i \u0026lt; count; i++ { wg.Add(1) go wait(i, c) } wg.Add(1) go broadcast(count, c) wg.Wait() } ","date":"2018-08-29T00:31:14Z","permalink":"/post/sync.cond%E7%9A%84%E4%BE%8B%E5%AD%90/","title":"sync.Cond的例子"},{"content":"\n自从阮一峰的博客中增加了每周分享栏目，自己每周五都是主动的浏览一下阮老师的每周分享，一来阮老师的涉猎非常广泛，可以提高自己的视野；二来，阮老师的文章都特别容易懂，给人一种一直想看下去的冲动。\n我个人平常也会看很多的技术类文章，也会遇到各种工具或者特别不错的文章，也有分享的冲动，也想搞一些分享的文章，当然我没有阮老师勤奋和涉猎广泛，但也期望能够对他人有所帮助，哪怕文中的一条分享能够让读者觉得有价值，那么也是值的做的一件事情。\n具体的分享版块可能不会特别固定，分享的间隔也不会特别勤快，很难做到阮老师的一周一次的频次。\n教程 1. The Go Memory Model Golang的内存模型，建议Golang开发者读一遍。\n2. 《极客时间》-左耳听风 知名博客酷壳的作者陈皓的技术专栏，花钱购买一下专栏还是非常值得的，尤其是最近写的程序员练级攻略系列，能提供大量有价值的学习资料及方向指导，非常赞。\nhttp://\n3. 深入解析 kubernetes 资源管理，容器云牛人有话说 对kubernetes的资源管理讲解的非常到位和深入，文章略长，需要花点时间才能读完，值的一看。\n4. 《TCP/IP详解 卷1: 协议》 网络方面的经典著作，每个工程师必读，虽然是写给工程师看的，但很多的学术著作中引用到了该书中内容。\n5. 《深入解析GO》 对于go的内部实现原理讲解的挺到位，对于理解go的原理挺有帮助。涉及少量汇编，我不太懂汇编，涉及汇编的地方直接跳过了。\n6. Red Hat Enterprise Linux Document RedHat官方的Linux文档，我个人还没怎么读过。\n资源 1. 腾讯大学-CEO来了 货真价实的互联网CEO的视频分享，谈创业、谈感悟，目前已经有蔚来汽车、VIPKID、每日优鲜、快手、Keep、知乎的CEO的分享。\n2. nginx-upsync-module 新浪微博开源的nginx module，用于动态更改upstream server。\n3. 语义化版本规范SemVer 软件版本在取名上会比较混乱，有的使用1.0.1，有的使用1.0等，SemVer用于规范软件版本的命名。\n4. 《见识》 吴军老师的最新图书，内容整理自吴军的专栏《硅谷来信》，每篇文章一个主题，值的一读。\n5. 嗨！济南 又听到了一首关于济南的歌曲，曾在济南生活多年，必须要分享一下。\n6. termtosvg Github上的开源项目，将命令行工具单独保存为SVG动画。\n7. teleport 提供了ssh的审计和回放，基于SSH的RBAC管理，同时还有一个带管理功能的ui界面，目的是用于取代系统自带的sshd。\n工具 1. cloc 统计代码行数的工具，下面是kubernetes项目的v1.11.2版本的代码行数统计，go的代码行数已经超过了100万行。\n2. SpaceVim 我个人不是vim工具党，刚毕业那会曾经一度热衷于将vim打造成为一个开发C++的IDE，但经过复杂的配置后仍然难以达到CLion这种IDE的水平。最近偶然看到SpaceVim，心中为之一振，这就是我想要的vim，虽达不到IDEA的高度，但已经可以跟vscode的易用度差不多了。\nSpaceVim的强大之处在于Space键的使用，默认情况下按下空格键会给出快捷键的提示，类似于桌面系统中的菜单功能。\n","date":"2018-08-26T17:38:29Z","permalink":"/post/knowledge-share-1/","title":"知识分享第一期"},{"content":"macvlan的原理是在宿主机物理网卡上虚拟出多个子接口，每个子接口有独立的mac地址，通过不同的MAC地址在数据链路层（Data Link Layer）进行网络数据转发的。达到的效果类似，一块物理网卡上有多个IP地址，多个IP地址有自己的mac地址。\n它是比较新的网络虚拟化技术，需要较新的内核支持（Linux kernel v3.9–3.19 and 4.0+）。\nmacvlan设备跟物理设备之间并不直接互通。\nmacvlan不以交换机端口来划分vlan，一个交换机端口可接收来自多个mac地址的数据。\n一个交换机端口要处理多个vlan的数据，需要开启trunk模式。\n四种模式 以下四种模式为每个macvlan设备单独配置，而不是一个物理设备就只有一个配置。\nVEPA 所有发送出去的报文都经过交换机，交换机再发送到对应的目标地址。默认模式。物理网卡接收到macvlan设备的数据后，总是将数据发送出去，即使是发往本设备上其他macvlan设备的数据包。这样在交换机设备上可以看到所有网络的流量。如果是本机的macvlan设备流量仍然是发往本机的macvlan设备，可能会被交换机的生成树协议阻止。需要交换机开启hairpin模式或者reflective relay模式，该模式在目前的交换机上未广泛支持，vepa模式的应用较少。\nlinux的网桥支持hairpin模式。\nbridge 最常用，同一个物理设备上的不同macvlan设备间的通讯可以直接转发，不再需要经过外部的交换机。转发非常快速，macvlan设备相对固定，不需要生成树协议。\nPrivate 本质上是VEPA模式，但同一个物理设备上的macvlan设备之间无法直接通讯，不常用。\nPassthru 后来增加的模式，比较少用\nvepa和passthru都会将不同macvlan接口之间的数据发送到交换机，然后发回，对性能的影响比较明显。\n物理网卡收到包后，根据包的mac地址来判断这个包交给哪个虚拟接口。\n手工创建实践 以下实验为在virturalbox虚拟机下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 创建两个network namespace net1和net2 ip netns add net1 ip netns add net2 # 创建macvlan接口 # enp0s8相当于物理网卡eth0 ip link add link enp0s8 mac1 type macvlan # 可以看到创建了接口mac1@enp0s8 [root@localhost vagrant]# ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff 3: enp0s8: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:cf:b0:b4 brd ff:ff:ff:ff:ff:ff 4: docker_gwbridge: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:8e:cf:a9:da brd ff:ff:ff:ff:ff:ff 5: br-b3e83aa45886: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:bf:b4:e5:36 brd ff:ff:ff:ff:ff:ff 6: docker0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:25:e7:40:32 brd ff:ff:ff:ff:ff:ff 19: br-ec6c4e77321d: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:e0:d8:38:6d brd ff:ff:ff:ff:ff:ff 24: mac1@enp0s8: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff 创建macvlan接口的格式为：ip link add link \u0026lt;PARENT\u0026gt; \u0026lt;NAME\u0026gt; type macvlan， 是macvlan接口的父接口名称，name是新创建的macvlan接口名称。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 将mac1放入到net1 namespace中 [root@localhost vagrant]# ip link set mac1 netns net1 [root@localhost vagrant]# ip netns exec net1 ip link 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 24: mac1@if3: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff link-netnsid 0 # 在net1中将mac1接口命名为eth0 [root@localhost vagrant]# ip netns exec net1 ip link set mac1 name eth0 [root@localhost vagrant]# ip netns exec net1 ip link 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 24: eth0@if3: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff link-netnsid 0 # 在net1中分配eth0网卡的ip地址为192.168.8.120 [root@localhost vagrant]# ip netns exec net1 ip addr add 192.168.8.120/24 dev eth0 [root@localhost vagrant]# ip netns exec net1 ip link set eth0 up [root@localhost vagrant]# ip netns exec net1 ip link 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 24: eth0@if3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff link-netnsid 0 在docker上的实践 1 2 3 4 5 6 7 8 9 10 11 12 [root@localhost vagrant]# docker network create -d macvlan --subnet=10.0.2.100/24 --gateway=10.0.2.2 -o parent=enp0s3 mcv 5c637798d559471bd8d1036cdd947d3949e1973f724568da066d9c60c00fb5e6 [root@localhost vagrant]# docker network ls NETWORK ID NAME DRIVER SCOPE b6a128f1730e bridge bridge local a65957cd6c3f docker_gwbridge bridge local 540bb390028a host host local b3e83aa45886 isolated_nw bridge local ec6c4e77321d local_alias bridge local 5c637798d559 mcv macvlan local 3d247d0414d0 none null local reference Some notes on macvlan/macvtap ","date":"2018-08-13T23:29:58Z","permalink":"/post/linux-macvlan-network/","title":"Linux macvlan network"},{"content":"pushd和popd命令的用法 在编写shell的时候，经常会在目录之间进行切换，如果使用cd命令经常会切换错误，pushd和popd使用栈的方式来管理目录。\ndirs 用于显示当前目录栈中的所有记录。\npushd 将目录加入到栈顶部，并将当前目录切换到该目录。若不加任何参数，该命令用于将栈顶的两个目录进行对调。\npopd 删除目录栈中的目录。若不加任何参数，则会首先删除目录栈顶的目录，并将当前目录切换到栈顶下面的目录。\n命令格式：pushd [-N | +N] [-n]\n+N 将第N个目录删除（从左边数起，数字从0开始） -N 将第N个目录删除（从右边数起，数字从0开始） -n 将目录出栈时，不切换目录 example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@localhost tmp]# mkdir /tmp/dir{1,2,3,4} [root@localhost tmp]# pushd /tmp/dir1 /tmp/dir1 /tmp [root@localhost dir1]# pushd /tmp/dir2 /tmp/dir2 /tmp/dir1 /tmp [root@localhost dir2]# pushd /tmp/dir3 /tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp [root@localhost dir3]# pushd /tmp/dir4 /tmp/dir4 /tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp # dirs的显示内容跟pushd完成后的输出一致 [root@localhost dir4]# dirs /tmp/dir4 /tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp [root@localhost dir4]# popd /tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp # 带有数字比较容易出错 [root@localhost dir3]# popd +1 /tmp/dir3 /tmp/dir1 /tmp # 清除目录栈 [root@localhost dir3]# dirs -c [root@localhost dir3]# dirs /tmp/dir3 ","date":"2018-08-10T18:09:56Z","permalink":"/post/pushd%E5%92%8Cpopd%E5%91%BD%E4%BB%A4%E7%9A%84%E7%94%A8%E6%B3%95/","title":"pushd和popd命令的用法"},{"content":"docker image结构 docker可以通过命令docker image inspect ${image}来查看image的详细信息，其中包含了所使用的底层文件系统及各层的信息。\ndocker container的存储结构分为了只读层、init层和可读写层。\n只读层跟docker image的层次结构恰好对应，主要包含操作系统的文件、配置、目录等信息，不包含操作系统镜像。\ninit层在只读层和读写层中间，用来专门存放/etc/hosts /etc/resolv.conf等信息，这些文件往往需要在启动的时候写入一些指定值，但不期望docker commit命令对其进行提交。\n可读写层为容器在运行中可以进行写入的层。\noverlay 采用了两层结构，lowerdir为镜像层，只读。upperdir为容器层。\n每层都会在/var/run/docker/overlay创建一个文件夹，文件夹中为实际层的内容，文件采用硬链接的方式链接到真实层中的文件，每一层都包含该层该拥有的所有文件，而该文件的真实存储可能是采用硬链接的方式链接到上层中的真实文件，因此比较耗费inode。\n创建一个容器时，会新增两个目录，一个为读写层，一个为初始层。初始层中保存了容器初始化时的环境信息，如hostname、hosts文件等。读写层用于记录容器的所有改动。\noverlay2 为了规避overlay消耗inode节点过多的问题，overlay2采用在每层中增加lower文件的方式来记录所有底层的信息，类似于链表的形式。\ndocker pull ubuntu\n1 2 3 4 5 6 7 8 9 10 [root@localhost runc]# docker pull ubuntu Using default tag: latest latest: Pulling from library/ubuntu c64513b74145: Pull complete 01b8b12bad90: Pull complete c5d85cf7a05f: Pull complete b6b268720157: Pull complete e12192999ff1: Pull complete Digest: sha256:3f119dc0737f57f704ebecac8a6d8477b0f6ca1ca0332c7ee1395ed2c6a82be7 Status: Downloaded newer image for ubuntu:latest 会在/var/run/docker/overlay2目录下创建如下文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 [root@localhost overlay2]# tree -L 2 . |-- 664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672 // 第0层 | |-- diff | `-- link // MZUEUOFHBNVTRCJYJEG7QY4VWT |-- 783ad02709b67ac47b55198e9659c4592f0972334987ab97f42fd10f1784cbba // 第2层 | |-- diff | |-- link // HXATFASQ4E2JBG434DUEN54EZZ | |-- lower // l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT | |-- merged | `-- work |-- 89f7a20dda3d868840e20d9e8f1bfe20c5cca51c27b07825f100da0f474672f6 // 第3层 | |-- diff | |-- link // 5PHT7S3MCZTTQOXVPA4CKJRRFD | |-- lower // l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT | |-- merged | `-- work |-- backingFsBlockDev |-- bad073a2d1f79a03af6caa0b3f51a22e6762cebbc0c30e45458fe6c1ff266f68 // 第4层 | |-- diff | |-- link // QMKHIPSDT4JTPE4FLT7QGJ33ND | |-- lower // l/5PHT7S3MCZTTQOXVPA4CKJRRFD:l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT | |-- merged | `-- work |-- cb40b5b47c699050305676b35b1cea1ce08b38604dd68243c4be48934125b1a3 // 第1层 | |-- diff | |-- link // WUZC5WSTQTPJUJ4KFAYCUT5IPD | |-- lower // l/MZUEUOFHBNVTRCJYJEG7QY4VWT | |-- merged | `-- work `-- l |-- 5PHT7S3MCZTTQOXVPA4CKJRRFD -\u0026gt; ../89f7a20dda3d868840e20d9e8f1bfe20c5cca51c27b07825f100da0f474672f6/diff |-- HXATFASQ4E2JBG434DUEN54EZZ -\u0026gt; ../783ad02709b67ac47b55198e9659c4592f0972334987ab97f42fd10f1784cbba/diff |-- MZUEUOFHBNVTRCJYJEG7QY4VWT -\u0026gt; ../664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672/diff |-- QMKHIPSDT4JTPE4FLT7QGJ33ND -\u0026gt; ../bad073a2d1f79a03af6caa0b3f51a22e6762cebbc0c30e45458fe6c1ff266f68/diff `-- WUZC5WSTQTPJUJ4KFAYCUT5IPD -\u0026gt; ../cb40b5b47c699050305676b35b1cea1ce08b38604dd68243c4be48934125b1a3/diff l目录下为超链接，缩短后的目录，为了避免mount时超出页大小限制。\n每一层中的diff文件夹包含实际内容。\n每一层中都有一个link文件，内容为l目录中的超链接，超链接实际指向当前层目录中的diff文件夹。\n除去最底层的目录外，其余每一层中包含一个lower文件，包含了该层的所有更底层名称和顺序，可以根据该文件构建出整个镜像的层次结构。\nwork目录用于OverlayFS内部使用。\n最底层只有link文件，无lower文件，因此664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672为最底层。\n以上五层为lower，只读。\n当使用docker run -it ubuntu:latest /bin/bash启动一个容器后，在overlay2目录下会多出两个文件夹。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 [root@localhost overlay2]# tree -L 1 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08 l 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init |-- diff |-- link // ZJVMGTB2IOJ6QF57TYM5O7EWXW |-- lower // l/QMKHIPSDT4JTPE4FLT7QGJ33ND:l/5PHT7S3MCZTTQOXVPA4CKJRRFD:l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT |-- merged `-- work 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08 |-- diff |-- link |-- lower // l/ZJVMGTB2IOJ6QF57TYM5O7EWXW:l/QMKHIPSDT4JTPE4FLT7QGJ33ND:l/5PHT7S3MCZTTQOXVPA4CKJRRFD:l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT |-- merged `-- work l |-- 5PHT7S3MCZTTQOXVPA4CKJRRFD -\u0026gt; ../89f7a20dda3d868840e20d9e8f1bfe20c5cca51c27b07825f100da0f474672f6/diff |-- AOLYGFOHIAHWU5CBAJFULNAXI7 -\u0026gt; ../0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/diff |-- HXATFASQ4E2JBG434DUEN54EZZ -\u0026gt; ../783ad02709b67ac47b55198e9659c4592f0972334987ab97f42fd10f1784cbba/diff |-- MZUEUOFHBNVTRCJYJEG7QY4VWT -\u0026gt; ../664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672/diff |-- QMKHIPSDT4JTPE4FLT7QGJ33ND -\u0026gt; ../bad073a2d1f79a03af6caa0b3f51a22e6762cebbc0c30e45458fe6c1ff266f68/diff |-- WUZC5WSTQTPJUJ4KFAYCUT5IPD -\u0026gt; ../cb40b5b47c699050305676b35b1cea1ce08b38604dd68243c4be48934125b1a3/diff `-- ZJVMGTB2IOJ6QF57TYM5O7EWXW -\u0026gt; ../0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init/diff 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init用于存放容器初始化时的信息，通过下面查看更直观。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@localhost overlay2]# tree 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init |-- diff | |-- dev | | `-- console | `-- etc | |-- hostname | |-- hosts | |-- mtab -\u0026gt; /proc/mounts | `-- resolv.conf |-- link |-- lower |-- merged `-- work `-- work 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08的直接底层为init层，更详细的目录结构如下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 [root@localhost overlay2]# tree -L 2 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08 |-- diff |-- link |-- lower |-- merged | |-- bin | |-- boot | |-- dev | |-- etc | |-- home | |-- lib | |-- lib64 | |-- media | |-- mnt | |-- opt | |-- proc | |-- root | |-- run | |-- sbin | |-- srv | |-- sys | |-- tmp | |-- usr | `-- var `-- work `-- work merged文件夹中内容较多，为overlay2的直接挂载点，对容器的修改会反应到该目录中。例如在容器中增加/root/hello.txt文件，在merged目录下会增加root/hello.txt文件。\n1 2 3 [root@localhost overlay2]# mount | grep overlay2 /dev/mapper/centos-root on /var/lib/docker/overlay2 type xfs (rw,relatime,attr2,inode64,noquota) overlay on /var/lib/docker/overlay2/0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/ZJVMGTB2IOJ6QF57TYM5O7EWXW:/var/lib/docker/overlay2/l/QMKHIPSDT4JTPE4FLT7QGJ33ND:/var/lib/docker/overlay2/l/5PHT7S3MCZTTQOXVPA4CKJRRFD:/var/lib/docker/overlay2/l/HXATFASQ4E2JBG434DUEN54EZZ:/var/lib/docker/overlay2/l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:/var/lib/docker/overlay2/l/MZUEUOFHBNVTRCJYJEG7QY4VWT,upperdir=/var/lib/docker/overlay2/0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/diff,workdir=/var/lib/docker/overlay2/0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/work) ref Use the OverlayFS storage driver Docker存储驱动—Overlay/Overlay2「译」 ","date":"2018-07-28T21:30:25Z","permalink":"/post/%E7%90%86%E8%A7%A3overlayfs/","title":"理解OverlayFS"},{"content":"本文绝大多数题目来源于网络，部分题目为原创。\nslice相关 以下代码有什么问题，说明原因 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 type student struct { Name string Age int } func pase_student() { m := make(map[string]*student) stus := []student{ {Name: \u0026#34;zhou\u0026#34;, Age: 24}, {Name: \u0026#34;li\u0026#34;, Age: 23}, {Name: \u0026#34;wang\u0026#34;, Age: 22}, } for _, stu := range stus { m[stu.Name] = \u0026amp;stu } } 每次遍历的时候stu变量为值拷贝，stu变量的地址未改变，即\u0026amp;stu未改变，遍历结束后stu指向stus中的最后一个元素。\n使用reflect.TypeOf(str)打印出的类型为main.student，如果使用stu.Age += 10这样的语法是不会修改stus中的值的。\n可修改为如下形式：\n1 2 3 for i, _ := range stus { m[stus[i].Name] = \u0026amp;stus[i] } 有一个slice of object, 遍历slice修改name为指定的值 1 2 3 4 5 6 7 8 type foo struct { name string value string } func mutate(s []foo, name string) { // TODO } 意在考察range遍历的时候是值拷贝，以及slice的内部数据结构，slice的数据结构如下：\n1 2 3 4 5 6 struct Slice { // must not move anything byte* array; // actual data uintgo len; // number of elements uintgo cap; // allocated number of elements }; 执行append函数后会返回一个新的Slice对象，新的Slice对象跟旧Slice对象共用相同的数据存储，但是len的值并不相同。\n该题目中，可以通过下面的方式来修改值:\n1 2 3 4 5 6 7 8 9 // range方式 for i, _ := range s { s[i].name = name } // for i形式 for i:=0; i\u0026lt;len(s); i++ { s[i].name = name } 从slice中找到一个元素匹配name，并将该元素的指针添加到一个新的slice中，返回新slice 1 2 3 4 func find(s []foo, name string) []*foo { // TODO } 仍旧是考察range是值拷贝的用法，此处使用for i 循环即可\n1 2 3 4 5 6 7 8 9 10 11 func find(s []foo, name string) []*foo { res := []*foo{} for i := 0; i \u0026lt; len(s); i++ { if s[i].name == name { res = append(res, \u0026amp;(s[i])) break } } return res } 下面输出什么内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package main import \u0026#34;fmt\u0026#34; func m(s []int) { s[0] = -1 s = append(s, 4) } func main() { s1 := []int{1, 2, 3} m(s1) s2 := make([]int, 3, 6) m(s2) s2 = append(s2, 7) s3 := [3]int{1, 2, 3} fmt.Println(s1) fmt.Println(s2) fmt.Println(s3) } slice的函数传递为值拷贝方式，在函数m中对下标为0的元素的修改会直接修改原slice中的值，因为slice中的指针指向的地址是相同的。\nappend之后的slice虽然可能是在原数组上增加了元素，但原slice中的len字段并没有变化。\nmake([]int, 3, 6)虽然指定了slice的cap，但对于append没有影响，还是会在slice中最后一个元素的下一个位置增加新元素。\n数组由于是值拷贝，对新数组的修改不会影响到原数组。\n输出内容如下：\n1 2 3 [-1 2 3] [-1 0 0 7] [1 2 3] 下面输出什么内容 该题目为我自己想出来的，非来自于互联网，意在考察对slice和append函数的理解。\n1 2 3 4 5 6 7 8 9 func f() { s1 := make([]int, 2, 8) fmt.Println(s1) s2 := append(s1, 4) fmt.Println(s2) s3 := append(s1, 5) fmt.Println(s3) fmt.Println(s2) } 输出结果如下，在执行第二个append后，第一个append在内存中增加的元素4会被5覆盖掉。执行结果可以通过fmt.Println(s1, cap(s1), \u0026amp;s1[0])的形式将第一个元素的内存地址打印出来查看。\n1 2 3 [0 0 4] [0 0 5] [0 0 5] goroutine 以下代码输出内容： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; ) func main() { runtime.GOMAXPROCS(1) go func() { fmt.Println(1) }() for { } fmt.Println(1) } 不会有任何输出\n下面输出的内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type People struct{} func (p *People) ShowA() { fmt.Println(\u0026#34;showA\u0026#34;) p.ShowB() } func (p *People) ShowB() { fmt.Println(\u0026#34;showB\u0026#34;) } type Teacher struct { People } func (t *Teacher) ShowB() { fmt.Println(\u0026#34;teacher showB\u0026#34;) } func main() { t := Teacher{} t.ShowA() } 输出\n1 2 showA showB 有点出乎意料，可以举个反例，如果ShowA()方法会调用到Teacher类型的ShowB()方法，假设People和Teacher并不在同一个包中时，编译一定会出现错误。\nGo中没有继承机制，只有组合机制。\n下面代码会触发异常吗？请详细说明 1 2 3 4 5 6 7 8 9 10 11 12 13 func main() { runtime.GOMAXPROCS(1) int_chan := make(chan int, 1) string_chan := make(chan string, 1) int_chan \u0026lt;- 1 string_chan \u0026lt;- \u0026#34;hello\u0026#34; select { case value := \u0026lt;-int_chan: fmt.Println(value) case value := \u0026lt;-string_chan: panic(value) } } 会间歇性触发异常，select会随机选择。\n以下代码能编译过去吗？为什么？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;fmt\u0026#34; ) type People interface { Speak(string) string } type Student struct{} func (stu *Student) Speak(think string) (talk string) { if think == \u0026#34;bitch\u0026#34; { talk = \u0026#34;You are a good boy\u0026#34; } else { talk = \u0026#34;hi\u0026#34; } return } func main() { var peo People = Student{} think := \u0026#34;bitch\u0026#34; fmt.Println(peo.Speak(think)) } 不能编译过去，提示Stduent does not implement People (Speak method has pointer receiver)，将Speak定义更改为func (stu Stduent) Speak(think string) (talk string)即可编译通过。\nmain的调用方式更改为如下也可以编译通过var peo People = new(Stduent)。\nfunc (stu *Stduent) Speak(think string) (talk string)是*Student类型的方法，不是Stduent类型的方法。\n下面输出什么 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;runtime\u0026#34; ) func main() { runtime.GOMAXPROCS(1) arr := [10000]int{} for i:=0; i\u0026lt;len(arr); i++ { arr[i] = i } for _, a := range arr { go func() { fmt.Println(a) }() } for { time.Sleep(time.Second) } } 一直输出9999.涉及到goroutine的切换时机，仅系统调用或者有函数调用的情况下才会切换goroutine，for循环情况下一直没有系统调用或函数切换发生，需要等到for循环结束后才会启动新的goroutine。\n以下代码打印出来什么内容，说出为什么。。。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main import ( \u0026#34;fmt\u0026#34; ) type People interface { Show() } type Student struct{} func (stu *Student) Show() { } func live() People { var stu *Student return stu } func main() { if live() == nil { fmt.Println(\u0026#34;AAAAAAA\u0026#34;) } else { fmt.Println(\u0026#34;BBBBBBB\u0026#34;) } } 打印BBBBBBB。\nbyte与rune的关系 byte alias for uint8 rune alias for uint32，用来表示unicode 1 2 3 4 5 6 7 8 func main() { // range遍历为rune类型，输出int32 for _, w:=range \u0026#34;123\u0026#34; { fmt.Printf(\u0026#34;%T\u0026#34;, w) } // 取数组为byte类型，输出uint8 fmt.Printf(\u0026#34;%T\u0026#34;, \u0026#34;123\u0026#34;[0]) } 写出打印的结果 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type People struct { name string `json:\u0026#34;name\u0026#34;` } func main() { js := `{ \u0026#34;name\u0026#34;:\u0026#34;11\u0026#34; }` var p People p.name = \u0026#34;123\u0026#34; err := json.Unmarshal([]byte(js), \u0026amp;p) if err != nil { fmt.Println(\u0026#34;err: \u0026#34;, err) return } fmt.Println(\u0026#34;people: \u0026#34;, p) } 打印结果为people: {123}\n下面函数有什么问题？ 1 2 3 func funcMui(x,y int)(sum int,error){ return x+y,nil } 函数返回值命名 在函数有多个返回值时，只要有一个返回值有指定命名，其他的也必须有命名。 如果返回值有有多个返回值必须加上括号； 如果只有一个返回值并且有命名也需要加上括号； 此处函数第一个返回值有sum名称，第二个为命名，所以错误。\n以下函数输出什么 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package main func main() { println(DeferFunc1(1)) println(DeferFunc2(1)) println(DeferFunc3(1)) println(DeferFunc4(1)) } func DeferFunc1(i int) (t int) { t = i defer func() { t += 3 }() return t } func DeferFunc2(i int) int { t := i defer func() { t += 3 }() return t } func DeferFunc3(i int) (t int) { defer func() { t += i }() return 2 } func DeferFunc4(i int) (t int) { t = 10 return 2 } 输出结果为: 4 1 3 2\nreturn语句不是一个原子指令，分为两个阶段，执行return后面的表达式和返回表达式的结果。defer函数在返回表达式之前执行。\n执行return后的表达式给返回值赋值 调用defer函数 空的return DeferFunc1在第一步执行表达式后t=1，执行defer后t=4，返回值为4\nDeferFunc2在第一步执行表达式后t=1，执行defer后t=4，返回值为第一步表达式的结果1\nDeferFunc3在第一步表达式为t=2，执行defer后t=3，返回值为t=3\nDeferFunc4在第一步执行表达式后t=2，返回值为t=2\n是否可以编译通过？如果通过，输出什么？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 package main import ( \u0026#34;fmt\u0026#34; ) func main() { sn1 := struct { age int name string }{age: 11, name: \u0026#34;qq\u0026#34;} sn2 := struct { age int name string }{age: 11, name: \u0026#34;qq\u0026#34;} sn3 := struct { name string age int }{age: 11, name: \u0026#34;qq\u0026#34;} if sn1 == sn2 { fmt.Println(\u0026#34;sn1 == sn2\u0026#34;) } if sn1 == sn3 { fmt.Println(\u0026#34;sn1 == sn3\u0026#34;) } sm1 := struct { age int m map[string]string }{age: 11, m: map[string]string{\u0026#34;a\u0026#34;: \u0026#34;1\u0026#34;}} sm2 := struct { age int m map[string]string }{age: 11, m: map[string]string{\u0026#34;a\u0026#34;: \u0026#34;1\u0026#34;}} if sm1 == sm2 { fmt.Println(\u0026#34;sm1 == sm2\u0026#34;) } } 结构体比较 进行结构体比较时候，只有相同类型的结构体才可以比较，结构体是否相同不但与属性类型个数有关，还与属性顺序相关。\n还有一点需要注意的是结构体是相同的，但是结构体属性中有不可以比较的类型，如map,slice。 如果该结构属性都是可以比较的，那么就可以使用“==”进行比较操作。\n是否可以编译通过？如果通过，输出什么？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import ( \u0026#34;fmt\u0026#34; ) func Foo(x interface{}) { if x == nil { fmt.Println(\u0026#34;empty interface\u0026#34;) return } fmt.Println(\u0026#34;non-empty interface\u0026#34;) } func main() { var x *int = nil Foo(x) } 输出“non-empty interface”\n交替打印数字和字母 使用两个 goroutine 交替打印序列，一个 goroutine 打印数字， 另外一个 goroutine 打印字母， 最终效果为: 12AB34CD56EF78GH910IJ1112KL1314MN1516OP1718QR1920ST2122UV2324WX2526YZ2728\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { number, letter := make(chan bool), make(chan bool) wait := new(sync.WaitGroup) go func () { num := 1 for { \u0026lt;-number fmt.Printf(\u0026#34;%d%d\u0026#34;, num, num+1) num += 2 letter \u0026lt;- true if num \u0026gt; 28 { break } } wait.Done() }() go func () { begin := \u0026#39;A\u0026#39; for { \u0026lt;- letter if begin \u0026lt; \u0026#39;Z\u0026#39; { fmt.Printf(\u0026#34;%c%c\u0026#34;, begin, begin+1) begin+=2 number \u0026lt;- true } else { break } } wait.Done() }() number \u0026lt;- true wait.Add(2) wait.Wait() } struct类型的方法调用 假设T类型的方法上接收器既有T类型的，又有T指针类型的，那么就不可以在不能寻址的T值上调用T接收器的方法。\n请看代码,试问能正常编译通过吗？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import ( \u0026#34;fmt\u0026#34; ) type Lili struct{ Name string } func (Lili *Lili) fmtPointer(){ fmt.Println(\u0026#34;poniter\u0026#34;) } func (Lili Lili) fmtReference(){ fmt.Println(\u0026#34;reference\u0026#34;) } func main(){ li := Lili{} li.fmtPointer() } 能正常编译通过，并输出\u0026quot;poniter\u0026quot;\n请接着看以下的代码，试问能编译通过？\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import ( \u0026#34;fmt\u0026#34; ) type Lili struct{ Name string } func (Lili *Lili) fmtPointer(){ fmt.Println(\u0026#34;poniter\u0026#34;) } func (Lili Lili) fmtReference(){ fmt.Println(\u0026#34;reference\u0026#34;) } func main(){ Lili{}.fmtPointer() } 不能编译通过。 “cannot call pointer method on Lili literal” “cannot take the address of Lili literal”\n其实在第一个代码示例中，main主函数中的“li”是一个变量，li的虽然是类型Lili，但是li是可以寻址的，\u0026amp;li的类型是Lili，因此可以调用Lili的方法。\ngolang context包的用法 goroutine之间的传值 goroutine之间的控制 在单核cpu的情况下，下面输出什么内容？ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { wg := sync.WaitGroup{} for _, i:=range []int{1, 2, 3, 4, 5} { wg.Add(1) go func() { defer wg.Done() fmt.Println(i) } () } wg.Wait() } 考察golang的runtime机制，goroutine的切换时机只有在有系统调用或者函数调用时才会发生，本例子中的for循环结束之前不会发生goroutine的切换，所以最终输出结果为5.\n下面输出什么 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \u0026#34;fmt\u0026#34; ) type People interface { Speak(string) string } type Stduent struct{} func (stu *Stduent) Speak(think string) (talk string) { if think == \u0026#34;bitch\u0026#34; { talk = \u0026#34;You are a good boy\u0026#34; } else { talk = \u0026#34;hi\u0026#34; } return } func main() { var peo People = Stduent{} think := \u0026#34;bitch\u0026#34; fmt.Println(peo.Speak(think)) } 编译不通过，仅*Student实现了People接口，更改为var peo People = \u0026amp;Student{}即可编译通过。\n下面输出什么 1 2 3 4 5 6 7 8 9 10 package main const cl = 100 var bl = 123 func main() { println(\u0026amp;bl, bl) println(\u0026amp;cl, cl) } 编译失败，常量cl通常在预处理阶段会直接展开，无法取其地址。\n以下代码是否存在问题，请解释你的判断和理由 1 2 3 4 5 6 7 8 import \u0026#34;sync\u0026#34; func f(m sync.Mutex) { m.Lock() defer m.Unlock() // Do something... } Mutex对象不能被值拷贝,后续传递需要使用指针的形式\n以下代码输出是什么 解释一下 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func main() { case1() case2() } func case1() { s1 := make([]string, 1, 20) s1[0] = \u0026#34;hello\u0026#34; p1 := \u0026amp;s1[0] s1 = append(s1, \u0026#34;world\u0026#34;) *p1 = \u0026#34;hello2\u0026#34; fmt.Printf(\u0026#34;value of p1 is %s, value of s1[0] is %s \\n\u0026#34;, *p1, s1[0]) } func case2() { s1 := make([]string) s1[0] = \u0026#34;hello\u0026#34; p1 := \u0026amp;s1[0] s1 = append(s1, \u0026#34;world\u0026#34;) *p1 = \u0026#34;hello2\u0026#34; fmt.Printf(\u0026#34;value of p1 is %s, value of s1[0] is %s \\n\u0026#34;, *p1, s1[0]) } 本题意在考察string和slice的数据结构，string的数据结构如下：\ncase1的内存结构变化情况如下：\ncase2由于s1默认长度为0，直接使用s1[0]复制会出现panic错误。\nref golang 面试题 Go面试题答案与解析 golang面试笔试题(第二版) interview-go Awesome Go Interview Questions and Answers Golang面试题解析（二） Golang面试题解析（三） golang错题集 ","date":"2018-07-27T20:25:38Z","permalink":"/post/golang%E9%9D%A2%E8%AF%95%E9%A2%98/","title":"Golang面试题"},{"content":"通过使用Google的登陆二步验证（即Google Authenticator服务），我们在登陆时需要输入额外由手机客户端生成的一次性密码。大大提高登陆的安全性。\n实现Google Authenticator功能需要服务器端和客户端的支持。服务器端负责密钥的生成、验证一次性密码是否正确。客户端记录密钥后生成一次性密码。\ngoogle实现了基于时间的TOTP算法（Time-based One-time Password），客户端实现包括了android和ios。\n算法为公开算法，google没有提供服务端的实现，各个语言都有单独的实现。自己系统使用可以直接使用网上的代码。\nlinux下有libpam-google-authenticator模块，可以使用yum或者源码编译安装，github上有源码，编译出来的为so文件，可以加到sshd的配置文件中，用于给sshd提供二次认证机制。\n客户端和服务端存在时间差的问题，google authenticator的超时时间为30s，服务端可以使用两个30s的时间来验证，包括当前和上一个30s。\n","date":"2018-02-27T22:34:17Z","permalink":"/post/google-autheticator%E5%BA%94%E7%94%A8%E7%8E%B0%E7%8A%B6/","title":"google autheticator应用现状"},{"content":"曾经使用多说和网易云评论作为博客的评论系统，不幸都相继倒闭后，博客就一直没有评论系统。虽博客的访问量可以忽略不计，但本着折腾和好奇的原则，还是折腾一下gitment。\n更新hexo-theme-next主题 最新版本的next主题已经默认支持gitment，需要将next主题升级到最新版本。\n我的hexo-theme-next使用单独的git项目进行管理，git地址为：https://github.com/kuring/hexo-theme-next。接下来需要将fork出来的git项目跟next的git项目进行同步。\n在本地创建名字为upstream的remote，指向地址为：git remote add upstream https://github.com/theme-next/hexo-theme-next.git\n拉取next项目到本地分支，本地的分支，执行git fetch upstream\n将upsteam/master分支合并到master分支上\n1 2 3 4 5 6 7 8 9 10 git checkout master # 由于修改了_config.yml文件，存在冲突，合并失败 lvkai@osx:~/blog/kuring/themes/hexo-theme-next% git merge upstream/master 128 ↵ Removing source/css/_common/components/third-party/gentie.styl Removing layout/_third-party/comments/gentie.swig Auto-merging _config.yml CONFLICT (content): Merge conflict in _config.yml Removing README.en.md Automatic merge failed; fix conflicts and then commit the result. 解决冲突后提交并将master分支push到github仓库 注册gitment 前往：https://github.com/settings/profile\nDeveloper settings -\u0026gt; Register a new application\n在界面中输入如下内容：\n获取到Client ID和Client Secret.\n新建github repo 创建新的github项目：https://github.com/kuring/gitment-comments\n在next主题中设置gitment next主题的配置文件为theme/next/_config.yml，修改其中的gitment设置如下，\nclient_id为在github中注册所获取到的client id client_secret为在github中注册所获取到的client secret github_repo为上面新创建的github repo名称 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Gitment # Introduction: https://imsun.net/posts/gitment-introduction/ gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide \u0026#39;Powered by ...\u0026#39; on footer, and more language: # Force language, or auto switch by theme github_user: kuring # MUST HAVE, Your Github ID github_repo: gitment-comments # MUST HAVE, The repo you use to store Gitment comments client_id: xxx # MUST HAVE, Github client id for the Gitment client_secret: xxxx # EITHER this or proxy_gateway, Github access secret token for the Gitment proxy_gateway: # Address of api proxy, See: https://github.com/aimingoo/intersect redirect_protocol: # Protocol of redirect_uri with force_redirect_protocol when mint enabled 执行hexo clean \u0026amp;\u0026amp; hexo g \u0026amp;\u0026amp; hexo s重新生成页面并在本地运行，可以看到gitment组件已经可以显示了，但是提示Error: Comments Not Initialized错误，点击login，然后允许认证，即可消除该错误。\n在界面上添加评论后，可以在github repo的issuse中看到，整个搭建完毕。\n","date":"2018-02-22T22:57:06Z","permalink":"/post/hexo%E6%B7%BB%E5%8A%A0gitment%E8%AF%84%E8%AE%BA%E7%B3%BB%E7%BB%9F/","title":"hexo添加gitment评论系统"},{"content":"概要 netfilter与iptables的关系 linux在内核中对数据包过滤和修改的模块为netfilter，netfilter模块本身并不对数据包进行过滤，只是允许将过滤数据包或修改数据包的函数hook到内核网络协议栈的适当位置。\niptables是用户态的工具，用于向netfilter中添加规则从而实现报文的过滤和修改等功能，工作在ip层。ebtables工作在数据链路层，用于处理以太网帧。\n图中绿色代表iptables的表，可以看到有部分位于了数据链路层，之所以产生这种奇怪的架构，原因是bridge_nf模块，因为bridge工作在数据链路层，不一定会经过网络层，但仍然需要iptables的功能。详细信息可以在ebtables/iptables interaction on a Linux-based bridge中了解。\n概念：tables -\u0026gt; chains -\u0026gt; rules\niptabels介绍 chain 每个表都由一组内置的链，还可以添加用户自定义链，只是用户自定义链没有钩子可以触发，需要从其他链通过-j即JUMP进行触发。\nINPUT 链：发往本机的报文 OUTPUT 链：由本机发出的报文 FORWARD 链：经由本机转发的报文 PREROUTING 链：报文到达本机，进行路由决策之前 POSTROUTING 链：报文由本机发出，进行路由决策之后 从chain的角度考虑数据包的流向：\n到本机某进程的报文：PREROUTING -\u0026gt; INPUT 由本机转发的报文：PREROUTING -\u0026gt; FORWARD -\u0026gt; POSTROUTING 由本机某进程发出的报文：OUTPUT -\u0026gt; POSTROUTING 当一个网络包进入一台机器的时候，首先拿下 MAC 头看看，是不是我的。如果是，则拿下 IP 头来。得到目标 IP 之后呢，就开始进行路由判断。在路由判断之前，这个节点我们称为 PREROUTING。如果发现 IP 是我的，包就应该是我的，就发给上面的传输层，这个节点叫作 INPUT。如果发现 IP 不是我的，就需要转发出去，这个节点称为 FORWARD。如果是我的，上层处理完毕完毕后，一般会返回一个处理结果，这个处理结果会发出去，这个节点称为 OUTPUT，无论是 FORWARD 还是 OUTPUT，都是路由判断之后发生的，最后一个节点是 POSTROUTING。\ntable 有了chain的概念后，为了便于chain中rule的管理，又引入了table的概念，用于存放相同功能的rule，不同功能的rule放到不同的table中。\n包括：filter nat mangle raw\nfilter 默认表，管理本机数据包的进出，用于实现包的过滤，对应内核模块iptables_filter\ninput：想要进入linux主机的包 output：linux主机要发送的包 forward：传递包到后端计算机，与nat table关联较多\nnat 管理后端主机进出，与linux主机没有关系，与linux后的主机有关\nprerouting：进行路由判断之前的规则(dnat/redirect) postrouting：路由判断之后执行的规则(snat/masquerade) output：与发出去的包有关\nmangle 较少使用，用于拆解报文，修改数据包，并重新封装。\nraw raw表的主要作用是允许我们给某些特定的数据包打上标记。\nrule 包含了匹配条件和处理动作。\n匹配条件包括：source ip、destination ip、source port、destination port\n处理动作包括：\naccept: 将包交给协议栈 drop：直接丢弃数据包，不给任何回应 reject：拒绝数据包通过，并给一个响应信息，客户端会收到拒绝消息 queue: 交个某个用户态进程处理 dnat：目的地址转换 snat：源地址转换，必须要指定SNAT地址，即\u0026ndash;to-source参数，可以是单个ip，也可以是网段。用在POSTROUTING链上。 masquerade: 源地址伪装，跟snat类似，不需要指定SNAT地址，会自动从服务器上获取SNAT的ip地址。如果有多个网卡的情况下，会使用路由选择算法。 mark: 对数据包进行打标签操作 table filter rule的关系 这三者之间的关系还是相当的绕。\n链中的规则存在的表 chain中存放了rule，某些chain中注定不包含某些rule。例如prerouting链中的rule仅存在于nat raw mangle三张表中。\nprerouting链中的规则存在的表：raw mangle nat input链中的规则存在的表：mangle filter nat forward链中的规则存在的表：mangle filter output链中的规则存在的表：raw mangle filter nat postrouting链中的规则存在的表：mangle nat\n表中的规则可以被哪些链使用 raw表中的规则可以被链使用：prerouting output\n表的名字为小写，链的名字为大写\n常用操作 查询 iptables 查询默认的表为filter，默认会列出表中所有链的规则\n-t 用于指定要操作的表，支持raw mangle filter nat，省略-t选项，默认使用filter表 -L 列出rule -v 可查看更详细的信息 -n 规则中以ip地址的形式进行显示 \u0026ndash;line-number 显示规则的编号 -x 包的计数以精确数字显示 iptables -t filter -L：从表的角度查询规则，用于查看filter表中的所有规则\niptables -L INPUT: 从链的角度查询规则，用于查看INPUT链中的所有规则\niptables -vL INPUT: 从链的角度查询规则，用于查看INPUT链中的所有规则，可查看更详细信息，包含了规则的匹配信息\niptables -nvL：以精确数字显示\n修改 -F: 清空规则 -I: 表示插入规则 -A: 表示以追加的方式插入规则 --dport: 目的端口 --sport: 源端口 -s: 源ip -d: 目的ip --match-set：匹配ipset iptables -F INPUT：清空filter表中的INPUT链中的所有规则。\n删除 -D: 删除规则，iptables -D 链名 规则编号，其中规则编号可以通过--line-number查看到。 -F: 清空规则，iptables -F 链名 -t 表名 -X: 删除链 iptables -X 链名 -t 表名 trace 开启trace功能\n1 2 3 4 5 # centos7 系统下有效，centos6下内核模块为ipt_LOG $ modprobe nf_log_ipv4 # 用来验证module是否加载成功 $ sysctl net.netfilter.nf_log.2 要开启icmp协议的追踪，执行如下的命令\n1 2 iptables -t raw -A OUTPUT -p icmp -m comment --comment \u0026#34;TRACE\u0026#34; -j TRACE iptables -t raw -A PREROUTING -p icmp -m comment --comment \u0026#34;TRACE\u0026#34; -j TRACE 可以通过如下的命令看到插入的iptabels规则：\n1 iptables -t raw -nvL --line-number 追踪日志最终会在/var/log/message或者/var/log/kern下看到：\n1 Feb 6 11:22:04 c43k09006.cloud.k09.am17 kernel: TRACE: raw:PREROUTING:policy:3 IN=docker0 OUT= PHYSIN=bond0.9 MAC=02:42:30:fb:43:94:5c:c9:99:de:c4:8b:08:00 SRC=10.45.8.10 DST=10.45.4.99 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=25550 DF PROTO=ICMP TYPE=0 CODE=0 ID=24191 SEQ=2 格式这块的含义如下：\n\u0026ldquo;TRACE: tablename:chainname:type:rulenum \u0026quot; where type can be \u0026ldquo;rule\u0026rdquo; for plain rule, \u0026ldquo;return\u0026rdquo; for implicit rule at the end of a user defined chain and \u0026ldquo;policy\u0026rdquo; for the policy of the built in chains.\n环境清理，删除刚刚创建的规则即可，其中1为规则的编号：\n1 2 3 4 5 # 可以通过此来查询之前创建的规则编号 iptables -t raw --line-number -nvL # 删除规则 iptables -t raw -D PREROUTING 1 iptables -t raw -D OUTPUT 1 实战 试验1 基本规则管理 插入规则 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 # 清空filter表中的input链规则 [vagrant@localhost ~]$ sudo iptables -F INPUT # 查看filter表中的详细规则，此时从其他机器上ping该ip是通的 [vagrant@localhost ~]$ sudo iptables -nvL INPUT Chain INPUT (policy ACCEPT 7 packets, 388 bytes) pkts bytes target prot opt in out source destination # 增加规则，拒绝192.168.33.1上的请求 # -I：表示插入 # INPUT为要插入的链 # -s：表示源ip地址 # -j：表示要执行的动作 [vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.1 -j DROP # 再次查询filter表中的规则，此时192.168.33.1上的报文已经不通 [vagrant@localhost ~]$ sudo iptables -t filter -nvL Chain INPUT (policy ACCEPT 107 packets, 6170 bytes) pkts bytes target prot opt in out source destination 0 0 DROP all -- * * 192.168.33.1 0.0.0.0/0 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 56 packets, 4355 bytes) pkts bytes target prot opt in out source destination # appent一条接收192.168.33.1的请求规则 [vagrant@localhost ~]$ sudo iptables -t filter -A INPUT -s 192.168.33.1 -j ACCEPT # 新增加的序号为2，192.168.33.1的包匹配到1后就停止往下走，因此192.168.33.1还是ping不通当前主机 [vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-number Chain INPUT (policy ACCEPT 65 packets, 3572 bytes) num pkts bytes target prot opt in out source destination 1 9 756 DROP all -- * * 192.168.33.1 0.0.0.0/0 2 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 # 插入一条ACCEPT rule，此时192.168.33.1可以ping通当前主机，新插入的规则优先 [vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.1 -j ACCEPT [vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-number Chain INPUT (policy ACCEPT 7 packets, 388 bytes) num pkts bytes target prot opt in out source destination 1 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 2 10 840 DROP all -- * * 192.168.33.1 0.0.0.0/0 3 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 # 新插入一条accept 192.168.33.2的规则，插入位置为2，可以看到插入到2的位置了 [vagrant@localhost ~]$ sudo iptables -t filter -I INPUT 2 -s 192.168.33.2 -j ACCEPT [vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-number Chain INPUT (policy ACCEPT 7 packets, 388 bytes) num pkts bytes target prot opt in out source destination 1 1 84 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 2 0 0 ACCEPT all -- * * 192.168.33.2 0.0.0.0/0 3 10 840 DROP all -- * * 192.168.33.1 0.0.0.0/0 4 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 删除规则 接下在上面实验的基础上测试删除规则\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 删除刚刚创建的规则2 [vagrant@localhost ~]$ sudo iptables -t filter -D INPUT 2 [vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-number Chain INPUT (policy ACCEPT 7 packets, 388 bytes) num pkts bytes target prot opt in out source destination 1 1 84 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 2 10 840 DROP all -- * * 192.168.33.1 0.0.0.0/0 3 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 # 删除source为192.168.33.1，动作为ACCEPT的规则，实际此时执行一次命令仅能删除一条 [vagrant@localhost ~]$ sudo iptables -t filter -D INPUT -s 192.168.33.1 -j ACCEPT [vagrant@localhost ~]$ sudo iptables -nvL INPUT --line Chain INPUT (policy ACCEPT 13 packets, 736 bytes) num pkts bytes target prot opt in out source destination 1 11 936 DROP all -- * * 192.168.33.1 0.0.0.0/0 修改规则 在上面实验的基础上修改规则\n1 2 3 4 5 6 7 8 9 10 # 将规则动作从REJECT更改为REJECT [vagrant@localhost ~]$ sudo iptables -t filter -R INPUT 1 -s 192.168.33.1 -j REJECT [vagrant@localhost ~]$ sudo iptables -nvL INPUT --line Chain INPUT (policy ACCEPT 7 packets, 388 bytes) num pkts bytes target prot opt in out source destination 1 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable # 每个链都有一个默认规则，当前INPUT链中的默认为ACCEPT # 以下可以修改INPUT链的默认规则为DROP，远程连接慎用，不要问我为什么 [vagrant@localhost ~]$ sudo iptables -t filter -P INPUT DROP 保存规则 防火墙的所有修改都是临时的，重启系统后会失效。iptables会读取/etc/sysconfig/iptables中的规则。\n1 2 3 4 5 # iptables-save命令仅会打印当前的规则，需要使用重定向当前规则到文件中 [root@localhost system]# iptables-save \u0026gt; /etc/sysconfig/iptables # 可以从规则文件中载入规则 [root@localhost system]# iptables-restore \u0026lt; /etc/sysconfig/iptables 实验二 各类匹配条件的使用 匹配条件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 可一次性插入两条规则 [vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.1,192.168.33.2 -j DROP [vagrant@localhost ~]$ sudo iptables -t filter -nvL INPUT --line Chain INPUT (policy ACCEPT 31 packets, 1744 bytes) num pkts bytes target prot opt in out source destination 1 0 0 DROP all -- * * 192.168.33.2 0.0.0.0/0 2 0 0 DROP all -- * * 192.168.33.1 0.0.0.0/0 # 可指定ip网段 [vagrant@localhost ~]$ sudo iptables -t filter -F INPUT [vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.0/24 -j DROP [vagrant@localhost ~]$ sudo iptables -t filter -nvL INPUT --line Chain INPUT (policy ACCEPT 7 packets, 388 bytes) num pkts bytes target prot opt in out source destination 1 0 0 DROP all -- * * 192.168.33.0/24 0.0.0.0/0 [vagrant@localhost ~]$ sudo iptables -t filter -I INPUT ! -s 192.168.33.0/24 -j DROP [vagrant@localhost ~]$ sudo iptables -t filter -nvL INPUT --line Chain INPUT (policy ACCEPT 19 packets, 1048 bytes) num pkts bytes target prot opt in out source destination 1 0 0 DROP all -- * * !10.0.2.0/24 0.0.0.0/0 协议类型 使用-p来指定协议类型，支持tcp udp icmp等，不指定时默认匹配所有协议\n网卡接口 -i来指定从某个网卡进入的流量，仅使用于PREROUTING INPUT FORWARD三条链。\n-o来指定从某个网络流出的流量，仅适用于FORWARD OUTPUT POSTROUTING三条链。\n实验三 扩展模块 端口 使用了扩展模块tcp udp，默认可以省略\n--dport来匹配报文的目的端口，使用时必须指定协议，即-p选项。 --sport来匹配报文的源端口，使用时必须指定协议，即-p选项。\n端口可以指定范围，例如22:25表示22-25之间的所有端口，22,25表示22和25端口，还可以配合起来使用，比如22,80:88表示22和80-88之间的端口。\n1 2 3 4 5 6 # 可以指定目的端口的范围 [root@localhost vagrant]# iptables -t filter -I INPUT -s 192.168.33.1 -p tcp --dport 22:25 -j REJECT [root@localhost vagrant]# iptables -t filter -nvL INPUT --line Chain INPUT (policy ACCEPT 70 packets, 4024 bytes) num pkts bytes target prot opt in out source destination 1 0 0 REJECT tcp -- * * 192.168.33.1 0.0.0.0/0 tcp dpts:22:25 reject-with icmp-port-unreachable iprange扩展模块 iprange扩展模块可以指定一段连续的ip地址范围。\n--src-range和--dst-range用来指定源地址和目的范围。\n1 2 3 4 5 [root@localhost vagrant]# iptables -t filter -I INPUT -m iprange --src-range 192.168.33.1-192.168.33.10 -j DROP [root@localhost vagrant]# iptables -t filter -nvL INPUT --line Chain INPUT (policy ACCEPT 17 packets, 968 bytes) num pkts bytes target prot opt in out source destination 1 0 0 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 source IP range 192.168.33.1-192.168.33.10 string扩展模块 匹配报文中包含的字符串\n1 2 3 4 5 6 # 匹配报文中包含XXOO的报文 [root@localhost vagrant]# iptables -t filter -I INPUT -m string --algo bm --string \u0026#34;XXOO\u0026#34; -j REJECT [root@localhost vagrant]# iptables -t filter -nvL INPUT --line Chain INPUT (policy ACCEPT 15 packets, 852 bytes) num pkts bytes target prot opt in out source destination 1 0 0 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 STRING match \u0026#34;XXOO\u0026#34; ALGO name bm TO 65535 reject-with icmp-port-unreachable 其他扩展 time扩展用来根据时间段进行匹配\nconnlimit用来对ip的并发连接数进行限制\nlimit模块限制单位时间内进出包的数量\ntcp扩展中可以使用--tcp-flags可根据tcp flag进行匹配\nstate扩展可根据tcp的连接状态进行匹配\n实验四 自定义链 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # 创建自定义链 IN_WEB [root@localhost vagrant]# iptables -t filter -N IN_WEB [root@localhost vagrant]# iptables -nvL Chain INPUT (policy ACCEPT 31 packets, 1780 bytes) pkts bytes target prot opt in out source destination Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 16 packets, 1216 bytes) pkts bytes target prot opt in out source destination Chain IN_WEB (0 references) pkts bytes target prot opt in out source destination [root@localhost vagrant]# iptables -t filter -I IN_WEB -s 192.168.33.1 -j REJECT [root@localhost vagrant]# iptables -t filter -I IN_WEB -s 192.168.33.2 -j REJECT [root@localhost vagrant]# iptables -t filter -nvL IN_WEB --line Chain IN_WEB (0 references) num pkts bytes target prot opt in out source destination 1 0 0 REJECT all -- * * 192.168.33.2 0.0.0.0/0 reject-with icmp-port-unreachable 2 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable # 将IN_WEB自定义链添加到INPUT链上 [root@localhost vagrant]# iptables -t filter -I INPUT -p tcp --dport 80 -j IN_WEB # 可以看到INPUT链中多出了IN_WEB链 [root@localhost vagrant]# iptables -nvL Chain INPUT (policy ACCEPT 35 packets, 2012 bytes) pkts bytes target prot opt in out source destination 0 0 IN_WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 18 packets, 1408 bytes) pkts bytes target prot opt in out source destination Chain IN_WEB (1 references) pkts bytes target prot opt in out source destination 0 0 REJECT all -- * * 192.168.33.2 0.0.0.0/0 reject-with icmp-port-unreachable 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable # 重新定义自定链名字 [root@localhost vagrant]# iptables -E IN_WEB WEB [root@localhost vagrant]# iptables -nvL Chain INPUT (policy ACCEPT 39 packets, 2244 bytes) pkts bytes target prot opt in out source destination 0 0 WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80 Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 20 packets, 1520 bytes) pkts bytes target prot opt in out source destination Chain WEB (1 references) pkts bytes target prot opt in out source destination 0 0 REJECT all -- * * 192.168.33.2 0.0.0.0/0 reject-with icmp-port-unreachable 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable # 由于iptables有自定义链，不能删除 [root@localhost vagrant]# iptables -X WEB iptables: Too many links. # 将INPUT链引用的WEB链删除 [root@localhost vagrant]# iptables -D INPUT 1 # 此时仍不能删除自定义链，因为自定义链删除，需要上面没有任何规则 [root@localhost vagrant]# iptables -X WEB iptables: Directory not empty. # 先清空自定义链的规则后可以删除 [root@localhost vagrant]# iptables -F WEB [root@localhost vagrant]# iptables -X WEB ref iptables详解系列 Iptables Tutorial 1.2.2 ","date":"2018-01-31T23:25:12Z","permalink":"/post/iptables%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/","title":"iptables基础知识"},{"content":"为了其他主机可访问docker registry，必须采用https协议。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 registry_data_dir=~/docker_registry/data cert_dir=~/docker_registry/certs signdomain=mycert # 制作证书 mkdir -p ${cert_dir} openssl req -nodes -subj \u0026#34;/C=CN/ST=BeiJing/L=BeiJing/CN=$signdomain\u0026#34; -newkey rsa:4096 -keyout ${cert_dir}/$signdomain.key -out ${cert_dir}/${signdomain}.csr openssl x509 -req -days 3650 -in ${cert_dir}/$signdomain.csr -signkey ${cert_dir}/${signdomain}.key -out ${cert_dir}/$signdomain.crt # 从docker hub拉取registry镜像，并启动镜像 mkdir -p ${registry_data_dir} docker run -d -p 15000:5000 --restart=always --name registry \\ -v ${registry_data_dir}:/var/lib/registry \\ -v ${cert_dir}:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/${signdomain}.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/${signdomain}.key \\ registry:2 停止registry镜像并删除的命令为：\n1 docker stop registry \u0026amp;\u0026amp; docker rm -v registry 下载最新的centos7镜像\n1 docker pull centos:7.3.1611 将centos7镜像增加tag\n1 2 3 4 5 6 7 8 docker tag centos:7.3.1611 127.0.0.1:15000/centos:7.3 # 可以看到列表中会多出一个镜像 [root@103-17-184-lg-201-k08 data]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/registry 2 047218491f8c 4 weeks ago 33.17 MB 103-17-184-lg-201-k08.yidian.com:5000/centos 7.3 67591570dd29 3 months ago 191.8 MB docker.io/centos 7.3.1611 67591570dd29 3 months ago 191.8 MB docker push命令仅支持https协议，签名已经启动了自签名的https协议的registry，为了能够让docker能够信任registry，需要在/etc/docker/certs.d/目录下增加相应的crt文件，增加后的目录结构为/etc/docker/certs.d/103-17-184-lg-201-k08.yidian.com:5000/103-17-184-lg-201-k08.yidian.com.crt，添加完成后需要重启docker服务。\n将image push到registry\n1 docker push 103-17-184-lg-201-k08.yidian.com:5000/centos:7.3 api 列出images：https://10.103.17.184:5000/v2/_catalog 列出image的tags：https://10.103.17.184:5000/v2/centos/tags/list 可以直接通过curl命令来访问api：curl --cacert 103-17-184-lg-201-k08.yidian.com.crt -v https://103-17-184-lg-201-k08.yidian.com:5000/v2\nref registry docker创建私有仓库 Docker Registry（官方教程） Registry API ","date":"2017-12-07T18:44:59Z","permalink":"/post/docker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%90%AD%E5%BB%BA/","title":"docker私有仓库搭建"},{"content":"某些情况下需要搭建自己的yum源，比如维持特定的软件包版本等，只需要从网上下载合适的rpm包，即可构建yum源。\nrepodata数据 创建/data/yum.repo目录用来存放rpm包。\n可以使用yumdownloader命令来下载rpm包到本地，并且不安装。这里以安装mesos为例，在/data/yum.repo目录下执行yumdownloader mesos即可下载mesos的rpm包到本地。\n安装createrepo：yum install createrepo，用来根据rpm包产生对应的包信息。\n每加入一个rpm包需要更新下repo的信息，执行createrepo --update /data/yum.repo。会自动产生repodata目录。\n搭建web服务 需要对外提供web服务，通常会使用nginx或者apache来对外提供服务，这里使用python SimpleHTTPServer来对外提供服务，执行cd /data/yum.repo \u0026amp;\u0026amp; python -m SimpleHTTPServer 1080。\n客户端的repo文件设置 安装yum优先级插件，用来设置yum源的优先级: yum install -y yum-plugin-priorities\n每个需要使用该yum源的客户端需要在/etc/yum.repo.d/目录下增加devops.repo文件。\n1 2 3 4 5 6 [devops] name=dev-ops baseurl=http://10.103.17.184:1080/ enabled=1 gpgcheck=0 priority=1 ","date":"2017-12-07T12:10:19Z","permalink":"/post/yum%E6%BA%90%E6%90%AD%E5%BB%BA/","title":"yum源搭建"},{"content":"本次grafana的升级从版本3.1.1，变更为4.4.3，涉及到一个大的版本跨度。同时之前在使用的存储为sqlite，趁着这次升级更改为mysql。\ngrafana升级 直接从官网下载对应的4.4.3版本的二进制包，修改部分配置即可，该部分没任何难度。\nsqlite to mysql 由于grafana使用的表结构在3.1.1到4.4.3之间有变更，不能直接将3.1.1版本的sqlite中的数据导入到4.4.3的mysql中。我的方法为先使用3.1.1版grafana将数据从sqlite导入到mysql中，然后再升级grafana的版本，grafana可以自动修改表结构。\n在的mysql中创建grafana的数据库，并修改数据库的编码为utf-8.\n修改grafana 3.1.1配置文件conf/defaults.ini如下：\n1 2 3 4 5 6 7 8 9 10 11 [database] # You can configure the database connection by specifying type, host, name, user and password # as separate properties or as on string using the url property. # Either \u0026#34;mysql\u0026#34;, \u0026#34;postgres\u0026#34; or \u0026#34;sqlite3\u0026#34;, it\u0026#39;s your choice type = mysql host = xx.xx.xx.xx:3306 name = grafana user = dev # If the password contains # or ; you have to wrap it with triple quotes. Ex \u0026#34;\u0026#34;\u0026#34;#password;\u0026#34;\u0026#34;\u0026#34; password = dev 启动grafana后会自动在grafana数据库中创建相应的表结构，接下来就是将sqlite中的数据导入到mysql中。\n在data目录下增加如下脚本sqlitedump.sh，并执行sh sqlitedump.sh grafana.db \u0026gt; grafana.sql。\n1 2 3 4 5 6 7 8 9 #!/bin/sh DB=$1 TABLES=$(sqlite3 $DB .tables | grep -v migration_log) for t in $TABLES; do echo \u0026#34;TRUNCATE TABLE $t;\u0026#34; done for t in $TABLES; do echo -e \u0026#34;.mode insert $t\\nselect * from $t;\u0026#34; done | sqlite3 $DB 然后将grafana.sql导入到新创建的mysql。\n将grafana 3.1.1版本停掉，将grafana 4.4.3版本的配置指向到mysql数据库，启动grafana 4.4.3后，mysql中的表结构会自动变更。\n至此，grafana的升级完成。\n","date":"2017-09-13T00:22:51Z","permalink":"/post/grafana%E5%8D%87%E7%BA%A7/","title":"grafana升级"},{"content":"由于不允许通过ssh直接连接服务器，即服务器的22端口是不开放的，但是其他端口号可以访问。这就造成了往服务器上传输文件会特别麻烦，需要通过relay中转一下。\nrsync命令有shell模式和daemon模式，为了解决该问题，可以通过rsync的daemon模式，rysnc的daemon模式会默认使用873端口，不使用ssh协议，以此来绕过ssh的22端口限制。\n最终可以实现在本地通过rsync一条命令直接同步文件或文件夹到服务器的指定目录下。\n首先在服务器上搭建rsync的服务端，rsync的安装不再介绍。\n修改服务器的rsync配置文件/etc/rsyncd.conf如下：\n1 2 3 4 5 6 [worker] path = /home/worker list = true uid = worker gid = worker read only = false 这里为了简便，并未设置rsync的用户名和密码。\n客户端同步文件的命令如下：\n1 rsync -avz $SRC worker@$HOST::worker --exclude=target --exclude=.git --exclude=.idea --delete 命令中的第一个worker为HOST的登录用户名，第二个worker为rysncd配置文件中配置的组名。\u0026ndash;exclude选项可以用来屏蔽需要同步的文件夹。\u0026ndash;delete选项用来同步删除的文件或文件夹。\ndaemon模式跟ssh模式相比，无法指定服务器的具体某一个路径，使用不够灵活，但也基本可以满足需求。只能通过daemon配置文件中配置的组中的path参数，同步时仅能通过::组名的形式来指定。\n","date":"2017-06-07T11:58:54Z","permalink":"/post/%E9%80%9A%E8%BF%87rsync%E6%9D%A5%E7%BB%95%E8%BF%87relay%E5%90%8C%E6%AD%A5%E6%96%87%E4%BB%B6/","title":"通过rsync来绕过relay同步文件"},{"content":"最近公司需要首先登录跳板机relay，然后通过跳板机才能登录服务器，操作上略显麻烦。为了节省登录服务器的时间，我编写了一个简单的脚本来简化登录操作。\n实现效果为在本地terminal下，执行wrelay $host，即可自动登录到相应的主机。\n在relay服务器上增加对其他服务器的免登录命令 在relay服务器上ssh到其他主机时需要输入密码，使用expect命令来登录到其他主机时通过expect脚本来实现自动输入密码并登录的功能。\n在/home/$user目录下新建mybin文件夹，并将mybin文件夹添加到$PATH环境变量中，具体修改方法不展开。\n在mybin目录下增加gw脚本，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/expect if {$argc \u0026lt; 1} { puts \u0026#34;Usage:cmd \u0026lt;host\u0026gt;\u0026#34; exit 1 } set host [lindex $argv 0] # 在这里填写要登录的用户 set username \u0026#34;worker\u0026#34; # 在这里填写要登录的密码 set password \u0026#34;worker\u0026#34; spawn ssh $username@$host set timeout 2 expect { \u0026#34;*password:\u0026#34; { send \u0026#34;$password\\n\u0026#34; } \u0026#34;Are you sure you want to continue connecting (yes/no)?\u0026#34; { send \u0026#34;yes\\r\u0026#34; exp_continue } } expect \u0026#34;*#\u0026#34; interact 执行gw 10.1.1.8，即可登录到对应的主机上。\n本地主机免登录relay服务器，并自动登录到对应的服务器 在本地自动登录relay主机同样使用expect的方式，脚本名称为wrelay，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #!/usr/bin/expect if {$argc \u0026lt; 1} { puts \u0026#34;Usage:cmd \u0026lt;remote_host\u0026gt;\u0026#34; exit 1 } # 下面指定relay主机 set host \u0026#34;relay.name\u0026#34; # 这里输入relay的用户名 set username \u0026#34;\u0026#34; # 这里输入relay的密码 set password \u0026#34;\u0026#34; set remote_host [lindex $argv 0] spawn ssh $username@$host set timeout 2 expect { \u0026#34;*password:\u0026#34; { send \u0026#34;$password\\n\u0026#34; } \u0026#34;Are you sure you want to continue connecting (yes/no)?\u0026#34; { send \u0026#34;yes\\r\u0026#34; exp_continue } } expect \u0026#34;*#\u0026#34; sleep 0.1 # 在relay上自动登录到其他服务器主机 send \u0026#34;gw $remote_host\\n\u0026#34; interact ","date":"2017-05-17T16:04:01Z","permalink":"/post/%E8%87%AA%E5%8A%A8%E9%80%9A%E8%BF%87%E8%B7%B3%E6%9D%BF%E6%9C%BA%E7%99%BB%E5%BD%95%E5%88%B0%E5%85%B6%E4%BB%96%E6%9C%8D%E5%8A%A1%E5%99%A8/","title":"自动通过跳板机登录到其他服务器"},{"content":"清明节假期突然想起了我好久不更的blog，看到Farbox官网上的《2016，终结了几个产品》，说明Farbox已经停止更新了。虽然我挺喜欢Farbox这个项目，也见证了Farbox的成长及作者做产品的思考，在这里也向作者致敬。\n我当时开始准备启用Farbox之前试用过jekyll，翻遍了整个github，也没找到个合我心意的theme。幸好是Farbox的出现，让我眼前一亮，这就是我想好的blog系统了。Farbox的停更使我不得不考虑重新换个blog，虽然2016的文章数量仅为罕见的个位数，但有可能今年有时间会多写一些。\n近几年hexo特别的火，在试看了官方文档了解功能及考虑了blog的迁移成本后，心想，这就是我想要的blog系统了。hexo该有的功能全都有，甚至比Farbox要强大很多。Farbox的很多设计思想跟hexo相仿，但hexo显的更加自由，blog需要自己一手搭建完成。\n当然hexo要想使用的好，做一些全面的了解及折腾是必不可少的，毕竟最终利用的Github pages是个静态的系统。早已没有了想当年翻遍整个github上jekyll theme的精力了，我这次的基调是能少折腾就少折腾，毕竟blog我也不是经常写，访问量也更是少的可怜，就当全面了解下当前最火的hexo就好了。\ntheme 本着不折腾原则，直接启用了很火的hexo-theme-next，文档比较全，维护比较及时。基本上按照文档走一遍，该配置的就都可以配置上了。\n代码同步 代码通过git同步是必备技能。\nhexo项目代码同步 hexo采用的是node.js环境，而Github pages是静态的，因此Github pages上仅能存储的是hexo编译后的静态文件，这些静态文件直接通过hexo d部署到kuring.github.com仓库中就可以了。\n而对于项目中的_config.yml、md文件我直接用git同步到Github上另外一个项目hexo_bak中了。网上还有思路是同步到kuring.github.com上的另外一个分支，我感觉太啰嗦，容易出错，还不如直接分开来的简便。\ntheme项目的代码同步 theme项目中也包含了部分自己的配置及修改，我这里选择的同步策略为从github上fork对应的theme项目，然后clone fork下来的项目到本地，然后直接在theme的项目中通过git命令同步到github fork的项目中。\n网上也有思路是通过git subtree的方式来解决，我仍然感觉太啰嗦，不采用。\n但这样一个blog项目需要多个git仓库，git push起来会比较麻烦，好在theme一般不怎么修改。\n评论系统 之前用多说的时候也没几个评论的，用起来还不错，至少比被墙了的disqus要好很多，可是多说这么好的项目要关闭了。我直接使用了国内的网易云跟帖来满足评论的需求。\n站内搜索 站内搜索是必不可少的功能，next主题提供了多种选择，我直接使用了hexo-generator-searchdb通过本地搜索来完成，生成的xml文件目前还比较小，效果还可以。\n常用命令 启用本地server端：hexo clean \u0026amp;\u0026amp; hexo g \u0026amp;\u0026amp; hexo s 部署到github：hexo d 发布文章：hexo new 文章url 使用hexo new draft test会在source/_drafts目录下创建对应文件，此时文件不会生成页面，用于存放未写完的文章。hexo publish draft test命令可将_drafts下的文章移动到_posts目录下，并添加创建时间等信息。\n收个尾 blog总算迁移完成了，期望今年能多写上几篇。\n","date":"2017-04-06T23:33:47Z","permalink":"/post/blog%E4%BB%8Efarbox%E8%BF%81%E7%A7%BB%E5%88%B0%E4%BA%86hexo/","title":"blog从farbox迁移到了hexo"},{"content":"目前php-fpm的服务部署在了docker中，对php-fpm的log和php error log可以通过syslog协议的形式发送出去，而php-fpm的slow log却不能配置为syslog协议，只能输出到文件中，因为一条slow log的是有多行组成的。\n在docker中使用时发现fpm-slowlog不能正常输出，后经发现是docker默认没有ptrace系统调用的权限，而slow log的产生需要该系统调用。通过在docker启动的时候增加\u0026quot;\u0026ndash;cap-add SYS_PTRACE\u0026quot;启动项可修正该问题。\n为了收集slow log，可以通过logstash、flume等工具进行收集，本文采用logstash对slow log进行收集，并将收集的log写入到kafka中，便于后续的处理。logstash的input采用读取文件的方式，即跟tail -f的原理类似。为了能够将多行日志作为一行，采用了filter中的multiline来对多行日志进行合并操作。logstash的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 input { file { path =\u0026gt; [“/var/log/php-fpm/fpm-slow.log\u0026#34;] } } filter { multiline { pattern =\u0026gt; \u0026#34;^$\u0026#34; negate =\u0026gt; true what =\u0026gt; \u0026#34;previous\u0026#34; } } output { stdout{codec =\u0026gt; rubydebug} kafka { codec =\u0026gt; plain { format =\u0026gt; “tag|%{host}%{message}\u0026#34; } topic_id =\u0026gt; \u0026#34;fpm-slowlog\u0026#34; bootstrap_servers =\u0026gt; “kafka1.hostname:8082,kafka2.hostname:8082\u0026#34; } } ","date":"2016-06-10T00:00:00Z","permalink":"/post/logstash-php-fpm/","title":"使用logstash收集php-fpm slow log"},{"content":"ELK解析nginx日志\n最近使用ELK搭建了一个nginx的日志解析环境，中间遇到一些挫折，好不容易搭建完毕，有必要记录一下。\nnginx nginx配置文件中的日志配置如下：\n1 2 3 4 5 6 error_log /var/log/nginx/error.log; log_format main \u0026#39;$remote_addr [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log main; logstash 由于是测试环境，我这里使用logstash读取nginx日志文件的方式来获取nginx的日志，并且仅读取了nginx的access log，对于error log没有关心。\n使用的logstash版本为2.2.0，在log stash程序目录下创建conf文件夹，用于存放解析日志的配置文件，并在其中创建文件test.conf，文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 input { file { path =\u0026gt; [\u0026#34;/var/log/nginx/access.log\u0026#34;] } } filter { grok { match =\u0026gt; { \u0026#34;message\u0026#34; =\u0026gt; \u0026#34;%{IPORHOST:clientip} \\[%{HTTPDATE:time}\\] \\\u0026#34;%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\\\u0026#34; %{NUMBER:http_status_code} %{NUMBER:bytes} \\\u0026#34;(?\u0026lt;http_referer\u0026gt;\\S+)\\\u0026#34; \\\u0026#34;(?\u0026lt;http_user_agent\u0026gt;\\S+)\\\u0026#34; \\\u0026#34;(?\u0026lt;http_x_forwarded_for\u0026gt;\\S+)\\\u0026#34;\u0026#34; } } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;10.103.17.4:9200\u0026#34;] index =\u0026gt; \u0026#34;logstash-nginx-test-%{+YYYY.MM.dd}\u0026#34; workers =\u0026gt; 1 flush_size =\u0026gt; 1 idle_flush_time =\u0026gt; 1 template_overwrite =\u0026gt; true } stdout{codec =\u0026gt; rubydebug} } 需要说明的是，filter字段中的grok部分，由于nginx的日志是格式化的，logstash解析日志的思路为通过正则表达式来匹配日志，并将字段保存到相应的变量中。logstash中使用grok插件来解析日志，grok中message部分为对应的grok语法，并不完全等价于正则表达式的语法，在其中增加了变量信息。\n具体grok语法不作过多介绍，可以通过logstash的官方文档中来了解。但grok语法中的变量类型如IPORHOST并未找到具体的文档，只能通过在logstash的安装目录下通过grep -nr \u0026quot;IPORHOST\u0026quot; .来搜索具体的含义。\n配置文件中的stdout部分用于打印grok解析结果的信息，在调试阶段一定要打开。\n可以通过这里来验证grok表达式的语法是否正确，编写grok表达式的时候可以在这里编写和测试。\n对于elasticsearch部分不做过多介绍，网上容易找到资料。\nkibana kibana不做过多介绍，使用可以查看官方文档和自己摸索。\nreference logstash中的grok插件介绍\n","date":"2016-02-18T00:00:00Z","permalink":"/post/elk_nginx/","title":"ELK解析nginx日志"},{"content":"曾经使用过多种科学上网方式，​最近尝试了使用aws的免费试用一年的功能搭建shadowsocks，访问google的速度非常不错，比很多收费的服务要好用，amazon真是良心企业！\n本文用于记录在aws上搭建服务的步骤及其中的一些注意事项，步骤不会太详细，aws上关于主机的功能需要读者自己在试验的过程中去自己探索。\n注册aws账号 为了能够搭建搭建aws服务，拥有一个amazon账号是必须的，在aws免费套餐的页面点击『创建免费账号』按钮即可按照步骤创建aws账号。\n值得一提的是，注册aws的账号需要一张信用卡。\n开启EC2主机实例 该步骤的目的是开启aws上的主机实例。​\n进入aws的控制面板，在左上角的服务中选择EC2，aws提供了多种类型的主机，这里选择EC2即可。\n在EC2控制面板界面中需要选择右上角的区域，这个用于选择EC2主机所在的机房，不同机房之间主机是不可以共享的。我这里选择了『美国西部（俄勒冈）』，感觉速度还不错，没有试验过亚洲地区的，新加坡的速度是不是会更好些。后续经过验证，首尔的服务器确实速度更快一些。 ​ 下面即可创建EC2的实例了，点击界面上的『启动实例』按钮即可按照步骤创建EC2实例了，创建实例的时候一定要选择免费的EC2主机，否则就会悲剧了。我选择了ubuntu14.04的主机，redhat7.2的主机yum源不太全，没有选择使用。\n最终会得到ssh登录用的pem文件，用于ssh远程登录主机。并在界面上启动刚刚创建的实例。\n按照shadowsocks 接下来就是在EC2实例上安装sock5代理工具了。\n登录刚刚启动的EC2，需要pem文件。可以通过ssh -i \u0026quot;key.pem\u0026quot; ubuntu@ec2-52-26-2-14.us-west-2.compute.amazonaws.com命令来登录到远程主机，其他工具请自行google。\n使用命令pip install shadowsocks来安装shadowssocks，pip命令的安装自行解决。\n在ubuntu的home目录下执行mkdir shadowsocks创建保存配置文件的文件夹，并创建配置文件config.json，内容如下：\n1 2 3 4 5 6 7 8 { \u0026#34;server\u0026#34;:\u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_port\u0026#34;:10001, \u0026#34;local_port\u0026#34;:1080, \u0026#34;password\u0026#34;:\u0026#34;xxx\u0026#34;, \u0026#34;timeout\u0026#34;:600, \u0026#34;method\u0026#34;:\u0026#34;bf-cfb\u0026#34; } 需要说明的是最好配置一下server_port选项，更改shadowsocks的默认端口号。method选项用于控制加密方式，我这里更改为了bf-cfb。\n执行nohup ssserver -c config.json \u0026amp;命令即可启动shadowsocks服务。\n由于对外增加了10001端口号，aws的默认安全策略为仅对外提供22端口，需要在EC2主机的安全策略中增加外放访问tcp端口10001的权限。\n脚本 为了安装方便，我简单写了个脚本如下\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 yum -y install epel-release #yum update -y yum install python2-pip -y pip install shadowsocks mkdir ~/shadowsocks echo \u0026#39;{ \u0026#34;server\u0026#34;:\u0026#34;0.0.0.0\u0026#34;, \u0026#34;server_port\u0026#34;:10001, \u0026#34;local_port\u0026#34;:1080, \u0026#34;password\u0026#34;:\u0026#34;xxx\u0026#34;, \u0026#34;timeout\u0026#34;:600, \u0026#34;method\u0026#34;:\u0026#34;aes-256-cfb\u0026#34; }\u0026#39; \u0026gt; ~/shadowsocks/config.json systemctl disable firewalld.service systemctl stop firewalld.service nohup ssserver -c ~/shadowsocks/config.json \u0026amp; 在某些云主机的CentOS7系统发现无法使用yum install python2-pip进行安装，原因是有些源被禁用了，可以使用yum repolist disabled来查看被禁用的源，其中会包含epel源。可以使用yum install python2-pip -y --enablerepo=epel的方式来安装。\n安装shadowsocks客户端 这里是支持的客户端列表，​我这里仅使用的mac客户端ShadowsocksX，支持Auto Proxy Mode和Global Mode两种方式，其中Auto方式会自动下载使用sock5代理的列表，非常方便。\nkcptun 为了加快访问速度，推荐使用kcp + shadowsocks\nkcp的服务端配置如下，即启用20001端口，该端口会将流量导入到127.0.0.1:10001端口，即本机的shadowsocks端口\n1 2 3 4 cd ~ \u0026amp;\u0026amp; mkdir kcptun \u0026amp;\u0026amp; cd kcptun wget https://github.com/xtaci/kcptun/releases/download/v20190109/kcptun-linux-amd64-20190109.tar.gz tar zvxf kcptun-linux-amd64-20190109.tar.gz nohup ./server_linux_amd64 -l :20001 -t 127.0.0.1:10001 -key xxx -mode fast2 --log ~/kcptun/20001.log \u0026amp; 配置了kcptun的shadowsocks客户端仅需要配置代理为远程的kcpdun端口即可，不再需要指定shadowsocks的端口，相当于shadowsocks是透明的。\n监控 为了避免aws产生额外的费用，一定要设置一下费用报警，否则被扣费了就麻烦了。\n另外，可定期查看下aws的费用。试用期为一年，一年后一定要记得停掉aws服务。\n最后，祝你玩的愉快！\n","date":"2016-02-17T00:00:00Z","permalink":"/post/aws_net/","title":"利用aws科学上网"},{"content":"按照惯例，年终总结依旧是按照农历算，农历乙未年的年终总结。\n2015年的夏天对我而言是个转折点，终于实现了我工作以来一直想北漂的梦想，受够了各种束缚，受够了带同事的各种无奈，受够了跟同事没有话题的工作，受够了天天雾霾比北京不知严重多少倍却没有一点声音，受够了满城市找不到一家互联网公司，受够了满城市找不到一个技术会议，受够了于为了工作而工作的同事们共事，受够了天天受到官本位思想的侵蚀。\n上半年在济南工作生活，下半年在北京工作生活。\n似乎一直以来我的一些人生大事都是在夏天发生的。\n工作 在济南的工作没什么好总结的，我已经在济南工作多年，似乎也没有太大的变化。\n由于互联网公司和传统行业软件公司存在较大的差异，来到一点资讯后大约适应了两个月才完全适应。到现在，让我回想之前的工作状态，感觉好陌生，好遥远。\n我的岗位为运维开发，叫做基础平台的研发更合适些。由于之前没有运维方面的工作经验，对运维开发这样的职位没有清晰的认识。对运维的需求也是在工作中逐步去摸索的。\n之前在济南工作的时候，由于从事的是传统软件行业，对技术的使用比较保守，公司中很少会采用很新的技术，比如storm、kafka等。很多行业我个人利用业余时间倒是学习了很多互联网中会用到的技术，但是由于缺乏实践机会，时间一长就忘记了。\n另外由于处理业务类型不同，使用的技术往往也不一样。比如haproxy这种反向代理软件在非互联网行业中其实用到比较少，因为非互联网行业面向的群体往往是政府和企业类的，不是使用互联网的广大用户群体，因此反向代理软件很少有永无之地。我之前待过的几家有些技术背景的非互联网公司往往会自己开发一些适合公司自己业务的类库或者技术架构，很多技术含量也是蛮高的，只是不为外界所知，组件无法公用，更没有开源的。\n当然了，随着互联网行业的发展，传统的软件企业所使用的技术也在更新，也在采用互联网企业使用的技术。\n因此，来到了互联网行业对我而言最重要的就是掌握互联网行业中会用到的技术，除了工作过程中会使用一些新技术之外，晚上下班之后也会将时间充分利用上，学习一些工作中会用到的技术。由于要学的技术实在太多了，至于先学什么后学什么，我采取了“用到什么就学什么，广撒网，后深入”的原则，这样既能达到不太影响工作，又可以达到一定广度，等到广度够了再深入了解各个技术。\n由于自己的各种问题，也造成过几次系统的线上的故障，靠谱程度还有待提高。\n在济南的环境下从来没有见过@福波和@凯荣为了工作这般拼得的同事，虽然他们这种拼得方式我学不来，但至少要从精神上拼一下。\n工作的原因，正在使用的编程也变得更多，工作中会用到C++、Java、Python、Golang、前端技术等，不同的语言使用于不同的场景下。\n工作上并没有太大的成就，主要是因为需要学习的东西太多，需需要了解，各种技术需要学习，一些开源的大数据处理技术和存在的坑也需要学习。相信随着学习的不断深入，2016年工作能小有成就。\n学习 由于我学习新知识很多时候还是看技术类的书籍，比较喜欢系统一点的学习，纸质书是我的最爱。今年买过不少的技术书籍，特别是下半年以来，至少有20本技术书的样子。之前都会系统的讲书籍看完，下半年买的技术书籍基本都是看上一半或找个技术重点就扔到一边去了。主要是因为没有了足够的时间来系统的学习整本书的内容，更多的时候是对症下药而已。\n学习的方向上逐渐转为务实，为工作所用，解决工作之所需，不像之前学的很多东西都是纸上谈兵，缺少实践的机会。\n生活 上半年生活在济南，下半年生活在北京。\n在济南的生活相对惬意，加班相对少些，虽然是单双周轮休的生活，但周末仍然是有时间出去转转的。\n来北京后，很多时候变成了工作在北京，周末回济南的生活，有时候两周回去一次，有时候一周回去一次，一般周一早上坐最早的高铁回北京。这样的生活相对单调，也确实非常辛苦，基本被工作和学习占去了绝大多数时间，剩下的时间非常有限。可以说工作就是我的生活，我的生活就是工作。\n由于很多时间都是远离家的，自己为家庭付出的确实非常少，这点深表遗憾。\n运动 之前上班的时候每天可以骑一个多小时的自行车来运动，而且坡度也比较大。后来，运动对我而言就相当匮乏了，没有活动的次数都非常有限，运动这一点做的相当不好。\n今年从夏天开始学习游泳，终于将游泳学会了，而且游泳的次数也是很多的，虽然仅仅熟练的是蛙泳。公司每周三下午有个活动时间，有一段时间每周三下午都会去游泳的，正好此时学会了游泳。\n偶然的一次机会，同学邀请打了一次台球，竟对台球感兴趣起来。\n北京对我而言还是个陌生的地方，偶尔周末有时间，会去一些地方转转。\n健康 今年下半年开始，职业症状逐渐明显，每天坐得时间一长，腰疼就来了，而且有愈演愈烈之势。后来尝试了站着办公，本来以为每天只要站着办公腰疼就不要紧了，试验后才发现站久了也是会腰疼的。目前每天基本都是站着办公了，只有站累了的情况下才会坐下来办公。\n来到北京后睡的床垫不是太舒服，后来干脆更改为了睡硬板床，早上起来后确实能明显感觉到睡硬板床轻松的多。\n颈椎也不是太好，虽然感觉到时候没有那么多，但至少没有那么健康了。\n做按摩的时候才发现自己的腰肌劳损挺严重了，也许是按摩师故意说的严重，至少按摩时疼的我直叫，也能感觉到按摩时腰部的肌肉硬块。听到按摩师的一句话挺伤感的，『你为了工作也是够拼的，把腰伤成这样』。\n汽车 一向比较排斥汽车的我也在今年的三月份拿到了驾照，并且一度产生了年底购买汽车的想法，曾经也是痴迷过一段时间的汽车节目，在做饭和骑着自行车去上班的路上听汽车广播，对常见的车型都有所了解，还去过春季车展，进过4S店，自己对车的了解也是与日俱增。曾经想过目标车型为标致2008，后来更换为马六，后来更换为昂科塞拉。曾经痴迷到在马路上见到不认识的车型就打开汽车之家的app来查看详情的地步。\n但后来来北京工作后，之前做饭和骑自行车上班的时光都变成了在公司上班，也就没有了时间来听汽车广播，渐渐的对汽车的兴趣在逐渐下降，直到现在再也不关注任何和汽车相关的信息。\n通过此来看，兴趣是可以培养的，但培养起来的兴趣，一旦放下了，兴趣就会逐渐淡掉，直到恢复到最初的状态。\n旅游 说来惭愧，2015年没有任何的旅行计划。我唯一的一次旅行是公司校园招聘时在哈尔滨稍微转了下。曾多少次家人建议出去旅行，都被我否决了。理由要么是不愿跟团，要么时间不够，要么自己不愿去，要么考虑钱的问题。\n游戏 最近几年每到周末是必然会晚上几盘dota的，主要是用来放松，另外也有点玩游戏的瘾。我之所以一直没有戒掉玩游戏，是因为我心里清楚到了一定的年龄段对dota自然不就感兴趣了。只是没想到来的这么快，近几个月对游戏玩的越来越少，有时候一周都不会晚上一次了，而且也没有特别想玩的冲动了。\n也许是因为玩游戏需要久坐，而久坐容易腰痛，也许是因为我现在已经达到了特定的年龄段，总之，都在说明，我在变老，精力确实没有之前旺盛了。\n展望 深入学习各种技术，技术更上多层楼。 至少要旅行一次，期望的目的地是日本或韩国，如果实在不行就国内。 期望能学会自由泳，蝶泳估计没戏。 多加强台球的练习。 熟悉下北京，多转转北京，去过的地方还非常非常的少。 如果有时间的话，多学一些锻炼智力的游戏，比如魔方。 工作中提高些效率，多一些生活。 多读英文技术文档。 多参加一些技术交流活动或其他活动。 \u0026hellip; ","date":"2016-02-02T00:00:00Z","permalink":"/post/2015_summary/","title":"2015年年终总结"},{"content":"本文讲述的是我家狗的普通一生，我家的狗即没有名贵的种族，也没有喜人的外貌，就是农村家中最常见的看家狗。\n狗的一生中连个正儿八经的名字都没有，父母平日里呼唤狗都是我们那方言中通用的『嗷（一声）嗷(三声)嗷（一声）』（我们那方言对家里的牲口都有一种特殊的呼唤方式），而我通常会通过舌头跟上颚发出的声音来呼唤。以至于在写本文时，我不知道该如何称呼了，只能约定俗成为『狗』。\n『狗』在家中的作用为看家护院的作用，在村中养狗的目的大抵如此，狗的作用也仅仅是传统意义上的一条狗。村民们还没有清闲到靠养狗来娱乐的地步，何况邻里邻外的都认识，随便找个人都能聊上个把小时，靠拉呱聊天来娱乐比遛狗更丰富直接。\n自打我记事起，家里共养过三只狗，第一只养了至少五年的样子，最终已经记不清楚为何而失去了，或是因为出去走丢了，亦或是因为误食了东西而死去。第三只狗是只有种族的狗，仅仅是中间的过客，确实家人的最爱，有个优雅的名字『点点』，仅此一点就能将此『狗』秒杀N条街，事实也是如此。\n家中也养过一些其他的动物，猪、猫、鸡等。猪等到长肥了也就卖掉了，也就四五个月的样子。小时候家里养过母猪，估计也得有个五年以上，但母猪除了吃和睡之外，似乎也没啥了。猫养过多只，确实跟有些猫是有感情的，但大都比较短暂，猫经常跑出去就回不来了。有些鸡在家里也养过四五年，但鸡给我留下的印象中除了吃和下蛋之外，就剩下美味的鸡汤了。唯独『狗』在家中的时间最长，在家中的地位也最高，给家里的贡献也是最大的，给我留下的印象也是颇为深刻。\n『狗』大概是我在刚上初中那会，父亲从集市上花了15或者30元钱抑或60元买的。刚买来的时候记得还不满月的样子，特别的可爱，白灰色毛居多，背上有个大大的灰黑色大圆点，不正不斜圆心就在脊梁骨的位置，而且圆是非常的标准的圆，头和尾巴是黄色的毛。由于小，不会造成什么破坏，就直接放在屋子总天天跑，我们吃饭时，它就在下面转啊转，等待着给点馒头或者菜来吃，一见到有东西吃，那尾巴就摇啊摇，摇啊摇。有时候在屋子里碍事了，我就用手揪住它背上的肉皮，扔到一边去，不一会又会回来，然后又让我给揪到一边去，这样揪来揪去的好好玩，听大人们说，这样狗是不疼的。\n揪着揪着『狗』就这样被我揪大了，大了之后自然要发挥指责了，总不能天天让家里白白养着，凡事必然有其存在价值，它的价值就是看家护院。这样一干可就是一辈子，它的一辈子就在家里不大的院子里面绕着铁链转，转啊转，直到再也没有力气转下去。自从被拴上铁链的时刻开始，直到生命的最后一刻，再也没有进过曾经在饭桌底下绕啊绕找食物的屋子，饭桌下再也容不下它。\n『狗』的看家还是非常尽职尽责，当然了，要不怎么是狗呢。只要是家里来的是陌生人，都会咬个不停，甚至是我爷爷来我家，都会咬个不停。要是觉得听起来太吵了，只要对它喊一声「狗」就会消停很多，跟家人的默契配合的相当不错。\n『狗』的记性也是相当不错的，记得上高中时，一个月只能回一次家，每次回家狗都会认识，从来不会当成陌生人狂咬。后来，上了大学，成了半年回一次家了，刚进家门狗就开始叫，也就是脚踏进院子没多久，狗就不叫了，已经认出我也曾经是这里的主人。每次回家都会跟『狗』玩上半天，喂点吃的，挠挠肚子，看着它围着我转。\n虽然对陌生人总是咬个不停，但是当人走到旁边时却不会上去咬人，对它凶点甚至还会吓得跑到狗窝里去，完全不是疯狗那样会咬人。但也确实咬过一次人，记得那是上高一那会，姨夫在我家喝多了，走到它旁边，它还在不停的咬，姨夫由于喝的多，对其踹了几脚，抑或身上酒味过重，这下『狗』可不干了，照着姨夫的脚脖子就咬了一口。这是已知的仅有的一次咬人经历。\n『狗』是母狗，大概一岁多的时候生过三只狗，其中两只已经夭折了，另外一只目前在我爷爷家里养着。家里来陌生人时，狗都会从狗窝猛然间钻出来，仿佛早发现一会陌生人就能领到奖赏一般。两只小狗的死都是它猛然间出来时将正在吃奶的小狗用铁链给带出来摔死的，挺可怜的。不知是不是脑子缺根筋，看家的本领远比看护自己孩子来的高超，就好比在休产假的妈妈，却天天想着工作，自此之后再也没生过小狗。\n『狗』的一声是用铁链禁锢的一生，成年后99%的时光都是用铁链拴着的，比互联网公司的服务可靠性都好的多。偶尔铁链会松掉，它仍然不知道自己的地盘之外仍然可以活动，直到偶然间走出了自己的活动范围，才发现原来自己的地盘外也可以肆意走动了，不过美好的时光不会太长，因为等着家人发现了，禁锢的铁链又跑到自己的脖子里了。有时家人发现的不及时，发现它又回到了原地了，也许它发现原来曾经向往的陌生地方也不过如此，远没有自己的地盘来的安全可靠和温馨。\n记得有一次冬天大雾，能见度非常低，『狗』跑出了家门，一路向北，由于雾太大，迷失了方向。我跑出去在家附近找了半个小时未果，爷爷骑着自行车出去打听到别人看见过，父亲终于在离家二里地外的地方找到了，也是虚惊一场。\n『狗』对吃得从来不挑，只要有吃的就行。记得之前家里还喂猪的时候，它就在旁边等着猪们吃完后再去吃猪剩下的，每次都会把猪槽舔的溜滑，比可以刷过的还干净。后来家里不喂猪了，就在喂鸡的时候一块喂一下。夏天吃完西瓜后，把西瓜皮扔过去，狗也可以啃得仅剩下一层薄薄的皮。偶尔忘记喂了，『狗』就会zhengzheng的叫，家人自然也能领悟『狗』的意图。\n大约刚开始工作那会，姑家的狗『点点』由于没时间照顾在我家里养了半年，『点点』是姐姐给取的名字，长得一副小巧可爱样，在城市的笼子中靠吃狗粮长大。在我们那喂狗几乎是不用狗粮的，估计也买不到，因为没市场。『点点』自然是不能跟『狗』吃一样的，每天都会味一些狗粮，后来家人就买了些过期的方便面和馒头搀和着来喂，而『狗』的主食依旧是吃着跟鸡一样的凉水活玉米面。『点点』白天都在笼子里，只有放出来的时候才会找个地方拉屎撒尿，自然比随地大小便的『狗』更讨人喜欢。晚上『点点』是放在院子里的，院子里的任何角落都是『点点』的活动范围，这点也是『狗』可望不可及的。白天『点点』的笼子是放在大门下的，逢人从门口经过都能看到可人的样子，自然也会吸引很多人的目光。\n也就是从『点点』刚来我家的时候，发现有时候从它身旁经过，它都没有任何反应了，因为『狗』的耳朵不好用了。人老了耳朵会聋，狗亦如此。有时候陌生人都进到家里了，发现『狗』仍然在狗窝里或外面太阳下睡觉，即使耳朵是贴在地面上的。狗天生骄傲的就是耳朵，试想一名工程师如果不能用双手敲代码，那怎么称之为码农。\n自从『狗』聋了后，明显感觉到『狗』的情绪变得低沉了，也许是因为耳朵不好用了，也许是进入晚年了，也许是因为『点点』的缘故，虽贵为正室，却不再受宠幸。直到半年后的『点点』走后，也一直没有缓过来。\n近几年，我每次回家都能感受到『狗』是一年不如一年，最后一次见到是在2015年的十一，那时它很多时候都是趴在窝里睡觉的，除了看家，也没啥本事，既然看家技能已失，那就只能睡觉了。\n2016.1.22傍晚听到了『狗』去世的消息，脑海中自然也是联想关于『狗』的往事，历历在目，眼角泪花直流。没有任何仪式，『狗』悄悄的离开了奉献了自己一生的岗位，摆脱了束缚了自己一辈子的锁链，黯然离开了自己的主人。\n当天天气极度寒冷，应该是一年中最冷的天气了，家中下起了大雪，也许是老天送给『狗』的葬礼，雪花仿佛跟『狗』的灵魂一般纯洁。\n算下来从2000年左右到现在该有16岁的样子了，这16年是我人生中最年轻的时光，我从懵懂的少年已变成了社会中的青年，却是『狗』的一生。\n下图是2016.1.22上午时的照片，已经生命垂危，生命在倒计时。\n一个月前，父亲又弄了条小狗，凑巧的是，小狗的模样跟『狗』长的颇为相似，毛色也一致，甚至连背上也有一个灰黑色的圆点。虽没当面见过，但看小狗的照片不禁联想到『狗』的小时候仿佛是狗的小时候，这也许是『狗』的生命的延续。\n","date":"2016-01-23T00:00:00Z","permalink":"/post/mydog/","title":"『狗』的故事"},{"content":"keepalived的作用为保持存活服务，服务启动后会在两台物理机器之间维护一个vip，但是仅有一台物理机器拥有该vip，这样就保证了两台机器之间是主备。\n安装 在ubuntu下直接执行：sudo apt-get install keepalived.\n使用 本例子两台机器的物理ip地址分别为10.101.185和10.101.1.186，要增加的虚拟ip地址为10.101.0.101、10.101.0.102、10.101.0.107和10.101.0.108，其中10.101.0.101和10.101.0.102在10.101.185上为主，10.101.0.107和10.101.0.108在10.101.1.186上为主。\nkeepalived的默认配置文件位于/etc/keepalived/keepalived.conf目录下，由于两台物理机器之间的主辅关系不同，配置文件也不相同。\n10.101.185机器上的配置文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 ! Configuration File for keepalived global_defs { # 报警邮箱配置 notification_email { ops@yidian-inc.com } smtp_server 10.101.1.139 smtp_connect_timeout 30 router_id 101-1-185-lg-201-l10.yidian.com // 运行机器的唯一标识，每个机器应该都不一样，可以直接使用hostname代替，具体用在什么地方暂时不是很清楚 } vrrp_instance ha-internal-1 { state MASTER interface eth0 virtual_router_id 1\t// VRID标记，可以设置为0-255，对应VRRD协议中的Virtual Rtr Id priority 100 // 对应VRRD协议中的priority选项 advert_int 1\t// 检测间隔，默认为1s，对应VRRD协议中的adver int authentication { auth_type PASS // 认证方式，支持PASS和AH auth_pass 1-internal-ha // 认证的密码，从抓取的包中看到 } // 声明的虚拟ip地址，这些ip会在VRRP一些的一个包发送 // 另外VRRP协议中还有一个Count IP Addrs用来指明需要声明多少个VIP virtual_ipaddress { 10.101.0.101/22 dev eth0 10.101.0.102/22 dev eth0 } } vrrp_instance ha-internal-2 { state BACKUP interface eth0 virtual_router_id 2 priority 99 advert_int 1 authentication { auth_type PASS auth_pass 2-internal-ha } virtual_ipaddress { 10.101.0.107/22 dev eth0 10.101.0.108/22 dev eth0 } } 10.101.1.186上的配置文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ! Configuration File for keepalived global_defs { notification_email { ops@yidian-inc.com } smtp_server 10.101.1.139 smtp_connect_timeout 30 router_id 101-1-186-lg-201-l10.yidian.com } vrrp_instance ha-internal-1 { state BACKUP interface eth0 virtual_router_id 1 priority 99 advert_int 1 authentication { auth_type PASS auth_pass 1-internal-ha } virtual_ipaddress { 10.101.0.101/22 dev eth0 10.101.0.102/22 dev eth0 } } vrrp_instance ha-internal-2 { state MASTER interface eth0 virtual_router_id 2 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 2-internal-ha } virtual_ipaddress { 10.101.0.107/22 dev eth0 10.101.0.108/22 dev eth0 } } 配置文件搭建完毕后，通过sudo service keepalived start即可启动服务，执行ip addr命令即可看到vip。需要注意的是，通过ifconfig命令是看不到vip的。\n有了vip，其他服务就可以利用该vip做一些绑定vip的端口来作为主辅热备模式了。\nabout vrrp 关于VRRP的详细说明可以查看RFC3768，我这里记录几点说明。\n协议中的以太网Destination Address的值必须为多播地址224.0.0.18。\n当前正在使用的VRRP版本为version 2，认证功能已经取消，但为了向下兼容，仍然可用。在抓取的包中，仍在使用认证信息\ndownload 这里提供两个vrrp协议的pcap包\n","date":"2015-10-23T00:00:00Z","permalink":"/post/keepalived_easy/","title":"keepalived简易教程"},{"content":"一直对堆排序算法用的不错，但是又是在排序中挺重要的算法，并且可以求解其他问题，比如top k问题。已经对堆排序学习过好多次了，无奈每次都记不太清楚具体的细节问题，本文对堆的问题进行整理。本文的排序例子来源于严蔚敏的《数据结构》，本文的知识点来自《算法导论》。\n最大堆为堆中的最大元素位于根节点，顾名思义，最小堆的根节点为最小值。堆的性质决定了堆中节点一定大于等于其子节点。在堆排序算法中用到的是最大堆，最小堆用于构造优先队列，要是使用最小堆进行排序，得到的排序结果为倒序。\n堆的结构为完全二叉树，因此可以用数组存储来代替树的链式存储结构。\n建最大堆过程 我这里通过图表的形式对建大顶堆的过程进行了展示，不再对文字进行叙述。在堆排序的过程中会不断进行建最大堆过程的调用。\n堆排序的核心步骤 清楚了堆的初始化，再看一下下面的堆排序步骤就非常清楚了，不需要图片进行描述了。堆排序的步骤：\n将待排序的数组初始化为大顶堆，该过程即建堆。 将堆顶元素与最后一个元素进行交换，除去最后一个元素外可以组建为一个新的大顶堆。 新建立的堆不是大顶堆，需要重新建立大顶堆。重复上面的处理流程，直到堆中仅剩下一个元素。 top k问题的堆解法 该问题最常规和通用的解决思路为使用快速排序，还可以在N的范围不大的情况下采用哈希（桶）的方式。\n另外一种解法就是堆排序的思路来解决。这里用到的为最小堆，用最小堆来存储最大的k个数，其中堆顶元素为最大k个数种最小的数。这个解法初看有些别扭，求最大的k个数，居然会用到小顶堆。\n该算法必然是一个个遍历N个数一次就够了，这点很好理解。每读取一个新的数x，如果x比堆顶的元素y小，则抛弃；如果x比y大，则用x替换y，并重新更新堆。\n","date":"2015-10-13T00:00:00Z","permalink":"/post/algorithm_heap/","title":"大顶堆小顶堆与堆排序"},{"content":"十一回农村老家，有件小事有些感悟。\n只要家里有人，家里的大门白天一直都是敞着的，外面的人都可以直接走进来，这在农村是很正常的一件事情，门敞着才说明家里是有人的，农村的人没有城里人这么多的隔阂。\n正巧地里的活都干得差不多了，母亲在院子里晒着太阳，我在屋子里收拾东西。从门口走到院里一个人来，只见一身道姑打扮，还带一顶帽子，嘴里振振有辞，说是泰山娘娘庙里的保佑家里平安之类的。母亲见到来者第一反应就是骗子，立马上前说是地里有活要干，马上要出去了。来者压根不理会母亲的话，依旧是保佑平安之类的，像极了大街上迎上前去得乞讨者。来者说道，要捐款之类的，并有个小本子，上面写着捐款者的名字。母亲看了眼捐款者的名单，很多都是邻居家的，说明来者刚从家里过来，迟疑了一会便签上了名字，回屋里去取钱。\n这时还在屋子里的我才看到并明白过来是这么一回事，我一看母亲名字都签上了，钱肯定是要给的了，想逃掉避免少不了一番纠结。我便回屋子去取钱，在城市待习惯了，知道乞讨者五毛一块就能打发的很高兴，我便从钱包里去了两张一块的，其中一张还是备用的，我先给一块，要是嫌少再给两块。我先于母亲出了屋子，给到了来者一块钱，岂知来者说道别人都是给三十五十的，这些太少了我不要。靠！这年头乞讨还嫌钱少啊，还是我太out了？我直接说那你走吧，我没钱，并转头回屋子，可最烦的是她也跟着往屋里走。\n就在这时母亲从屋子里走出来并拿着10块钱，迎上前去给了她。又是嫌少之类的话，最后也不情愿的收下了，并留下了一根红丝带，说些全家平安之类的话就走了，当然了骗子的目的已经完成了，只需匆匆收场就行了。\n此事的经过到此结束，但却有个问题挺令我深思的，这也是为什么写下本文的原因。\n父母并不富裕，家里一直也是过得比较平淡的生活，在我看来勤奋和勤俭节约一直是我们老家那块的美德。平常家里买个菜什么的都要为了几毛钱掂量半天，但却在面对乞讨施舍这种事情上扔掉了10块钱的巨款，而且母亲对于骗钱这件事从始至终都是知情的。\n事后，我的同样上当受骗的邻居也来到了我家，通过邻居和我母亲的谈话我大体理解了他们在经历此事时的心理活动。由于每年都会有多次来到家里进行骗钱的，骗钱的方式是多种多样的，无非是找不到孩子，回不了家之类的，母亲一开始见到骗子就知道是个骗子，这第一印象的判断是过关的。\n母亲对骗子的第二个行为是阻拦，母亲用了要去地里干活的信息来阻拦，但是阻拦不彻底，见阻拦不成功就放弃了。这一点上就显现出了的缺点，不知道用合理的手段来保护自己，并完全从主动状态变为了被动状态。之所以意志这么不坚定，其中有一个很大的因素就是母亲知道强加阻拦的后果就是骗子可能会爆粗口，完全不想听骗子絮絮叨叨个没完没了，撵都撵不走，还不如给点钱省事。另外得罪的骗子的后果可能会更麻烦，毕竟骗子往往都是一个团伙，且知道家庭住址，怕有什么报复行为。所以之所有给钱的原因就是花钱买个安宁，免得带来一身的麻烦，当然从这个出发点上给钱是对的。骗子也正是利用了这一点才在农村屡试不爽。\n但给钱的数目跟平时的生活水平是完全不相符的，要知道在农村买上10块钱的菜可以吃上好几顿。我觉得之所以出现这个问题，根源在于对自己的不够重视。在中国这种权力的社会中，母亲一直觉得处于权力的最底层，事实也确实如此。即使在自己的家中也很容易变主动为被动，让骗子得手。\n可能有人会说农村的法律意识淡薄，不知道合理的维权，完全可以打110来解决。我曾经打110处理过店铺扰民这种鸡毛小事，110给的处理时间为5天，这还是在城市里。我估计换做农村的派出所，这种小事估计110是请不动的。\n我在此次事件中并没有第一时间作出应有的反应，这点我需要反省。首先，我见到母亲签字后没有跟骗子要一下工作证明，这样子至少让骗子没这么容易得逞，给我的反驳增加大大的筹码。其次，没有阻拦母亲给骗子送钱，按照我的意思是跟骗子死缠到底的，毕竟是在我家，给不给钱也是我的自由。但我怕在家里生活时间不长，骗子的规矩我不懂，还是顺从了母亲的行为。\n之所以写这篇文章是想梳理下农民身上的共同缺点及我身上的缺点。我不是歧视农民，我是农村出来的，我知道农村人的辛苦，忙起来的时候他们的辛苦程度是我等码农不能企及的。\n在这里为广大奋斗在田地里的农民致敬！\n","date":"2015-10-10T00:00:00Z","permalink":"/post/20151001/","title":"记2015年十一假期的一件小事"},{"content":"本文在学习saltstack的过程中编写，内容比较基础，方便使用时查阅命令。\n安装 为了方便起见，直接采用yum的安装方式，centos源中并没有salt，需要手工添加一下。\nCentOS 7 安装master\n1 2 rpm -Uvh http://ftp.jaist.ac.jp/pub/Linux/Fedora/epel/7/x86_64/e/epel-release-7-5.noarch.rpm yum install salt-master 修改/etc/salt/master配置文件，在其中指定salt文件根目录位置，默认路径为/srv/salt/。\n1 2 3 file_roots: base: - /svr/salt/ salt在安装的时候已经创建了systemctl命令启动程序需要的service文件，位于/usr/lib/systemd/system/salt-master.service，重启systemctl restart salt-master.service生效。\nCentOS 6.5 安装minion\n1 2 rpm -Uvh http://ftp.linux.ncsu.edu/pub/epel/6/i386/epel-release-6-8.noarch.rpm yum install salt-minion 修改/etc/salt/minion配置文件，在其中指定master主机的地址\n1 master: 192.168.204.128 执行service salt-minion restart对服务进行重启。\n连通性测试 执行salt-key -L命令可以看到已认证和未认证的minion，执行salt-key -a 192.168.204.149可接收minion。\n在master主机中执行salt '*' test.ping可测试连接的minion主机。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ➜ stats salt-key -L Accepted Keys: Denied Keys: Unaccepted Keys: 192.168.204.149 Rejected Keys: ➜ stats salt-key -a 192.168.204.149 The following keys are going to be accepted: Unaccepted Keys: 192.168.204.149 Proceed? [n/Y] y Key for minion 192.168.204.149 accepted. ➜ stats salt \u0026#39;*\u0026#39; test.ping 192.168.204.149: True state 可以通过预先定义好的sls文件对被控主机进行管理，这里演示一个简单的文件复制的例子，该例子可以将master主机上的vimrc文件复制到目标主机上。\n在master主机的/svr/salt/edit目录下新建vim.sls文件，文件内容如下：\n1 2 3 4 5 6 /etc/vimrc: file.managed: - source: salt://edit/vimrc - mode: 644 - user: root - group: root 另外在edit目录下需要存在一个空的init.sls，以确保state.sls可以找到该目录下的sls文件。同时该目录下还需要存在要复制的vimrc文件。\n执行salt '*' state.sls edit.vim即可以执行该命令。\n如果将vim.sls更改为init.sls文件，执行salt '*' state.sls edit命令即可。\n常用命令 salt \u0026lsquo;192.168.204.149\u0026rsquo; cmd.run \u0026lsquo;free -m\u0026rsquo;\nsalt \u0026lsquo;192.168.204.149\u0026rsquo; sys.list_modules 列出minion支持哪些模块，默认已经支持很多模块\nsalt \u0026lsquo;192.168.204.149\u0026rsquo; cp.get_file salt://test_file /root/test_file 将master主机file_roots目录下的文件复制到minion任意目录下，该命令不可以将master主机任意目录下的文件进行复制\nsalt \u0026lsquo;192.168.204.149\u0026rsquo; cp.get_dir salt://test_dir/ /root/ 实验未成功\nsalt \u0026lsquo;*\u0026rsquo; file.mkdir dir_path=/root/test_dir user=root group=root mode=700 在minion主机上创建目录s\n参考文章 *saltstack官方文档\n*《python自动化运维技术与最佳实践》\n","date":"2015-08-21T00:00:00Z","permalink":"/post/saltstack/","title":"SaltStack使用"},{"content":"前段时间我的工作有了一个比较大的变动，我的工作地点从济南到了北京，离开了待了9年的济南，离开了温馨的家，离开了我的亲人，独自一人开启了北漂模式。\n本文不打算叙述面试细节问题，具体的面试细节我自己在印象笔记中整理过，但不打算放出来。本文仅选取有代表性的几家公司来叙述。\n之所以有如此大的变动，最大的因素是我的个人技术的发展遇到了瓶颈。我近几年对个人发展的定位是在技术上能够更上一层楼，尽量不走管理路线，还是以踏踏实实学技术为主要任务。可是在济南工作已经慢慢被推上了管理的岗位，我怕会逐渐脱离技术，直到完全走上了管理的岗位，这跟我对技术非常感兴趣的初衷是有些违背的。\n工作这几年，技术的进步主要来自于自己业余时间的学习，工作中越来越学不太着东西，不是因为工作中不需要牛逼的技术，而是没有人和精力去研究新技术，济南缺乏这个环境，而且是非常匮乏。我已经深深感觉到济南的IT行业未来会逐渐缩小，实际上目前济南的圈子就是很小的，稍微有点规模的公司也就那么几家而已。\n我自从开始工作就以互联网公司为目标，可以济南没有一家真正意义上的互联网公司，用的技术也都不咋地，工作五年后，有了一定的技术积累，家庭也算稳定了，是时候出来闯闯了，不能在济南的安逸环境中，像水煮青蛙般等待着济南IT业的下滑。\n这次来北京找工作的目标非常明确，互联网公司，最好是规模能够稍微大些的，BAT更好，不想加入A轮的公司，我需要的是成熟的互联网公司的环境和技术，来洗刷我已经在传统行业奋斗了多年的旧习。\n我是裸辞的，因为毕竟面试是需要去北京的，在职请假面太麻烦，且不能够全身心的找工作。我给自己找工作的期限为一个月的时间，我最终上班的时间是在20天多点的时间。\n在辞职后的第一周我在家里边休息边看了一遍《STL源码剖析》，之前一直觉得没必要看此书，结果看起来效果还不错，比我想象的要简单的多的多。另外，我制作了自己的简历，包括了pdf版本和markdown版本，并在拉勾网上投递了几份简历。\n就这样第一周过去了，而我没有收到任何的面试通知，第二周我必须加紧开始找工作了。首先在100offer上申请拍卖了我的简历，之前一直关注100offer，微信公众账号和知乎上经常看到100offer的文章，感觉是个靠谱的平台，事实证明确实是一个靠谱的平台。另外，恰巧我在微信公共账号“余晟以为”的文章看到了怎样写简历的文章，就跟作者聊了一会，并且作者在twitter上推荐了我的简历，我的博客有史以来日pv达到了600多，这是我没有想到的，在这里非常感谢余晟的无私帮助。\n100offer上拍卖后没多久就收到了二个面试通知，周三上午就赶到了北京参加面试。上午参加的一家游戏公司的面试，用C++做后台的业务逻辑处理，其实我并不喜欢游戏类工作，来面试也仅仅是为了了解北京面试的流畅增加一些资历而已。两天后收到了该公司的offer，而我理所当然是拒绝了。\n周四上午参加了一个创业团队的面试，面试官年龄跟我差不多，问的问题非常多，非常杂，最后还有一个我最讨厌的逻辑题，一直面试到下午一点。这家公司的面试是通过twitter上看到我的简历联系我的，我来的目的其实也是出于学习和了解行业。面试完后跟团队成员一起吃了个饭，令我感动的是我面试完已经一点钟了，而大家都在等着我一起吃饭，团队的成员都是做技术的码农，还是非常好相处的。如果我已经有了几年的工作经历，或许我会选择这样一家公司。\n下午参加了我目前所在公司一点资讯的面试，面试流程还是非常nice的，感觉跟我心目中的互联网公司基本吻合，虽然面试的时候问的问题过于简单些，令我有点怀疑公司的真实技术实力。另外还问了一堆我并不深入的web技术问题，令我非常捉急。\n没有面试的时候，我就直接回济南了，毕竟在自己家里比北京不知舒服多少倍。\n第三周的时候，我已经开始有些慌了。该用的能有面试机会的方式我都用了，而却收不到面试的邀请了。我用到了100offer、拉勾网、内推、jobdeer、同学内推的方式。难道是我简历写的太水了，可是我已经很难改进自己的简历了，我不想将简历写的夸张了。\n我开始反思原因。我发现互联网用到的我擅长的C++技术的公司非常少，互联网追求的是短平快，C++并不具备开发速度快的特点，因此并不受互联网公司欢迎。像BAT类的公司用C++技术还是比较多的，因为做到一定程度会深究程序的性能，而这是C++擅长的。而BAT类的公司，我并没有任何优势，我虽有几年工作经验，但是都是在传统行业，互联网行业的经验却为0。很多大公司的hr在看到我的简历后直接就给pass掉了，压根没有面试的机会。\n我找同学内推了百度的简历，另外在拉勾网上也投了一些百度的简历。在我入职之前的几天百度的hr妹子给我电话沟通说一周之内会给我面试通知，最终的结果是一周后百度hr妹子给我打电话让我去面试，看来大公司的流程真是复杂，连面试都得排队，而那天是我入职的第一天。没办法，只能委婉的拒绝了，hr倒是很爽快的挂掉了电话。\n目前，我已经入职有三周的时间了，工作已经趋于稳定。我也搬到了公司附近，离公司就几分钟的路程，毕竟就自己一个人，没必要离公司太远，上下班太折腾。晚上一般会在公司待到十点以后，毕竟回去了也没太有什么事情。周末会抽时间回济南跟家人团聚，或者家人来北京，不期望因为工作的原因而对家庭有损伤。工作方面的内容还算满意，能涉及到很多新技术，对个人的成长还不错，只是组内的人员较少，沟通交流的机会不够多，通过招聘慢慢就会解决了。由于用到的很多技术都不够熟悉，自己俨然变成了一个菜鸟，有大量的技术需要学习。同事也还比较给力，大家对工作也很认真负责，团队的凝聚力也符合我的预期。\n总结来看，这段找工作的经历虽有很多失误的地方，错误的对行业的需求进行了估计，以为C++程序员很抢手，事实并不是如此，但结果还算满意。现在自己一个人在北京奋斗，期望通过自己的努力能够有个好的收货。要想写的东西很多，很多都一笔带过了，非技术类的文章写起来还是挺费脑力的。\n","date":"2015-07-18T00:00:00Z","permalink":"/post/2015_interview/","title":"2015年找工作的一段经历"},{"content":"最近在开发程序的过程中遇到了一个getaddrinfo函数的问题，令我感到非常奇怪。\n程序中调用了librdkafka库，当程序选择用-static方式链接所有库时程序会在librdkafka库中某个函数core dump，但是选择动态链接系统库（包括libpthread、libdl、libz、libm、libc等）时程序却能正常运行。\n每次程序都回core dump在getaddrinfo函数中，经过搜索发现有人跟我遇到同样的问题，但是却没有解决方案。\n我这里实验了文中提到了例子，在静态链接的时候确实会报错，动态链接却非常正常，编译选项为g++ -o test_getaddrinfo test_getaddrinfo.cpp -lpthread -Wall -static。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 include \u0026lt;stdio.h\u0026gt; #include \u0026lt;netdb.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; void *test(void *) { struct addrinfo *res = NULL; fprintf(stderr, \u0026#34;x=\u0026#34;); int ret = getaddrinfo(\u0026#34;localhost\u0026#34;, NULL, NULL, \u0026amp;res); fprintf(stderr, \u0026#34;%d \u0026#34;, ret); return NULL; } int main() { for (int i = 0; i \u0026lt; 512; i++) { pthread_t thr; pthread_create(\u0026amp;thr, NULL, test, NULL); } sleep(5); return 0; } 发现程序在链接的时候会提示如下警告：\n1 2 3 4 /tmp/cc0WILtn.o: In function `test(void*)\u0026#39;: test_getaddrinfo.cpp:(.text+0x49): warning: Using \u0026#39;getaddrinfo\u0026#39; in statically linked applications requires at runtime the shared libraries from the glibc version used for linking /usr/lib/gcc/x86_64-redhat-linux/4.8.3/../../../../lib64/libpthread.a(libpthread.o): In function `sem_open\u0026#39;: (.text+0x685b): warning: the use of `mktemp\u0026#39; is dangerous, better use `mkstemp\u0026#39; 从网上查看有该警告的人还是非常多的，都是在-static方式链接glibc库时遇到的，但是没有发现很好的解决方案。该问题的原因 产生估计是glibc在静态链接时调用libnss库存在问题，因此不提倡静态链接方式。\n我看到了两种解决方案：\n方案一：用newlib或uClibc来代替glibc来静态链接，这种方案我没有去尝试是否可行。\n方案二：用--enable-static-nss重新编译glibc。我试了一下问题仍然存在。\n我之所以采用静态链接的方式，是因为开发机器和运行机器的glibc版本不一致造成的。我尝试将libc.so相关文件复制运行机器上，并让程序链接我复制过去的文件，ldd查看可执行文件没有错误，但是当运行程序时会报如下错误：\n1 ./xxx: relocation error: /home/kuring/lib/libc.so.6: symbol _dl_starting_up, version GLIBC_PRIVATE not defined in file ld-linux-x86-64.so.2 with link time reference 最终，我放弃了静态链接的方式，采用了动态链接方式来暂时解决了问题。如果你知道解决方案，请告诉我。\n参考文章 getaddrinfo causes segfault if multithreaded and linked statically\nEven statically linked programs need some shared libraries which is not acceptable for me.\nCreate statically-linked binary that uses getaddrinfo?\nStatic Linking Considered Harmful\n","date":"2015-07-17T00:00:00Z","permalink":"/post/question_getaddrinfo/","title":"getaddrinfo函数调用问题"},{"content":"这是一篇拿来主义的文章，所有的安装步骤仅为互联网上查找，网络上的教程各种凌乱，这里根据我的实践情况进行了更改，本文仅记录了我的安装过程，由于不同环境可能导致安装步骤不甚相同。\nMAC OS X 10.10 php Mac OSX 10.10的系统自带了php、php-fpm，省去了安装的麻烦，可以执行php -v查看php的版本。这里需要简单地修改下php-fpm的配置，否则运行php-fpm会报错。\n1 2 sudo cp /private/etc/php-fpm.conf.default /private/etc/php-fpm.conf vim /private/etc/php-fpm.conf 修改php-fpm.conf文件中的error_log项，默认该项被注释掉，这里需要去注释并且修改为error_log = /usr/local/var/log/php-fpm.log。如果不修改该值，运行php-fpm的时候会提示log文件输出路径不存在的错误。\n如果系统中存在多个php-fpm.conf，不知道需要编辑哪一个，可以执行php-fpm -t命令查看php-fpm要读取的配置文件。\n通过php-fpm -D来启动php-fpm，可以通过lsof -Pni4 | grep LISTEN | grep php命令来查看php-fpm是否监听在9000端口。\nnginx 这里为了简单，直接采用了brew的方式安装。执行\n1 brew install nginx nginx的配置文件位于/usr/local/etc/nginx/nginx.conf，默认只能解析html文件，需要配置后才能调用php-fpm解析php文件。下面内容为该修改后的文件全部有效内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 8080; server_name localhost; root /Users/kuring/www;\t// 页面存放路径 location / { index index.html index.htm index.php; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } } include servers/*; 然后执行nginx命令即可启动，默认监听的端口为8080，在浏览器中输入http://127.0.0.1:8080即可看到nginx的初始界面。nginx要想监听1024以下端口还需要进一步的配置，8080端口既能满足我需求，不再更改。\nmysql 1 brew install mysql 在启动mysql之前可对mysql的配置文件进行更改，我这里需要更改mysql的编码方式，将所有的编码方式都更改为utf8，防止乱码问题的发生。mysql的配置文件为my.cnf，我的位于/usr/local/Cellar/mysql/5.6.25/my.cnf，对文件添加如下内容，有些选项不存在，可手动添加。\n1 2 3 4 5 6 7 8 [mysqld] character-set-server=utf8 [client] default-character-set=utf8 [mysqld_safe] default-character-set=utf8 输入mysqld命令即可启动mysql，启动mysql后输入mysql_secure_installation命令对mysql进行配置，可以设置root用户的密码。\n通过mysql -uroot -p命令连接到mysql后，输入status命令可查看刚才更改的编码是否生效。\n由于不需要长期使用mysql，这里不设置mysql自启动命令。\nCentOS6 我首先采用的方案为完全用普通用户安装，尝试失败后采用root安装依赖库普通用户编译程序的方案。\n普通用户安装依赖库 我首选选择在普通用户kuring下进行安装和运行整个web环境。因此不能使用yum安装方式，必须采用源码安装的方式。通过普通用户安装，最麻烦的地方就在于需要安装很多的依赖库，而依赖的库的安装可能又有需要的库，且库之间存在版本问题。\n首先普及几个小知识：\nbash查找命令的先后顺序为：\nalias别名 shell中的关键字，如if等 shell中的函数 shell内置命令，如echo等 $PATH环境变量，PATH中的匹配顺序为从前向后的。 程序查找lib库的先后顺序为：\n编译程序时指定的链接库路径，g++编译器可以通过-Wl,-rpath,路径来指定链接库的路径。 环境变量LD_LIBRARY_PATH指定的搜索路径。 /etc/ld.so.conf指定的路径。 默认的系统动态库搜索路径，如/usr/lib64、/usr/local/lib64等。 很多程序采用pkg-config程序来检查库的版本号，pkg-config命令依赖于动态链接库对应的.pc文件，这些.pc文件一般位于系统的/usr/local/lib/pkgconfig目录下。为了能够将安装完成的库通过pkg-config找到对应的.pc文件，需要将.pc文件所在的路径/home/kuring/local/lib/pkg-config设置到环境变量PKG_CONFIG_PATH中。\n安装php依赖的libxml2库时提示找不到libtool、autoconf和automake，首先安装libtool。执行./configure --prefix=/home/kuring/local;make; make install将其安装到当前用户的local目录下。\n用同样的步骤安装autoconf，执行./configure --prefix=/home/kuring/local;make; make install。\n为了能够将安装的程序起作用，需要将/home/kuring/local目录添加到PATH环境变量中，在.bash_profile文件中添加PATH=$PATH:$HOME/local/bin语句，并执行source ~/.bash_profile。\n安装之前需要先安装libxml2库，下载地址采用git clone git://git.gnome.org/libxml2的方式下载。在执行sh autogen产生configure配置文件的过程中，发现提示\n1 2 ./configure: line 13094: syntax error near unexpected token `LZMA,liblzma,\u0026#39; ./configure: line 13094: ` PKG_CHECK_MODULES(LZMA,liblzma,\u0026#39; 经过发现是由于找不到PKG_CHECK_MODULES造成的，正常情况下该函数定义在aclocal.m4文件，而该情况下aclocal.m4文件中并不存在该函数。之所以不存在是由于aclocal命令找不到pkg.m4文件造成的，可以通过aclocal --print命令查看查找的pkg.m4文件的路径。我这里的解决思路为直接从其他机器上复制一个pkg.m4文件过来。\n在产生了configure命令后，执行./configure --prefix=/home/kuring/local命令后发现提示找不到Python.h命令的错误。\n鉴于遇到了如此之多的错误，本着不浪费生命的原则还是采用yum来安装依赖库吧。\nphp 这里的mysql直接采用了yum命令安装的。\n在执行php的./configure命令后提示libxml2找不到错误，直接yum install libxml-devel命令安装libxml-devel即可。然后执行./configure --enable-fpm --prefix=/home/kuring/php5.5 --with-mysqli=/usr/bin/mysql_config;make; make install;。在编译php的时候要加上php-fpm选项来安装php-fpm命令。\n安装后配置~/.bash_profile文件的$PATH环境变量的值为：PATH=$HOME/bin:$HOME/php5.5/bin:$HOME/php5.5/sbin:$PATH。\n此时即可通过php-fpm -D命令来启动php-fpm命令了。\n安装完成后通过phpinfo()函数查看里面有MySQLi的选项，但是实际程序运行的时候居然不支持mysqli的一些力函数，说明mysqli的扩展安装不成功。在/home/kuring/php5.5/include/php/ext/mysqli目录中找到了对应的.h文件，却没有找到mysqli.so的动态链接库文件。大概是由于在编译php时mysql的路径配置有些问题造成的，因为mysql是通过yum安装的，路径比较乱一些。\n为了能够产生mysqli.so文件，采用单独编译的方式，在php的源码目录中已经包含了mysqli的源码，进入mysqli源码目录下执行phpize;./configure --prefix=/home/kuring/php5.5/mysqli --with-php-config=/home/kuring/php5.5/bin/php-config --with-mysqli=/usr/bin/mysql_config;make;make install；。将mysqli.so文件安装到了/home/kuring/php5.5/lib/php/extensions/no-debug-non-zts-20121212目录下，不知道为什么目录末尾还要加个这么长的文件夹名，直接将文件复制到上一级目录下。\n在/home/kuring/php5.5目录下没有找到php.ini文件，通过php --ini命令查看php的配置文件路径为/home/kuring/php5.5/lib，直接从php的源码文件中复制一个php.ini文件到该目录下。并将php.ini中的增加如下内容：\n1 2 extension_dir = \u0026#34;/home/kuring/php5.5/lib/php/extensions\u0026#34; extension=mysqli.so 再运行程序，发现mysqli的系列函数已经支持了，好一段折腾。\nphp-fpm 执行cp $HOME/php5.5/etc/php-fpm.conf.default $HOME/php5.5/etc/php-fpm.conf来增加配置文件。\nnginx 首先安装pcre库，该库为正则表达式库。下载后通过\n下载源码后执行./configure --prefix /home/kuring/nginx;make;make install;即可安装完成。\n修改nginx的配置文件为如下内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 8080; server_name localhost; root /home/kuring/www; location / { index index.html index.htm index.php; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } } } include servers/*; 常见操作 nginx nginx -s stop：关闭 nginx -t：检测nginx的配置是否正确 mysql mysqld_safe：启动mysql mysqladmin shutdown -u root -p：关闭mysql create user kuring identified by \u0026lsquo;kuring_pass\u0026rsquo;：mysql创建用户（我尝试过几次，每次创建的用户密码都为空） drop user kuring：删除一个用户 grant all privileges on . to \u0026lsquo;root\u0026rsquo;@\u0026rsquo;%\u0026rsquo; identified by \u0026lsquo;root\u0026rsquo; with grant option ：允许mysql的root用户通过远程登录 创建用户的操作 使用create user kuring identified by 'kuring_pass'命令创建用户kuring。默认创建完成的用户在本机无法登陆，但是远程却可以登陆。 这是因为mysql数据库中的user表中存在一条记录造成的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 use mysql; select user,host,password from user; // 在表中存在一条用户名为空的记录 +---------+-----------+-------------------------------------------+ | User | host | password | +---------+-----------+-------------------------------------------+ | root | localhost | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B | | root | 127.0.0.1 | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B | | root | ::1 | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B | | | localhost | | | root | % | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B | | report1 | % | *884CAA4D6FA1C3F7E4849C8DAF1B5B37FCB3EC0B | +---------+-----------+-------------------------------------------+ // 将mysql中的为空的记录删除掉，这样就可以通过创建的用户连接了 在mysql命令行中执行grant all privileges on kuring_db.* to kuring identified by 'kuring_pass'命令可以给刚创建的用户对数据库的权限。\n修改mysql用户密码 mysql将用户名和密码存放到了mysql数据库的user表中，在mysql命令行中执行use mysql;update user set password=password(\u0026quot;new password\u0026quot;) where user=\u0026quot;username\u0026quot;;flush privileges;即可更新相应用户的密码。\nphp-fpm php-fpm -D：启动php-fpm，如果需要指定php.ini文件，可以使用-c参数 php-fpm -t：检查php-fpm的配置文件 kill -USR2: 重启php-fpm kill -INT: 停止php-fpm php php \u0026ndash;ini：显示php.ini文件路径 参考文章 运用Autoconf和Automake生成Makefile的学习之路 ","date":"2015-07-16T00:00:00Z","permalink":"/post/build_lnmp/","title":"LNMP开发环境搭建"},{"content":"我在软件行业工作已经有五个年头了，在现在这家公司已经有两个年头了。虽然身为公司的研发部经理可以参与公司的一些决定，但是没有绝对的话语权，对于公司的很多决定我深知是错误的，虽然后来也证明是错误的，但是我仍然无能为力。这里总结一下在公司中遇到的问题。\n宁可招聘一个技术水平高的也不愿招聘三个技术水平低的。在工作中能够非常有力的证明这一点，三个刚工作的技术人员，尤其对于C++这样门槛稍微高一些，需要工作经验来弥补C++中坑的语言，三个C++技术人员远没有一个高水平的工作效率高，因为三个菜鸟需要将大牛踩过的坑全部踩一遍，踩过多少坑就代表走了多少弯路。\n兴趣是最大的老师。我带过不少人，很多都是新人，我给他们制定了学习计划，期望他们能够在业余时间多学习，但实际上哪有几个人能够充分利用业余时间的。我就非常怀疑他们对技术的兴趣问题，如果他们对技术不感兴趣那为什么要加入该行业，为没有兴趣的工作而工作就是自己对自己耍流氓。如果他们对技术感兴趣，那只能说明他们业余时间中有更大的诱惑。\n在招聘中不要过于在意金钱，便宜无好货在招聘行业中仍然非常适用。在招聘中千万不要吝惜给员工的那点钱，因为一千块钱而错失一个好的员工是非常不值得的。\n盈利模式决定了公司对产品的态度。我所在的软件行业属于传统的软件行业，传统软件行业的盈利模式为销售，由于软件具有可复制性的特点，因此只要一套产品卖的越多就赚的越多。对于传统软件行业的产品使用者很多情况下就是几个人，至少跟互联网产品的用户数量不在一个量级。使用的人数决定了传统软件行业的用户体验可以做的很烂，技术水平可以不用那么高，只要能用就行，慢点无所谓，只要能卖出去就行了。身为一个技术人员，一个对技术有追求的技术人员，这令我非常反感，我做技术我不能对技术无所谓，我讨厌听到无所谓这样的字眼。\n一定要明确公司的定位，明白什么时候应该干什么，什么应该干，野心太大也是问题。公司处于成长阶段提出了今年营业额比去年增长10倍的目标，我听到之后就是嗤之以鼻，这压根就是不可能的任务，而事实证明这也根本不可能完成，实际上当年营业额仅比去年增长了一倍。\n一家公司一定要有自己的明确产品线，要抵住外界的诱惑。公司的产品线本来是非常明确的，后来由于客户需求和各种方面的原因，开始考虑疯狂扩展产品，这就造成了本来人手就紧蹙的情况下，没有时间去改善现有的系统，不得不去研发新的产品。自己没有的产品甚至跟客户合作或者完全购买别人的产品，导致公司很多人都在考虑跟其他公司合作的事宜。结果可想而知，新产品的销售并不理想，旧有的产品升级维护的也开始变慢。ps：我是非常讨厌在技术上跟其他公司之间考虑合作的问题，因为这从本质上讲并没有产生任何的社会价值，技术上必然涉及到接口的问题，只要是接口必然会有很多细节问题，这些往往会出现技术人员扯皮的问题，一个问题你可以解决他也可以解决，但是谁都不愿意解决，你说烦不烦。\n技术人员后来要么转行要么做管理了。在济南技术人员就这两种出路吧，没见过多少大龄的程序员，很多情况下写着写着程序突然发现自己转为公司的中层了，比如我，并逐渐参与公司的事务。很多对程序不感兴趣的，可能就直接换个行业或者转行做销售了。\n有些人再怎么培养也成不了高手。在工作我发现，有些人即使有了几年的工作经验，对公司的产品也非常了解，但是在解决问题的时候总是找不到点子上，占了一大堆资源，最后解决起问题来即慢又绕弯路，还留下一堆bug。对于这部分人，我想说也许这个行业不适合你。\n领导千万不可三天两头一个想法，这在员工看来就是一个不靠谱的领导。谁都不愿意追随一个拿着自己当猴耍的领导，一会一个想法只能说明领导不够成熟，不适合做领导。跟随杰出的人，为杰出的人工作。\n搞公司最好不要搞施工太久的。公司很多做工程的都在为现场的情况忙碌，一个点架设完毕后往往还需要耗费大量的时间来维护，维护对于公司而言牵涉到精力太大，尽量避免需要整天跟客户打交道和整天维护的业务。\n专科生是很难撑起一家科技企业。虽然我不完全认同学历就能决定能力，但学历跟能力之间是成正比关系的。我的朋友中有专科生在工作几年后可以做专业的视频教程，并且业余时间写过几部玄幻小说。由于学习经历的不同这就造就了科班出身的程度不同，自然能力之间是有差异的。虽然中国的大学教育跟工作很脱节，但是在工作中还是能够跟大学教育挂起钩来的。学历跟素质之间也是成正比关系的，这里的素质体现在工作中就包括了工作中的责任心，工作态度等方面，这里就不展开了，要展开的话我可以举出非常多活生生的例子。因此，我非常不提倡在公司招聘中招聘专科生。我发现在很多情况下，很多专科人员是连普通话都不会的，操着各种方言或各种被普通话的方言。基本上能不能说普通话也是一个断定人素质的标准，扩展到其他行业同样适用。\n也许本文的观点有些偏激，没错我就是一个偏激的IT工程师，就酱。\n","date":"2015-06-01T00:00:00Z","permalink":"/post/company_question/","title":"公司问题及经验总结"},{"content":"在C++98中有左值和右值的概念，不过这两个概念对于很多程序员并不关心，因为不知道这两个概念照样可以写出好程序。在C++11中对右值的概念进行了增强，我个人理解这部分内容是C++11引入的特性中最难以理解的了。该特性的引入至少可以解决C++98中的移动语义和完美转发问题，若你还不清楚这两个问题是什么，请向下看。\n温馨提示，由于内容比较难懂，请仔细看。C++已经够复杂了，C++11中引入的新特性令C++更加复杂了。在学习本文的时候一定要理解清楚左值、右值、左值引用和右值引用。\n移动构造函数 首先看一个C++98中的关于函数返回类对象的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 class MyString { public: MyString() { _data = nullptr; _len = 0; printf(\u0026#34;Constructor is called!\\n\u0026#34;); } MyString(const char* p) { _len = strlen (p); _init_data(p); cout \u0026lt;\u0026lt; \u0026#34;Constructor is called! this-\u0026gt;_data: \u0026#34; \u0026lt;\u0026lt; (long)_data \u0026lt;\u0026lt; endl; } MyString(const MyString\u0026amp; str) { _len = str._len; _init_data(str._data); cout \u0026lt;\u0026lt; \u0026#34;Copy Constructor is called! src: \u0026#34; \u0026lt;\u0026lt; (long)str._data \u0026lt;\u0026lt; \u0026#34; dst: \u0026#34; \u0026lt;\u0026lt; (long)_data \u0026lt;\u0026lt; endl; } ~MyString() { if (_data) { cout \u0026lt;\u0026lt; \u0026#34;DeConstructor is called! this-\u0026gt;_data: \u0026#34; \u0026lt;\u0026lt; (long)_data \u0026lt;\u0026lt; endl; free(_data); } else { std::cout \u0026lt;\u0026lt; \u0026#34;DeConstructor is called!\u0026#34; \u0026lt;\u0026lt; std::endl; } } MyString\u0026amp; operator=(const MyString\u0026amp; str) { if (this != \u0026amp;str) { _len = str._len; _init_data(str._data); } cout \u0026lt;\u0026lt; \u0026#34;Copy Assignment is called! src: \u0026#34; \u0026lt;\u0026lt; (long)str._data \u0026lt;\u0026lt; \u0026#34; dst\u0026#34; \u0026lt;\u0026lt; (long)_data \u0026lt;\u0026lt; endl; return *this; } operator const char *() const { return _data; } private: char *_data; size_t _len; void _init_data(const char *s) { _data = new char[_len+1]; memcpy(_data, s, _len); _data[_len] = \u0026#39;\\0\u0026#39;; } }; MyString foo() { MyString middle(\u0026#34;123\u0026#34;); return middle; } int main() { MyString a = foo(); return 1; } 该例子在编译器没有进行优化的情况下会输出以下内容，我在输出的内容中做了注释处理，如果连这个例子的输出都看不懂，建议再看一下C++的语法了。我这里使用的编译器命令为g++ test.cpp -o main -g -fno-elide-constructors，之所以要加上-fno-elide-constructors选项时因为g++编译器默认情况下会对函数返回类对象的情况作返回值优化处理，这不是我们讨论的重点。\n1 2 3 4 5 6 Constructor is called! this-\u0026gt;_data: 29483024 // middle对象的构造函数 Copy Constructor is called! src: 29483024 dst: 29483056 // 临时对象的构造，通过middle对象调用复制构造函数 DeConstructor is called! this-\u0026gt;_data: 29483024 // middle对象的析构 Copy Constructor is called! src: 29483056 dst: 29483024\t// a对象构造，通过临时对象调用复制构造函数 DeConstructor is called! this-\u0026gt;_data: 29483056 // 临时对象析构 DeConstructor is called! this-\u0026gt;_data: 29483024 // a对象析构 在上述例子中，临时对象的构造、复制和析构操作所带来的效率影响一直是C++中为人诟病的问题，临时对象的构造和析构操作均对堆上的内存进行操作，而如果_data的内存过大，势必会非常影响效率。从程序员的角度而言，该临时对象是透明的。而这一问题正是C++11中需要解决的问题。\n在C++11中解决该问题的思路为，引入了移动构造函数，移动构造函数的定义如下。\n1 2 3 4 5 6 MyString(MyString \u0026amp;\u0026amp;str) { cout \u0026lt;\u0026lt; \u0026#34;Move Constructor is called! src: \u0026#34; \u0026lt;\u0026lt; (long)str._data \u0026lt;\u0026lt; endl; _len = str._len; _data = str._data; str._data = nullptr; } 在移动构造函数中我们窃取了str对象已经申请的内存，将其拿为己用，并将str申请的内存给赋值为nullptr。移动构造函数和复制构造函数的不同之处在于移动构造函数的参数使用*\u0026amp;\u0026amp;*，这就是下文要讲解的右值引用符号。参数不再是const，因为在移动构造函数需要修改右值str的内容。\n移动构造函数的调用时机为用来构造临时变量和用临时变量来构造对象的时候移动语义会被调用。可以通过下面的输出结果看到，我们所使用的编译参数为g++ test.cpp -o main -g -fno-elide-constructors --std=c++11。\n1 2 3 4 5 6 Constructor is called! this-\u0026gt;_data: 22872080 // middle对象构造 Move Constructor is called! src: 22872080 // 临时对象通过移动构造函数构造，将middle申请的内存窃取 DeConstructor is called! // middle对象析构 Move Constructor is called! src: 22872080 // 对象a通过移动构造函数构造，将临时对象的内存窃取 DeConstructor is called! // 临时对象析构 DeConstructor is called! this-\u0026gt;_data: 22872080 // 对象a析构 通过输出结果可以看出，整个过程中仅申请了一块内存，这也正好符合我们的要求了。\nC++98中的左值和右值 我们先来看下C++98中的左值和右值的概念。左值和右值最直观的理解就是一条语句等号左边的为左值，等号右边的为右值，而事实上该种理解是错误的。左值：可以取地址，有名字的值，是一个指向某内存空间的表达式，可以使用\u0026amp;操作符获取内存地址。右值：不能取地址，即非左值的都是右值，没有名字的值，是一个临时值，表达式结束后右值就没有意义了。我想通过下面的例子，读者可以清楚的理解左值和右值了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // lvalues: // int i = 42; i = 43; // i是左值 int* p = \u0026amp;i; // i是左值 int\u0026amp; foo(); foo() = 42; // foo()返回引用类型是左值 int* p1 = \u0026amp;foo(); // foo()可以取地址是左值 // rvalues: // int foobar(); int j = 0; j = foobar(); // foobar()是右值 int* p2 = \u0026amp;foobar(); // 编译错误，foobar()是右值不能取地址 j = 42; // 42是右值 C++11右值引用和移动语义 在C++98中有引用的概念，对于const int \u0026amp;m = 1，其中m为引用类型，可以对其取地址，故为左值。在C++11中，引入了右值引用的概念，使用*\u0026amp;\u0026amp;*来表示。在引入了右值引用后，在函数重载时可以根据是左值引用还是右值引用来区分。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 void fun(MyString \u0026amp;str) { cout \u0026lt;\u0026lt; \u0026#34;left reference\u0026#34; \u0026lt;\u0026lt; endl; } void fun(MyString \u0026amp;\u0026amp;str) { cout \u0026lt;\u0026lt; \u0026#34;right reference\u0026#34; \u0026lt;\u0026lt; endl; } int main() { MyString a(\u0026#34;456\u0026#34;); fun(a); // 左值引用，调用void fun(MyString \u0026amp;str) fun(foo()); // 右值引用，调用void fun(MyString \u0026amp;\u0026amp;str) return 1; } 在绝大多数情况下，这种通过左值引用和右值引用重载函数的方式仅会在类的构造函数和赋值操作符中出现，被例子仅是为了方便采用函数的形式，该种形式的函数用到的比较少。上述代码中所使用的将资源从一个对象到另外一个对象之间的转移就是移动语义。这里提到的资源是指类中的在堆上申请的内存、文件描述符等资源。\n前面已经介绍过了移动构造函数的具体形式和使用情况，这里对移动赋值操作符的定义再说明一下，并将main函数的内容也一起更改，将得到如下输出结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 MyString\u0026amp; operator=(MyString\u0026amp;\u0026amp; str) { cout \u0026lt;\u0026lt; \u0026#34;Move Operator= is called! src: \u0026#34; \u0026lt;\u0026lt; (long)str._data \u0026lt;\u0026lt; endl; if (this != \u0026amp;str) { if (_data != nullptr) { free(_data); } _len = str._len; _data = str._data; str._len = 0; str._data = nullptr; } return *this; } int main() { MyString b; b = foo(); return 1; } // 输出结果，整个过程仅申请了一个内存地址 Constructor is called! // 对象b构造函数调用 Constructor is called! this-\u0026gt;_data: 14835728 // middle对象构造 Move Constructor is called! src: 14835728 // 临时对象通过移动构造函数由middle对象构造 DeConstructor is called! // middle对象析构 Move Operator= is called! src: 14835728 // 对象b通过移动赋值操作符由临时对象赋值 DeConstructor is called! // 临时对象析构 DeConstructor is called! this-\u0026gt;_data: 14835728 // 对象b析构函数调用 在C++中对一个变量可以通过const来修饰，而const和引用是对变量约束的两种方式，为并行存在，相互独立。因此，就可以划分为了const左值引用、非const左值引用、const右值引用和非const右值引用四种类型。其中左值引用的绑定规则和C++98中是一致的。\n非const左值引用只能绑定到非const左值，不能绑定到const右值、非const右值和const左值。这一点可以通过const关键字的语义来判断。\nconst左值引用可以绑定到任何类型，包括const左值、非const左值、const右值和非const右值，属于万能引用类型。其中绑定const右值的规则比较少见，但是语法上是可行的，比如const int \u0026amp;a = 1，只是我们一般都会直接使用int \u0026amp;a = 1了。\n非const右值引用不能绑定到任何左值和const右值，只能绑定非const右值。\nconst右值引用类型仅是为了语法的完整性而设计的， 比如可以使用const MyString \u0026amp;\u0026amp;right_ref = foo()，但是右值引用类型的引入主要是为了移动语义，而移动语义需要右值引用是可以被修改的，因此const右值引用类型没有实际意义。\n我们通过表格的形式对上文中提到的四种引用类型可以绑定的类型进行总结。\n引用类型/是否绑定 非const左值 const左值 非const右值 const右值 备注 非const左值引用 是 否 否 否 无 const左值引用 是 是 是 是 全能绑定类型，绑定到const右值的情况比较少见 非const右值引用 否 否 是 否 C++11中引入的特性，用于移动语义和完美转发 const值引用 是 否 否 否 没有实际意义，为了语法完整性而存在 下面针对上述例子，我们看一下foo函数绑定参数的情况。\n如果只实现了void foo(MyString \u0026amp;str)，而没有实现void fun(MyString \u0026amp;\u0026amp;str)，则和之前一样foo函数的实参只能是非const左值。\n如果只实现了void foo(const MyString \u0026amp;str)，而没有实现void fun(MyString \u0026amp;\u0026amp;str)，则和之前一样foo函数的参数即可以是左值又可以是右值，因为const左值引用是万能绑定类型。\n如果只实现了void foo(MyString \u0026amp;\u0026amp;str)，而没有实现void fun(MyString \u0026amp;str)，则foo函数的参数只能是非const右值。\n强制移动语义std::move() 前文中我们通过右值引用给类增加移动构造函数和移动赋值操作符已经解决了函数返回类对象效率低下的问题。那么还有什么问题没有解决呢？\n在C++98中的swap函数的实现形式如下，在该函数中我们可以看到整个函数中的变量a、b、c均为左值，无法直接使用前面移动语义。\n1 2 3 4 5 6 7 template \u0026lt;class T\u0026gt; void swap ( T\u0026amp; a, T\u0026amp; b ) { T c(a); a=b; b=c; } 但是如果该函数中能够使用移动语义是非常合适的，仅是为了交换两个变量，却要反复申请和释放资源。按照前面的知识变量c不可能为非const右值引用，因为变量a为非const左值，非const右值引用不能绑定到任何左值。\n在C++11的标准库中引入了std::move()函数来解决该问题，该函数的作用为将其参数转换为右值。在C++11中的swap函数就可以更改为了：\n1 2 3 4 5 6 7 template \u0026lt;class T\u0026gt; void swap (T\u0026amp; a, T\u0026amp; b) { T c(std::move(a)); a=std::move(b); b=std::move(c); } 在使用了move语义以后,swap函数的效率会大大提升，我们更改main函数后测试如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int main() { // move函数 MyString d(\u0026#34;123\u0026#34;); MyString e(\u0026#34;456\u0026#34;); swap(d, e); return 1; } // 输出结果，通过输出结果可以看出对象交换是成功的 Constructor is called! this-\u0026gt;_data: 38469648 // 对象d构造 Constructor is called! this-\u0026gt;_data: 38469680 // 对象e构造 Move Constructor is called! src: 38469648 // swap函数中的对象c通过移动构造函数构造 Move Operator= is called! src: 38469680 // swap函数中的对象a通过移动赋值操作符赋值 Move Operator= is called! src: 38469648 // swap函数中的对象b通过移动赋值操作符赋值 DeConstructor is called! // swap函数中的对象c析构 DeConstructor is called! this-\u0026gt;_data: 38469648 // 对象e析构 DeConstructor is called! this-\u0026gt;_data: 38469680 // 对象d析构 右值引用和右值的关系 这个问题就有点绕了，需要开动思考一下右值引用和右值是啥含义了。读者会凭空的认为右值引用肯定是右值，其实不然。我们在之前的例子中添加如下代码，并将main函数进行修改如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 void test_rvalue_rref(MyString \u0026amp;\u0026amp;str) { cout \u0026lt;\u0026lt; \u0026#34;tmp object construct start\u0026#34; \u0026lt;\u0026lt; endl; MyString tmp = str; cout \u0026lt;\u0026lt; \u0026#34;tmp object construct finish\u0026#34; \u0026lt;\u0026lt; endl; } int main() { test_rvalue_rref(foo()); return 1; } // 输出结果 Constructor is called! this-\u0026gt;_data: 28913680 Move Constructor is called! src: 28913680 DeConstructor is called! tmp object construct start Copy Constructor is called! src: 28913680 dst: 28913712 // 可以看到这里调用的是复制构造函数而不是移动构造函数 tmp object construct finish DeConstructor is called! this-\u0026gt;_data: 28913712 DeConstructor is called! this-\u0026gt;_data: 28913680 我想程序运行的结果肯定跟大多数人想到的不一样，“Are you kidding me?不是应该调用移动构造函数吗？为什么调用了复制构造函数？”。关于右值引用和左右值之间的规则是：\n如果右值引用有名字则为左值，如果右值引用没有名字则为右值。\n通过规则我们可以发现，在我们的例子中右值引用str是有名字的，因此为左值，tmp的构造会调用复制构造函数。之所以会这样，是因为如果tmp构造的时候调用了移动构造函数，则调用完成后str的申请的内存自己已经不可用了，如果在该函数中该语句的后面在调用str变量会出现我们意想不到的问题。鉴于此，我们也就能够理解为什么有名字的右值引用是左值了。如果已经确定在tmp构造语句的后面不需要使用str变量了，可以使用std::move()函数将str变量从左值转换为右值，这样tmp变量的构造就可以使用移动构造函数了。\n而如果我们调用的是MyString b = foo()语句，由于foo()函数返回的是临时对象没有名字属于右值，因此b的构造会调用移动构造函数。\n该规则非常的重要，要想能够正确使用右值引用，该规则必须要掌握，否则写出来的代码会有一个大坑。\n完美转发 前面已经介绍了本文的两大主题之一的移动语义，还剩下完美转发机制。完美转发机制通常用于库函数中，至少在我的工作中还是很少使用的。如果实在不想理解该问题，可以不用向下看了。在泛型编程中，经常会遇到的一个问题是怎样将一组参数原封不动的转发给另外一个函数。这里的原封不动是指，如果函数是左值，那么转发给的那个函数也要接收一个左值；如果参数是右值，那么转发给的函数也要接收一个右值；如果参数是const的，转发给的函数也要接收一个const参数；如果参数是非const的，转发给的函数也要接收一个非const值。\n该问题看上去非常简单，其实不然。看一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #include \u0026lt;iostream\u0026gt; using namespace std; void fun(int \u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;lvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } void fun(int \u0026amp;\u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;rvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } void fun(const int \u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;const lvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } void fun(const int \u0026amp;\u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;const rvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } template\u0026lt;typename T\u0026gt; void PerfectForward(T t) { fun(t); } int main() { PerfectForward(10); // rvalue ref int a; PerfectForward(a); // lvalue ref PerfectForward(std::move(a)); // rvalue ref const int b = 8; PerfectForward(b); // const lvalue ref PerfectForward(std::move(b)); // const rvalue ref return 0; } 在上述例子中，我们想达到的目的是PerfectForward模板函数能够完美转发参数t到fun函数中。上述例子中的PerfectForward函数必然不能够达到此目的，因为PerfectForward函数的参数为左值类型，调用的fun函数也必然为void fun(int \u0026amp;)。且调用PerfectForward之前就产生了一次参数的复制操作，因此这样的转发只能称之为正确转发，而不是完美转发。要想达到完美转发，需要做到像转发函数不存在一样的效率。\n因此，我们考虑将PerfectForward函数的参数更改为引用类型，因为引用类型不会有额外的开销。另外，还需要考虑转发函数PerfectForward是否可以接收引用类型。如果转发函数PerfectForward仅能接收左值引用或右值引用的一种，那么也无法实现完美转发。\n我们考虑使用const T \u0026amp;t类型的参数，因为我们在前文中提到过，const左值引用类型可以绑定到任何类型。但是这样目标函数就不一定能接收const左值引用类型的参数了。const左值引用属于左值，非const左值引用和非const右值引用是无法绑定到const左值的。\n如果将参数t更改为非const右值引用、const右值也是不可以实现完美转发的。\n在C++11中为了能够解决完美转发问题，引入了更为复杂的规则：引用折叠规则和特殊模板参数推导规则。\n引用折叠推导规则 为了能够理解清楚引用折叠规则，还是通过以下例子来学习。\n1 2 3 4 5 6 7 8 9 10 typedef int\u0026amp; TR; int main() { int a = 1; int \u0026amp;b = a; int \u0026amp; \u0026amp;c = a; // 编译器报错，不可以对引用再显示添加引用 TR \u0026amp;d = a; // 通过typedef定义的类型隐式添加引用是可以的 return 1; } 在C++中，不可以在程序中对引用再显示添加引用类型，对于int \u0026amp; \u0026amp;c的声明变量方式，编译器会提示错误。但是如果在上下文中（包括使用模板实例化、typedef、auto类型推断等）出现了对引用类型再添加引用的情况，编译器是可以编译通过的。具体的引用折叠规则如下，可以看出一旦引用中定义了左值类型，折叠规则总是将其折叠为左值引用。这就是引用折叠规则的全部内容了。另外折叠规则跟变量的const特性是没有关系的。\n1 2 3 4 A\u0026amp; \u0026amp; =\u0026gt; A\u0026amp; A\u0026amp; \u0026amp;\u0026amp; =\u0026gt; A\u0026amp; A\u0026amp;\u0026amp; \u0026amp; =\u0026gt; A\u0026amp; A\u0026amp;\u0026amp; \u0026amp;\u0026amp; =\u0026gt; A\u0026amp;\u0026amp; 特殊模板参数推导规则 下面我们再来学习特殊模板参数推导规则，考虑下面的模板函数，模板函数接收一个右值引用作为模板参数。\n1 2 template\u0026lt;typename T\u0026gt; void foo(T\u0026amp;\u0026amp;); 说白点，特殊模板参数推导规则其实就是引用折叠规则在模板参数为右值引用时模板情况下的应用，是引用折叠规则的一种情况。我们结合上文中的引用折叠规则，\n如果foo的实参是上文中的A类型的左值时，T的类型就为A\u0026amp;。根据引用折叠规则，最后foo的参数类型为A\u0026amp;。 如果foo的实参是上文中的A类型的右值时，T的类型就为A\u0026amp;\u0026amp;。根据引用折叠规则，最后foo的参数类型为A\u0026amp;\u0026amp;。 解决完美转发问题 我们已经学习了模板参数为右值引用时的特殊模板参数推导规则，那么我们利用刚学习的知识来解决本文中待解决的完美转发的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #include \u0026lt;iostream\u0026gt; using namespace std; void fun(int \u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;lvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } void fun(int \u0026amp;\u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;rvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } void fun(const int \u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;const lvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } void fun(const int \u0026amp;\u0026amp;) { cout \u0026lt;\u0026lt; \u0026#34;const rvalue ref\u0026#34; \u0026lt;\u0026lt; endl; } //template\u0026lt;typename T\u0026gt; //void PerfectForward(T t) { fun(t); } // 利用引用折叠规则代替了原有的不完美转发机制 template\u0026lt;typename T\u0026gt; void PerfectForward(T \u0026amp;\u0026amp;t) { fun(static_cast\u0026lt;T \u0026amp;\u0026amp;\u0026gt;(t)); } int main() { PerfectForward(10); // rvalue ref，折叠后t类型仍然为T \u0026amp;\u0026amp; int a; PerfectForward(a); // lvalue ref，折叠后t类型为T \u0026amp; PerfectForward(std::move(a)); // rvalue ref，折叠后t类型为T \u0026amp;\u0026amp; const int b = 8; PerfectForward(b); // const lvalue ref，折叠后t类型为const T \u0026amp; PerfectForward(std::move(b)); // const rvalue ref，折叠后t类型为const T \u0026amp;\u0026amp; return 0; } 例子中已经对完美转发的各种情况进行了说明，这里需要对PerfectForward模板函数中的static_cast进行说明。static_cast仅是对传递右值时起作用。我们看一下当参数为右值时的情况，这里的右值包括了const右值和非const右值。\n1 2 3 4 5 6 7 // 参数为右值，引用折叠规则引用前 template\u0026lt;int \u0026amp;\u0026amp; \u0026amp;\u0026amp;T\u0026gt; void PerfectForward(int \u0026amp;\u0026amp; \u0026amp;\u0026amp;t) { fun(static_cast\u0026lt;int \u0026amp;\u0026amp; \u0026amp;\u0026amp;\u0026gt;(t)); } // 引用折叠规则应用后 template\u0026lt;int \u0026amp;\u0026amp;T\u0026gt; void PerfectForward(int \u0026amp;\u0026amp;t) { fun(static_cast\u0026lt;int \u0026amp;\u0026amp;\u0026gt;(t)); } 可能读者仍然没有发现上述例子中的问题，“不用static_cast进行强制类型转换不是也可以吗？”。别忘记前文中仍然提到一个右值引用和右值之间关系的规则，如果右值引用有名字则为左值，如果右值引用没有名字则为右值。。这里的变量t虽然为右值引用，但是是左值。如果我们想继续向fun函数中传递右值，就需要使用static_cast进行强制类型转换了。\n其实在C++11中已经为我们封装了std::forward函数来替代我们上文中使用的static_cast类型转换，该例子中使用std::forward函数的版本变为了：\n1 2 template\u0026lt;typename T\u0026gt; void PerfectForward(T \u0026amp;\u0026amp;t) { fun(std::forward\u0026lt;T\u0026gt;(t)); } 对于上文中std::move函数的实现也是使用了引用折叠规则，实现方式跟std::forward一致。\n引用 《深入理解C++11-C++11新特性解析与应用》 C++11 标准新特性: 右值引用与转移语义 如何评价 C++11 的右值引用（Rvalue reference）特性？ C++11 完美转发 C++ Rvalue References Explained 详解C++右值引用 （对C++ Rvalue References Explained的翻译） ","date":"2015-05-18T00:00:00Z","permalink":"/post/cpp11_right_reference/","title":"C++11中的右值引用"},{"content":"最近粗读了一遍《大型网站技术架构-核心原理与案例分析》，并对其中的内容通过思维导图的形式进行了整理。本书的所讲解的内容均为大型网站中涉及到的问题及相关技术，但并未展开深入讨论相关技术的解决办法，非常适合入门。下面我将我的思维导图以图片的形式贴出来，并提供XMind编辑的.xmid格式的文件。\n下载 大型网站技术架构读书笔记\n","date":"2015-05-13T00:00:00Z","permalink":"/post/large_website_architecture/","title":"大型网站技术架构读书笔记"},{"content":"问题描述 无线wifi的essid支持英文和中文，中文的编码在802.11协议并没有规定，对于802.11协议而言仅将essid看作是二进制。而中文又存在多种编码方式，最常见的就是GB18030（我这里直接用GB18030代替了GB系列的字符集）和UTF-8了。\niwlist程序通过命令iwlist wlan0 scanning可以在终端上正常显示UTF-8编码的essid，对于其他编码的中文仍然是乱码，这也就非常容易理解了。因为具体的essid能否将中文正常显示在终端屏幕上跟essid的编码和当前终端环境的编码是否能够匹配有关，如果essid的编码和当前终端环境的编码均为UTF-8，则essid可以在屏幕上正常显示。如果当前网络中的可以搜索到的essid即包含了GB18030编码又包含了UTF-8编码，则打印在终端上的essid必然会有乱码的情况出现。\nairodump-ng程序问题 对于airodump-ng程序而言，即时是essid的编码和终端编码一致也会出现某些中文字符乱码的问题，这一点比较奇怪。比如“免费”中的“免”字是乱码，“费”却能正常显示。通过这一现象有理由怀疑airodump-ng对essid做了某些处理。\n经过查看源码发现，在airodump-ng.c文件中存在三处如下类似代码，作用为将essid中的ascii值在(126,160)之间的转换为\u0026quot;.\u0026quot;。看来airodump-ng程序并没有考虑到中文的情况，仅将ascii中无法显示的字符做了转换。将程序中的三处代码注释后就可以正常显示了。具体三处代码可以通过搜索\u0026rsquo;.\u0026lsquo;来查找。\n1 2 3 4 5 6 7 8 9 for( i = 0; i \u0026lt; n; i++ ) { c = p[2 + i]; if( c == 0 || ( c \u0026gt; 126 \u0026amp;\u0026amp; c \u0026lt; 160 ) ) { c = \u0026#39;.\u0026#39;; //could also check ||(c\u0026gt;0 \u0026amp;\u0026amp; c\u0026lt;32) } st_cur-\u0026gt;probes[st_cur-\u0026gt;probe_index][i] = c; } NetworkManager 通过实践发现，GNOME和KDE桌面下的查看无线网络连接的ssid是可以正常显示的，即可以正常显示GB18030，又可以正常显示UTF-8编码的essid。则可以推测，在桌面环境下的搜索网络的程序肯定对编码做了某些处理，顺着这个思路，就可以查找GNOME或KDE的代码了。\n在GNOME的源码中看到了network-manager-applet，该程序即为桌面上查看无线网络连接的小控件。在applet-device-wifi.c文件中看到了如下代码，其中的nm_utils_ssid_to_utf8函数即为将其他编码转换为UTF-8编码的函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 static char * get_ssid_utf8 (NMAccessPoint *ap) { char *ssid_utf8 = NULL; const GByteArray *ssid; if (ap) { ssid = nm_access_point_get_ssid (ap); if (ssid) ssid_utf8 = nm_utils_ssid_to_utf8 (ssid); } if (!ssid_utf8) ssid_utf8 = g_strdup (_(\u0026#34;(none)\u0026#34;)); return ssid_utf8; } nm_utils_ssid_to_utf8函数定义在NetworkManager工程中的nm-utils.c文件中。该函数的代码如下，该函数具体功能可以查看代码中的注释，已经非常详细了。其中以g_开头的函数是glib库中的函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 char * nm_utils_ssid_to_utf8 (const GByteArray *ssid) { char *converted = NULL; char *lang, *e1 = NULL, *e2 = NULL, *e3 = NULL; g_return_val_if_fail (ssid != NULL, NULL); if (g_utf8_validate ((const gchar *) ssid-\u0026gt;data, ssid-\u0026gt;len, NULL)) return g_strndup ((const gchar *) ssid-\u0026gt;data, ssid-\u0026gt;len); /* LANG may be a good encoding hint */ g_get_charset ((const char **)(\u0026amp;e1)); if ((lang = getenv (\u0026#34;LANG\u0026#34;))) { char * dot; lang = g_ascii_strdown (lang, -1); if ((dot = strchr (lang, \u0026#39;.\u0026#39;))) *dot = \u0026#39;\\0\u0026#39;; get_encodings_for_lang (lang, \u0026amp;e1, \u0026amp;e2, \u0026amp;e3); g_free (lang); } converted = g_convert ((const gchar *) ssid-\u0026gt;data, ssid-\u0026gt;len, \u0026#34;UTF-8\u0026#34;, e1, NULL, NULL, NULL); if (!converted \u0026amp;\u0026amp; e2) converted = g_convert ((const gchar *) ssid-\u0026gt;data, ssid-\u0026gt;len, \u0026#34;UTF-8\u0026#34;, e2, NULL, NULL, NULL); if (!converted \u0026amp;\u0026amp; e3) converted = g_convert ((const gchar *) ssid-\u0026gt;data, ssid-\u0026gt;len, \u0026#34;UTF-8\u0026#34;, e3, NULL, NULL, NULL); if (!converted) { converted = g_convert_with_fallback ((const gchar *) ssid-\u0026gt;data, ssid-\u0026gt;len, \u0026#34;UTF-8\u0026#34;, e1, \u0026#34;?\u0026#34;, NULL, NULL, NULL); } return converted; } nm_utils_ssid_to_utf8该函数位于libnm-util.so.1动态库中，可通过nm -D /usr/lib64/libnm-util.so.1 | grep nm_utils_ssid_to_utf8命令查看导出表中存在该函数。但是系统中并不存在该函数的头文件libnm-util.h，给该库的调用增加了不少难度。可以通过将相关头文件引入到该工程编译的方式来完成，但是可能会牵涉到的头文件比较多，比较繁琐。\n我这里直接采用了将NetworkManager中相关代码抓取出来的思路，并将其封装成类的形式以方便调用。具体代码可以参照demo中的例子。\nglib glib是GTK底层调用的核心库，跟glibc是没有关系的，虽然名字中仅差一个字母。为了调用该库需要在编译的时候添加*pkg-config --cflags --libs glib-2.0*信息，以引入需要的头文件和要链接的库。\n相关下载 文中用到的软件源码和程序demo\n引用 GNOME源码列表 ","date":"2015-05-04T00:00:00Z","permalink":"/post/airodump-ng_ssid_messy/","title":"解决airodump-ng显示ssid名称的乱码问题"},{"content":"题目一 Single Number Given an array of integers, every element appears twice except for one. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?\n题目二 Single Number II Given an array of integers, every element appears three times except for one. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory?\n题目一分析及解答 针对题目一，一看就能看出是考察异或操作的特点，并迅速写出了解答方法。\n1 2 3 4 5 6 7 8 9 10 11 class Solution { public: int singleNumber(int A[], int n) { int result = 0; for (int i = 0; i \u0026lt; n; i++) { result ^= A[i]; } return result; } }; 题目二分析及解答 要想实现时间复杂度为O(n)，空间复杂度为O(1)的算法，还是跟题目一一样需要充分利用位操作特性，但是并没有直接可用的位操作特性可以完成，于是想到肯定是各种位操作的组合操作，但是并没有继续向下想到具体的算法。本质上该题目就是模拟一个三进制的操作，当一个位的最大值为2，当为3时直接清0。\n参照网上的算法，利用一个int类型的数组来模拟一个三进制数，每个int值的最大值为3，当然这样存在一定空间上的浪费。算法需要将A中的每个值通过移位运算获取到该位的状态，并将值添加到用来模拟三进制的int数组中相应的位置，最后将模拟三进制int数组中的值为3的更改为0。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public: int singleNumber(int A[], int n) { int count[32] = {0}; int result = 0; for (int i = 0; i \u0026lt; 32; i++) { for (int j = 0; j \u0026lt; n; j++{ if ((A[j] \u0026gt;\u0026gt; i) \u0026amp; 1) { count[i]++; } } result |= ((count[i] % 3) \u0026lt;\u0026lt; i); } return result; } }; 另外，还有上述算法的改进算法，更为节省空间，效率更高，但是确实不容易理解和记忆，属于下次仍然无法记忆的算法类型。这里仅提供代码，不再给出解释，自己领悟。\n1 2 3 4 5 6 7 8 9 10 11 12 int singleNumber(int A[], int n) { int ones = 0, twos = 0, threes = 0; for (int i = 0; i \u0026lt; n; i++) { twos |= ones \u0026amp; A[i]; ones ^= A[i];// 异或3次 和 异或 1次的结果是一样的 //对于ones 和 twos 把出现了3次的位置设置为0 （取反之后1的位置为0） threes = ones \u0026amp; twos; ones \u0026amp;= ~threes; twos \u0026amp;= ~threes; } return ones; } ","date":"2015-03-31T00:00:00Z","permalink":"/post/leetcode_single_number/","title":"leetcode题目之Single Number"},{"content":"前段时间参加了牛客网的答题活动，共两套试题，每套题目3个算法题，我只做了每套题的前两道。最近想查看之前做的题目的答案，却发现非常不方便，特此将我做过的4道题目记录一下，算法的思路就不再解释了。\n题目一 奇数位上都是奇数或者偶数位上都是偶数 给定一个长度不小于2的数组arr。 写一个函数调整arr，使arr中要么所有的偶数位上都是偶数，要么所有的奇数位上都是奇数上。 要求：如果数组长度为N，时间复杂度请达到O(N)，额外空间复杂度请达到O(1),下标0,2,4,6\u0026hellip;算作偶数位,下标1,3,5,7\u0026hellip;算作奇数位，例如[1,2,3,4]调整为[2,1,4,3]即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 class Solution { public: void oddInOddEvenInEven(vector\u0026lt;int\u0026gt;\u0026amp; arr, int len) { int odd = 1; int even = 0; while (odd \u0026lt; len \u0026amp;\u0026amp; even \u0026lt; len) { if (arr[odd] % 2 == 0) { while (arr[even] % 2 == 0) { even += 2; } if (even \u0026lt; len) { int tmp = arr[even]; arr[even] = arr[odd]; arr[odd] = tmp; } else { break; } } else { odd += 2; } } } }; 题目二 求正数数组的最小不可组成和 给定一个全是正数的数组arr，定义一下arr的最小不可组成和的概念： 1，arr的所有非空子集中，把每个子集内的所有元素加起来会出现很多的值，其中最小的记为min，最大的记为max； 2，在区间[min,max]上，如果有一些正数不可以被arr某一个子集相加得到，那么这些正数中最小的那个，就是arr的最小不可组成和； 3，在区间[min,max]上，如果所有的数都可以被arr的某一个子集相加得到，那么max+1是arr的最小不可组成和； 举例： arr = {3,2,5} arr的min为2，max为10，在区间[2,10]上，4是不能被任何一个子集相加得到的值中最小的，所以4是arr的最小不可组成和； arr = {3,2,4} arr的min为2，max为9，在区间[2,9]上，8是不能被任何一个子集相加得到的值中最小的，所以8是arr的最小不可组成和； arr = {3,1,2} arr的min为1，max为6，在区间[2,6]上，任何数都可以被某一个子集相加得到，所以7是arr的最小不可组成和； 请写函数返回arr的最小不可组成和。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 class Solution { public: int getFirstUnFormedNum(vector\u0026lt;int\u0026gt;\u0026amp; arr, int len) { set\u0026lt;int\u0026gt; res; for (int i=0; i\u0026lt;len; i++) { set\u0026lt;int\u0026gt; tmp = res; for (set\u0026lt;int\u0026gt;::iterator iter = res.begin(); iter != res.end(); iter++) { tmp.insert(*iter + arr[i]); } res = tmp; res.insert(arr[i]); } set\u0026lt;int\u0026gt;::iterator iter = res.begin(); int before = *iter; iter++; for (; iter != res.end(); iter++) { if (*iter - before \u0026gt; 1) { return before + 1; } before = *iter; } return before + 1; } }; 题目三 最大的LeftMax与rightMax之差绝对值 给定一个长度为N的整型数组arr，可以划分成左右两个部分： 左部分arr[0..K]，右部分arr[K+1..arr.length-1]，K可以取值的范围是[0,arr.length-2] 求这么多划分方案中，左部分中的最大值减去右部分最大值的绝对值，最大是多少？ 例如： [2,7,3,1,1] 当左部分为[2,7]，右部分为[3,1,1]时，左部分中的最大值减去右部分最大值的绝对值为4; 当左部分为[2,7,3]，右部分为[1,1]时，左部分中的最大值减去右部分最大值的绝对值为6; 最后返回的结果为6。 注意：如果数组的长度为N，请尽量做到时间复杂度O(N)，额外空间复杂度O(1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Solution { public: int getMaxABSLeftAndRight(vector\u0026lt;int\u0026gt; vec, int len) { if (len == 0) { return 0; } // find the max in array int max = vec[0]; for (int i=1; i\u0026lt;(int)vec.size(); i++) { if (vec[i] \u0026gt; max) { max = vec[i]; } } // compare the head and tail in array if (vec[0] \u0026lt; vec[len - 1]) { return max - vec[0]; } return max - vec[len - 1]; } }; 题目四 按照左右半区的方式重新组合单链表 给定一个单链表的头部节点head，链表长度为N。 如果N为偶数，那么前N/2个节点算作左半区，后N/2个节点算作右半区； 如果N为奇数，那么前N/2个节点算作左半区，后N/2+1个节点算作右半区； 左半区从左到右依次记为L1-\u0026gt;L2-\u0026gt;\u0026hellip;，右半区从左到右依次记为R1-\u0026gt;R2-\u0026gt;\u0026hellip;。请将单链表调整成L1-\u0026gt;R1-\u0026gt;L2-\u0026gt;R2-\u0026gt;\u0026hellip;的样子。 例如： 1-\u0026gt;2-\u0026gt;3-\u0026gt;4 调整后：1-\u0026gt;3-\u0026gt;2-\u0026gt;4 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5 调整后：1-\u0026gt;3-\u0026gt;2-\u0026gt;4-\u0026gt;5 要求：如果链表长度为N，时间复杂度请达到O(N)，额外空间复杂度请达到O(1)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 class Solution { public: void relocateList(struct ListNode* head) { if (head == NULL || head-\u0026gt;next == NULL) { return ; } // use one loop, find the right head ListNode *right_head = head; ListNode *node = head; while (node != NULL) { if (node-\u0026gt;next == NULL) { break; } if (node-\u0026gt;next-\u0026gt;next == NULL) { right_head = right_head-\u0026gt;next; break; } right_head = right_head-\u0026gt;next; node = node-\u0026gt;next-\u0026gt;next; } ListNode *left_node = head; ListNode *right_node = right_head; while (left_node-\u0026gt;next != right_head) { ListNode *tmp = left_node-\u0026gt;next; left_node-\u0026gt;next = right_node; right_node = right_node-\u0026gt;next; left_node-\u0026gt;next-\u0026gt;next = tmp; left_node = left_node-\u0026gt;next-\u0026gt;next; } left_node-\u0026gt;next = right_node; } }; ","date":"2015-03-25T00:00:00Z","permalink":"/post/nowcoder_2015.3.12/","title":"牛客网内推笔试卷题目2015.3.12"},{"content":"题目 Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times. You may assume that the array is non-empty and the majority element always exist in the array.\n分析 本题是一道非常简单的题目，但我能想到的思路有限，仅能想到排序法和哈希法两种算法，在Solution中提供了另外几种方法，这是非常值得我学习和思考的。本文仅将网站的思路拿过来，可以直接看该问题的Solution。\n解答 暴力枚举法 最原始的解决办法，逐个元素比较是否为该数组中的最多元素，只要满足条件即可终止。时间复杂度为O(n^2)。\n哈希表法 将数组中的元素遍历一遍，并将数组中元素的个数保存到哈希中。然后遍历哈希，从哈希中找到最多元素。时间复杂度O(n)，但需要占用一定的空间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 int majorityElement(vector\u0026lt;int\u0026gt; \u0026amp;num) { std::map\u0026lt;int, int\u0026gt; result_map; for (vector\u0026lt;int\u0026gt;::iterator iter = num.begin(); iter != num.end(); iter++) { if (result_map.find(*iter) == result_map.end()) { result_map.insert(map\u0026lt;int, int\u0026gt;::value_type(*iter, 1)); } else { result_map[*iter]++; } } int max_count = 0; int result; for (std::map\u0026lt;int, int\u0026gt;::iterator iter = result_map.begin(); iter != result_map.end(); iter++) { if (iter-\u0026gt;second \u0026gt; max_count) { result = iter-\u0026gt;first; max_count = iter-\u0026gt;second; } } return result; } 排序法 直接对元素进行排序，排序后元素的中间元素即为要求的最多元素。时间复杂度为O(nlogn)。\n1 2 3 4 int majorityElementSort(vector\u0026lt;int\u0026gt; \u0026amp;num) { sort(num.begin(), num.end()); return num[num.size() / 2]; } 随机抽取法 随机从数组中抽取元素，然后遍历数组判断该元素是否为最多元素。该算法利用了最多元素被随机抽取的概率最大的特点，但该算法效率的随机性较大，最好时间复杂度为O(n)，最坏情况下一直随机不到最多元素。\n二分法 将数组均分为两份，分别求出两个数组中的最多元素A和B，则整个数组中的最多元素必然在两个子数组的最多元素A和B中，这一点可以通过举例子的方式来证明，但是仅凭感觉不太容易得出该结论。如果A==B，则结果就是A。如果A!=B，则分别求出A和B在这个数组中的元素个数。时间复杂度接近O(nlogn)。\n其他 还有一些比较不容易想到的算法，这里就不列举了。至少我看过一次之后，下次这些算法仍然是记不住的。\n","date":"2015-03-11T00:00:00Z","permalink":"/post/leetcode_majority_element/","title":"leetcode题目之Majority Element"},{"content":"asleap是一个开源的vpn破解工具，最近查看了asleap的源码，该项目地址。本文的重点是对其中的带索引的字典文件的产生过程进行介绍，产生带索引的字典文件并不复杂，但是要想用简洁易懂的语言将该问题描述明白却不容易。\nasleap破解vpn的机制是通过字典文件暴力破解的方式，该字典文件有dat数据文件和idx索引文件两个文件组成，两个文件均为二进制格式。asleap工程中自带了genkey程序，可以将文本的字典文件转换为asleap程序需要的带索引的字典文件。\n本文以字典文件为以下内容讲解：\n1 2 3 turquoise da test 读取字典文件并产生md4值 md4编码占16个字节，三个字典进行md4编码后的结果分别为：\n1 2 3 18 07 33 43 f6 30 b5 f8 2c 38 c0 34 37 f2 81 6b 01 19 a3 80 94 40 60 3c 57 39 5e 73 f3 60 95 98 0c b6 94 88 05 f7 97 bf 2a 82 80 79 73 b8 95 37 将字典信息写入到临时文件 为了能够对最终生成的dat文件中的内容进行排序和便于索引，程序生成了256个临时文件，文件名格式为从genk-bucket-00.tmp到genk-bucket-ff.tmp。程序根据md4编码中的第14位将字典对应的信息分别写入到临时文件中，一个字典写入到临时文件的内容如下，如果一个临时文件中存在多个字典则依次存放：\n1 2 3 4 5 struct hashpass_rec { unsigned char rec_size;\t// 一个字典占用文件的大小，包括该变量+字典+字典对应的md4值共占用的字节数 char *password;\t// 字典 unsigned char hash[16];\t// 字典对应的md4值 } __attribute__ ((packed)); 本例子中turquoise对应结构体会写入到genk-bucket-81.tmp中，da和test对应结构体会依次写入到genk-bucket-95.tmp中。\n读取临时文件并写入到dat数据文件中 最终dat文件中的数据内容为hashpass_rec的有序集合，排序的原则是按照md4的第14和15两个字节。依次读取256个临时文件中的hashpass_rec可以保证dat文件中的数据内容是按照第14字节排序的，但是不能够保证是按照第15个字节排序的。为了保证最终dat文件中的数据内容是按照第14和15字节有序的，在将一个临时文件中的内容写入到dat文件中前需要对该临时文件中的hashpass_rec结果按照hash变量的第15字节进行排序，直接使用C语言中的qsort进行排序。\n该例子中da和test位于同一个临时文件中，需要根据hash变量的第15字节排序的结果为test、da，最终写入到dat文件中的排序结果为turquoise、test、da。\n根据dat数据文件产生idx索引文件 idx索引文件中存放的是多个hashpassidx_rec结果，最多有256*256项，其结构定义如下：\n1 2 3 4 5 struct hashpassidx_rec { unsigned char\thashkey[2];\t// 对应md4编码的第14和15字节 off_t offset;\t// 第一个匹配的hashpass_rec结构在dat文件中的偏移，占用4个字节 unsigned long long int numrec;\t// dat文件中共有多少个匹配的hashpass_rec结果 } __attribute__ ((packed));\t// 字节对齐，需要填充4个字节 最终完成的dat文件和idx文件的指向如下图所示：\nbug genkeys.c文件中在读取字典文件时存在bug，在文件的207行将内容更改为：\n1 2 3 4 5 6 while (!feof(inputfl)) { memset(password, 0, MAX_NT_PASSWORD + 1); fgets(password, MAX_NT_PASSWORD+1, inputfl); if (strlen(password) == 0) { continue; } 相关下载 字典文件等相关文件下载\n","date":"2015-02-28T00:00:00Z","permalink":"/post/aleap_idx/","title":"asleap中的简单文件索引机制"},{"content":"本文对我编写的常用的排序算法进行整理和总结，方便用时进行查阅和参考。\n快速排序 快速排序是实际应用中的最好选择，采用了分治法的思想。通过一趟排序将待排序记录分割成独立的两部分，其中一部分的关键字均比另外一部分的小，分别对这两部分记录进行排序，已达到整个有序。\n是否稳定：不稳定\n时间复杂度：O(nlogn)\n空间复杂度：O(logn)，需要栈来实现递归用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;iostream\u0026gt; int partition(std::vector\u0026lt;int\u0026gt; \u0026amp;numbers, int low, int high) { int pivotkey = numbers[low]; while (low \u0026lt; high) { while (low \u0026lt; high \u0026amp;\u0026amp; numbers[high] \u0026gt;= pivotkey) { --high; } numbers[low] = numbers[high]; while (low \u0026lt; high \u0026amp;\u0026amp; numbers[low] \u0026lt;= pivotkey) { ++low; } numbers[high] = numbers[low]; numbers[low] = pivotkey; } return low; } void quick_sort(std::vector\u0026lt;int\u0026gt; \u0026amp;numbers, int low, int high) { if (low \u0026lt; high) { int pivotloc = partition(numbers, low, high); quick_sort(numbers, low, pivotloc - 1); quick_sort(numbers, pivotloc + 1, high); } } void quick_sort(std::vector\u0026lt;int\u0026gt; \u0026amp;numbers) { quick_sort(numbers, 0, numbers.size() - 1); } int main() { std::vector\u0026lt;int\u0026gt; numbers = {49, 38, 65, 97, 76, 13, 27, 49}; quick_sort(numbers); for (int i=0; i\u0026lt;numbers.size(); i++) { printf(\u0026#34;%d\\t\u0026#34;, numbers[i]); } printf(\u0026#34;\\n\u0026#34;); } 归并排序 将两个或两个以上的有序表组合成一个新的有序表。合并两个有序表的方法为：比较两个有序表中第一个数，谁小先取谁。继续进行比较，只要有一个有序表为空，直接将另一个有序表取出即可。\n是否稳定：稳定\n时间复杂度：O(nlogn)\n空间复杂度：O(n) （当使用顺序存储时，为了能够实现两个有序表之间的合并），或O(1)（当使用链式存储的时候，不再需要临时的空间来存储排序的结果）\n顺序存储代码 以下为采用顺序存储结构的代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 /** * 归并排序使用递归算法的效率比较低，具体应用中会采用非递归算法代替 */ void merge(std::vector\u0026lt;int\u0026gt; \u0026amp;numbers, std::vector\u0026lt;int\u0026gt; \u0026amp;extra, int low, int high, int middle) { int i = low, j = middle+1, k = low; for (; i\u0026lt;=middle \u0026amp;\u0026amp; j\u0026lt;=high; k++) { if (numbers[i] \u0026lt;= numbers[j]) { extra[k] = numbers[i]; i++; } else { extra[k] = numbers[j]; j++; } } while (i \u0026lt;= middle) { extra[k++] = numbers[i++]; } while (j \u0026lt;= middle) { extra[k++] = numbers[j++]; } for (int m = low; m \u0026lt;= high; m++) { numbers[m] = extra[m]; } } void merge_sort(std::vector\u0026lt;int\u0026gt; \u0026amp;numbers, std::vector\u0026lt;int\u0026gt; \u0026amp;extra, int low, int high) { if (low == high) { return ; } int middle = (low + high) / 2; merge_sort(numbers, extra, low, middle); merge_sort(numbers, extra, middle + 1, high); merge(numbers, extra, low, high, middle); } void merge_sort(std::vector\u0026lt;int\u0026gt; \u0026amp;numbers) { // 申请额外的存储空间来用于排序处理 std::vector\u0026lt;int\u0026gt; extra = numbers; merge_sort(numbers, extra, 0, numbers.size() - 1); } int main() { std::vector\u0026lt;int\u0026gt; numbers = {49, 38, 65, 97, 76, 13, 27, 49}; merge_sort(numbers); for (int i=0; i\u0026lt;numbers.size(); i++) { printf(\u0026#34;%d\\t\u0026#34;, numbers[i]); } printf(\u0026#34;\\n\u0026#34;); } 链式存储代码 以下为采用链式存储结构的代码，本答案为我在LeetCode上的Sort List 题目的答案，源码放在我的Github上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 struct ListNode { int val; ListNode *next; ListNode(int x) : val(x), next(NULL) {} }; /** * 使用归并排序方法，核心思想为将数组拆分为两半，分别对两半进行排序，排序完成后再进行一次排序，排序算法就可以采用插入排序的方式。 * 对两半排序的算法仍然采用归并排序算法，即问题为递归问题 * 在使用线性存储结果的归并排序算法中，会使用额外的空间来存储临时结果，空间复杂度为O(n)，而在链式存储中，空间复杂度为O(1) * 归并排序的时间复杂度为O(nlogn) * 如果存储结构为双向链表，可以使用快速排序 */ ListNode* sortList(ListNode* head) { if (head == nullptr || head-\u0026gt;next == nullptr) { return head; } if (head-\u0026gt;next-\u0026gt;next == nullptr) { // 仅有两个元素，对两个元素进行排序后直接返回 if (head-\u0026gt;val \u0026lt; head-\u0026gt;next-\u0026gt;val) { return head; } else { ListNode *tmp = head-\u0026gt;next; tmp-\u0026gt;next = head; tmp-\u0026gt;next-\u0026gt;next = nullptr; return tmp; } } // 为了找到中间节点，这里采用快慢指针的方式，否则需要使用先遍历一次取长度，然后找到中间位置的两次遍历方式 ListNode *fast = head; ListNode *slow = head; ListNode *slow_prev = nullptr; while (fast != nullptr \u0026amp;\u0026amp; fast-\u0026gt;next != nullptr) { fast = fast-\u0026gt;next-\u0026gt;next; slow_prev = slow; slow = slow-\u0026gt;next; } fast = slow_prev-\u0026gt;next; slow_prev-\u0026gt;next = nullptr; // 分别对两段链表进行排序 slow = sortList(head); fast = sortList(fast); // 对两段链表进行合并 ListNode *node = nullptr, *result = nullptr; while (slow != nullptr \u0026amp;\u0026amp; fast != nullptr) { if (slow-\u0026gt;val \u0026lt; fast-\u0026gt;val) { if (result != nullptr) { node-\u0026gt;next = slow; node = node-\u0026gt;next; } else { node = slow; result = slow; } slow = slow-\u0026gt;next; } else { if (result != nullptr) { node-\u0026gt;next = fast; node = node-\u0026gt;next; } else { node = fast; result = fast; } fast = fast-\u0026gt;next; } } if (slow != nullptr) { node-\u0026gt;next = slow; } if (fast != nullptr) { node-\u0026gt;next = fast; } return result; } 直接插入排序 该排序算法的时间复杂度为O(n^2)，算法复杂度过高。分为顺序存储和链式存储两种算法，其中顺序存储每比较一个元素是从该元素往前比较的，而链式存储是从链头开始比较的，这点有所不同，造成不同的是由存储结构决定的。\n顺序存储代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;vector\u0026gt; using namespace std; void insertion_sort(std::vector\u0026lt;int\u0026gt; \u0026amp;numbers) { if (numbers.size() \u0026lt;= 1) { return; } for (int i = 1; i \u0026lt; numbers.size(); i++) { for (int j = i; j \u0026gt; 0; j--) { if (numbers[j] \u0026lt; numbers[j - 1]) { swap(numbers[j], numbers[j - 1]); } else { break; } } } } int main() { int a[] = { 49, 38, 65, 97, 76, 13, 27, 49 }; vector\u0026lt;int\u0026gt; numbers(a, a + sizeof(a) / sizeof(int)); insertion_sort(numbers); for (int i = 0; i\u0026lt;numbers.size(); i++) { printf(\u0026#34;%d\\t\u0026#34;, numbers[i]); } return 0; } 链式存储代码 以下代码为LeetCode上的链式存储的情况时的直接插入排序算法的代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 ListNode* insertionSortList(ListNode* head) { if (head == NULL) { return NULL; } // 初始化 ListNode *node = head-\u0026gt;next; ListNode *new_head = head; new_head-\u0026gt;next = NULL; while (node != NULL) { ListNode *node_next = node-\u0026gt;next; // 先将当前遍历的下一个节点保存 // 将当前节点插入到新链表中 ListNode *new_node_tmp = new_head; if (node-\u0026gt;val \u0026lt; new_node_tmp-\u0026gt;val) { // 当前节点插入新链表的第一个位置 node-\u0026gt;next = new_head; new_head = node; } else { // 将当前节点插入到中间 while (new_node_tmp-\u0026gt;next != NULL) { if (node-\u0026gt;val \u0026lt; new_node_tmp-\u0026gt;next-\u0026gt;val) { node-\u0026gt;next = new_node_tmp-\u0026gt;next; new_node_tmp-\u0026gt;next = node; break; } new_node_tmp = new_node_tmp-\u0026gt;next; } // 将该节点插入到最后位置 if (new_node_tmp-\u0026gt;next == NULL) { new_node_tmp-\u0026gt;next = node; node-\u0026gt;next = NULL; } } // 开始遍历当前节点的下一个节点 node = node_next; } return new_head; } ","date":"2015-02-25T00:00:00Z","permalink":"/post/algorithm_sort_code/","title":"常用排序算法整理及代码实现"},{"content":"我的年总结是依照农历的，因为在我心中春节才算是一年的真正开始，因为只有春节的时候才能找到年的滋味，年的感觉，才能称之为年，阳历的年只能称之为year。\n2014年又在不经意间过去了，很多地方跟2013年一样是平淡，工作和学习仍然是生活的主旋律，闲暇时间抽个时间玩玩dota放松一下，周末偶尔爬个小山锻炼下身体，对我的人生中算是比较重要的一年。\n学习 2014年经历了结婚、毕业答辩和考驾照几件占用时间的事情，留给我业余时间用来学习的就少之又少了。结婚占用两个月时间，毕业论文占用了我两个月时间，考驾照占用了多个周末，工作出差占用了我一个月时间，留给我能够独立学习的晚上也就6个月时间。\n开始的时候小看了硕士论文，以为很简单一事情，搞过这么多软件项目还搞不了一篇硕士论文。一直以来我看不起软件工程类论文，就一项目套个模板一介绍就是一篇论文，于是我选择了写一篇理论研究类论文，没有高大上的理论，而是在公司实践中真正用到的，《将Windows平台的C/C++程序向Linux平台移植的技术研究》，选择论文的时候我已经看到了该题目不太适合作为硕士论文，当我还是毅然作为了我的硕士论文题目，不得不说这是我今年的一大败笔。第一篇论文失败后，我重新走起了保守路线，以之前熟悉的系统为主线，辅以各种文档的拼凑，完成了一篇我曾经嗤之以项目类硕士论文，并顺利通过答辩。对于我这样的新手而言，写论文是一件漫长又痛苦的过程，占用了我大量的宝贵时间。\n一直以来对嵌入式linux方向比较好奇，今年终于抵不住好奇，买了个2140开发板自己捣鼓了一段时间，由于时间关系虽然到现在也没有入门，多多少少对嵌入式已经有所了解。\n感谢我的另一半，给我提供了足够的时间来干我想干的事情。\n书籍 比起2013年，今年读过的书籍少了一些。\n《Linux/Unix系统编程手册》 《编程珠玑》 《LINUX设备驱动程序》（部分章节） 《LINUX内核设计与实现》（大部分章节） 《深度探索linux操作系统》 《剑指Office》 《大规模C++程序设计》 《程序员的自我修养》（第二遍） 《文明之光》 《程序员健康指南》 《黄金时代》 《一只特立独行的猪》 《算法导论》（部分章节） 工作 已经比较熟练掌握了公司产品的大部分技术，工作起来算是得心应手，但是却少了许多挑战，是时候该接收大挑战的了。工作的职位从后台组长到研发部副经理到研发部经理，开始了工作的转型，这是我不期望这么早来到的，我自认为技术的成长空间还很大，不想过早的接触管理岗位。\n五月份的出差成为了我心中抹不去的痛，莫名其妙接受了任务，匆忙出差，不过坑才刚刚开始。要维护的是一个我没见过界面的产品，不过基本原理我是清楚的。产品有很多bug，这我可以理解，要不然也不会让我出差了，但要命的是我没有产品的代码，我接受的任务仅是去应付客户、发现bug后反馈、更新产品、施工，这明白着是市场+测试+维护的话，跟我半毛钱关系都没有。以上这些都无所谓，要知道我可不是一个顽固不化的程序员，但工作地点竟然是机房，而且机房是我见过最脏最乱的机房，我就站在一排机柜的后面的一堆烂纸箱子上吹着空调的冷风和机器的热风办公，时而蹲着，时而站着，时而坐着，时而将衣领撩起保暖，时而浑身打颤，以至于到现在我留下了膝盖隐隐作痛的毛病。而且系统bug不断，一连在不吃晚饭的情况下加班到大半夜好多次，而我竟然坚持下来了。原本三五天的出差计划，一待就是二十多天。这短短的二十多天成为了我今年最难忘的痛，多少次期望回到家里温暖的被窝，多少次期望在家吃着我做的炖土豆。\n今年面试了不少人，通过面试也发现大部分技术人员的水平太差了，我甚至都搞不明白他们是怎么厚着脸皮来面试技术岗位的。济南软件行业实在是不景气，甚至找一个靠谱点的web或者php程序员都成为了公司的一大难题。\n生活 长达五年的恋情在今年终于得到了升华，当然这升华是我不愿意这么快就看到的，多么希望能迟来几年，给原本一直以为自己还是个孩子的我一个接受的缓冲区，但站在两家人面前，我的想法竟然不能主导我。我对结婚这件事情持怎么简单怎么办的态度，结婚本就一仪式，豪华也罢，没有也罢，都是过眼云烟，为一天忙碌了一阵子仅为了那一天，而之后又有谁记得。结婚之所以在中国的古代非常重视，那是因为在农业社会中人民的娱乐方式非常单一，结婚可以成为人民心中的一个盼头和没有灯光的饭后侃的资本，现在娱乐方式早已多元化，相比之下结婚的光鲜早已显得微不足道。可结婚毕竟不是我一个人的事，甚至不是两个人的事，而是两家人的事。\n结婚定在酷夏，定下来的时间比较匆忙，从定下来要结婚到结婚仅一个月时间，对我来说是莫大的好事，因为拖得时间越长占用的准备时间就越多。半年的婚前和半年的婚后生活，其实真的没人什么两样，都是美满的二人世界，希望这种状况能持续几年。\n我一向对车比较排斥，始终认为汽车是一个比较失败的发明，用户体验特别差。好的设计应该让用户忽略其内部实现细节，好的发明不应该让用户花费大量的时间来学习怎样使用，甚至需要多个课时的专业培训。汽车不仅是一个毫无用户体验的发明，而且危险到极致，危险到一失误就会要掉人的性命。\n但今年我随波逐流了，毕竟驾照是早晚要考的，晚考成本只会更高。于是考驾照提上了议程，8月初已经计划报名，只可惜流程过于复杂，到现在也才到了科目二的程度。先是报名需要办暂住证，暂住证一办就是15个工作日，直接拖到了十一之后。找个离家近的驾校报个名，一等就是一个月才考科目一。科目一考完一等又是一个月才开始分车学科目二。科目二刚开始学又开始继续了，一共练了两个工作日后驾校又开始集训了，又没我啥事了。好在我找了个陪练，练了几把就顺手了。\n虽然驾照没有考出来，仅考到了科目二，算是完成了驾照的一半，但却耗去了我的部分经历。找驾校、准备科目一、学习科目二、找陪练，这些花费的都是我的时间。要是驾校培训行业能够再成熟些，再人性化些能给多少学车的人带来方便。\n玩游戏多少有些过了，虽然每周也就不想学习的两个晚上用来玩游戏，但我深知自己不是玩游戏的料。很多时候为了能够赢一局，会熬夜到下半夜，这是非常不理智的。另外，以后尽量用其他方式来代替游戏放松，当然我深知其中的苦难。\n旅行 去过一次云南，都在这里了。\n清明节时间去过一次天津，天津比我想象的要好很多，各个地方特色比较明显，有别墅区、意大利风情区、现代的商业区等，这之间能够非常明显的区分，不像济南太混杂。不过天津的人却给我留下的印象不是很好，这也是小小的遗憾。\n展望 在我心中已经为2015年制定好了一些计划，为家庭，为自己，2015年会是我人生的一个转折点，期望2015年能够顺利。我会努力的。\n","date":"2015-02-13T00:00:00Z","permalink":"/post/2014_summary/","title":"2014年总结"},{"content":"题目 Find the contiguous subarray within an array (containing at least one number) which has the largest sum.\nFor example, given the array [−2,1,−3,4,−1,2,1,−5,4], the contiguous subarray [4,−1,2,1] has the largest sum = 6.\n分析 该题目为经典题目，存在多种解题思路。\n动态规划 求动态规划的关键在于找到状态方程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /** * 问题的关键是找到状态方式，找到状态方程后问题就迎刃而解 * 状态方程如下： * b[j]表示第j处，以a[j]结尾的子序列的最大和 * b[j]=max(a[j] + b[j-1], a[j]) * b数据的最大值即为问题的解 * 问题转换为求解b数组 * 时间复杂度为O(1)，空间复杂度为(n)，空间复杂度可以降为O(1)，为了使程序易读，不做调整 */ int maxSubArray(int A[], int n) { if (n == 0) { return 0; } int *b = new int[n]; b[0] = A[0]; int max_b = b[0]; for (int i=1; i\u0026lt;n; i++) { b[i] = std::max(A[i] + b[i-1], A[i]); if (max_b \u0026lt; b[i]) { max_b = b[i]; } } delete[] b; return max_b; } 分治法 《算法导论》的分治策略一章有关于该问题的详细解释。该题利用分治法来解决要比二分查找类最简单的分治算法要复杂。将数组一分为二后，最大数组存在三种情况：在左半或右半部分、跨越中点分别占据左部分一点和右部分一点。对于跨越中点的情况，转化为求从中点开始向左的最大值和从中点开始向右的最大值之和。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class Solution { public: int compare_array[4]; int maxSubArray(int A[], int n) { return maxSubArray(A, 0, n - 1); } /** * leetcode not support stdarg.h */ int max(int count, ...) { va_list ap; va_start(ap, count); int max = INT_MIN; for (int i=0; i\u0026lt;count; i++) { int temp = va_arg(ap, int); if (max \u0026lt; temp) { max = temp; } } va_end(ap); return max; } int max_compare_array() { int max_num = compare_array[0]; for (int i=1; i\u0026lt;4; i++) { if (max_num \u0026lt; compare_array[i]) { max_num = compare_array[i]; } } return max_num; } int maxSubArray(int A[], int begin, int end) { //printf(\u0026#34;begin : %d, end : %d\\n\u0026#34;, begin, end); if (begin == end) { return A[begin]; } else if ((end - begin) == 1) { //return max(3, A[begin], A[begin] + A[end], A[end]); compare_array[0] = A[begin]; compare_array[1] = A[begin] + A[end]; compare_array[2] = A[end]; compare_array[3] = INT_MIN; return max_compare_array(); } int middle = (begin + end) / 2; // 处理左边子数组 int max_left = maxSubArray(A, begin, middle); // 处理右边子数组 int max_right = maxSubArray(A, middle + 1, end); // 处理跨越中点的情况 int max_cross = maxCrossMiddle(A, begin, end); printf(\u0026#34;begin : %d, end : %d, max_left = %d, max_right = %d, max_cross = %d\\n\u0026#34;, begin, end, max_left, max_right, max_cross); // 返回三者中的最大值 compare_array[0] = max_left; compare_array[1] = max_right; compare_array[2] = max_cross; compare_array[3] = INT_MIN; return max_compare_array(); } /** * 处理跨越中点的情况 */ int maxCrossMiddle(int A[], int begin, int end) { if (begin == end) { return A[begin]; } int middle = (begin + end) / 2; // 求得[begin -- middle-1]的最大值 int max_left = A[middle - 1]; int sum = 0; for (int i=middle - 1; i\u0026gt;=begin \u0026amp;\u0026amp; i \u0026gt;= 0; i--) { sum += A[i]; if (max_left \u0026lt; sum) { max_left = sum; } } // 求得[middle+1 -- end]的最大值 int max_right = A[middle + 1]; sum = 0; for (int i=middle + 1; i\u0026lt;=end; i++) { sum += A[i]; if (max_right\u0026lt; sum) { max_right = sum; } } compare_array[0] = A[middle]; compare_array[1] = A[middle] + max_left; compare_array[2] = A[middle] + max_right; compare_array[3] = A[middle] + max_left + max_right; return max_compare_array(); } }; 扫描算法 《编程珠玑》一书8.4节提到该算法，时间复杂度为O(1)，是解决该问题最好的算法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 int maxSubArray(int A[], int n) { if (n == 0) { return 0; } int current_sum = 0; int max_sum = INT_MIN; for (int i=0; i\u0026lt;n; i++) { if (current_sum \u0026lt;= 0) { current_sum = A[i]; } else { current_sum += A[i]; } if (current_sum \u0026gt; max_sum) { max_sum = current_sum; } } return max_sum; } ","date":"2015-01-02T00:00:00Z","permalink":"/post/leetcode_maximum_subarray/","title":"leetcode题目之Maximum Subarray"},{"content":"最近在看《Linux/Unix系统编程手册》一书，这里对书中提到的函数类型进行总结。\n可重入 POSIX标准中的解释如下：\nReentrant Function: A function whose effect, when called by two or more threads,is guaranteed to be as if the threads each executed thefunction one after another in an undefined order, even ifthe actual execution is interleaved.\n可重入函数跟信号相关，一种更容易理解的解释为：\n程序执行到某个函数foo()时，收到信号，于是暂停目前正在执行的函数，转到信号处理函数，而这个信号处理函数的执行过程中，又恰恰也会进入到刚刚执行的函数foo()，便发生了所谓的重入。此时如果foo()能够正确的运行，而且处理完成后，之前暂停的foo()也能够正确运行，则说明它是可重入的。\n可重入函数需要满足如下几个条件：\n不在函数内部使用静态或全局数据 不返回静态或全局数据，所有数据均有函数调用者提供 使用本地数据或通过复制全局数据来保护全局数据 不调用不可重入函数 标准的异步安全信号函数 异步信号安全的函数指当从信号处理函数调用时，可保证实现是安全的。如果某一个函数是可重入的，或者信号处理函数无法将其中断时，称该函数是异步信号安全的。\n我的理解是可重入函数和标准的异步安全信号函数基本等同，只是描述层面不同。\n线程安全 若函数可同时供多个线程安全的调用，则该函数为线程安全的函数。比较容易理解。\n线程安全与可重入之间的关系 可重入函数一定为线程安全的函数。线程安全函数不一定是可重入函数。\n不可重入函数，函数调用结果不具有可再现性，可通过互斥锁等机制供多个线程安全的调用，这样该不可重入函数即为线程安全的函数。\nmalloc函数内部维护了全局数据结构，因此为不可重入的，但是内部通过递归互斥量来确保为线程安全的函数。并且该互斥量必须是可递归的，否则当malloc函数重入的情况下，会造成死锁。在glibc中，malloc有线程安全和非线程安全两个版本，两个区别在于内部是否使用递归锁，当编译程序时使用了_pthreads选项时使用线程安全版本，否则使用非线程安全版本。\n自动重启 Linux中的某些系统调用在阻塞的过程中，如果接受到信号并转去处理信号处理函数，当从信号处理函数返回时这些阻塞的系统调用默认会返回EINTR。为了避免信号处理函数对阻塞中的系统调用的打断，可以通过设置SA_RESTART标志的sigaction()来建立信号处理函数，从而令内核代表进程自动重启系统调用，而无需处理系统调用返回的EINTR错误。\n并非所有的系统调用都支持自动重启，具体可参考《Linux/Unix系统编程手册（上册）》的21.5节。\n参考资料 《Linux/Unix系统编程手册（上册）》\n对可重性和线程安全的小结\n","date":"2014-12-22T00:00:00Z","permalink":"/post/linux_funtion_advance_feature/","title":"Linux函数高级特性"},{"content":"[TOC]\n信号机制在Linux编程中一直是一个难点，因为信号往往跟进程、线程、定时器、I/O等多个层面都有牵涉，这些情况存在错综复杂的关系，堪比娱乐圈错综复杂的男女关系，要想全面理解信号机制确实不易。\n信号种类 在Linux中可以通过如下命令来查看所有的信号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 [kuring@localhost ~]$ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP 21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+8 43) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-2 63) SIGRTMAX-1 64) SIGRTMAX 共64个信号，分为两种信号：非实时信号和实时信号。其中1-31个信号为非实时信号，32-64为实时信号。\n当一信号在阻塞状态下产生多次信号，当解除该信号的阻塞后，非实时信号仅传递一次信号，而实时信号会传递多次。\n对于非实时信号：内核会为每个信号维护一个信号掩码，并阻塞信号针对该进程的传递。如果将阻塞的信号发送给某进程，对该信号的传递将延时，直至从进程掩码中移除该信号为止。当从进程掩码中移除该信号时该信号将传递给该进程。如果信号在阻塞期间传递过多次该信号，信号解除阻塞后仅传递一次。\n对于实时信号：实时信号采用队列化处理，一个实时信号的多个实例发送给进程，信号将会传递多次。可以制定伴随数据，用于产生信号时的数据传递。不同实时信号的传递顺序是固定的，优先传递信号编号小的。\n信号阻塞 内核会为每个信号维护一个信号掩码，来阻塞内核将信号传递给该进程。如果将阻塞的信号发送给该进程，信号的传递将延后，从进程信号掩码中移除该信号后内核立刻将信号传递给该进程。如果一个信号在阻塞状态下产生多次，对于非实时信号稍后仅会传递一次，对于实时信号内核会进行排队处理，会传递多次。\n信号处理函数 要想在进程中设置信号处理函数有两种选择：signal()和sigaction()。其中signal()函数提供的接口比较简单，但是在不同的UNIX系统之间存在差异，跨平台特性不是很好,signal()函数由于是C库函数，实现往往是采用sigaction()系统调用完成。sigaction()具有很好的跨平台性，但是使用较为复杂，但是却可以在信号处理程序中完成阻塞信号的作用。\n在sigaction函数中可以指定调用信号处理函数时要阻塞的信号集，不允许这些信号中断信号处理函数的调用，直到信号处理函数调用完毕后信号才会传递。这一点通过signal函数是完不成的，利用signal函数设定的信号处理函数只能在信号处理函数开始时使用sigprocmask设置要阻塞的信号，在信号处理函数尾部利用sigprocmask还原信号，但在调用第一次调用sigprocmask函数之前和第二次调用sigprocmask函数之后的空白期内却无法防止要阻塞信号的传递。\n信号处理函数中调用的函数尽量是异步信号安全的，C库中的函数不是异步信号安全的函数。\n在信号处理函数中尽量避免访问全局变量，要访问全局变量可以使用volatile sig_atomic_t flag，volatile防止将编译器将变量优化到内存中，sig_atomic_t是一种整形数据类型，用来保证读写操作的原子性。\n系统调用的中断 当系统调用阻塞时，之前创建了处理函数的信号传递过来。在信号处理函数返回后，默认情况下，系统调用会失败，并将errno置为EINTR。\n如果调用指定了SA_RESTART标志的sigaction()函数来创建信号处理器函数，内核会在信号处理函数返回后自动重启系统调用，从而避免了信号处理函数对阻塞的系统调用产生的影响。比较不幸的是，并非所有的系统调用都支持该特性。\n信号的同步生成和异步生成 这里的同步是对信号产生方式的描述，跟具体哪个信号无关。所有的信号均可同步生成，也可异步生成。\n异步生成：引发信号产生的事件与进程的执行无关。例如，用户输入了中断字符、子进程终止等事件，这些信号的产生该进程是无法左右的。\n同步生成：当执行特定的机制指令产生硬件异常时或进程使用raise()、kill()等向自身发生信号时，信号是同步传递的。这些信号的产生时间该进程是可以左右的。\n信号传递的时机和顺序 同步产生的信号会立即传递给该进程。例如，当使用raise()函数向自身发送信号时，信号会在raise()调用前发生。\n异步产生一个信号时，且在进程并未阻塞的情况下，信号也不会立即被传递。当且仅当进程正在执行，并且由内核态到用户态的下一次切换时才会传递信号。说人话就是在以下两种情况下会传递信号：进程获得调度时和系统调用完成时。这是因为内核会在进程在内核态和用户态进行的切换的时候才会检测信号。\n非实时信号的传递顺序无法保障，实时信号的传递顺序是固定的，当多个不同的实时信号处于等待状态时，优先传递最小编号的信号。\n信号和线程 信号模型是基于进程模型而设计的，应尽量避免在多线程中使用信号模型。\n信号的发送可以针对整个进程，也可以针对特定线程。\n当进程收到一个信号后，内核会任选一个线程来接收信号，并调用信号处理函数对信号进行处理。\n每个线程可以独立设置信号掩码。\n如果信号处理程序中断了对pthread_mutex_lock()和pthread_cond_wait()的调用，该调用会自动重启。\n参考文章 《Linux/Unix系统编程手册》\n","date":"2014-12-22T00:00:00Z","permalink":"/post/linux_signal/","title":"Linux信号机制学习"},{"content":"第一遍阅读unpv3后，对书中讲述的内容有了大体的认识，但对书中的具体细节地方却是早已忘记。重新阅读unpv3，这次不希望仍然是阅后即忘，于是通过编写代码的方式对书中的例子和注意事项加深理解。\n为了能够将书中的很多细节问题理解清楚并且便于记忆，本文采用了编写书中代码并运行的方式，并将书中容易出错和意想不到的问题记在代码中。\n本文的代码实例并未完全按照书中的代码实例，本着单个文件即能编译通过并运行的原则，本文对于很多系统调用并未做防御式编程处理。针对每个版本的程序中缺点和注意事项在代码中已经进行了标注。\n鉴于高性能的epoll机制出现比较晚，晚于unp的编写时间，书中并未做介绍。\nTCP客户端程序 客户端函数执行效率情况：select非阻塞式I/O版本\u0026gt;线程化版本\u0026gt;fork版本\u0026gt;select阻塞式I/O版本\u0026gt;停等版本，停等版本的执行效率非常低，在实际生产环境中不建议使用。\n其中poll和select机制基本类似，书中并未给出poll版本。\n停-等版本 最常规的实现思路，但效率非常低，且当程序阻塞在读取要发送内容时，程序是无法收到服务端的状态变化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 /** * 停-等版本 * 该版本缺陷为当服务端发生某些事件时，客户端可能仍然阻塞于fgets调用中 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE 4096\t/* max text line length */ void str_cli(FILE *fp, int sockfd) { char sendline[MAXLINE], recvline[MAXLINE]; while (fgets(sendline, MAXLINE, fp) != NULL) { /* 当阻塞在fgets函数时将服务器进程关闭时虽然给客户端发送了FIN信号，客户端并不会知道， * 服务端关闭时第一次调用write服务器会返回RST， * 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 * 该问题需要使用I/O复用技术来解决，或者使用fork处理的方式来解决 * */ write(sockfd, sendline, strlen(sendline)); int n = read(sockfd, recvline, MAXLINE); if (n == 0) { printf(\u0026#34;str_cli: server terminated prematurely\\n\u0026#34;); exit(1); } // 向标准输出写内容，既可以使用write也可以使用fputs write(STDOUT_FILENO, recvline, n); // 使用fputs时需要注意将recvline数组有效内容的后面一位设置为\u0026#39;\\0\u0026#39; //\trecvline[n] = \u0026#39;\\0\u0026#39;; //\tfputs(recvline, stdout); } } int main(int argc, char **argv) { int sockfd; struct sockaddr_in\tservaddr; if (argc != 2) { printf(\u0026#34;usage: tcpcli \u0026lt;IPaddress\u0026gt;\\n\u0026#34;); exit(1); } // 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 // 最好的方式是忽略此信号的处理方式，并在程序下面处理该异常情况 signal(SIGPIPE, SIG_IGN); sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], \u0026amp;servaddr.sin_addr); connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)); str_cli(stdin, sockfd);\t/* do it all */ exit(0); } fork版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 /** * 阻塞式I/O的fork版本 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE 4096\t/* max text line length */ /** * 即使服务端已经退出，子进程的read方法仍然能够感知到并且退出while循环，并给父进程发送SIGTERM,父进程对该信号的默认处理方式为退出 * 优点：代码量比较少，每个进程只处理2个I/O流，从一个复制到另一个 */ void str_cli(FILE *fp, int sockfd) { char sendline[MAXLINE], recvline[MAXLINE]; pid_t pid; if ((pid = fork()) == 0) { // child process : server -\u0026gt; stdout int n; while ((n = read(sockfd, recvline, MAXLINE)) \u0026gt; 0) { recvline[n] = \u0026#39;\\0\u0026#39;; fputs(recvline, stdout); } kill(getppid(), SIGTERM); exit(0); } // parent process : stdin -\u0026gt; server while (fgets(sendline, MAXLINE, fp) != NULL) { write(sockfd, sendline, strlen(sendline)); } shutdown(sockfd, SHUT_WR); pause(); return ; } int main(int argc, char **argv) { int sockfd; struct sockaddr_in\tservaddr; if (argc != 2) { printf(\u0026#34;usage: tcpcli \u0026lt;IPaddress\u0026gt;\\n\u0026#34;); exit(1); } // 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 // 最好的方式是忽略此信号的处理方式，并在程序下面处理该异常情况 signal(SIGPIPE, SIG_IGN); sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], \u0026amp;servaddr.sin_addr); if (connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) != 0) { printf(\u0026#34;connect error...\\n\u0026#34;); exit(1); } str_cli(stdin, sockfd);\t/* do it all */ exit(0); } 阻塞式I/O的select版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 /** * 阻塞式I/O的select版本 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE 4096\t/* max text line length */ /** * 缺点：使用了阻塞式I/O，如果在向套接字调用write发送给服务器时，套接字缓冲区已满，write调用会阻塞，从而影响了后续的套接字缓冲区的读取 */ void str_cli(FILE *fp, int sockfd) { int maxfdp1; fd_set rset; char sendline[MAXLINE], recvline[MAXLINE]; int stdineof = 0; FD_ZERO(\u0026amp;rset); for (; ;) { // select FD_SET(fileno(fp), \u0026amp;rset); FD_SET(sockfd, \u0026amp;rset); maxfdp1 = (fileno(fp) \u0026gt; sockfd ? fileno(fp) : sockfd) + 1; select(maxfdp1, \u0026amp;rset, NULL, NULL, NULL); // socket if (FD_ISSET(sockfd, \u0026amp;rset)) { int n = read(sockfd, recvline, MAXLINE); if (n == 0) { if (stdineof == 1) { return ; } else { printf(\u0026#34;str_cli: server terminated prematurely\\n\u0026#34;); exit(1); } } else if (n == -1) { exit(1); } recvline[n] = \u0026#39;\\0\u0026#39;; fputs(recvline, stdout); //\twrite(STDOUT_FILENO, recvline, n); } // input if (FD_ISSET(fileno(fp), \u0026amp;rset)) { // 此处不能使用fgets函数，该函数带有缓冲区功能，select跟带有缓冲区的c函数混合使用有问题 //\tif (fgets(sendline, MAXLINE, fp) == NULL) //\t{ //\treturn ; //\t} int n = read(fileno(fp), sendline, MAXLINE); if (n == 0) { stdineof = 1; shutdown(sockfd, SHUT_WR);\t// 关闭写 FD_CLR(fileno(fp), \u0026amp;rset); continue; } else if (n == -1) { exit(1); } write(sockfd, sendline, n); } } } int main(int argc, char **argv) { int sockfd; struct sockaddr_in\tservaddr; if (argc != 2) { printf(\u0026#34;usage: tcpcli \u0026lt;IPaddress\u0026gt;\\n\u0026#34;); exit(1); } sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], \u0026amp;servaddr.sin_addr); if (connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) != 0) { printf(\u0026#34;connect error...\\n\u0026#34;); exit(1); } str_cli(stdin, sockfd);\t/* do it all */ exit(0); } 非阻塞式I/O的select版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 /** * 非阻塞式I/O的select版本 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE 4096\t/* max text line length */ #define max(a,b) ( ((a)\u0026gt;(b)) ? (a):(b) ) /** * 优点：速度是最快的，可以防止进程在做任何工作时发生阻塞 * 缺点：同时管理4个不同的I/O流，每个流都是非阻塞的，需要考虑到4个流的部分读和部分写问题。编码量是最多的，需要引入缓冲区管理机制。 */ void str_cli(FILE *fp, int sockfd) { // 将socket、标准输入和标准输出描述符设置为非阻塞方式 int val = fcntl(sockfd, F_GETFL, 0); fcntl(sockfd, F_SETFL, val | O_NONBLOCK); val = fcntl(STDIN_FILENO, F_GETFL, 0); fcntl(STDIN_FILENO, F_SETFL, val | O_NONBLOCK); val = fcntl(STDOUT_FILENO, F_GETFL, 0); fcntl(STDOUT_FILENO, F_SETFL, val | O_NONBLOCK); char to[MAXLINE], fr[MAXLINE]; char *toiptr, *tooptr, *friptr, *froptr; toiptr = tooptr = to; friptr = froptr = fr; int stdineof = 0; int maxfdp1 = max(max(STDIN_FILENO, STDOUT_FILENO), sockfd) + 1; fd_set rset, wset; for (; ;) { FD_ZERO(\u0026amp;rset); FD_ZERO(\u0026amp;wset); if (stdineof == 0 \u0026amp;\u0026amp; toiptr \u0026lt; \u0026amp;to[MAXLINE]) { FD_SET(STDIN_FILENO, \u0026amp;rset); } if (friptr \u0026lt; \u0026amp;fr[MAXLINE]) { FD_SET(sockfd, \u0026amp;rset); } if (tooptr != toiptr) { FD_SET(sockfd, \u0026amp;wset); } if (froptr != friptr) { FD_SET(STDOUT_FILENO, \u0026amp;wset); } select(maxfdp1, \u0026amp;rset, \u0026amp;wset, NULL, NULL);\t// select函数仍然是阻塞的 // 标准输入 if (FD_ISSET(STDIN_FILENO, \u0026amp;rset)) { int n; if ((n = read(STDIN_FILENO, toiptr, \u0026amp;to[MAXLINE] - toiptr)) \u0026lt; 0) { // 对于非阻塞式IO，如果操作不能满足，相应系统调用会返回EWOULDBLOCK错误 if (errno != EWOULDBLOCK) { printf(\u0026#34;read error on stdin\\n\u0026#34;); exit(1); } } else if (n == 0) { fprintf(stderr, \u0026#34;EOF on stdin\\n\u0026#34;); stdineof = 1; if (tooptr == toiptr) { shutdown(sockfd, SHUT_WR);\t// 缓冲区中没有数据要发送，关闭socket } } else { fprintf(stderr, \u0026#34;read %d bytes from stdin\\n\u0026#34;, n); toiptr += n; FD_SET(sockfd, \u0026amp;wset); } } // 从套接字读 if (FD_ISSET(sockfd, \u0026amp;rset)) { int n; if ((n = read(sockfd, friptr, \u0026amp;fr[MAXLINE] - friptr)) \u0026lt; 0) { if (errno != EWOULDBLOCK) { printf(\u0026#34;read error on socket\\n\u0026#34;); exit(1); } } else if (n == 0) { fprintf(stderr, \u0026#34;EOF on socket\\n\u0026#34;); if (stdineof) { return ; } else { printf(\u0026#34;server terminated prematurely\\n\u0026#34;); exit(1); } } else { fprintf(stderr, \u0026#34;read %d bytes from socket\\n\u0026#34;, n); friptr += n; FD_SET(STDOUT_FILENO, \u0026amp;wset); } } // 标准输出 int n; if (FD_ISSET(STDOUT_FILENO, \u0026amp;wset) \u0026amp;\u0026amp; ((n = friptr - froptr) \u0026gt; 0)) { int nwritten; if ((nwritten = write(STDOUT_FILENO, froptr, n)) \u0026lt; 0) { if (errno != EWOULDBLOCK) { printf(\u0026#34;write error to stdout\\n\u0026#34;); exit(1); } } else { fprintf(stderr, \u0026#34;wrote %d bytes to stdout\\n\u0026#34;, nwritten); froptr += nwritten; if (froptr == friptr) { froptr = friptr = fr; } } } // 向socket写 if (FD_ISSET(sockfd, \u0026amp;wset) \u0026amp;\u0026amp; ((n = toiptr - tooptr)) \u0026gt; 0) { int nwritten; if ((nwritten = write(sockfd, tooptr, n)) \u0026lt; 0) { if (errno != EWOULDBLOCK) { printf(\u0026#34;write error to socket\\n\u0026#34;); exit(1); } } else { fprintf(stderr, \u0026#34;wrote %d bytes to socket\\n\u0026#34;, nwritten); tooptr += nwritten; if (tooptr == toiptr) { toiptr = tooptr = to; if (stdineof) { shutdown(sockfd, SHUT_WR); } } } } } return ; } /** * connect的非阻塞版本 * 连接建立成功时，描述符变为可写；连接建立错误时，描述符变为即可读又可写 * 优点： * 1、阻塞式的connect调用会消耗CPU时间，非阻塞式connect可以充分利用CPU时间，在等待的过程中可以处理其他工作 * 2、可以同时建立多个连接，浏览器中会用到此技术 * 3、阻塞式connect的函数超时过长，可以通过该函数设置超时时间 * 4、阻塞式的套接字调用connect时，在TCP的三次握手完成之前被某些信号中断时并且connect未设置内核自动重启的标志时，connect将返回EINTR错误 * 当再次调用connect等待未完成的连接时将会返回EADDRINUSE错误 */ int connect_nonb(int sockfd, const struct sockaddr *saptr, socklen_t salen, int nsec) { // 将套接字设置为非阻塞状态 int flags = fcntl(sockfd, F_GETFL, 0); fcntl(sockfd, F_SETFL, flags | O_NONBLOCK); int error = 0; int n; if ((n = connect(sockfd, saptr, salen)) \u0026lt; 0) { // 连接未成功建立，正常情况下返回EINPROGRESS错误，表示操作正在处理 if (errno != EINPROGRESS) { // EINPROGRESS表示连接建立已经启动，但是尚未完成 return -1; } } else if (n == 0) { // 当服务器和客户端在一台主机上时会立即建立连接 goto done; } // 当代码执行到如下过程中时，connect正在建立连接，可以在此位置执行业务相关代码 // 当然真正使用时，在此位置加入其他代码并不合适，需要根据具体情况重新调整代码 // 可以参照书中的web客户程序例子 fd_set rset, wset; FD_ZERO(\u0026amp;rset); FD_SET(sockfd, \u0026amp;rset); wset = rset; struct timeval tval; tval.tv_sec = nsec; tval.tv_usec = 0; if ((n = select(sockfd + 1, \u0026amp;rset, \u0026amp;wset, NULL, nsec ? \u0026amp;tval : NULL)) == 0) { // 发生超时 close(sockfd); errno = ETIMEDOUT; return -1; } // 当连接建立成功时sockfd变为可写，当连接建立失败时sockfd变为即可读又可写 if (FD_ISSET(sockfd, \u0026amp;rset) || FD_ISSET(sockfd, \u0026amp;wset)) { int len = sizeof(error); // 非可移植性函数，连接建立成功返回0，连接建立失败将错误值返回给error // 连接建立失败时，有返回-1和返回0的情况 if (getsockopt(sockfd, SOL_SOCKET, SO_ERROR, \u0026amp;error, \u0026amp;len) \u0026lt; 0) { // solaris连接建立失败返回-1 return -1; } } else { printf(\u0026#34;select error:sockfd not set\u0026#34;); exit(1); } done: // 恢复套接字的文件状态标志 fcntl(sockfd, F_SETFL, flags); if (error) { close(sockfd); errno = error; return -1; } return 0; } int main(int argc, char **argv) { int sockfd; struct sockaddr_in\tservaddr; if (argc != 2) { printf(\u0026#34;usage: tcpcli \u0026lt;IPaddress\u0026gt;\\n\u0026#34;); exit(1); } // 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 // 最好的方式是忽略此信号的处理方式，并在程序下面处理该异常情况 signal(SIGPIPE, SIG_IGN); sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], \u0026amp;servaddr.sin_addr); //connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)); if (connect_nonb(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr), 50) \u0026lt; 0) { printf(\u0026#34;socket connect error\\n\u0026#34;); exit(1); } str_cli(stdin, sockfd);\t/* do it all */ exit(0); } TCP服务端程序 服务器程序要处理大量并发，在设计时更要注重效率。\nfork版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 /** * fork版本 * PPC(Process per Connection)模型 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;strings.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE\t4096\t/* max text line length */ void sig_chld(int signo) { if (signo != SIGIO) { return; } int stat; /* 此处不可以使用wait函数，当多个SIGCHLD信号同时发出时会因为信号覆盖而出现僵尸进程的情况 pid_t pid = wait(\u0026amp;stat); printf(\u0026#34;child %d terminated\\n\u0026#34;, pid);\t// 非异步信号安全函数，此处不应该调用 */ /* 使用非阻塞的参数WNOHANG来循环处理信号，避免信号丢失问题 */ pid_t pid; while ((pid = waitpid(-1, \u0026amp;stat, WNOHANG)) \u0026gt; 0) { printf(\u0026#34;child %d terminated\\n\u0026#34;, pid); } } void str_echo(int sockfd) { ssize_t\tn; char buf[MAXLINE]; again: while ( (n = read(sockfd, buf, MAXLINE)) \u0026gt; 0) { write(sockfd, buf, n); } if (n \u0026lt; 0 \u0026amp;\u0026amp; errno == EINTR) goto again; else if (n \u0026lt; 0) printf(\u0026#34;str_echo: read error\\n\u0026#34;); } /** * fork版本 * 缺点： * 1.fork需要将父进程的内存映像复制到子进程，并在子进程中复制所有的描述符，尽管现在的操作系统已经都实现了写时复制技术，但是耗时仍然比较多 * 2.父进程和子进程之间需要IPC机制进行通信，从子进程返回信息到父进程比较麻烦 */ int main(int argc, char *argv[]) { signal(SIGCHLD, sig_chld); int listenfd, connfd; pid_t childpid; struct sockaddr_in cliaddr, servaddr; // socket listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) { printf(\u0026#34;socket error\\n\u0026#34;); exit(1); } // bind bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) == -1) { printf(\u0026#34;bind error\\n\u0026#34;); exit(1); } // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) { printf(\u0026#34;listen error\\n\u0026#34;); exit(1); } for ( ; ; ) { socklen_t clilen = sizeof(cliaddr); // 处理accept被信号中断时返回EINTR错误 if ((connfd = accept(listenfd, (struct sockaddr*)\u0026amp;cliaddr, \u0026amp;clilen)) \u0026lt; 0) { if (errno == EINTR) { continue; } else { printf(\u0026#34;accept error\u0026#34;); exit(1); } } if ((childpid = fork()) == 0) { /* child process */ close(listenfd); /* close listening socket */ str_echo(connfd); /* process the request */ exit(0); } close(connfd); /* parent closes connected socket */ } } select版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 /** * select版本 * 缺点： * 1. 有最大并发数限制，一个进程最多打开FD_SETSIZE个文件描述符，FD_SETSIZE往往是1024或2048字节 * 2. select每次调用都会线性扫描全部的FD集合，这样效率就会呈现线性下降，把FD_SETSIZE改大的后果就是所有FD处理都慢慢来 * 3. 内核/用户空间内存拷贝问题，内核把FD消息通知给用户空间采取了内存拷贝方法 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;sys/select.h\u0026gt; #include \u0026lt;strings.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE\t4096\t/* max text line length */ /** * 使用select的需要维护client数组和allset的描述符集 */ int main(int argc, char *argv[]) { struct sockaddr_in cliaddr, servaddr; // socket int listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) { printf(\u0026#34;socket error\\n\u0026#34;); exit(1); } printf(\u0026#34;finish socket...\\n\u0026#34;); // bind bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) == -1) { printf(\u0026#34;bind error\\n\u0026#34;); exit(1); } printf(\u0026#34;finish bind...\\n\u0026#34;); // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) { printf(\u0026#34;listen error\\n\u0026#34;); exit(1); } printf(\u0026#34;finish listening...\\n\u0026#34;); int maxfd = listenfd; int maxi = -1; int client[FD_SETSIZE]; for (int i=0; i\u0026lt;FD_SETSIZE; i++) { client[i] = -1; } fd_set allset; FD_ZERO(\u0026amp;allset); FD_SET(listenfd, \u0026amp;allset); for ( ; ; ) { fd_set rset = allset; int nready = select(maxfd + 1, \u0026amp;rset, NULL, NULL, NULL); if (FD_ISSET(listenfd, \u0026amp;rset)) { // 设置client数组 socklen_t clilen = sizeof(cliaddr); // 调用select时有个问题，见书中16.6节 // 如果调用accept时客户端已经关闭连接，此时accept会阻塞并直到新的客户端连接到来 // 为了解决该问题可以将套接字设置为非阻塞再调用accept int connfd = accept(listenfd, (struct sockaddr*)\u0026amp;cliaddr, \u0026amp;clilen); printf(\u0026#34;accept one client:%d...\\n\u0026#34;, connfd); int i = 0; for (; i\u0026lt;FD_SETSIZE; i++) { if (client[i] \u0026lt; 0) { client[i] = connfd; break; } } FD_SET(connfd, \u0026amp;allset); if (i == FD_SETSIZE) { printf(\u0026#34;too many clients\u0026#34;); exit(-1); } if (connfd \u0026gt; maxfd) { maxfd = connfd; } if (i \u0026gt; maxi) { maxi = i; } if (--nready \u0026lt;= 0) { continue; } } // 检测所有客户端的数据 for (int i=0; i\u0026lt;=maxi; i++) { if (client[i] \u0026lt; 0) { continue; } if (FD_ISSET(client[i], \u0026amp;rset)) { int n; char buf[MAXLINE]; printf(\u0026#34;start reading form one client...\\n\u0026#34;); if ((n = read(client[i], buf, MAXLINE)) == 0) { close(client[i]); FD_CLR(client[i], \u0026amp;allset); client[i] = -1; } else { write(client[i], buf, n); } if (--nready \u0026lt;= 0) { break; } } } } } poll版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 /** * poll版本 * poll版本的解决了select文件描述符限制问题，但是仍然具备select的缺点中的2和3 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;strings.h\u0026gt; #include \u0026lt;poll.h\u0026gt; #include \u0026lt;stropts.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE\t4096\t/* max text line length */ #define OPEN_MAX 1024 // 该宏已经从limit.h中移除，用来表示一个进程可以打开的最大描述符数目 /** * 使用select的缺点为需要维护client数组 */ int main(int argc, char *argv[]) { struct sockaddr_in cliaddr, servaddr; // socket int listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) { printf(\u0026#34;socket error\\n\u0026#34;); exit(1); } printf(\u0026#34;finish socket...\\n\u0026#34;); // bind bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) == -1) { printf(\u0026#34;bind error\\n\u0026#34;); exit(1); } printf(\u0026#34;finish bind...\\n\u0026#34;); // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) { printf(\u0026#34;listen error\\n\u0026#34;); exit(1); } printf(\u0026#34;finish listening...\\n\u0026#34;); struct pollfd client[OPEN_MAX]; client[0].fd = listenfd; client[0].events = POLLIN; for (int i=1; i\u0026lt;OPEN_MAX; i++) { client[i].fd = -1; } int maxi = 0;\t// 当前client正在使用的最大下标 for ( ; ; ) { int nready = poll(client, maxi + 1, -1); if (client[0].revents \u0026amp; POLLIN) { // 设置client数组 socklen_t clilen = sizeof(cliaddr); int connfd = accept(listenfd, (struct sockaddr*)\u0026amp;cliaddr, \u0026amp;clilen); printf(\u0026#34;accept one client:%d...\\n\u0026#34;, connfd); int i = 1; for (; i\u0026lt;OPEN_MAX; i++) { if (client[i].fd \u0026lt; 0) { client[i].fd = connfd; break; } } if (i == OPEN_MAX) { printf(\u0026#34;too many clients\u0026#34;); exit(-1); } client[i].events = POLLIN; if (i \u0026gt; maxi) { maxi = i; } if (--nready \u0026lt;= 0) { continue; } } // 检测所有客户端的数据 for (int i=0; i\u0026lt;=maxi; i++) { if (client[i].fd \u0026lt; 0) { continue; } if (client[i].revents \u0026amp; (POLLIN | POLLERR)) { int n; char buf[MAXLINE]; printf(\u0026#34;start reading form one client...\\n\u0026#34;); if ((n = read(client[i].fd, buf, MAXLINE)) \u0026lt; 0) { if (errno == ECONNRESET) { close(client[i].fd); client[i].fd = -1; } else { printf(\u0026#34;read client error\\n\u0026#34;); exit(-1); } } else if (n == 0) { printf(\u0026#34;client %d close\\n\u0026#34;, client[i].fd); close(client[i].fd); client[i].fd = -1; } else { write(client[i].fd, buf, n); } if (--nready \u0026lt;= 0) { break; } } } } } 多线程版本 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 /** * 多线程版本 * TPC(Thread Per Connection)模型 * 线程的开销虽然比进程小，但是仍然有比较大开销，因此并发数不是很高 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;strings.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE\t4096\t/* max text line length */ void str_echo(int sockfd) { ssize_t\tn; char buf[MAXLINE]; again: while ( (n = read(sockfd, buf, MAXLINE)) \u0026gt; 0) { write(sockfd, buf, n); } if (n \u0026lt; 0 \u0026amp;\u0026amp; errno == EINTR) goto again; else if (n \u0026lt; 0) printf(\u0026#34;str_echo: read error\\n\u0026#34;); } static void *doit(void *arg) { pthread_detach(pthread_self()); str_echo((int)arg); close((int)arg); printf(\u0026#34;close socket...\\n\u0026#34;); return NULL; } int main(int argc, char *argv[]) { int listenfd, connfd; struct sockaddr_in cliaddr, servaddr; // socket listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) { printf(\u0026#34;socket error\\n\u0026#34;); exit(1); } // bind bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) == -1) { printf(\u0026#34;bind error\\n\u0026#34;); exit(1); } // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) { printf(\u0026#34;listen error\\n\u0026#34;); exit(1); } for ( ; ; ) { socklen_t clilen = sizeof(cliaddr); // 处理accept被信号中断时返回EINTR错误 if ((connfd = accept(listenfd, (struct sockaddr*)\u0026amp;cliaddr, \u0026amp;clilen)) \u0026lt; 0) { if (errno == EINTR) { continue; } else { printf(\u0026#34;accept error\u0026#34;); exit(1); } } printf(\u0026#34;receive new client...\\n\u0026#34;); pthread_t tid; pthread_create(\u0026amp;tid, NULL, \u0026amp;doit, (void *)connfd); } } UDP 由于udp比较简单，书中并未将udp协议当做重点来讲解。\nUDP客户端程序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;strings.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE\t4096\t/* max text line length */ // sendto、recvfrom方式 void dg_cli(FILE *fp, int sockfd, const struct sockaddr *pservaddr, socklen_t servlen) { char sendline[MAXLINE], recvline[MAXLINE + 1]; struct sockaddr *preply_addr = (struct sockaddr *)malloc(servlen); while (fgets(sendline, MAXLINE, fp) != NULL) { sendto(sockfd, sendline, strlen(sendline), 0, pservaddr, servlen); int len = servlen; int n = recvfrom(sockfd, recvline, MAXLINE, 0, preply_addr, \u0026amp;len); // 为了防止接收到其他进程的数据，通过条件判断去除 if (len != servlen || memcmp(pservaddr, preply_addr, len) != 0) { printf(\u0026#34;reply from others (!ignore)\\n\u0026#34;); continue; } recvline[n] = 0; fputs(recvline, stdout); } } // connect、write、read方式 void dg_cli2(FILE *fp, int sockfd, const struct sockaddr *pservaddr, socklen_t servlen) { char sendline[MAXLINE], recvline[MAXLINE + 1]; connect(sockfd, (struct sockaddr *)pservaddr, servlen); while (fgets(sendline, MAXLINE, fp) != NULL) { write(sockfd, sendline, strlen(sendline)); int n = read(sockfd, recvline, MAXLINE); recvline[n] = 0; fputs(recvline, stdout); } } int main(int argc, char *argv[]) { if (argc != 2) { printf(\u0026#34;usage: tcpcli \u0026lt;IPaddress\u0026gt;\\n\u0026#34;); exit(1); } struct sockaddr_in servaddr; bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], \u0026amp;servaddr.sin_addr); int sockfd = socket(AF_INET, SOCK_DGRAM, 0); dg_cli2(stdin, sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)); exit(0); } UDP服务端程序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;strings.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE\t4096\t/* max text line length */ void dg_echo(int sockfd, struct sockaddr *pcliaddr, socklen_t clilen) { char mesg[MAXLINE]; for (;;) { socklen_t len = clilen; int n; bzero(mesg, MAXLINE); if ((n = recvfrom(sockfd, mesg, MAXLINE, 0, pcliaddr, \u0026amp;len)) \u0026lt; 0) { close(sockfd); printf(\u0026#34;recvfrom error, error=%m\\n\u0026#34;); exit(1); } printf(\u0026#34;recv %s\\n\u0026#34;, mesg); sendto(sockfd, mesg, n, 0, pcliaddr, len); } } int main(int argc, char *argv[]) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); struct sockaddr_in servaddr, cliaddr; bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) \u0026lt; 0) { printf(\u0026#34;bind error\\n\u0026#34;); exit(1); } dg_echo(sockfd, (struct sockaddr*)\u0026amp;cliaddr, sizeof(cliaddr)); } UDP服务端信号驱动式I/O版本 信号驱动式I/O：进程执行I/O系统调用告知内核启动某个I/O操作，内核启动I/O操作后立即返回到进程。进程在I/O操作发生期间继续执行。当操作完成或遇到错误时，内核以进程在I/O系统调用中指定的方式通知进程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 /** * 信号驱动式I/O在TCP套接字用途不大，该信号产生的过于频繁，它的出现并未指示发生的事情 */ #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;errno.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;netinet/in.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; #include \u0026lt;strings.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #define SERV_PORT 9877 #define MAXLINE\t4096\t/* max text line length */ static int sockfd; #define MAXDG 4096 typedef struct { void *dg_data;\t// 实际数据 size_t dg_len;\t// 实际数据长度 struct sockaddr *dg_sa;\t// 包含客户端地址 socklen_t dg_salen;\t// 客户端地址长度 } DG; #define QSIZE 8 static DG dg[QSIZE];\t// 存放数据的环形缓冲区 static long cntread[QSIZE + 1]; // 需要处理的下一个数据元素的下标 static int iget; // 存放数据元素的下一个位置 static int iput; static int nqueue;\t// 队列中的数据个数 static socklen_t clilen; static void sig_hup(int signo) { int i=0; for (; i \u0026lt;= QSIZE; i++) { printf(\u0026#34;cntread[%d = %ld\\n\u0026#34;, i, cntread[i]); } } static void sig_io(int signo) { int nread; // 为了解决非实时信号不排队问题，采用循环读取方式 for (nread = 0; ; ) { // 检查队列是否已满 if (nread \u0026gt;= QSIZE) { printf(\u0026#34;receive overflow\\n\u0026#34;); exit(1); } DG *ptr = \u0026amp;dg[iput]; ptr-\u0026gt;dg_salen = clilen; ssize_t len = recvfrom(sockfd, ptr-\u0026gt;dg_data, MAXDG, 0, ptr-\u0026gt;dg_sa, \u0026amp;ptr-\u0026gt;dg_salen); if (len \u0026lt; 0) { if (errno == EWOULDBLOCK) { break; } else { printf(\u0026#34;recvfrom error\\n\u0026#34;); exit(1); } } ptr-\u0026gt;dg_len = len; nread++; nqueue++; if (++iput \u0026gt;= QSIZE) { iput = 0; } } cntread[nread]++; } void dg_echo(int sockfd_arg, struct sockaddr *pcliaddr, socklen_t clilen_arg) { sockfd = sockfd_arg; clilen = clilen_arg; int i = 0; for (; i\u0026lt;QSIZE; i++) { dg[i].dg_data = malloc(MAXDG); dg[i].dg_sa = (struct sockaddr *)malloc(clilen); dg[i].dg_salen = clilen; } iget = iput = nqueue = 0; signal(SIGHUP, sig_hup); // 在启动信号I/O前设置信号处理函数 signal(SIGIO, sig_io); // 设置接收信号通知的进程，让本进程接收SIGIO信号 fcntl(sockfd, F_SETOWN, getpid()); // 为了能够在得到I/O事件后重复执行I/O操作，需要将文件描述符设置为非阻塞方式 // O_ASYNC表示在文件描述符上使用信号驱动I/O int flags = fcntl(sockfd, F_GETFL); fcntl(sockfd, F_SETFL, flags | O_ASYNC | O_NONBLOCK); sigset_t zeromask, newmask, oldmask; sigemptyset(\u0026amp;zeromask); sigemptyset(\u0026amp;newmask); sigemptyset(\u0026amp;oldmask); // 设置新的信号掩码，阻塞SIGIO信号 sigaddset(\u0026amp;newmask, SIGIO); sigprocmask(SIG_BLOCK, \u0026amp;newmask, \u0026amp;oldmask); for (; ;) { while (nqueue == 0) { // 挂起进程直到收到任何信号，该函数返回后SIGIO继续被阻塞 sigsuspend(\u0026amp;zeromask); } // 解除SIGIO的阻塞 sigprocmask(SIG_SETMASK, \u0026amp;oldmask, NULL); sendto(sockfd, dg[iget].dg_data, dg[iget].dg_len, 0, dg[iget].dg_sa, dg[iget].dg_salen); if (++iget \u0026gt;= QSIZE) { iget = 0; } // 为了能够修改nqueue的值，阻塞SIGIO信号 sigprocmask(SIG_BLOCK, \u0026amp;newmask, \u0026amp;oldmask); nqueue--; } } int main(int argc, char *argv[]) { int sockfd = socket(AF_INET, SOCK_DGRAM, 0); struct sockaddr_in servaddr, cliaddr; bzero(\u0026amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) \u0026lt; 0) { printf(\u0026#34;bind error\\n\u0026#34;); exit(1); } dg_echo(sockfd, (struct sockaddr*)\u0026amp;cliaddr, sizeof(cliaddr)); return 1; } 相关下载 本文中的实例，代码采用eclipse CDT编写，可以直接导入eclipse中运行。\n下载实例\n","date":"2014-12-22T00:00:00Z","permalink":"/post/linux_unp/","title":"UNIX网络编程读书笔记"},{"content":"最近这段时间回顾了下python，距离上次使用python已经超过两年的时间了。\n相对于c++语言，python要灵活许多，对于工作中的一些小问题的解决可以通过python来实现比较高效和方便，比如网页的抓取和解析。甚至对于非IT的工作，也可以通过脚本的方式来解决，只要是工作中遇到反复处理的体力活劳动就可以考虑利用编程方式来解决。\n本文以我的博客的文档列表页面为例，利用python对页面中的文章名进行提取。\n文章列表页中的文章列表部分的url如下：\n1 2 3 4 5 6 7 \u0026lt;ul class=\u0026#34;listing\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;listing-item\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;date\u0026#34;\u0026gt;2014-12-03\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026#34;/post/linux_funtion_advance_feature\u0026#34; title=\u0026#34;Linux函数高级特性\u0026#34; \u0026gt;Linux函数高级特性\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;listing-item\u0026#34;\u0026gt;\u0026lt;span class=\u0026#34;date\u0026#34;\u0026gt;2014-12-02\u0026lt;/span\u0026gt;\u0026lt;a href=\u0026#34;/post/cgdb\u0026#34; title=\u0026#34;cgdb的使用\u0026#34; \u0026gt;cgdb的使用\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; ... \u0026lt;/ul\u0026gt; requests模块的安装 requests模块用于加载要请求的web页面。\n在python的命令行中输入import requests，报错说明requests模块没有安装。\n我这里打算采用easy_install的在线安装方式安装，发现系统中并不存在easy_install命令，输入sudo apt-get install python-setuptools来安装easy_install工具。\n执行sudo easy_install requests安装requests模块。\nBeautiful Soup安装 为了能够对页面中的内容进行解析，本文使用Beautiful Soup。当然，本文的例子需求较简单，完全可以使用分析字符串的方式。\n执行sudo easy_install beautifulsoup4即可安装。\n编码问题 python的编码问题确实是一个很头大的问题，尤其是对于不熟悉python的菜鸟。\npython自身的编码问题就已经够头大的了，碰巧requests模块也有一个编码问题的bug，具体的bug见参考文章。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/usr/bin/env python # -*- coding: utf-8 -*- \u0026#39; a http parse test programe \u0026#39; __author__ = \u0026#39;kuring lv\u0026#39; import requests import bs4 # 使用相对路径，可以适配不同的部署域名（GitHub Pages 或 Cloudflare Pages） archives_url = \u0026#34;/archive\u0026#34; # 如果需要使用绝对 URL，可以根据实际部署域名修改，例如： # archives_url = \u0026#34;https://kuring.github.io/archive\u0026#34; # GitHub Pages # archives_url = \u0026#34;https://kuring.pages.dev/archive\u0026#34; # Cloudflare Pages def start_parse(url) : print \u0026#34;开始获取(%s)内容\u0026#34; % url response = requests.get(url) print \u0026#34;获取网页内容完毕\u0026#34; soup = bs4.BeautifulSoup(response.content.decode(\u0026#34;utf-8\u0026#34;)) #soup = bs4.BeautifulSoup(response.text); # 为了防止漏掉调用close方法，这里使用了with语句 # 写入到文件中的编码为utf-8 with open(\u0026#39;archives.txt\u0026#39;, \u0026#39;w\u0026#39;) as f : for archive in soup.select(\u0026#34;li.listing-item a\u0026#34;) : f.write(archive.get_text().encode(\u0026#39;utf-8\u0026#39;) + \u0026#34;\\n\u0026#34;) print archive.get_text().encode(\u0026#39;utf-8\u0026#39;) # 当命令行运行该模块时，__name__等于\u0026#39;__main__\u0026#39; # 其他模块导入该模块时，__name__等于\u0026#39;parse_html\u0026#39; if __name__ == \u0026#39;__main__\u0026#39; : start_parse(archives_url) 参考文章 廖雪峰的python教程\nBeautiful Soup 4.2.0 文档\n使用 Python 轻松抓取网页\nPython+Requests抓取中文乱码改进方案\n","date":"2014-12-14T00:00:00Z","permalink":"/post/python_parse_web/","title":"通过python来抓取和解析网页内容"},{"content":"问题描述 一个最多包含n个正整数的文件，每个数小于n，其中n为10000000。要求升序排列整数列表，最多使用1MB的内存，运行时间尽可能短。\n代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #define BITSPERWORD 32 #define SHIFT 5 #define MASK 0x1F #define N 10000000 int a[1 + N/BITSPERWORD] = {0}; /** * i\u0026gt;\u0026gt;SHIFT相当于i/32，用于确定i在第几个int数组中 * 其中i\u0026amp;MASK含义为i%32，用于确定在int中的第几位 */ void set(int i) { a[i \u0026gt;\u0026gt; SHIFT] |= (1 \u0026lt;\u0026lt; (i \u0026amp; MASK)); } void clr(int i) { a[i \u0026gt;\u0026gt; SHIFT] \u0026amp;= (0 \u0026lt;\u0026lt; (i \u0026amp; MASK)); } int test(int i) { return a[i \u0026gt;\u0026gt; SHIFT] \u0026amp; (1 \u0026lt;\u0026lt; (i \u0026amp; MASK)); } int main(void) { int i; while (scanf(\u0026#34;%d\u0026#34;, \u0026amp;i) != EOF) { set(i); } for (i=0; i\u0026lt;N; i++) { if (test(i)) { printf(\u0026#34;%d\\n\u0026#34;, i); } } return 0; } 习题5 通过shell命令echo \u0026quot;scale=2; 10000000 / 1024 / 8 / 1024.0\u0026quot; | bc计算该程序运行时至少需要的存储空间为1.19MB，如果仅提供了1MB的存储空间，则需要更改上述程序的处理方式。\n可采用多趟算法，多趟读入输入数据，每次完成一步。针对该题，可采用2步来完成，int数组的大小变更为5000000/8，比之前小了一半。第一步处理0-4999999之间的数据，第二步处理5000000-999999之间的数据。\n习题6 如果是每个整数至少出现10次，而不是原先的一次。可以使用4bit来统计出现的次数，申请的数组大小变为了10000000/2。只要是每个整数有出现的最多次数上限该种处理方式就合适，当然整数出现的上限不能太大，否则该算法就没有了任何优势。\n习题9 对一个大的数组的初始化操作需要耗费一些时间，为了消除数组的初始化，可以通过两个额外的数组来解决，这是典型的用空间换时间的方法。\n+---+---+---+---+---+---+---+----+ data | | | 3 | | 2 | | 8 | | +---+---+---+---+---+---+---+----+ +---+---+---+---+---+---+---+----+ from | | | 0 | | 2 | | 1 | | +---+---+---+---+---+---+---+----+ +---+---+---+---+---+---+---+----+ to | 1 | 5 | 3 | | | | | | +---+---+---+---+---+---+---+----+ ^ + top 上图中data为要初始化的数组，from和to为辅助数组。如果data[i]已经初始化，则from[i]\u0026lt;top，to[from[i]]=i。from是一个简单的标识，to和top确保了from中不会写入内存中的随机内容。\n习题11 该题的答案太他妈逗了，为了能够解决两地之间的数据传输瓶颈，作者给出的答案居然是用信鸽传输图片的底片后再将底片放大的方式来代替原先的用汽车运输的方式，这就是中国古代的飞鸽传书啊。\n习题12 该题在《三傻大闹宝莱坞》中见过，这跟编程毛线关系也没有啊。\n","date":"2014-11-23T00:00:00Z","permalink":"/post/programming_pearls_1/","title":"编程珠玑读书笔记第1章开篇"},{"content":"最近学习了《GNU Make项目管理》，改进了我之前一直在用的Makefile文件，解决我之前的Makefile中一直存在的修改依赖头文件后不能自动编译cpp文件的问题。本文列举了我常用的两个Makefile文件，其中第一个为我常用的Makefile，第二个为从网上找到的其他Makefile文件。\n第一个Makefile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 all: INCLUDE = -I./ FLAGS = -g -Wall $(INCLUDE) FLAGS += -fPIC LIBDIR = -lz -lm -lcrypto LINK = $(LIBDIR) -lpthread GCC = g++ # for C++ language CODE.cpp = main.cpp \\ trim.cpp CPP.o = $(CODE.cpp:.cpp=.o) OBJS.d = $(CODE.cpp:.cpp=.d) OBJS.o = $(CPP.o) # 解决头文件依赖 -include $(subst .cpp,.d,$(CODE.cpp)) %.d: %.cpp $(GCC) -M $(FLAGS) $\u0026lt; \u0026gt; $@.$$$$;\t\\ sed \u0026#39;s,\\($*\\)\\.o[ :]*,\\1.o $@ : ,g\u0026#39; \u0026lt; $@.$$$$ \u0026gt; $@;\t\\ rm -f $@.$$$$ # rule for C++ language %.o : %.cpp\t$(GCC) $(FLAGS) -o $@ -c $\u0026lt;\t@echo $*.o build successfully!...... TARGET = main $(TARGET) : $(OBJS.o) $(GCC) $(OBJS.o) -o $(TARGET) $(LINK) @echo $(TARGET) BUILD OK!......... all : $(TARGET) .PHONY: clean: rm -rf $(TARGET) rm -rf $(OBJS.o) rm -rf $(OBJS.d) rm -rf *.d 该文件特点为需要手工将需要编译的源文件手动添加到Makefile中，可能比较麻烦，但是编译时比较灵活。可以随意修改需要编译源文件的顺序和是否需要编译源文件。\n第二个Makefile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 ########################################################### # # KEFILE FOR C/C++ PROJECT # Author: swm8023 \u0026lt;swm8023@gmail.com\u0026gt; # Date: 2014/01/30 # ########################################################### .PHONY: all clean all: # annotation when release version DEBUG := y TARGET_PROG := main # project directory\tDEBUG_DIR := ./Debug RELEASE_DIR := ./Release BIN_DIR := $(if $(DEBUG), $(DEBUG_DIR), $(RELEASE_DIR)) # shell command CC := gcc CXX := g++ RM := rm -rf MKDIR := mkdir -p SED := sed MV := mv # init sources \u0026amp; objects \u0026amp; depends sources_all := $(shell find . -name \u0026#34;*.c\u0026#34; -o -name \u0026#34;*.cpp\u0026#34; -o -name \u0026#34;*.h\u0026#34;) sources_c := $(filter %.c, $(sources_all)) sources_cpp := $(filter %.cpp, $(sources_all)) sources_h := $(filter %.h, $(sources_all)) objs := $(addprefix $(BIN_DIR)/,$(strip $(sources_cpp:.cpp=.o) $(sources_c:.c=.o))) deps := $(addprefix $(BIN_DIR)/,$(strip $(sources_cpp:.cpp=.d) $(sources_c:.c=.d))) # create directory $(foreach dirname,$(sort $(dir $(sources_c) $(sources_cpp))),\\ $(shell $(MKDIR) $(BIN_DIR)/$(dirname))) # complie \u0026amp; link variable CFLAGS := $(if $(DEBUG),-g -O, -O2) CFLAGS += $(addprefix -I ,$(sort $(dir $(sources_h)))) CXXFLAGS = $(CFLAGS) LDFLAGS := LOADLIBES += #-L/usr/include/mysql LDLIBS += #-lpthread -lmysqlclient # add vpath vpath %.h $(sort $(dir $(sources_h))) vpath %.c $(sort $(dir $(sources_c))) vpath %.cpp $(sort $(dir $(sources_cpp))) # generate depend files # actually generate after object generated, beacasue it only used when next make) ifneq \u0026#34;$(MAKECMDGOALS)\u0026#34; \u0026#34;clean\u0026#34; sinclude $(deps) endif # make-depend(depend-file,source-file,object-file,cc) define make-depend $(RM) $1; \\ $4 $(CFLAGS) -MM $2 | \\ $(SED) \u0026#39;s,\\($(notdir $3)\\): ,$3: ,\u0026#39; \u0026gt; $1.tmp; \\ $(SED) -e \u0026#39;s/#.*//\u0026#39; \\ -e \u0026#39;s/^[^:]*: *//\u0026#39; \\ -e \u0026#39;s/ *\\\\$$//\u0026#39; \\ -e \u0026#39;/^$$/ d\u0026#39; \\ -e \u0026#39;s/$$/ :/\u0026#39; \u0026lt; $1.tmp \u0026gt;\u0026gt; $1.tmp; \\ $(MV) $1.tmp $1; endef # rules to generate objects file $(BIN_DIR)/%.o: %.c @$(call make-depend,$(patsubst %.o,%.d,$@),$\u0026lt;,$@,$(CC)) $(CC) $(CFLAGS) -o $@ -c $\u0026lt; $(BIN_DIR)/%.o: %.cpp @$(call make-depend,$(patsubst %.o,%.d,$@),$\u0026lt;,$@,$(CXX)) $(CXX) $(CXXFLAGS) -o $@ -c $\u0026lt; # add-target(target,objs,cc) define add-target REAL_TARGET += $(BIN_DIR)/$1 $(BIN_DIR)/$1: $2 $3 $(LDFLAGS) $$^ $(LOADLIBES) $(LDLIBS) -o $$@ endef # call add-target $(foreach targ,$(TARGET_PROG),$(eval $(call add-target,$(targ),$(objs),$(CXX)))) all: $(REAL_TARGET) $(TARGET_LIBS) clean: $(RM) $(BIN_DIR) 该Makefile为从一个通用的C/C++ Makefile中直接获得的，为了避免原博客以后不能访问的情况，这里备份一下。\n该Makefile可以动检测Makefile所在目录及其子目录中的.c和.cpp文件，并进行编译，不需要手动修改Makefile来填写需要编译的源文件，比较自动化。\n相关参考 第二个Makefile文件的作者博客中的两篇文章：GNU Make学习总结（一）和GNU Make学习总结（二）\n相关下载 一个包含上述两个Makefile的例子\n","date":"2014-11-23T00:00:00Z","permalink":"/post/my_makefile/","title":"我的Makefile文件"},{"content":"为了避免linux下的控制台程序A死掉，可以通过一个另外一个程序B来监听A程序，当A程序异常退出时将B程序带起来。当然程序设计的最好方式为程序不崩溃，但是程序中存在bug很难避免，该方法还是有一定的实践意义。对于B程序可以通过shell脚本或者单独一个应用程序来解决。本文将通过shell脚本来解决此问题。\nshell脚本的内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash check_process() { # check parameter if [ $1 = \u0026#34;\u0026#34; ]; then return -1 fi # get the running process process_names=$(ps -ef | grep $1 | grep -v grep | awk \u0026#39;{print $8}\u0026#39;) for process_name in $process_names do if [ $process_name = $1 ] ; then return 1 fi done # not run and run the process echo \u0026#34;$(date) : process $1 not run, just run it\u0026#34; $1 return 0 } while [ 1 ];do check_process \u0026#34;/usr/bin/app/process\u0026#34;\t# programe path sleep 5 done 将shell脚本在脱离控制台下可以运行 一旦断开了控制台，shell脚本就会由于接收到SIGHUP信号而退出。这里有两种思路来解决该问题，一种是通过系统的crontab来定期调用脚本程序，另外一种是通过神奇的screen程序来解决该问题，我这里通过screen程序来解决该问题，具体screen程序的应用见我的另外一篇文章《》。\n应用程序为daemon方式运行 为了能够保证该脚本监控多个应用程序，需要将应用程序设置为daemon方式运行，可以调用函数daemon实现。也可以调用单独实现的daemon函数，具体代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 void init_daemon(void) { int pid; int i; if(pid=fork()) exit(0);//是父进程，结束父进程 else if(pid\u0026lt; 0) exit(1);//fork失败，退出 //是第一子进程，后台继续执行 setsid();//第一子进程成为新的会话组长和进程组长 //并与控制终端分离 if(pid=fork()) exit(0);//是第一子进程，结束第一子进程 else if(pid\u0026lt; 0) exit(1);//fork失败，退出 //是第二子进程，继续 //第二子进程不再是会话组长 for(i=0;i\u0026lt; NOFILE;++i)//关闭打开的文件描述符 close(i); chdir(\u0026#34;/tmp\u0026#34;);//改变工作目录到/tmp umask(0);//重设文件创建掩模 return; } ","date":"2014-11-06T00:00:00Z","permalink":"/post/linux_listen_shell/","title":"一个Linux下的监听脚本程序"},{"content":"本文以从服务器下载一个文件为例，讲解HTTP的断点续传功能。\n客户端IP地址为：192.168.1.2 服务器IP地址为：192.168.1.3\n客户端向服务器发送请求 客户端向服务器发送的请求为：\n1 2 3 4 5 6 7 GET /deepc.a HTTP/1.1 Host: 192.168.100.189 Connection: keep-alive Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8 User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36 Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 从中可以看出请求的文件名为deepc.a文件。\n客户端向服务器发送具体请求 1 2 3 4 5 6 7 8 9 GET /deepc.a HTTP/1.1 Host: 192.168.100.189 Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36 Accept: */* Referer: http://192.168.100.189/deepc.a Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 Range: bytes=0-32767 Range字段表示请求文件的范围为0-32767。\n服务器响应 第一次服务器的HTTP响应报文如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 GET /deepc.a HTTP/1.1 Host: 192.168.1.3 Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36 Accept: */* Referer: http://192.168.1.3/deepc.a Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 Range: bytes=0-32767 HTTP/1.1 206 Partial Content Date: Sun, 04 May 2014 05:14:54 GMT Server: Apache/2.4.4 (Win32) PHP/5.4.16 Last-Modified: Sat, 03 May 2014 00:43:22 GMT ETag: \u0026#34;7efce6-4f8742f6ed9b2\u0026#34; Accept-Ranges: bytes Content-Length: 32768 Content-Range: bytes 0-32767/8322278 Keep-Alive: timeout=5, max=100 Connection: Keep-Alive HTTP的状态为206，表示服务器已经处理了部分HTTP相应。其中Content-Range字段表示服务器已经响应了0-32767个字节的文件内容。8322278表示文件的总长度为8322278字节。\n客户端继续向服务器发送请求 客户端根据上次HTTP报文中服务器已经返回给的客户端的数据情况继续向服务器发送请求报文，向服务器发送的请求报文内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 GET /deepc.a HTTP/1.1 Host: 192.168.100.189 Connection: keep-alive User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36 Accept: */* Referer: http://192.168.100.189/deepc.a Accept-Encoding: gzip,deflate,sdch Accept-Language: zh-CN,zh;q=0.8,en;q=0.6 Range: bytes=32768-8322277 If-Range: \u0026#34;7efce6-4f8742f6ed9b2\u0026#34; HTTP/1.1 206 Partial Content Date: Sun, 04 May 2014 05:14:54 GMT Server: Apache/2.4.4 (Win32) PHP/5.4.16 Last-Modified: Sat, 03 May 2014 00:43:22 GMT ETag: \u0026#34;7efce6-4f8742f6ed9b2\u0026#34; Accept-Ranges: bytes Content-Length: 8289510 Content-Range: bytes 32768-8322277/8322278 Keep-Alive: timeout=5, max=98 Connection: Keep-Alive Content-Range的内容表示客户端向服务器请求文件中32768-8322277之间的字节数据。\n第二次服务器的HTTP响应报文如下： 1 2 3 4 5 6 7 8 9 10 HTTP/1.1 206 Partial Content Date: Sun, 04 May 2014 05:14:54 GMT Server: Apache/2.4.4 (Win32) PHP/5.4.16 Last-Modified: Sat, 03 May 2014 00:43:22 GMT ETag: \u0026#34;7efce6-4f8742f6ed9b2\u0026#34; Accept-Ranges: bytes Content-Length: 8289510 Content-Range: bytes 32768-8322277/8322278 Keep-Alive: timeout=5, max=98 Connection: Keep-Alive 表示服务器已经相应完成了32768-8322277之间的数据。\n","date":"2014-11-06T00:00:00Z","permalink":"/post/http_range/","title":"一个实例讲解HTTP的断点续传"},{"content":"在github上可以fork别人的项目成为自己的项目，但是当fork的项目更新后自己fork的项目应该怎么怎么更新呢？我从网上看到了两种方式，一种是采用github的web界面中的操作来实现，具体是通过“Pull Request”功能来实现；另外一种是通过在本地合并代码分支的方式来解决。本文将采用第二种方式，以我最近fork的项目为例来说明。\n将fork后自己的项目clone到本地 执行git clone https://github.com/kuring/leetcode.git即可将自己fork的代码更新到本地。\nfork完成后的远程分支和所有分支情况如下：\n1 2 3 4 5 6 7 kuring@T420:/data/git/leetcode$ git remote -v origin\thttps://github.com/kuring/leetcode.git (fetch) origin\thttps://github.com/kuring/leetcode.git (push) kuring@T420:/data/git/leetcode$ git branch -a * master remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master 将fork之前的项目clone到本地 将fork之前的项目添加到本地的远程分支haoel中，执行git remote add haoel https://github.com/haoel/leetcode。\n再查看一下远程分支和所有分支情况：\n1 2 3 4 5 6 7 8 9 kuring@T420:/data/git/leetcode$ git remote -v haoel\thttps://github.com/haoel/leetcode (fetch) haoel\thttps://github.com/haoel/leetcode (push) origin\thttps://github.com/kuring/leetcode.git (fetch) origin\thttps://github.com/kuring/leetcode.git (push) kuring@T420:/data/git/leetcode$ git branch -a * master remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master 将远程代码halel分支fetch到本地 执行git fetch haoel，此时的所有分支情况如下，可以看出多了一个remotes/haoel/master分支。\n1 2 3 4 5 kuring@T420:/data/git/leetcode$ git branch -a * master remotes/haoel/master remotes/origin/HEAD -\u0026gt; origin/master remotes/origin/master 将halel分支merge到本地的分支 执行git merge remotes/haoel/master，此时发现有冲突，提示内容如下：\n1 2 3 4 uring@T420:/data/git/leetcode$ git merge haoel/master 自动合并 src/reverseInteger/reverseInteger.cpp 冲突（内容）：合并冲突于 src/reverseInteger/reverseInteger.cpp 自动合并失败，修正冲突然后提交修正的结果。 之所以出现上述错误，这是由于我在fork之后在本地修正了源代码中的一处bug，而在fork之后到现在的时间间隔内原作者haoel也正好修正了该bug。打开文件后发现存在如下的内容，其实就是代码风格的问题，我这里将错误进行修正。\n1 2 3 4 5 36 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD 37 while( x != 0 ){ 38 ======= 39 while( x != 0){ 40 \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; haoel/master 如果没有冲突的情况下通过merge命令即会将haoel/master分支合并master分支并执行commit操作。可以通过git status命令看到当前冲突的文件和已经修改的文件。执行git status命令可以看到如下内容，说明未冲突的文件已经在暂存区，冲突的文件需要修改后执行add操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 kuring@T420:/data/git/leetcode$ git status 位于分支 master 您的分支与上游分支 \u0026#39;origin/master\u0026#39; 一致。 您有尚未合并的路径。 （解决冲突并运行 \u0026#34;git commit\u0026#34;） 要提交的变更： 修改: src/3Sum/3Sum.cpp 修改: src/4Sum/4Sum.cpp 修改: src/LRUCache/LRUCache.cpp ......\t// 此处省略了很多重复的 ...... 未合并的路径： （使用 \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; 标记解决方案） 双方修改： src/reverseInteger/reverseInteger.cpp 解决完冲突后执行add操作后再通过git status命令查看的内容如下。通过git status命令却看不到已经解决的冲突文件，对于这一点我还是很理解，参考文章中的Git 分支 - 分支的新建与合并是可以看到已经解决的冲突文件的，因为执行git add后将解决完成冲突的文件放到了暂存区中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kuring@T420:/data/git/leetcode$ git status 位于分支 master 您的分支与上游分支 \u0026#39;origin/master\u0026#39; 一致。 所有冲突已解决但您仍处于合并中。 （使用 \u0026#34;git commit\u0026#34; 结束合并） 要提交的变更： 修改: src/3Sum/3Sum.cpp 修改: src/4Sum/4Sum.cpp 修改: src/LRUCache/LRUCache.cpp ......\t// 此处省略了很多重复的 ...... 这里冲突后merge操作并没有执行commit操作，需要解决冲突后再手工执行commit操作，此时整个的同步操作就已经完成了。\n结尾 如果隔一段时间后又需要同步项目了仅需要执行git fetch haoel命令以下的操作即可。\n参考 Git 分支 - 分支的新建与合并 如何在github上fork一个项目来贡献代码以及同步原作者的修改 Github上更新自己fork的代码 由于git命令较多，为了便于查阅增加一处git data transprot commands\n","date":"2014-11-02T00:00:00Z","permalink":"/post/github_fork_sync/","title":"在github上同步fork的项目"},{"content":"问题 Given n, generate all structurally unique BST\u0026rsquo;s (binary search trees) that store values 1\u0026hellip;n.\nFor example, Given n = 3, your program should return all 5 unique BST\u0026rsquo;s shown below.\n1 3 3 2 1 \\ / / / \\ \\ 3 2 1 1 3 2 / / \\ \\ 2 1 2 3 分析 要想能够生成多个树并存储到vector中，最容易想到的就是递归算法。要想能够递归，题目中提供的函数仅有一个参数，结合题目不能够完成递归的条件，考虑到unique binary search trees中的解法，需要递归具有两个参数的函数。\n考虑到了递归的问题，还需要利用循环不断将树添加到vector中，这编写起来也是比较有难度，需要掌握循环的次数和什么时候将树添加到vector中。\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 vector\u0026lt;TreeNode *\u0026gt; generateTrees(int n) { vector\u0026lt;TreeNode *\u0026gt; sub_tree = generateTrees(1, n); return sub_tree; } vector\u0026lt;TreeNode *\u0026gt; generateTrees(int low, int high) { vector\u0026lt;TreeNode *\u0026gt; result; if (low \u0026gt; high) { result.push_back(NULL); return result; } else if (low == high) { TreeNode *node = new TreeNode(low); result.push_back(node); return result; } for (int i=low; i\u0026lt;=high; i++) { vector\u0026lt;TreeNode *\u0026gt; left = generateTrees(low, i - 1); vector\u0026lt;TreeNode *\u0026gt; right = generateTrees(i + 1, high); for (int j=0; j\u0026lt;left.size(); j++) { for (int k=0; k\u0026lt;right.size(); k++) { TreeNode *root = new TreeNode(i); root-\u0026gt;left = left[j]; root-\u0026gt;right = right[k]; result.push_back(root); } } } return result; } ","date":"2014-09-29T00:00:00Z","permalink":"/post/leetcode_unique_binary_search_trees_ii/","title":"Unique Binary Search Trees II"},{"content":"[TOC]\n近期准备复习数据结构和算法的知识，参照了网络上各路大神的学习攻略，大部分算法学习的思路为参照一些经典书籍（如算法导论）并结合一些代码的实践来完成，并未找到一条适合我的算法学习之路。经过思考后决定采用代码编写曾经的教科书中代码实例的方式来学习，曾经接触的算法教科书包括《数据结构（C语言版）》和《计算机算法基础》。一来这些算法已经基本熟悉，只是时间久远有些已经忘记；二来，通过思考后编写代码增强自己的记忆。\n同时我编写的这些实例可以作为leetcode上的很多题目的基础，为下一个阶段刷leetcode上的题目打好基础。\n树的存储形式包括了顺序存储（采用数组形式）和链式存储，其中链式存储更为灵活，可以表示任意形式的树，本文中的代码将采用树的链式存储方式。\n树的构建 树的构建有多种方式，本文使用字符串采用了自顶向下、自左到右的顺序构建树，跟leetcode的形式一致。其中\u0026rsquo;#\u0026lsquo;表示该节点为空，如果该节点为空节点，其左右子孩子节点也要用\u0026rsquo;#\u0026lsquo;表示，而不能不用任何字符表示。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 /** * 二叉树的链式存储结构 */ struct TreeNode { char data; struct TreeNode *left; struct TreeNode *right; TreeNode(char data) : data(data), left(NULL), right(NULL) {} }; /** * 构造二叉树，要构造的字符串采用了自顶向下、自左到右的顺序，跟leetcode的形式一致 */ TreeNode *create_binary_tree(const char *str) { if (!str || strlen(str) == 0) { return NULL; } // 对每个节点分配存储空间 int node_size = strlen(str); TreeNode **tree = new TreeNode*[node_size]; for (int i=0; i\u0026lt;node_size; i++) { if (str[i] == \u0026#39;#\u0026#39;) { tree[i] = NULL; } else { tree[i] = new TreeNode(str[i]); } } for (int i=0, j=0; i\u0026lt;node_size \u0026amp;\u0026amp; j\u0026lt;node_size; i++) { if (tree[i] != NULL) { if ((j + 1) \u0026lt; node_size) { tree[i]-\u0026gt;left = tree[++j]; tree[i]-\u0026gt;right = tree[++j]; } } else { j += 2; } } return *tree; } 二叉树的先序遍历 递归实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * 先序遍历二叉树的递归形式 */ void preorder_traverse_recursion(TreeNode *root) { if (!root) { return; } printf(\u0026#34;%c\\t\u0026#34;, root-\u0026gt;data); preorder_traverse_recursion(root-\u0026gt;left); preorder_traverse_recursion(root-\u0026gt;right); } 非递归实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /** * 先序遍历的非递归形式 */ void preorder_traverse_not_recursion(TreeNode *root) { if (!root) { return ; } std::stack\u0026lt;TreeNode *\u0026gt; tree_stack; tree_stack.push(root); while (tree_stack.size() \u0026gt; 0) { TreeNode *node = tree_stack.top(); tree_stack.pop(); printf(\u0026#34;%c\\t\u0026#34;, node-\u0026gt;data); if (node-\u0026gt;right != NULL) { tree_stack.push(node-\u0026gt;right); } if (node-\u0026gt;left != NULL) { tree_stack.push(node-\u0026gt;left); } } } 二叉树的中序遍历 递归实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * 中序遍历二叉树的递归形式 */ void inorder_traverse_recursion(TreeNode *root) { if (!root) { return; } inorder_traverse_recursion(root-\u0026gt;left); printf(\u0026#34;%c\\t\u0026#34;, root-\u0026gt;data); inorder_traverse_recursion(root-\u0026gt;right); } 非递归实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /** * 中序遍历二叉树的非递归形式 */ void inorder_traverse_not_recursion(TreeNode *root) { if (!root) { return ; } std::stack\u0026lt;TreeNode *\u0026gt; tree_stack; while (root != NULL || tree_stack.size() \u0026gt; 0) { // 遍历到左子树的叶子节点 while (root) { tree_stack.push(root); root = root-\u0026gt;left; } // 遍历栈顶节点 root = tree_stack.top(); tree_stack.pop(); printf(\u0026#34;%c\\t\u0026#34;, root-\u0026gt;data); root = root-\u0026gt;right; } } 二叉树的后序遍历 递归实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * 后序遍历二叉树的递归形式 */ void postorder_traverse_recursion(TreeNode *root) { if (!root) { return; } postorder_traverse_recursion(root-\u0026gt;left); postorder_traverse_recursion(root-\u0026gt;right); printf(\u0026#34;%c\\t\u0026#34;, root-\u0026gt;data); } 非递归实现 仅用一个栈不能够实现后序遍历非递归算法，需要保存一个上次访问过节点的变量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 /** * 后序遍历二叉树的非递归形式 */ void postorder_traverse_not_recursion(TreeNode *root) { if (!root) { return ; } std::stack\u0026lt;TreeNode *\u0026gt; tree_stack; TreeNode *visited = NULL; while (root != NULL || tree_stack.size() \u0026gt; 0) { // 遍历到左子树的叶子节点 while (root) { tree_stack.push(root); root = root-\u0026gt;left; } root = tree_stack.top(); if (root-\u0026gt;right == NULL || root-\u0026gt;right == visited) { // 如果没有右孩子，或者右孩子刚刚访问过，则访问当前节点 printf(\u0026#34;%c\\t\u0026#34;, root-\u0026gt;data); tree_stack.pop(); visited = root; root = NULL; } else { root = root-\u0026gt;right; } } } 总结 二叉树遍历的递归形式程序结构类似，编写相对简单。但是递归方法在C语言中存在执行效率差（需要维护函数栈），容易出现栈溢出的异常的问题。任何递归问题问题都可以转化为非递归问题，转化的思路包括了直接转化法和间接转化法。直接转化法可以通过循环来解决，间接转化法需要借助栈加循环来解决。\n二叉树遍历的非递归形式相对复杂，二叉树的先序遍历的非递归形式容易理解，二叉树的中序遍历稍微困难，后序遍历的非递归形式最复杂。\n相关下载 程序源代码\n","date":"2014-09-28T00:00:00Z","permalink":"/post/traverse_tree/","title":"二叉树的遍历"},{"content":"图的存储 在leetcode中图的存储形式如下，这种形式的图只能适合用来存储是连通图的情况，且根据leetcode提供的_{0,1,2#1,2#2,2}_格式的字符串通过程序来自动构造图比较麻烦，预知字符串的含义请移步到leetcode的解释。\n1 2 3 4 5 6 7 /** * Definition for undirected graph. * struct UndirectedGraphNode { * int label; * vector\u0026lt;UndirectedGraphNode *\u0026gt; neighbors; * UndirectedGraphNode(int x) : label(x) {}; * }; 本文为了能够用字符串表示所有图，并且便于程序的构造，使用了邻接表的形式来对图进行存储，即可以用来存储有向图，有可以存储无向图。图一个节点的结构如下：\n1 2 3 4 5 6 7 8 9 /** * 图节点的邻接表表示形式 */ struct GraphNode { std::string label; std::vector\u0026lt;GraphNode *\u0026gt; neighbors; bool visited;\t// 深度优先搜索和广度优先搜索的遍历都需要visited数组，为了简化程序，直接在节点的存储结构中设置visited变量 GraphNode(std::string x) : label(x), visited(false) {}; }; 图的创建方面为了简化算法实现，对程序的效率没做太多关注，算法复杂度稍高。本算法的难点在于对字符串的拆解，并根据字符串找到对应的节点指针。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 /** * 图节点的邻接表表示形式 */ struct GraphNode { std::string label; std::vector\u0026lt;GraphNode *\u0026gt; neighbors; bool visited; GraphNode(std::string x) : label(x), visited(false) {}; }; /** * 通过字符串的值，找到该字符串对应的图节点 */ GraphNode *get_one_node(const std::vector\u0026lt;GraphNode *\u0026gt; \u0026amp;node_vector, std::string str) { for (std::vector\u0026lt;GraphNode *\u0026gt;::const_iterator iter = node_vector.begin(); iter != node_vector.end(); iter++) { if ((*iter)-\u0026gt;label == str) { return *iter; } } return NULL; } /** * 时间复杂度高,对图的构建一般效率要求较低 * 对于查找某个节点的邻接点的指针操作可以使用map来提高查询效率 * 或者可以通过不需要初始化所有节点的方式来构造图，而是采用需要哪个节点构造哪个节点的方式 */ std::vector\u0026lt;GraphNode *\u0026gt; create_graph(std::string str) { std::vector\u0026lt;GraphNode *\u0026gt; node_vector; // init all nodes for (size_t pos = 0; pos \u0026lt; str.length() - 1;) { size_t end = str.find(\u0026#39;,\u0026#39;, pos); if (end != std::string::npos) { GraphNode *node = new GraphNode(str.substr(pos, end - pos)); node_vector.push_back(node); } else { break; } pos = str.find(\u0026#39;#\u0026#39;, pos); if (pos == std::string::npos) { break; } else { pos++; } } // add neighbors in every node for (size_t pos = 0; pos \u0026lt; str.length() - 1; ) { GraphNode *current_node = NULL; size_t current_end = str.find(\u0026#39;,\u0026#39;, pos); if (current_end != std::string::npos) { current_node = get_one_node(node_vector, str.substr(pos, current_end - pos)); pos = current_end + 1; } else { break; } size_t node_end = str.find(\u0026#39;#\u0026#39;, pos); // 当前节点的字符串的结束位置 if (node_end == std::string::npos) { node_end = str.length(); } else { node_end--; } for ( ; ; ) { current_end = str.find(\u0026#39;,\u0026#39;, pos); if (current_end \u0026gt; node_end || current_end == std::string::npos) { // 一个节点的最后一个邻接点 current_end = node_end; } else { current_end--; } GraphNode *node = get_one_node(node_vector, str.substr(pos, current_end - pos + 1)); if (node != NULL) { current_node-\u0026gt;neighbors.push_back(node); std::cout \u0026lt;\u0026lt; current_node-\u0026gt;label \u0026lt;\u0026lt; \u0026#34; add \u0026#34; \u0026lt;\u0026lt; node-\u0026gt;label \u0026lt;\u0026lt; std::endl; } if (current_end == node_end) { // 一个节点的最后一个邻接点 break; } else { pos = current_end + 2; // 该节点之后还有其他邻接点 } } pos = node_end + 2; } return node_vector; } 深度优先搜索 深度优先搜索遵循贪心算法的原理，如果孩子节点不为空，则一直遍历下去。\n递归算法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 void DFS_traverse_recursion(GraphNode *node) { if (!node-\u0026gt;visited) { std::cout \u0026lt;\u0026lt; node-\u0026gt;label \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39;; node-\u0026gt;visited = true; } for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter = node-\u0026gt;neighbors.begin(); iter != node-\u0026gt;neighbors.end(); iter++) { if (!(*iter)-\u0026gt;visited) { DFS_traverse_recursion(*iter); } } } /** * 图的深度优先搜索的递归形式 */ void DFS_traverse_recursion(std::vector\u0026lt;GraphNode *\u0026gt; \u0026amp;graph) { for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) { (*iter)-\u0026gt;visited = false; } for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) { if (!(*iter)-\u0026gt;visited) DFS_traverse_recursion(*iter); } } 非递归算法 使用栈来实现非递归。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /** * 图的深度优先搜索的非递归形式 */ void DFS_traverse_not_recursion(std::vector\u0026lt;GraphNode *\u0026gt; \u0026amp;graph) { for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) { (*iter)-\u0026gt;visited = false; } for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) { std::stack\u0026lt;GraphNode *\u0026gt; node_stack; if ((*iter)-\u0026gt;visited) { continue; } node_stack.push(*iter); while (!node_stack.empty()) { GraphNode *node = node_stack.top(); node_stack.pop(); if (node-\u0026gt;visited) continue; std::cout \u0026lt;\u0026lt; node-\u0026gt;label \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39;; node-\u0026gt;visited = true; /* 使用反向迭代器遍历后将节点加入到栈中 */ for (std::vector\u0026lt;GraphNode *\u0026gt;::reverse_iterator iter2 = node-\u0026gt;neighbors.rbegin(); iter2 != node-\u0026gt;neighbors.rend(); iter2++) { if (!(*iter2)-\u0026gt;visited) { node_stack.push(*iter2); } } } } } 广度优先搜索 该算法不存在递归算法，仅有非递归版本。需要利用队列来保存需要遍历的节点，占用的存储空间稍多。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 /** * 图的广度优先搜索的非递归形式 */ void BFS_traverse_not_recursion(std::vector\u0026lt;GraphNode *\u0026gt; \u0026amp;graph) { for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) { (*iter)-\u0026gt;visited = false; } for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) { std::queue\u0026lt;GraphNode *\u0026gt; node_queue; if ((*iter)-\u0026gt;visited) { continue; } node_queue.push(*iter); while (!node_queue.empty()) { GraphNode *node = node_queue.front(); node_queue.pop(); if (node-\u0026gt;visited) continue; std::cout \u0026lt;\u0026lt; node-\u0026gt;label \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39;; node-\u0026gt;visited = true; for (std::vector\u0026lt;GraphNode *\u0026gt;::iterator iter2 = node-\u0026gt;neighbors.begin(); iter2 != node-\u0026gt;neighbors.end(); iter2++) { if (!(*iter2)-\u0026gt;visited) { node_queue.push(*iter2); } } } } } 相关下载 本文相关源码\n","date":"2014-09-28T00:00:00Z","permalink":"/post/traverse_graph/","title":"图的存储和遍历"},{"content":"题目 Memory Limit Exceeded\nDesign a stack that supports push, pop, top, and retrieving the minimum element in constant time.\npush(x) \u0026ndash; Push element x onto stack. pop() \u0026ndash; Removes the element on top of the stack. top() \u0026ndash; Get the top element. getMin() \u0026ndash; Retrieve the minimum element in the stack.\n错误代码 看到此题目，以为是用实现一个简单的栈结构，于是就直接写下了如下代码，采用了双向链表的方式来实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class MinStack { public: struct Node { int data; Node *next; Node *pre; Node() : next(NULL),pre(NULL) {}; }; MinStack() { header = new Node(); tail = header; } void push(int x) { Node *node = new Node(); node-\u0026gt;data = x; node-\u0026gt;pre = tail; tail-\u0026gt;next = node; tail = node; } void pop() { Node *pre = tail-\u0026gt;pre; delete tail; tail = pre; tail-\u0026gt;next = NULL; } int top() { if (tail == header) { return -1; } return tail-\u0026gt;data; } int getMin() { Node *begin = header-\u0026gt;next; int min = INT_MIN; while(begin) { if (min \u0026gt; begin-\u0026gt;data) { min = begin-\u0026gt;data; } begin = begin-\u0026gt;next; } return min; } private: Node *header; Node *tail; }; 提交后提示Memory Limit Exceeded错误，开始考虑是不是双向链表占用的空间过多。\n正确代码 仔细看题目后发现retrieving the minimum element in constant time，即时间复杂度为O(1)，上述实现代码时间复杂度为O(n)，明显不符合要求。看来理解题目有误，不是为了实现栈类，而是为了利用数据结构解决获取最小值问题。\n经过考虑后可以通过在类内部维护存储最小值的栈来解决，存储最小值的栈除了需要存储最小值外，还需要维护最小值个数。\n实现代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class MinStack { struct MinUnit { int count; int value; MinUnit(int count, int value) : count(count), value(value) {}; }; public: void push(int x) { data_stack.push(x); if (min_stack.empty() || x \u0026lt; min_stack.top()-\u0026gt;value) { MinUnit *p_unit = new MinUnit(1, x); min_stack.push(p_unit); } else if (x == min_stack.top()-\u0026gt;value) { min_stack.top()-\u0026gt;count++; } } void pop() { if (data_stack.top() == min_stack.top()-\u0026gt;value) { if (min_stack.top()-\u0026gt;count == 1) { MinUnit *punit = min_stack.top(); min_stack.pop(); delete punit; } else { min_stack.top()-\u0026gt;count--; } } data_stack.pop(); } int top() { return data_stack.top(); } int getMin() { return min_stack.top()-\u0026gt;value; } private: std::stack\u0026lt;int\u0026gt; data_stack; // 最小栈，为了便于修改栈单元中的count值，采用存储指针方式 std::stack\u0026lt;MinUnit*\u0026gt; min_stack; }; ","date":"2014-09-27T00:00:00Z","permalink":"/post/leetcode_min_stack/","title":"leetcode题目之Min Stack"},{"content":"在Windows下可以通过计算器进行进制之间的转换，非常方便。本文总结Linux下可用的进行进制之间的转换方法。\n利用shell shell脚本默认数值是由10进制数处理，除非这个数字某种特殊的标记法或前缀开头，才可以表示其它进制类型数值。如：以0开头就是8进制、以0x开头就是16进制数。使用“BASE#NUMBER”这种形式可以表示其它进制。BASE值的范围为2-64。\n其他进制转10进制\n1 2 3 4 5 6 7 8 9 10 11 12 kuring@T420:~$ echo $((16#4000000)) 67108864 kuring@T420:~$ echo $((2#111)) 7 kuring@T420:~$ echo $((0x10)) 16 kuring@T420:~$ echo $((010)) 8 // 对进制转换为10进制进行运算，稍显啰嗦 kuring@T420:~$ echo $(($((16#4000000))/1014/1024)); 64 利用let命令 let用来执行算数运算和数值表达式测试。可以利用该命令完成简单的计算，并将计算结果赋给其他变量。\n1 2 3 4 // 对进制转换为10进制后进行运算，比纯shell方式简洁 kuring@T420:~$ let a=$((16#4000000))/1014/1024; kuring@T420:~$ echo $a 64 利用bc命令 该命令是一个强大的计算器软件，可以利用其中的ibase和obase进行输入进制的转换，ibase表示输入数字的进制，obase表示输出数字的进制。\n1 2 3 4 5 6 // 如果没有制定，则默认为10进制 // 对于16进制，要使用F，而不能使用f kuring@T420:~$ echo \u0026#34;ibase=16;FF\u0026#34; | bc 255 kuring@T420:~$ echo \u0026#34;obase=2;ibase=16;FF\u0026#34; | bc 11111111 参考文章 linux shell 不同进制数据转换（二进制，八进制，十六进制，base64)\nShell进制转换小结\n","date":"2014-09-27T00:00:00Z","permalink":"/post/linux_convert_ary/","title":"Linux下通过命令行进行进制转换"},{"content":"由于I/O模型现在掌握还不够透彻，本文仅列出了一些I/O模型中的一些常用概念，对于select、poll、epoll和信号驱动I/O等模型理解还不够透彻，待理解透彻后补上。\n阻塞I/O与非阻塞I/O 阻塞I/O：当进行I/O操作后，程序处于等待状态，直到需要的资源可用为止。\n非阻塞I/O：当进行I/O操作后，函数直接返回，不需要等待。\n两种文件描述符准备就绪的通知模式 水平触发通知：如果文件描述符上可以非阻塞地执行I/O系统调用，此时认为它已经就绪。允许在任意时刻重复检测I/O状态，没有必要每次当文件描述符就绪后尽可能多的执行I/O。其中select()和poll()属于水平触发通知模式。\n边缘触发通知：如果文件描述符自上次状态检查以来有了新的I/O活动，此时需要触发通知。只有当I/O事件发生后才会得到通知，在另外一个I/O事件到来之前不会得到通知。因此，在接收到一个I/O事件后，程序在某个时刻应该在相应的文件描述符上尽可能多地执行I/O。每个被检查的文件描述符被置为非阻塞模式，在得到I/O事件通知后重复执行I/O操作，直到系统调用返回错误为止。其中信号驱动I/O属于此种类型。\n将的通俗一点，假设通过异步I/O读取socket数据，当接收到100个字节的数据后，这时候会触发通知给应用程序。如果应用程序仅读取了50字节的数据，重新调用异步I/O函数时，在水平触发时仍然会得到通知，而采用边缘触发时不会得到通知。\nepoll即可以采用水平触发通知方式，也可以采用边缘触发通知方式。\n这两个概念对于理解Linux的I/O模型非常重要，但是却是不容易理解。可以通过举例说明：一个管道内收到了数据，注册该管道描述符的epoll返回，但是用户只读取了一部分数据，然后再次调用了epoll。这时，如果是水平触发方式，epoll将立刻返回，因为当前有数据可读，满足IO就绪的要求；但是如果是边沿触发方式，epoll不会返回，因为调用之后还没有新的IO事件发生，直到有新的数据到来，epoll才会返回，用户可以一并读到老的数据和新的数据。\n同步I/O与异步I/O 同步I/O：发出I/O操作后，后面操作不能进行，要么等待I/O操作完成，要么放弃I/O操作。后面的操作和当前的操作只能有一个进行。\n异步I/O：发出I/O操作后，马上返回继续执行后面的操作，而I/O操作同时执行。\n参考文章 《Linux/UNIX系统编程手册》第63章 浅析Linux IO技术体系 ","date":"2014-09-16T00:00:00Z","permalink":"/post/linux_io/","title":"Linux IO模型"},{"content":"\n前言 在这个炎热的夏天，在一个特殊的月份，我开启了一段8天7晚的云南之旅。出于资金方面和省心方面的考虑，选择跟团旅游。本次的旅行包含了昆明、大理、丽江和香格里拉四个地点，如果不是跟团，在短短的6天的旅行时间之内不可能走完四个地方的多个景点，因为这四个地方之间的距离坐车基本都在四个小时以上。大多数人去云南还是会选择跟团的，因为跟团省太多事了。如果是独自出行，每天光查路线路、找酒店、查攻略就会浪费掉很大一部分时间，肯定会更自由一些，但是绝对不会走过这么多地方。\n跟团也有很多不好的地方，本次出行旅行社制定的路线为“出发地-昆明-大理-丽江-香格里拉-丽江-大理-昆明-目的地”，可以看到路线是重复了一遍，相当于很大一部分时间都浪费在了路上，至少相当于浪费了4+4+4共12个小时的时间，这也就造成了每天早晨需要很早就起床，基本在6点半就坐在大巴上出发了。之所以每天早晨会很早就起床，另一个原因是因为现在是云南旅游的旺季，一旦稍微晚些，很多景点就会排队等好长时间，可能晚起10分钟，就要在景区多排一个小时的队。\n跟团的价格之所以便宜，听导游说，这是云南省旅游局的决定。云南绝大多数都是高原山区地貌，交通不够发达。旅游业在云南的经济中占有很大的比重，政府想通过旅游来拉动经济，跟团价格便宜，自然需要在旅游过程中的消费来赚钱。本次旅游的过程中，也有几次购物的经历，其中行程中的大理十八里铺和七彩云南是赤裸裸的购物环节，导游也明确的告知了。在去景点的过程中，很明显发现每个景点的必经通道中均有购物的部分，这是云南的景点和我去过的其他景点比较大的差别。最奇葩的是长江第一弯的观景楼，大约7层楼左右的样子，每层楼的楼梯都不在一个位置，从一层的楼梯走到另一层的楼梯必经卖土特产的柜台，楼层上全是卖土特产的。其实长江第一弯也仅有一个观景楼而已，因此是免费的。\n跟团很大一部分时间都在坐车上，游览景点的时间确实非常紧，而导游给的购物地方的时间确实非常充裕。比如游览石林仅给了一个半小时左右的时间，石林是国家的5A级景区，门票价格是170，是本次行程中最贵的景点了吧。石林虽然不大，但是要想在一个半小时内全部游览完是不可能的，因此只能走马观花，没办法，跟团走就只能认了。\n之所以要写下这段行程，是因为这段旅行对我而言有着不一般的意义，我想将这段记忆铭记在心。不幸的是，回家后发现单反的SD卡出问题了，照片丢失了很大一部分。我虽不是很认同旅游就是为了拍照，现在人们已经过度的重视拍照而忽略了旅行本身的意义，但是这些照片在这次旅行中确实不可或缺的。\nDay 1 下午三点多就到了春城昆明，抬头一看昆明的天果然不一般，云彩非常多，而且离地面非常近，轮廓感很强。阳光非常强烈，气温虽然不高，但是紫外线较强，阳光照在皮肤上生疼。不像在济南只能看到的雾霾，济南即使有云也是在雾霾之上，从地面看去毫无层次感。找到旅行社后机场接送人员送往酒店，稍事休息后去到昆明市中心的南屏街。从整体看，昆明的市容跟济南相仿，属于不是很繁华的类别，但是却属于省会的档次。下图照片我第一晚酒店的照片，大家感受一下云。\n晚上和朋友在一家彝族的菜馆吃饭，对于北方的我来说，南方的饭菜吃起来不太习惯，米饭是生硬的，汤是油腻的，菜是辣的且没滋味。\n吃完饭后准备坐公交回酒店，已经晚上九点半左右了，想抬头看看能不能看到星星，居然看到满天的云，真是奇特。在山东即使晚上有满天的云，也绝对达不到昆明的云的明亮程度。造成这种现象有两个原因，一是昆明属于东七区，比北京时间要晚上一个小时；二是，云南的云离得地面更近且多。\nDay 2 这是悲催的一天，按照行程，要走“昆明—大理—丽江（昆明大理行车约4.5小时，大理-丽江行车约2小时车程）”的路程。早晨一起床，要去餐厅吃早饭，发现餐厅乌秧乌秧的全是人啊，找个地方吃饭都难，在艰难中吃完了早餐。期间一女同胞不小心打碎了一个碗，餐厅老板要了女顾客10块钱，心想真黑啊，几毛钱一个碗至于要这么多吗？\n吃完饭后，餐厅外面和酒店大堂全是游客，经过电话沟通后找到了导游。导游点名后坐上了悲催的大巴。幸运的是，导游还不错，东北小伙，在云南上的大学。云南的旅游大巴都是车牌号都是统一编号的，车牌号以“L“开头，表示旅游的含义。旅游大巴启动后，雨就开始稀里哗啦的下起来了，运气还不错。\n旅游大巴在高速上要在好多地方做例行检查，大约9点左右行驶到楚雄地段的某个高速检查点时，我们大巴的右侧玻璃跟后面的大巴的左侧后视镜摩擦而过，因为我就坐在后排，眼睁睁的目睹了这一过程。结果我们大巴右侧玻璃和后面大巴的左侧后视镜全部挂掉了，幸好没有驴友受伤情况，有图有真相。\n在服务区经过1个小时的等待后大巴向前走了，目的地不是大理，而是前方的某个小镇。因为大巴没了挡风玻璃，速度也上不去了，在后面坐着还是相当凉快的。在小镇吃了个野生菌火锅后，开始了漫长的等待，等待着更换的大巴从大理赶来。从出现事故到新的大巴的到来，花掉了近5个小时的时间，这五个小时就算浪费了。\n大巴前往大理，到达大理时接近5点多了，在天龙八部影视城附近吃了个晚餐，开始了赶往丽江的行程。本来计划中的大理古城和束河古镇全部泡汤，只能将行程安排到回来时在游览。\n到达丽江时已经晚上十点的样子，刚刚下过一场雨，直接入住酒店。\nDay 3 相对来说最轻松的一天，今天的行程中仅有玉龙雪山景区和玉水寨。\n在云南给我们带团的导游属于省级导游，到了丽江这种地方性景点导游需要更换为地方性导游，这也是一种地方拉动就业的措施。昨天的导游直接休息一天，今天的导游是当地的纳西族导游带领。\n玉龙雪山是丽江旅游的必去之地，每天丽江的游客大约在2万人的样子，也就意味着每天也会有2万人会前往玉龙雪山。为了避免在景区排队，早晨6点半就出发前往玉龙雪山，一个小时左右时间到达了玉龙雪山的入口。下车后坐着环保大巴盘山而上，到达云杉坪脚下，乘索道而上到达云杉坪。云龙雪山终年云雾缭绕，尤其是在云南的雨季更是非常难以看到玉龙雪山的面貌，听导游讲他也仅仅看过10次左右的样子。我不是个幸运的人，自然也无法看到玉龙雪山的面貌，此时天还下着小雨，从云杉坪上望去仅有白茫茫的云而已。其实从索道下和索道上看到的玉龙雪山的景色几乎是完全一致的，增加索道仅是为了增加景点的可玩性。\n从索道下来后，开始乘坐电瓶车往下走，经过三站，每一站都会停下来观赏风景。三站观赏都是下山的白水河，只是分为了三段，三段景色不尽相同。河水属于碱性，河水颜色是蓝色且清澈，看起来相当漂亮，所以叫做蓝月谷。\n从雪山下来后，如果时间来的及可以看到张艺谋导演的《印象丽江》，500名纳西族的群众演员的表演。由于下来晚了，仅看了个5分钟的结尾，不过可以从网上找的到视频。自从有了老谋子的《印象丽江》，带团的必须跟着演出时间走，来晚了就看不上了。\n从云龙雪山出来后，前往玉水寨。玉水寨是丽江古城河水主要源头之一，是丽江东巴文化的传承圣地。景点面积不大，但属于东巴教圣地。既然属于圣地，照片就不上了。\n在回丽江市的途中，导游领我们去了所谓的螺旋藻配送中心，实实在在的购物环节。螺旋藻的确属于丽江特产，野生的螺旋藻需要生长在碱性环境中，而离丽江不远的程海湖是世界第三大碱性湖泊，绝对是野生螺旋藻的产地，就是不知道市面上的所谓程海湖螺旋藻有多少是真的，因为一个程海湖再大也供应不了市场上这么多的螺旋藻。螺旋藻可以被导游描述出一堆好处，什么降血压、调肠胃、增强抵抗力等。每罐螺旋藻290元，属于批发价。机灵的我在没有了解一件商品之前是不会冲动消费的，淘宝一下发现有更便宜的，好像这个品牌也不是什么最好的，自然不买，何况想买还非得从这里买不成。\n下午一点左右到达丽江古城北门后就是我们的自由时间。首先来到了黑龙潭公园，属于市民最常去的健身场所，跟济南的五龙潭公园类似，没什么特别之处，属于市民最常去的健身场所。\n从黑龙潭出来后直奔丽江古城，整个游览下来丽江古城给我的感觉就是一座已经完全商业化的现代古城，几乎跟我想象中的一致。导游跟我们说过，丽江古城内其中90%是外地人，因此要在这里找回古城的感觉是比较难了，但在这里文艺一把还是可以的。\n丽江古城面积虽大，但是游览下来古城内也就文艺客栈、普通餐馆、艳遇酒吧、彩条围巾、特色小饰品、银首饰、手敲鼓、写真。客栈几乎被鲜花包裹，随便找一家客栈都看上去很美很艺术，看上去像是一件艺术作品。餐馆跟古城外面的没什么两样，只是位置不同罢了。酒吧在古城中占有很大一部分席地，丽江是以艳遇闻名的，艳遇最多的地方还是得在酒吧。甚至大街上都有卖艳遇服装的，可见艳遇在游客心中的地位。彩条围巾基本拿回家都是送人和当桌布的，导游说围巾基本都产自浙江的，但在逛古城的时候买个披肩披在身上看起来还是不错的。导游说银首饰是买不得的，不知真假，不买为好。从店外面看去手敲鼓店里在边劈着腿坐着敲鼓边唱歌的女士怎么看怎么觉得别扭，但那音乐很特别。古城的街道挺多，但是走着走着却厌倦了，因为每条街道上的商品也就那些。\n边看地图边问路，再配合上不太好用的百度地图向酒店返回。丽江的特色菜为三文鱼、黑山羊和腊排骨。一路上想找一家可以吃腊排骨的店，愣是没有找到一家合适的。最后累并饿的不行了，在四方街找了家德克士填饱肚子算了。\nDay 4 一早起来，发觉胃部不适，不知是不是昨天在丽江古城吃了什么不合适的小吃还是晚上睡觉忘记开窗户了还是有点轻微的高原反应还是这几天的劳累还是水土不服。总之身体不舒服是旅游中的大忌，我恰巧就碰到了。稍微喝了点米粥后就上路了。约莫走了一个小时，吐了一次。还好，吐完后整个身体都轻松多了。\n在虎跳峡附近接上我们的香格里拉导游，地方导游自然是藏族人，叫什么措，我们的团改成叫“措团”。\n今天第一个景点为虎跳峡，虎跳峡为金沙江上的一个峡谷，下车点在峡谷上方，景点为沿着护栏峡谷底部行进，到达底部后再沿上行木栈道到达出发点。下车后听到最多的就是知了叫，知了数量非常之多，以至于木栈道上都有，伸手就可以抓到。沿木栈道而下，江水并没有想象中的急，不过在最下面的观景台上声音听起来确实很大。观景台对面有一个连接两山的石桥远远看去非常漂亮。由于照片全部损坏，不能贴图了。\n从虎跳峡走后下午一点抵达“心中的日月\u0026ndash;香格里拉”，香格里拉一名源于《消失的地平线》小说中虚构的地名。香格里拉的海拔在3200米，很多人在这种海拔高度会出现高原反应。\n下午一到酒店就趴在床上累的起不来了，沉睡了两个小时后，由于晚上要去藏族家里进行家访，不敢睡的太沉。醒后又在床上躺了一个多小时，一闭眼就能睡着，感觉从来没有这么累过。咬咬牙起来后，找个超时买点东西，感觉走起来也非常的累，心想，这辈子再也不会来这地方了，纯粹的花钱找罪受。\n吃过晚饭后，就去了藏族家。这个是额外收费的，而且费用不低。带牦牛肉和烤鸡肉的价格为一人260元，不带的为一人180元。胃部本来就不适的自然选择了180元的价格。等到我们去了之后问起其他团的，他们带牦牛和烤鸡的价格才100元，可见我们导游能黑我们多少money。这次家访纯粹是一次商业性质的家访，大巴在藏族家门口停下来，下车后就会有人给佩戴围巾，并且有专人拿着单反拍照。\n进屋后直接到二楼，屋里已经有好多个团围着屋中央而坐。坐下后，品尝酥油茶，青稞酒，还有酸奶干，甚至还有青稞拿来当瓜子吃的。\n等牦牛肉和烤鸡都上齐后，节目就开始了。主持人很明显经过些专业的训练，嗓门特别大，也特别擅长带动气氛。藏族人好客的方式就是跺地板加鼓掌，声音越大越好。地板为木制的，一群人跺起脚来后声音特别大。节目大约持续了三个小时，无非就是跳舞、唱歌、跺脚、教些藏族的语言等。其实节目挺无聊，但是热闹。挺佩服主持人能够将这么庸俗的一个节目坚持了三个小时之巨。本来身体非常难受的我，在欢呼中也变得热情高涨，使劲跺脚，使劲故障，全身心投入到节目中，最后居然难受的症状全部消失了。\n期间，藏民开始按照在门口时拍摄的照片找人，所有人的照片早已经洗好，并且用硬塑料膜保护着。照片20元一位，可见这是多么商业的一次活动。\n到了酒店已经11点了，导游嘱咐过不能早睡，不能洗澡，否则很容易出现高原头痛等高原反应的症状。\nDay 5 照旧是早起，今天的行程为普达措国家森林公园。为了防止出现高原反应带来的事故，同时也可以达到赚钱的目的，导游推荐我们在香格里拉买氧气灌和红景天口服液。仙剑奇侠传三中的景天大概就取自此药名吧。普达措的最高处海拔可以达到4200米，想想这个海拔高度，为了避免高原反应，再加上昨天我本来就很不舒服，为了保险了我买了两罐，并且购买了一盒红景天口服液。后来才发现这些花费都是多余的，普达措之所以称之为森林公园，森林肯定非常之多，空气中氧气含量不会很低。走在普达措中，感觉空气比氧气瓶中的更纯净。\n整个森林公园分为了三部分：属都湖、弥里塘和碧塔海。整个森林公园的面积非常之大，三个景点之间大约需要坐10-20分钟的电瓶车，电瓶车上都会有导游对景点进行讲解。\n普达措应该是我这次云南旅游中最值得一去的地方，整个游览下来就感觉这个地方好似人间仙境。这里的湖面非常静，远处牦牛和马儿低着头在不停的吃草，一副悠闲自得的样子。遍地的小野花，在牛粪堆中自由的生长。从山上留下来的水就在平地上冲出一条弯弯曲曲的河，令我一直在想河道为什么是S型，这说明山上留下来的水总是缓缓的。水杉树上挂满了蜘蛛网版的白色的毛，更添加了这里的仙境色彩，据说是寄生在水杉上的一种植物。山间偶尔会见到特别小的小松鼠在树下找点吃的，然后一会就不见了，仿佛刚才见到的不是真的。这中景色中有身临其境方能体会。\n属都湖为一个天然湖泊，远处就能看到牦牛和马儿。弥里塘是整个森林的海拔最高处，地方非常平静，视野相当开阔，牦牛和马儿在远处的草地上吃着小花小草。碧塔海是整个景区的核心景点，我们选择徒步4公里的路程，沿着碧塔海从湖的一边走到另外一边。在漫步中慢慢体会碧塔海的迷人之处。\n从普达措出来后，藏族导游跟我们说拜拜。在香格里拉吃过午饭后，开始马不停蹄的往丽江赶。期间路过“长江第一湾”景点，在观景楼上高高看去，除了恐高之外没有别的感觉。前文已经介绍过“长江第一湾”景点的一些坑况，不再对坑进行过多描述。\n下午三点多到达丽江的酒店，在酒店放下东西后直接前往大理古城，这里酒店离的大理古城的南门算是比较近。上次来没有吃上当地特色小吃腊排骨，在南门外找了家店品尝一番，吃起来跟我想象中的一致，除了有点排骨味之外还略带腊味。\n吃完腊排骨后已经是晚上8点左右的时间，上次逛丽江古城主要是逛了北门附近多些，这次逛上次没有逛过的南门附近。南门附近与北门不同的是，古城南边有很多的客栈，但是这里的胡同和客栈却略显凄凉，人少了需要，客栈也没有那么文艺。转了几个胡同后，在转了一个又一个弯后的胡同中找到了大名鼎鼎的木府。时间已晚，看着地图加导航接回酒店睡下了。\nDay 6 今天的行程为从丽江-大理-昆明。上次去大理仅是路过，这次是去体验，虽然时间也仅仅有一天。\n下关风，上关花，苍山雪，洱海月。风花雪月便是大理景色的最真实写照，大理城就被下关、上关、苍山和洱海包裹着。本次大理执行风花雪月均为体会到，甚至连洱海边都未到过，仅仅是去了洱源小西湖，也勉强算是到过洱海吧。\n快到洱源小西湖的路上遇到了集市，集市上有用竹篓子卖小猪，看起来挺滑稽挺有意思。\n进入洱源小西湖后导游会先带领到购物区，都是些当地的特产，价格也不贵。刚坐上木船会看到鱼鹰表演，木船绕着西湖转上一圈，欣赏西湖的美景。\n下船后，会去餐馆白族的歌舞表演，并品尝白族三道茶。三道茶“头苦、二甜、三回味”，喝起来味道怪怪的。从歌舞表演厅出来后就直接出西湖了。\n从西湖出来后，直接赶到白银购物中心十八里铺。大理是白银产地，在大理旅游购物自然少不了白银。一下车，在十八里铺的停车场已经停了几十辆大巴，显然都是拉来购物的。导游象征性的给我们介绍了下大理白族的房屋布置和婚房布置，顺便看了下几个人在手工制作白银，最后就带我们到白银购物大厅里了，然后导游就撤了。留下来选择购买白银的时间非常的长，有一个半小时之久。\n艰难的等待后，在十八里铺吃完午饭后，直接奔向大理古城。由于南门无法停车，大巴在离大理古城南门2里地的停车场停下，导游给我们游玩的时间是1个半小时，偌大一个大理古城仅有一个半小时的游玩时间，而一个十八里铺的购物大厅却也预留了差不多时间。\n大理古城相比丽江古城更现代化，古城内有相当多的现代商店，甚至都有超市的存在，除了建筑风格比较古之外，其他都是新的。古城的北面，直接就成了居民区了。不过相比丽江古城，我更喜欢大理古城，因为这里购物更方便，而且街道也建设的更加规整，不会出现迷路的情况。里面的物品价格也更加便宜，整个古城更像是城市中的步行街。\n从大理古城走后，直接赶回昆明，到达昆明酒店已经是晚上10点了。\nDay 7 早餐后赶往中国名牌企业七彩云南，顾名思义就是购物了。导游一而再再而三的强调，导游前五天是天使，第六天就是魔鬼。“什么都可以落在宾馆，唯独你们的武器\u0026ndash;钱包不能落在宾馆”。没有比这个更直白的了。\n七彩云南园区内共有七个展馆，首先带领我们去的是翡翠馆。翡翠主要产自缅甸和云南接壤地带，自然会成为云南的特产代表。导游带领我们体验一把模拟的翡翠形成过程的动画，紧接着就会带领我们到达翡翠销售大厅。购买翡翠的时间预留的非常充足，期间为了打发时间跑去了孔雀园转了一圈。整个行程中导游是只字未提孔雀园，但是孔雀园却是七彩云南园区中最适合游玩的地方。孔雀园内有几千只孔雀，植物的种类也非常多，还有一面非常大的花墙，看上去也是相当漂亮。\n集合后又餐馆了普洱茶馆，直接被关到屋里面品茶，品完熟茶品生茶，品完茶后自然是买茶。\n在七彩云南吃过午餐后，大巴赶往天造奇观的石林。去石林的路上，从未看到过石林内形状的石头，可是到了石林内部却到处都是竖状的石头，一直对竖状石头的成因感到颇为好奇。一个5A级的景区仅仅预留了一个半小时的时间游览，而且还包含了排队等电瓶车的时间。\n石林分大石林和小石林，大石林的石头看上去更高大一些，小石林的石头看上去稍微矮小一些。听景区的导游介绍，到了大石林找到“石林”二子后拍个照后就相当于花掉了100块钱的门票，到了小石林后找到“阿诗玛的化身”后就相当于又花掉了70元的门票。\n到达大石林后找到了“石林”二字拍完照后，没有听导游的话直接原路返回坐电瓶车到小石林，而是出于好奇在石林内部转起迷宫来，石林面积应该不小，至少我在内部至少走了40分钟后也没有走多少，而且重要的是我彻底迷路了，找不到入口也找不到出口。一看时间已经过去了一大半，还没到小石林，就抓紧时间往外赶，转了半天迷宫终于出来了。\n排队挤上电瓶车到达小石林，小石林石头之间的间隙要大很多，石头之间还有草地，走在里面不会存在迷路的可能。拐个弯找到阿诗玛的化身拍个照抓紧时间往出口赶。阿诗玛的化身石我怎么看也没看出像一个人来，石头背后的故事就不深究了。\n从石林归来后，又回到了七彩云南。这次购物的重点已经不再是翡翠，而是黄龙玉。黄龙玉产自云南，发现较晚，因此价格较便宜。我直接走向黄龙玉旁边的土特产超市，从里面买点鲜花饼、当地咖啡、普洱茶等，价格还算亲民，拿回来送朋友是个很不错的选择。\n在七彩云南吃过地道的过桥米线后直接入住温泉酒店，跟陪伴我们六天的导游和司机师傅说再见后，在温泉酒店做了个鱼疗后，收拾所有的物品打包准备回温馨的家。\nDay 8 8点20的飞机，五点半开始往机场赶。12点左右已经回到了温馨的家。\n杂 方向感 我无论到哪里都喜欢找找方向感，看看那是南，我是属于那种第一次到一个地方后只要认错了方向就再也调不过来的那种人。这一周的时间我却几乎没有掉过方向来，以至于到后来干脆就不考虑方向了，管他东西南北，只要能回得了酒店，只要能找到大巴就OK了。\n民族 本次行程中的几个城市分属于不同的少数民族，风俗习惯、文化传统和建筑风格各有特点。昆明和楚雄是彝族人居多，大理是白族自治州，丽江是纳西族的聚居地，香格里拉主要是藏族人。最有意思的是，这几个少数名族对男女的称呼各有特色。彝族人对男性称呼为“阿黑哥”，对女性称呼为“阿诗玛”；白族分别称呼为“金花”和“阿鹏”；纳西族分别称呼为“胖金妹”和“胖金哥”；藏族分别称呼为“扎西”和“卓玛”。\n尾 本来是计划写成散文的，文笔有限，写着写着就写成记叙文了。要是有足够的文采，绝对可以写出比《从百草园到三味书屋》中需要背诵的片段更优美的文字，可惜不是作家的料，仅能码字而已。\n旅游不易，码字更难，且行且珍惜。\n","date":"2014-08-22T00:00:00Z","permalink":"/post/yunan_travel/","title":"云南之旅"},{"content":"有些闲暇时间了解了下酷壳的谜题活动，共10道题，每道题都不是非常简单，我这里参考着攻略做了下。\n字符替换题 我编写的C++语言程序\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; char change(char input) { char *after = \u0026#34;abcdefghijklmnopqrstuvwxyz\u0026#34;; char *before = \u0026#34;pvwdgazxubqfsnrhocitlkeymj\u0026#34;; for (int i=0; i\u0026lt;strlen(before); i++) { if (before[i] == input) { return after[i]; } } return input; } int main() { char *input = \u0026#34;Wxgcg txgcg ui p ixgff, txgcg ui p epm. I gyhgwt mrl lig txg ixgff wrsspnd tr irfkg txui hcrvfgs, nre, hfgpig tcm liunz txg crt13 ra \\\u0026#34;ixgff\\\u0026#34; tr gntgc ngyt fgkgf.\u0026#34;; char *output = new char[strlen(input) + 1]; for (int i=0; i\u0026lt;strlen(input); i++) { output[i] = change(input[i]); } output[strlen(input)] = \u0026#39;\\0\u0026#39;; printf(\u0026#34;%s\\n\u0026#34;, output); return 1; } 好久没有用shell了，又写了个shell版本的解题方法，该问题可能有更简单的shell解决办法，我这里肯定写复杂了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #!/bin/bash result=\u0026#39;\u0026#39; function change() { result=$1 before=\u0026#39;pvwdgazxubqfsnrhocitlkeymj\u0026#39; after=\u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39; for ((i=0; i \u0026lt;= ${#before}; i++)) do if [[ ${before:${i}:1} = ${1} ]] then result=${after:${i}:1} break fi done } input=\u0026#39;Wxgcg txgcg ui p ixgff, txgcg ui p epm. I gyhgwt mrl lig txg ixgff wrsspnd tr irfkg txui hcrvfgs, nre, hfgpig tcm liunz txg crt13 ra \u0026#34;ixgff\u0026#34; tr gntgc ngyt fgkgf.\u0026#39; output=\u0026#39;\u0026#39; j=0 while [ \u0026#34;$j\u0026#34; -le ${#input} ] do change \u0026#34;${input:${j}:1}\u0026#34; output=\u0026#34;${output}${result}\u0026#34; j=$((j+1)) done echo ${output} 关于rol13的转码可以采用rot13这个网址来在线转码。\n穷举变量题 该题需要不断的请求url来获取最终的网址，我这里写一个shell脚本来穷举。\n1 2 3 4 5 6 7 8 #!/bin/bash res=2014 while [ ${#res} \u0026gt; 0 ] do res=`curl -s \u0026#34;http://fun.coolshell.cn/n/${res}\u0026#34;` echo $res done 得到答案tree\n参考 游戏页面 CoolShell puzzle game 攻略 我也不产生代码 \u0026ndash; Coolshell 谜题一游 ","date":"2014-08-20T00:00:00Z","permalink":"/coolshell_puzzle/","title":"coolshell博客解谜题游戏"},{"content":"Eclipse默认提供的主题过于刺眼，自己比较习惯黑色背景色。本文分享下修改Eclipse主题的步骤。我所使用的Eclipse版本为Kepler。相关的说明在插件的网站都有相关说明，本文不再赘述。安装完插件后的效果图如下：\n安装Eclipse Color Theme插件 该插件仅会更新编辑栏中的背景颜色和字体颜色等信息，不会修改Eclipse的整体主题颜色。该插件网址为：https://github.com/eclipse-color-theme/eclipse-color-theme。\n安装Eclipse UI Themes主题 该插件会更新除编辑栏之外的其他Eclipse背景色等信息，其Github地址为：http://eclipsecolorthemes.org/。\n相关下载 为了方便使用，我这里备份了插件放在百度网盘共享，相关插件下载地址：http://pan.baidu.com/s/1eQnDI86\n","date":"2014-06-23T00:00:00Z","permalink":"/post/eclipse_theme/","title":"修改eclipse主题"},{"content":"[TOC]\n本文研究了Linux下的一些网卡调优，用于提升网卡的性能。\n下文中的所有关于网络的参数可以在/etc/sysctl.conf文件中修改，如果没有相应的参数，可以添加。\n查看相应参数在当前运行机器的值可以通过/proc/sys/net/目录下的文件内容查看，也可以对该目录下相应文件的值进行修改，但是由于/proc目录下的文件全部位于内存中，修改的值不会保存到下次开机时。因此要修改参数的值可以通过修改/etc/sysctl.conf文件来完成。\n还可以通过sysctl -a命令来查看所有的系统配置参数。\nIP协议相关参数配置 net.ipv4.ip_default_ttl：设置从本机发出的ip包的生存时间，参数值为整数，范围为0～128，缺省值为64。如果系统经常得到“Time to live exceeded”的icmp回应，可以适当增大该参数的值，但是也不能过大，因为如果你的路由的环路的话，就会增加系统报错的时间。 TCP协议相关参数配置 TCP链接是有很多开销的，一个是会占用文件描述符，另一个是会开缓存，一般来说一个系统可以支持的TCP链接数是有限的。\nTCP常规参数 /proc/sys/net/ipv4/tcp_window_scaling：设置tcp/ip会话的滑动窗口大小是否可变。参数值为布尔值，为1时表示可变，为0时表示不可变。Tcp/ip 通常使用的窗口最大可达到65535字节，对于高速网络，该值可能太小，这时候如果启用了该功能，可以使tcp/ip滑动窗口大小增大数个数量级，从而提高数据传输的能力。\nnet.core.rmem_default：默认的接收窗口大小。\nnet.core.rmem_max：接收窗口的最大大小。\nnet.core.wmem_default：默认的发送窗口大小。\nnet.core.wmem_max：发送窗口的最大大小。\n/proc/sys/net/core/wmem_max。最大的TCP发送数据缓冲区大小。\n/proc/sys/net/ipv4/tcp_timestamps。时间戳在(请参考RFC 1323)TCP的包头增加10个字节，以一种比重发超时更精确的方法（请参阅 RFC 1323）来启用对 RTT 的计算；为了实现更好的性能应该启用这个选项。\n配置KeepAlive参数 这个参数的意思是定义一个时间，如果链接上没有数据传输，系统会在这个时间发一个包，如果没有收到回应，那么TCP就认为链接断了，然后就会把链接关闭，这样可以回收系统资源开销。（注：HTTP层上也有KeepAlive参数）对于像HTTP这样的短链接，设置一个1-2分钟的keepalive非常重要。\nnet.ipv4.tcp_keepalive_time：当keepalive打开的情况下，TCP发送keepalive消息的频率。缺省值为7200,即两小时，建议将其更改为1800。\nnet.ipv4.tcp_keepalive_probes：TCP发送keepalive探测以确定该连接已经断开的次数。(默认值是9，设置为5比较合适)。\nnet.ipv4.tcp_keepalive_intvl：探测消息发送的频率，乘以tcp_keepalive_probes就得到对于从开始探测以来没有响应的连接杀除的时间。(默认值为75秒，推荐设为15秒)\n配置建立连接参数 /proc/sys/net/ipv4/tcp_syn_retries：设置开始建立一个tcp会话时，重试发送syn连接请求包的次数。参数值为小于255的整数，缺省值为10。假如你的连接速度很快，可以考虑降低该值来提高系统响应时间，即便对连接速度很慢的用户，缺省值的设定也足够大了。\nnet.ipv4.tcp_retries1：建立一个连接的最大重试次数，默认为3,不建议修改。\n配置关闭连接参数 主动关闭的一方进入TIME_WAIT状态，TIME_WAIT状态将持续2个MSL(Max Segment Lifetime)，默认为4分钟，TIME_WAIT状态下的资源不能回收。有大量的TIME_WAIT链接的情况一般是在HTTP服务器上。\nnet.ipv4.tcp_retries2：普通数据的重传次数，在丢弃激活(已建立通讯状况)的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒)。\nnet.ipv4.tcp_tw_reuse：该文件表示是否允许重新应用处于TIME-WAIT状态的socket用于新的TCP连接。可以将其设置为1\nnet.ipv4.tcp_tw_recycle：打开快速TIME-WAIT sockets回收。默认关闭，建议打开。\nnet.ipv4.tcp_fin_timeout：在一个tcp会话过程中，在会话结束时，A首先向B发送一个fin包，在获得B的ack确认包后，A就进入FIN WAIT2状态等待B的fin包然后给B发ack确认包。这个参数就是用来设置A进入FIN WAIT2状态等待对方fin包的超时时间。如果时间到了仍未收到对方的fin包就主动释放该会话。参数值为整数，单位为秒，缺省为180秒，建议设置成30秒。\n网卡的参数设置 调整网卡的txqueuelen txqueuelen的涵义为网卡的发送队列长度，可以通过ifconfig命令找到网卡的txqueuelen参数配置，默认为1000,建议将其更改为5000。\n网卡的中断设置 网卡的读写是通过硬件的中断机制来实现的，默认网卡是中断在cpu的内核0上。CPU0非常重要，CPU0具有调整功能，如果CPU0利用率过高，其他cpu核心的利用率也会下降。因此可以考虑将linux的网卡中断绑定到其他的cpu内核上。\n可以通过/proc/interrupt文件内容来查看网卡在cpu核的中断情况。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 CPU0 CPU1 CPU2 CPU3 0: 16 0 0 0 IO-APIC-edge timer 1: 17144 2552 2312 2011 IO-APIC-edge i8042 5: 0 0 0 0 IO-APIC-edge parport0 8: 1 0 0 0 IO-APIC-edge rtc0 9: 0 0 0 0 IO-APIC-fasteoi acpi 19: 75011 144479 28826 16212 IO-APIC-fasteoi ata_piix, ata_piix 23: 214627 1 0 2 IO-APIC-fasteoi ehci_hcd:usb1, ehci_hcd:usb2 41: 0 0 0 0 PCI-MSI-edge xhci_hcd 42: 245675 11 17 6 PCI-MSI-edge eth0 43: 18 6 0 0 PCI-MSI-edge mei 44: 423481 80408 67559 54037 PCI-MSI-edge i915 45: 268 488 112 16 PCI-MSI-edge snd_hda_intel NMI: 21 18 23 18 Non-maskable interrupts LOC: 7076902 6761203 8711912 9042018 Local timer interrupts SPU: 0 0 0 0 Spurious interrupts PMI: 21 18 23 18 Performance monitoring interrupts IWI: 0 0 0 0 IRQ work interrupts RTR: 2 0 0 0 APIC ICR read retries RES: 365564 155654 52242 40754 Rescheduling interrupts CAL: 12775 15182 23713 23003 Function call interrupts TLB: 31028 26721 21584 25548 TLB shootdowns TRM: 0 0 0 0 Thermal event interrupts THR: 0 0 0 0 Threshold APIC interrupts MCE: 0 0 0 0 Machine check exceptions MCP: 32 32 32 32 Machine check polls ERR: 0 MIS: 0 可以看到eth0的中断是在CPU0上，可以通过/proc/irq/42/smp_affinity文件查看eth0默认的中断分配情况，文件的内容为1,对应二进制为0001,对应的为CPU0。\n要想修改中断分配方式，需要先停掉IRQ自动调节的服务进程。\n1 2 /etc/init.d/irqbalance stop echo \u0026#34;2\u0026#34; \u0026gt; /proc/irq/42/smp_affinity 这里的2表示将中断分配到CPU1上。\n双网卡负载均衡 两个网卡共用一个IP地址，中断用两个核，效率可以提升一倍。\n服务器的其他设置 修改服务器启动级别 可以通过runlevel命令查看机器的启动级别，带图形界面的启动级别为5。可以通过init 3来切换到启动级别3;可以修改/etc/inittab文件中id:5:initdefault:来修改默认级别。\n修改系统的文件描述符数 如果网络链接较多，可以修改每个进程打开的最大文件描述符数目，默认为1024.可以通过ulimit -a查看，可以通过ulimit -n 32768来修改。\n参考网页 性能调优攻略 Linux 内核网络参数配置资料 Linux 多核下绑定硬件中断到不同 CPU（IRQ Affinity） 计算 SMP IRQ Affinity 提高 Linux 上 socket 性能 ","date":"2014-06-14T00:00:00Z","permalink":"/post/linux_netcard_promote/","title":"Linux下的网卡速度提升方案"},{"content":"我的大部分读书时间都花在了技术类书籍上，对于扯淡类和鸡汤类的书籍不是我的最爱。本书受到了很多互联网从业人员的推荐，对于技术人员推荐的书籍感觉还是比较靠谱的，因此试着一读，读完之后果然收获颇多。吴军博士的很多观点都相当理性，很多对未来的预测也都在今天得以验证了。文章行为流畅，观点鲜明，能够窥见作者的写作功底。文中的一些观点非常值得记录和学习，本文主要是记录文中观点的摘要，并体现部分自己的理解。\n计算机行业发展规律 ##　摩尔定律\n摩尔定律：每18个月，计算机等IT产品的性能会翻一番，或者说相同性能的计算机等IT产品，每18个月价钱会降一半。\n摩尔定律对行业发展的影响主要体现在以下几个方面：\nIT公司必须在比较短的时间内完成下一代产品的开发。天下武功唯快不破，在互联网行业特别合适。 由于有了强有力的硬件支持，以前都不敢想的应用会不断涌现。手机行业正在深受摩尔定律的影响。 各个公司现在的研发必须针对多年后的市场。不知道google是否在研究PC版的操作系统来取代Windows了，不知道苹果公司内部是否在研究iphone10了？ 安迪-比尔定律 安迪-比尔定律：即比尔要拿走安迪所给的。安迪是原英特尔公司 CEO 安迪·格鲁夫（Andy Grove），比尔就是微软的创始人比尔·盖茨。在过去的二十年里，英特尔处理器的速度每十八个月翻一番，计算机内存和硬盘的容量以更快的速度在增长。但是，微软的操作系统等应用软件越来越慢，也越做越大。所以，现在的计算机虽然比十年前快了一百倍，运行软件感觉上还是和以前差不多。而且，过去整个视窗操作系统不过十几兆大小，现在要几千兆，应用软件也是如此。虽然新的软件功能比以前的版本强了一些，但是，增加的功能绝对不是和它的大小成比例的。因此，一台十年前的计算机能装多少应用程序，现在的也不过装这么多，虽然硬盘的容量增加了一千倍。更糟糕的是，用户发现，如果不更新计算机，现在很多新的软件就用不了，连上网也是个问题。而十年前买得起的车却照样可以跑。\n安迪-比尔定律把原本属于耐用消费品的电脑、手机等商品变成了消费性商品，刺激着整个IT领域的发展。\n反摩尔定律 反摩尔定律：Google的前CEO埃里克·施密特提出：如果你反过来看摩尔定律，一个IT公司如果今天和18个月前卖掉同样多的、同样的产品，它的营业额就要降一半。IT界把它称为反摩尔定律。反摩尔定理对于所有的IT 公司来讲，都是非常可悲的，因为一个IT 公司花了同样的劳动，却只得到以前一半的收入。反摩尔定理逼着所有的硬件设备公司必须赶上摩尔定理规定的更新速度。\n信息产业的规律性 70-20-10定律 当某个领域发展成熟后（而不是群雄争霸时期），一般在全球容不下三个以上的主要竞争者。这个行业一定有一个老大，斯库利把它比喻成一个猴王，它是这个行业的主导者。毫无疑问，它虽然想顺顺当当地统领好整个行业，就像猴王想让猴子们永远臣服一样，但是，它一定会遇到一两个主要的挑战者，也就是老二（也许还有一个老三）。剩下来的是一大群小商家，就像一大群猴子。老大是这个领域的主导者，不仅占据着超过一半，通常是百分之六七十的市场，并且制定了这个领域的游戏规则。老二有自己稳定的百分之二三十的市场份额，有时也会挑战老大并给老大一些颜色看看，但是总的来讲是受老大欺负的时间多。剩下的一群小猴子数量虽然多，但是却只能占到百分之十甚至更少的市场，它们基本上唯老大马首是瞻。老大总是密切注视着老二，并时不时地打压它，防止它做大。老大和老二通常都不会太在意剩下的小企业，这样就让这一群小的企业能有挣一些小钱的地方。\n诺维格定律 谷歌研究院院长彼得.诺威格博士说，当一个公司的市场占有率超过50% 以后，就不要 再指望在市场占有率上翻番了。\n基因决定定律 一个在某个领域特别成功的大公司一定已经被优化得非常适应这个市场，它的文化、做事方式、商业模式、市场定位等等已经非常适应，甚至过分适应自己传统的市场。这使得该公司获得成功的内在因素会渐渐地、深深地植入该公司，可以讲是这个公司的基因。\n基因对于一个公司非常的重要，会影响到公司的整个发展历程。济南的软件公司都是些传统行业的软件公司，在过去十年的时间中发展还算可以。因为行业软件是销售为王，只要销售搞的好，只要不是特别烂的技术都能混过去。现在随着互联网的兴起，很多企业开始转型搞互联网应用，却没有一家搞的起来，到现在济南都没有一家真正意义上的互联网公司，如果有请你告诉我。这些从本质上来说还是基因决定定律的影响。\n最佳商业模式 Google的广告系统 在这台印钞机里，运营的成本就是数据中心的费用和带宽的费用，而间接的成本则是打 造和改进这个印钞机的研发费用。在这台印钞机中，自动化程度必须到达一个阈值，它才能自动运转起来。而当它的自动化越高，成本就越低。\nEbay 和亚马逊的在线市场 提供在线交易和支付平台，买卖双方自由交易，赚取手续费用和少量提成。中国的淘宝也基本属于这种模式。\n戴尔的虚拟工厂 一个传统的制造业需要通过产品设计、原料采购、仓储运输、加工制造、订单处理、批 发经营和零售七个环节才能收回投资、获得利润。戴尔将上述七个环节减少到两个，仅留订单处理和零售。\n腾讯的虚拟物品和服务 销售虚拟商品商业模式必须解决好两个问题：虚拟商品的使用价值和虚拟社交网站中的用户不但是虚拟商品的消费者，还是它们的创造者。解决好上述两个问题，只要累计起足够多的用户基数，虚拟商品就成了社交网站的印钞机。\n下一个Google 微软是一家软件公司，百度是一家区域性的互联网公司，而Google更多地是一家科技公司。\n下一个Google不可能是搜索公司，而且不太可能是现在意义上依靠广告挣钱的互联网公司，因为这个互联网广告产业不够养活一个像Google这么大的公司。\n替代能源和电池在近期很难诞生一个Google这样的公司。\n生物和医疗技术有全社会的需求，但是创新周期特别长，加上法律上的风险，不可能在短期内核诞生下一个Google。\n在中国和一些亚洲国家，电子商务的潜力可以早就出Google这样的大型新型公司。\n如果通过云计算诞生一两个千亿美元的公司，首先会是Google自己，其次是控制开发平台的Facebook，而不会是新面孔。\n相关下载 非常精美的读书笔记PPT\n","date":"2014-06-02T00:00:00Z","permalink":"/post/on_top_of_tides/","title":"浪潮之巅读书笔记"},{"content":"在Linux下会父进程通过fork()出的子进程可能会由于某种原因死锁或睡眠而无法终止，这时候需要父进程杀死子进程。本程序是父进程检测到子进程运行一段时间后杀死子进程的例子。\n父进程的检测代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; int main(int argc, char *argv[]) { int pid = fork(); if (pid \u0026gt; 0) { // parent process int count = 0; while (1) { pid_t result = waitpid(pid, NULL, WNOHANG); if (result == 0) { printf(\u0026#34;child process is running\\n\u0026#34;); count++; if (count \u0026gt;= 60 * 60) // one hour { kill(pid, 9); // kill child process } } else if (result == pid) { printf(\u0026#34;child process has exit\\n\u0026#34;); break; } else { printf(\u0026#34;result=%d\\n\u0026#34;, result); } sleep(1); } } else if (pid == 0) { // child process execv(\u0026#34;/home/kuring/source/child\u0026#34;, NULL); _exit(1); } else { printf(\u0026#34;fork() error\\n\u0026#34;); return -1; } return 0; } 子进程调用execv()函数执行的child代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(int argc, char *argv[]) { while (1) { sleep(1); } return 1; } 例子比较简单，不作过多解释。\n","date":"2014-05-21T00:00:00Z","permalink":"/post/linux_kill_child_process/","title":"Linux下父进程定期杀死超时子进程的例子"},{"content":"在外地出差，趁着午休时间躺在机房走廊的地板上想小憩一会却怎么也进入不了状态。突然写着可以写写我心中目前的理想工作状态。随着年龄、阅历和知识水平的增长，每个人心中理想的工作状态也会在变。我现在的理想工作状态跟我刚毕业时肯定是有区别的，刚毕业那会初入茅庐，对公司的很多事情还是懵懂，自然期望找个高手带一带。\n我期望加入一家技术为王的公司。\n技术才是科技公司真正的核心竞争力，而非销售和公关能力。从硅谷中做大做强的公司哪个不是以技术作为领先竞争对手的筹码，十几年前的google的崛起、雅虎的衰落，近几年Github的火爆无不是技术因素在其决定作用。在中国软件行业还未发展成熟，觉大多数企业仍然在以销售为核心盈利方式的畸形发展方式中，但技术为王绝对是个趋势，这个不是靠关系靠政策可以改变的事实，这个是大势所趋、历史必然。\n我不奢望技术为王的公司中牛人辈出但是期望能多几个牛人就多几个牛人，至少不期望在公司中见到一看就没有做技术潜质的同事（一个人适不适合做技术有时候是可以看出来的）。\n我期望加入一家互联网行业的公司。\n软件行业按照产品类型大致可以分三类：外包、卖产品和做平台。\n外包公司完全以订单为导向，往往涉及到复杂的业务逻辑，技术含量低，自然不在我的考虑范围内。\n我之前从事的公司都是在卖产品的公司，往往卖产品的公司需要有一定的技术含量，但是技术不能决定公司发展，这一类公司是以销售为导向的公司，一套产品往往可以卖上多年，甚至十几年也不奇怪。\n做平台的公司往往不需要过多的销售，大多数是以用户量为王的，而要想拉拢用户到平台上往往需要靠技术或销售取胜，而技术因素相对更加关键。以Dota对战平台为例，前几年比较火爆的是浩方和VS，现在成了11对战平台的天下了。11对战平台后来者居上就是因为11对战平台率先推出了天梯模式和路人模式，从而落下了竞争对手一大截。互联网行业属于第三类，而济南几乎找不到这类公司。就算有一些这样的公司，往往也是以销售为导向的公司。韩都衣舍也有自己的网站，但是流量应该没有淘宝大，估计网站技术上的投入也一般般。银座集团在电商火爆了后也搞了个网上商城，单从界面看可跟苏宁拉在同一个档次，但效果如何影响怎么样可想而知，估计技术都是外包的，仅是领导拍拍脑袋的产物。我实在想不到在济南有什么互联网公司了，如果你知道请告诉我。\n我期望加入一家创业型的公司。\n我现在还不具备单独创业的实力，但我一直梦想能够加入一家有梦想有激情的创业公司。加入创业公司就意味着不会清闲，我只期望每天能够过的充实并快乐着。《黑客与画家》中提到快速致富的手段就是加入一家创业公司，在美国如此，在中国亦如此。\n总结一下，我想加入一家技术为王的互联网行业的创业公司。如果你知道在济南有哪家公司可以满足我的要求，请告诉我，谢谢！\n","date":"2014-05-20T00:00:00Z","permalink":"/post/my_perfect_work/","title":"我目前的理想工作状态"},{"content":"本实验为在虚拟机环境中实验，操作系统为Red Hat Enterprise6.0 32位，当前网卡列表如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@localhost ~]# ifconfig eth1 Link encap:Ethernet HWaddr 00:0C:29:8C:58:06 inet addr:192.168.124.140 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe8c:5806/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:123 errors:0 dropped:0 overruns:0 frame:0 TX packets:57 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:18770 (18.3 KiB) TX bytes:11684 (11.4 KiB) Interrupt:19 Base address:0x2024 eth2 Link encap:Ethernet HWaddr 00:50:56:3F:B3:90 inet addr:192.168.124.141 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::250:56ff:fe3f:b390/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:111 errors:0 dropped:0 overruns:0 frame:0 TX packets:43 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:16101 (15.7 KiB) TX bytes:10343 (10.1 KiB) Interrupt:19 Base address:0x20a4 目的为将网卡eth1更改为eth0，将eth2更改为eth3。\n修改grub.conf文件 在文件中内核启动时增加_biosdevname=0_选项。修改后的文件内容如下：\n1 2 3 4 5 6 7 8 default=0 timeout=5 splashimage=(hd0,0)/grub/splash.xpm.gz hiddenmenu title Red Hat Enterprise Linux (2.6.32-71.el6.i686) root (hd0,0) kernel /vmlinuz-2.6.32-71.el6.i686 ro root=/dev/mapper/VolGroup-lv_root rd_LVM_LV=VolGroup/lv_root rd_LVM_LV=VolGroup/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=zh_CN.UTF-8 KEYBOARDTYPE=pc KEYTABLE=us nomodeset crashkernel=auto rhgb quiet biosdevname=0 initrd /initramfs-2.6.32-71.el6.i686.img 更改网卡配置文件内容和文件名称 在/etc/sysconfig/network-scripts目录中将原有的网卡配置文件ifcfg_Auto_eth1和ifcfg_Auto_eth2更改为ifcfg_eth0和ifcfg_eth3，同时修改文件的内容，将文件的内容中的网卡设备名称进行替换。替换后的文件ifcfg_eth0内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 TYPE=Ethernet BOOTPROTO=dhcp DEFROUTE=yes IPV4_FAILURE_FATAL=yes IPV6INIT=no NAME=\u0026#34;Auto eth0\u0026#34; UUID=995d037e-3b65-4490-a1fa-f26f6abf066d ONBOOT=yes HWADDR=00:0C:29:8C:58:06 PEERDNS=yes PEERROUTES=yes DEVICE=eth0 删除70-persistent-net.rules文件 该文件存在于/etc/udev/rules.d目录下。该文件如果不存在，开始时会自动创建，里面包含了网卡名称的配置信息。\n在修改完上述内容后重新启动机器配置就修改过来了,修改完成之后的网卡配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@localhost rules.d]# ifconfig eth0 Link encap:Ethernet HWaddr 00:0C:29:8C:58:06 inet addr:192.168.124.140 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe8c:5806/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:87 errors:0 dropped:0 overruns:0 frame:0 TX packets:75 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:17174 (16.7 KiB) TX bytes:14520 (14.1 KiB) Interrupt:19 Base address:0x2024 eth3 Link encap:Ethernet HWaddr 00:50:56:3F:B3:90 inet addr:192.168.124.141 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::250:56ff:fe3f:b390/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:72 errors:0 dropped:0 overruns:0 frame:0 TX packets:76 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:14164 (13.8 KiB) TX bytes:14955 (14.6 KiB) Interrupt:19 Base address:0x20a4 ","date":"2014-05-02T00:00:00Z","permalink":"/post/linux_change_netcardname/","title":"Linux更改网卡名称"},{"content":"[TOC]\n＃ 安装kdevelop\n确保可以上网，这里采用yum的安装方式进行安装。 首先执行命令：yum install kdevelop，会出现如下提示：\n1 2 3 4 5 6 Loaded plugins: rhnplugin, security This system is not registered with RHN. RHN support will be disabled. file:///mnt/file/rh5/Cluster/repodata/repomd.xml: [Errno 5] OSError: [Errno 2] 没有那个文件或目录: \u0026#39;/mnt/file/rh5/Cluster/repodata/repomd.xml\u0026#39; Trying other mirror. Error: Cannot retrieve repository metadata (repomd.xml) for repository: Cluster. Please verify its path and try again 出现上述错误是由于redhat没有注册，所有不能使用它自身的源进行更新，可以更换为CentOS系统的源进行更新，操作步骤为： 1、进入/etc/yum.repos.d/目录。在命令行输入：wget http://docs.linuxtone.org/soft/lemp/CentOS-Base.repo。 2、ls 一下，会看到一个文件名为CentOS-Base.repo的文件。 3、将原来的文件rhel-debuginfo.repo改名为rhel-debuginfo.repo.bak。 4、将CentOS-Base.repo改名为rhel-debuginfo.repo\n再次运行命令：yum install kdevelop，就可以安装kdevelop了。\n安装过程中遇到了需要的pcre包无法从centos的源中下载的问题，解决方法为根据yum命令无法下载的包，在google中搜索，下载包然后再redhat上用rpm的升级命令来安装。具体下载网址为：电子科技大学星辰工作室开源镜像服务。\nrpm相关命令： 安装一个包：rpm -ivh 升级一个包：rpm -Uvh\n移走一个包：rpm -e\n安装konsole 安装上kdevelop后在执行程序的时候会提示/bin/sh:konsole:command not found。执行yum install kdebase命令来安装konsole。\n配置ssh服务 修改ssh服务的配置文件/etc/ssh/sshd_config文件，将文件中的#PasswordAuthentication yes注释打开。 修改ssh服务的配置文件/etc/ssh/sshd_config文件，将文件中的PermitRootLogin no更改为yes。这样即可以用ssh工具连接到该机器。\nxmanager连接配置 该部分参考文档的网址为：http://blog.csdn.net/gltyi99/article/details/6141972\n修改/usr/share/gdm/defaults.conf文件的权限，默认权限为444，chmod 700 /usr/share/gdm/defaults.conf。 在/usr/share/gdm/defaults.conf文件的末尾添加如下内容： 1 2 3 4 5 6 Enable=true DisplaysPerHost=10 Port=177 AllowRoot=true AllowRemoteroot=true AllowRemoteAutoLogin=false 修改/etc/gdm/custom.conf文件 1 2 [xdmcp] Enable=1 修改/etc/inittab文件，不修改原来的设置，在文件的最后增加一行： 1 x:5:respawn:/usr/sbin/gdm 修改/usr/share/gdm/defaults.conf文件，将其中的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [security] # Allow root to login. It makes sense to turn this off for kiosk use, when # you want to minimize the possibility of break in. AllowRoot=true # Allow login as root via XDMCP. This value will be overridden and set to # false if the /etc/default/login file exists and contains # \u0026#34;CONSOLE=/dev/login\u0026#34;, and set to true if the /etc/default/login file exists # and contains any other value or no value for CONSOLE. AllowRemoteRoot=false # This will allow remote timed login. AllowRemoteAutoLogin=false # 0 is the most restrictive, 1 allows group write permissions, 2 allows all # write permissions. RelaxPermissions=0 # Check if directories are owned by logon user. Set to false, if you have, for # example, home directories owned by some other user. CheckDirOwner=true # Number of seconds to wait after a failed login #RetryDelay=1 # Maximum size of a file we wish to read. This makes it hard for a user to DoS # us by using a large file. #UserMaxFile=65536 AllowRemoteRoot=false更改为AllowRemoteRoot=true。\n修改/etc/securetty文件，在文件底部添加如下内容： 1 2 3 4 5 pts/0 pts/1 pts/2 pts/3 pts/4 修改/etc/pam.d/login文件，将其中的一行注释 1 #auth [user_unknown=ignore success=ok ignore=ignore default=bad] pam_securetty.so 修改/etc/pam.d/remote，将其中的一行注释 1 #auth required pam_securetty.so 修改/etc/xinetd.d/krb5-telnet文件，将文件内容由 1 2 3 4 5 6 7 8 9 10 service telnet { flags = REUSE socket_type = stream wait = no user = root server = /usr/kerberos/sbin/telnetd log_on_failure += USERID disable = yes } 更改为：\n1 2 3 4 5 6 7 8 9 10 service telnet { flags = REUSE socket_type = stream wait = no user = root server = /usr/kerberos/sbin/telnetd log_on_failure += USERID disable = no } 同样将/etc/xinetd.d/ekrb5-telnet文件中的disable=yes更改为disable=no。\n安装中文字体 为了阅读代码方便，安装字体。\n将字体文件YaHei.Consolas.1.12.ttf放到Redhat的目录/usr/share/fonts/chinese/TrueType目录下 执行mkfontscale命令，重新生成fonts.scale文件 执行mkfontdir命令，重新生成了fonts.dir文件。 执行chkfontpath \u0026ndash;add /usr/share/fonts/chinese/TrueType 更改操作系统编码从utf8到gb18030 可以通过locale命令来查看操作系统编码。输出如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 LANG=zh_CN.UTF-8 LC_CTYPE=\u0026#34;zh_CN.UTF-8\u0026#34; LC_NUMERIC=\u0026#34;zh_CN.UTF-8\u0026#34; LC_TIME=\u0026#34;zh_CN.UTF-8\u0026#34; LC_COLLATE=\u0026#34;zh_CN.UTF-8\u0026#34; LC_MONETARY=\u0026#34;zh_CN.UTF-8\u0026#34; LC_MESSAGES=\u0026#34;zh_CN.UTF-8\u0026#34; LC_PAPER=\u0026#34;zh_CN.UTF-8\u0026#34; LC_NAME=\u0026#34;zh_CN.UTF-8\u0026#34; LC_ADDRESS=\u0026#34;zh_CN.UTF-8\u0026#34; LC_TELEPHONE=\u0026#34;zh_CN.UTF-8\u0026#34; LC_MEASUREMENT=\u0026#34;zh_CN.UTF-8\u0026#34; LC_IDENTIFICATION=\u0026#34;zh_CN.UTF-8\u0026#34; LC_ALL= 打开/etc/sysconfig/i18n文件，文件默认内容为：\n1 LANG=\u0026#34;zh_CN.UTF-8\u0026#34; 将文件内容修改为：\n1 LANG=\u0026#34;zh_CN.GBK\u0026#34; 相关下载 最适合程序员的字体：微软雅黑+Consolas\n","date":"2014-05-01T00:00:00Z","permalink":"/post/redhat_setup_base/","title":"Redhat安装完成之后的设置"},{"content":"声明和定义 声明将一个名字引入到了程序中，；定义提供了一个实体（如类型、实例、函数）在程序中的唯一描述，为变量分配存储空间；\n在一个给定的作用于中重复一个给定的声明是合法的，而重复定义是非法的。声明仅对当前编译单元有效，仅会在.o文件中加入了一个未定义符号。特定的类型不必与实际的定义类型匹配。\n内部链接和外部链接 内部链接和外部链接是根据链接过程中一个符号是在编译单元内部还是外部进行划分的，编译单元是按照一个.c或.cpp文件的作用域进行划分的。\n内部链接：一个标识符对于编译单元（一个目标文件）来说是局部的，并在链接时与其他编译单元中的标识符不冲突。\n内部链接包括：\nstatic类型的变量、函数 const类型的变量 枚举类型的定义 typedef定义的类型 class、struct、union的定义 inline函数 外部链接：在多文件程序中，链接时这个标识符可以和其他编译单元交互。这些外部符号在程序中必须是唯一的，用来被其他编译单元中未定义的符号访问。\n将带有外部链接的定义放在头文件中是错误的。\n外部链接包括：\n类的非内联成员函数 非内联且非static函数 类的static数据成员 const类型的变量为内部链接的实例 test1.h内容如下：\n1 void print1(); test1.cpp内容如下：\n1 2 3 4 5 6 7 8 #include \u0026lt;stdio.h\u0026gt; const int max_length = 256; void print1() { printf(\u0026#34;max_length=%d\\n\u0026#34;, max_length); } test2.h内容如下：\n1 void print2(); test2.cpp内容如下：\n1 2 3 4 5 6 7 8 #include \u0026lt;stdio.h\u0026gt; const int max_length = 128; void print2() { printf(\u0026#34;max_length=%d\\n\u0026#34;, max_length); } main.cpp内容如下：\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;stdio.h\u0026gt; #include \u0026#34;test1.h\u0026#34; #include \u0026#34;test2.h\u0026#34; int main() { print1(); print2(); return 1; } 从实例可以看出在全局作用域内声明的const类型的变量为内部链接的，在多个文件的作用域中分别定义相同的const类型的变量并不会产生冲突。\n基本规则 直接访问类的数据成员违反了面向对象中的封装原则，应该将类的数据成员私有化，并提供接口供部访问，类似于java bean规范。\n避免使用全局变量，应该将全局变量封装到类中，并提供接口供外部访问。\n为避免命名冲突，将全局函数封装到类中。\n避免在头文件的作用域中使用enum、typedef和const数据，应该将这些数据封装到头文件的类作用域中。\n在头文件中只应该包含如下内容：类、结构体、联合体和运算符函数的声明，类、结构体、联合体和内联函数的定义。\n","date":"2014-04-14T00:00:00Z","permalink":"/post/large_scale_c-_software_desigin_part_1/","title":"大规模C++程序设计第1部分读书笔记"},{"content":"本文为Linux设备驱动程序的入门实践文章，编写一个hello world程序，并在Linux上执行。\n编写驱动程序 驱动程序hello.c文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #ifndef __KERNEL__ # define __KERNEL__ #endif #ifndef MODULE # define MODULE #endif #include \u0026lt;linux/kernel.h\u0026gt; #include \u0026lt;linux/module.h\u0026gt; #include \u0026lt;linux/init.h\u0026gt; MODULE_LICENSE(\u0026#34;GPL\u0026#34;); int hello_init() { printk(KERN_WARNING \u0026#34;Hello kernel!\\n\u0026#34;); return 0; } void hello_exit() { printk(\u0026#34;Bye, kernel!\\n\u0026#34;); } module_init(hello_init); module_exit(hello_exit) 编写Makefile Makefile文件的写法可以采用传统的make方式，也可以采用kbuild的方式。\n采用传统的make方式的写法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ifeq ($(KERNELRELEASE),) KERNELDIR ?= /lib/modules/$(shell uname -r)/build PWD := $(shell pwd) modules: $(MAKE) -C $(KERNELDIR) M=$(PWD) modules modules_install: $(MAKE) -C $(KERNELDIR) M=$(PWD) modules_install clean: rm -rf *.o *~ core .depend .*.cmd *.ko *.mod.c .tmp_versions .PHONY: modules modules_install clean else obj-m := hello.o endif 采用kbuild方式的Makefile内容如下：\n1 2 3 4 5 6 7 obj-m := hello.o all : $(MAKE) -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules clean: $(MAKE) -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean 编译 将hello.c和Makefile文件放在任意目录中，执行make命令编译。\n安装 执行insmod hello.ko命令安装驱动程序，通过lsmod命令即可看到驱动程序已经安装。\n通过查看/var/log/messages文件即可看到hello驱动程序打印的内容。\n卸载 执行rmmod hello.ko命令即可卸载驱动程序模块。\n参考文章 《深入理解Linux设备驱动程序》 《Linux那些事之我是USB》 Ubuntu12.10 内核源码外编译 linux模块\u0026ndash;编译驱动模块的基本方法\n","date":"2014-04-02T00:00:00Z","permalink":"/post/linux_driver_hello_world/","title":"Linux设备驱动程序实例之hello world"},{"content":"本文仅为了练习Linux内核源码的编译安装，安装环境为VMWare下的CentOS，现有CentOS版本为2.6.32-358.el6.x86_64。 /boot/grub/grub.conf文件内容如下：\n1 2 3 4 5 6 7 8 9 # 注释部分去掉 default=0 timeout=5 splashimage=(hd0,0)/grub/splash.xpm.gz hiddenmenu title CentOS (2.6.32-358.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=/dev/mapper/vg_livedvd-lv_root rd_NO_LUKS rd_LVM_LV=vg_livedvd/lv_root rd_NO_MD crashkernel=auto rd_LVM_LV=vg_livedvd/lv_swap KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM LANG=zh_CN.UTF-8 rhgb quiet initrd /initramfs-2.6.32-358.el6.x86_64.img 获取内核源码 首先从Linux的官方网站下载最新版内核Linux3.13。\n执行tar Jxvf linux-3.13.tar.xz -C/usr/src/kernels命令将内核源码解压到内核源代码存放目录/usr/src/kernels/，该源码目录并不固定，但推荐将内核源码存放到该目录下。\n为了将上次编译时的目标文件及相关设置文件删除，执行make mrproper。\n挑选功能 可以采用了多种方式，这里采用make menuconfig的方式来挑选内核功能，该方式不需要X Window（make xconfig方式）的支持，而且要比纯命令行方式（make config）要直观。执行make menuconfig遇到如下错误：\n1 2 3 4 5 6 7 8 9 10 11 [root@localhost linux-3.13]# make menuconfig HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o *** Unable to find the ncurses libraries or the *** required header files. *** \u0026#39;make menuconfig\u0026#39; requires the ncurses libraries. *** *** Install ncurses (ncurses-devel) and try again. *** make[1]: *** [scripts/kconfig/dochecklxdialog] 错误 1 make: *** [menuconfig] 错误 2 这是因为需要ncurses库的支持，下面采用从源码安装的方式安装ncurses。\n从ncurses的官方网站下载最新版的ncurses-5.9.tar.gz。然后分别执行./configure、make、make install`命令安装。\n更改内核版本号标识 为了能够在编译完成后的内核版本中通过uname -r看到定义的内核版本号，修改Makefile文件。其中EXTRAVERSION字段值为空，将其赋值为kuring。\n编译内核 执行make命令，该过程需要话费很长时间，我在512MB的VM下跑，花费了大约1个半小时时间。\n编译内核模块 执行make modules命令。\n安装内核模块 执行make modules_install命令，会将内核模块安装到/lib/modules目录下。\n安装内核 执行make install命令，产生如下输出：\n1 2 3 4 5 6 sh /usr/src/kernels/linux-3.13/arch/x86/boot/install.sh 3.13.0kuring arch/x86/boot/bzImage \\ System.map \u0026#34;/boot\u0026#34; ERROR: modinfo: could not find module vmhgfs ERROR: modinfo: could not find module vsock ERROR: modinfo: could not find module vmware_balloon ERROR: modinfo: could not find module vmci 这个错误跟vmware的vmware tools有关，暂时不去管。\n这样再去看/boot/grub/grub.conf文件，会看到文件已经变化，已经将新内核添加到开机启动项中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 注释部分去掉 default=1 timeout=5 splashimage=(hd0,0)/grub/splash.xpm.gz hiddenmenu title CentOS (3.13.0kuring) root (hd0,0) kernel /vmlinuz-3.13.0kuring ro root=/dev/mapper/vg_livedvd-lv_root rd_NO_LUKS rd_LVM_LV=vg_livedvd/lv_root rd_NO_MD crashkernel=auto rd_LVM_LV=vg_livedvd/lv_swap KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM LANG=zh_CN.UTF-8 rhgb quiet initrd /initramfs-3.13.0kuring.img title CentOS (2.6.32-358.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=/dev/mapper/vg_livedvd-lv_root rd_NO_LUKS rd_LVM_LV=vg_livedvd/lv_root rd_NO_MD crashkernel=auto rd_LVM_LV=vg_livedvd/lv_swap KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM LANG=zh_CN.UTF-8 rhgb quiet initrd /initramfs-2.6.32-358.el6.x86_64.img 同时在/boot目录下已经多出了vmlinuz-3.13.0kuring、System.map-3.13.0kuring、initramfs-3.13.0kuring.img文件。\n重启系统后，在启动菜单中多出了新内核选项。进入新内核后，执行uname -r显示3.13.0kuring，说明新内核已经安装完成。\n","date":"2014-04-01T00:00:00Z","permalink":"/post/linux_kernel_setup/","title":"Linux下内核编译安装"},{"content":"在用CentOS默认的svn客户端工具访问Windows下搭建的subversion时会提示如下错误：\n1 2 [kuring@localhost 桌面]$ svn checkout https://192.168.100.100/svn/test svn: 方法 OPTIONS 失败于 “https://192.168.100.100/svn/test: SSL handshake failed: SSL 错误：Key usage violation in certificate has been detected. (https://192.168.100.100) 通过执行如下命令可以看到svn是支持https协议的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [kuring@localhost ~]$ svn --version svn，版本 1.6.11 (r934486) 编译于 Apr 11 2013，16:13:51 版权所有 (C) 2000-2009 CollabNet。 Subversion 是开放源代码软件，请参阅 http://subversion.tigris.org/ 站点。 此产品包含由 CollabNet(http://www.Collab.Net/) 开发的软件。 可使用以下的版本库访问模块: * ra_neon : 通过 WebDAV 协议使用 neon 访问版本库的模块。 - 处理“http”方案 - 处理“https”方案 * ra_svn : 使用 svn 网络协议访问版本库的模块。 - 使用 Cyrus SASL 认证 - 处理“svn”方案 * ra_local : 访问本地磁盘的版本库模块。 - 处理“file”方案 这是由于svn客户端在https协议中使用了GnuTLS库造成的，将其更改为使用openssl库即可。通过执行如下命令可以查看svn使用的库：\n1 2 3 [kuring@localhost bin]$ ldd svn | grep ssl [kuring@localhost bin]$ ldd svn | grep tls libgnutls.so.26 =\u0026gt; /usr/lib64/libgnutls.so.26 (0x00007f33004ad000) 下面选择重新编译的方式来安装svn。\n删除subversion 执行：yum remove subversion\n检查openssl安装情况 这里已经安装：\n1 2 3 [kuring@localhost tmp]$ rpm -qa | grep openssl openssl-1.0.1e-15.el6.x86_64 openssl-devel-1.0.1e-15.el6.x86_64 安装neon 这里选择的安装版本为0.29.6，subversion对neon的版本有要求。如果不是subversion的版本，在执行subversion下的configure文件时并不会报错\n1 2 3 4 5 [kuring@localhost software]$ tar zvxf neon-0.29.6.tar.gz [kuring@localhost software]$ cd neon-0.29.6 ./configure --with-ssl=openssl make make install 安装apr 1 2 3 4 5 tar zvxf apr-1.5.0.tar.gz cd apr-1.5.0 ./configure make make install 安装apr-util 1 2 3 4 5 tar zvxf apr-util-1.5.3.tar.gz cd apr-util-1.5.3 ./configure --with-apr=/usr/local/apr make make install 下载sqllite 1 2 3 unzip sqlite-amalgamation-3080401.zip mv sqlite-amalgamation-3080401 sqlite-amalgamation mv sqlite-amalgamation subversion-1.8.8/\t// 将其复制到subversion源码目录下 安装subversion 1 2 3 4 tar zvxf subversion-1.7.16.tar.gz ./configure --with-ssl --with-neon --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr make make install 然后再执行svn --version命令可以看到已经包含了https协议。\n参考资料 SSL handshake failed: SSL error: Key usage violation in certificate has been detected CentOS\n资料下载 需要的安装包下载\n","date":"2014-03-22T00:00:00Z","permalink":"/post/centos6.5_svn/","title":"CentOS6.5下安装svn客户端软件"},{"content":"从官方网站下载最新的viplugin插件：viPlugin_2.12.0。\n将viPlugin_2.12.0.zip文件解压到eclipse安装目录下的dropins目录下。\n该插件为收费插件，在eclipse安装的根目录下新建viPlugin2.lic文件，文件内容为:_q1MHdGlxh7nCyn_FpHaVazxTdn1tajjeIABlcgJBc20 _。\n重启eclipse后即可生效。\n官方参考文档地址：http://www.viplugin.com/files/User_Manual_viPlugin.pdf\n相关下载：viPlugin_2.12.0.zip\n","date":"2014-03-22T00:00:00Z","permalink":"/post/eclipse_viplugin/","title":"在Eclipse中安装viplugin插件"},{"content":"[TOC]\n在Linux操作系统中，每个文件都有一组9个权限位来控制谁能够读写和执行该文件的内容。这组权限对于文件的意义非常容易理解。但是对于目录而言就不是那么容易理解了。要想搞明白权限机制需要了解文件系统中的inode节点和block等概念，并大致了解文件系统的内部实现原理。\n文件的权限 这部分比较容易理解。比较容易搞错的为：\nw位：包含对文件的添加、修改该文件内容的权限，但不包含删除文件或移动该文件的权限，因为文件的文件名信息存储在父目录的block中，而不是在该文件的inode节点中。父目录的block中包含该文件的inode节点和文件名信息，通过父目录中的inode节点找到该文件。\n目录的权限 r位：表示具有读取该目录列表的权限。\nw位：对该文件夹下创建新的文件或目录进行增加、删除、修改操作。\nx位：这个稍微难以理解。表示用户能否进入该目录成为工作目录，即是否可以cd到该目录。通常和r位组合一块使用。\n例子一 假设root用户对testing目录和testing目录下的testing文件拥有的如下权限：\n1 2 3 [root@localhost tmp]# ls -ald testing/ testing/testing drwxr--r--. 2 root root 4096 3月 1 11:44 testing/ -rw-------. 1 root root 0 3月 1 11:44 testing/testing kuring用户拥有对该文件夹的读权限，但是没有x权限。当kuring用户访问时会提示如下内容：\n1 2 3 4 5 6 7 8 9 // 拥有r的权限可以查询文件名，但是没有x权限，不能读取除文件名外的其他信息，产生了问号。 [kuring@localhost tmp]$ ll testing/ ls: 无法访问testing/testing: 权限不够 总用量 0 -????????? ? ? ? ? ? testing // 没有x权限，不能进入该目录 [kuring@localhost tmp]$ cd testing/ bash: cd: testing/: 权限不够 例子二 在上述例子中，给kuring用户增加对目录testing的rwx权限，却只拥有testing目录下的testing文件的r权限。权限情况如下所示：\n1 2 3 [kuring@localhost tmp]$ ls -ald testing testing/testing drwxr--rwx. 2 root root 4096 3月 1 13:29 testing -rw-r--r--. 1 root root 0 3月 1 13:29 testing/testing kuring用户执行如下操作：\n1 2 3 4 5 6 7 8 9 10 // 当对文件进行更改时由于没有对文件的w权限，操作失败 [kuring@localhost tmp]$ echo \u0026#34;world\u0026#34; \u0026gt;\u0026gt; testing/testing bash: testing/testing: 权限不够 // 当对文件进行删除操作时却可以删除该文件，这是因为文件删除操作的权限是由该文件所在目录的w位决定的 // 文件删除操作会修改父目录中block节点中的文件名内容，而父目录的权限为rx，不可写。 [kuring@localhost tmp]$ rm testing/testing rm：是否删除有写保护的普通文件 \u0026#34;testing/testing\u0026#34;？y [kuring@localhost tmp]$ ls testing/ [kuring@localhost tmp]$ 只有了解了原理，就可以理解在多级目录并且目录的权限不一致的情况下相应的权限问题了。\n进程用户ID 要讲解set uid和set gid，就涉及到进程的用户ID概念。用户ID又可以分为两部分：\n实际用户ID和实际组ID：标识了究竟是哪个用户执行了该程序，跟命令行中的登录用户一致，可以通过id命令查看。\n有效用户ID和有效组ID：系统通过有效用户ID、有效组ID和附加组ID来决定进行对系统资源的访问权限。在Linux系统中一个用户可以属于多个组，在/etc/passwd文件中一个用户仅能标识出隶属于一个组ID，该组ID叫做默认的组ID。在/etc/group为文件中可以标识出一个用户隶属于多个组，这多个组中除去默认的组ID叫做附加组ID。\nset uid 该权限位仅对二进制文件有效 执行者对该程序需要具有x的权限 该权限仅在执行该程序过程中有效 执行者将具有该程序拥有者的权限 以/usr/bin/passwd命令为例，该程序的权限为：rwsr-xr-x。普通用户可以调用该程序修改自己的密码，而密码文件/etc/shadow未设置任何权限，即只有root用户可以操作该文件。\n1 2 3 4 [root@localhost tmp]# ll /usr/bin/passwd -rwsr-xr-x. 1 root root 30768 2月 22 2012 /usr/bin/passwd [root@localhost tmp]# ll /etc/shadow ----------. 1 root root 1248 11月 30 10:50 /etc/shadow passwd文件的s标志表明setuid位被设置。\npasswd的拥有者为root；当普通用户执行passwd命令时，普通用户具有该命令的执行权限；passwd执行该命令时会暂时获得root用户权限；/etc/shadow就可以被修改。\n也许看到这里就会有疑问：那岂不是普通用户可以通过/etc/passwd命令修改其他用户的密码了。之所以不可以这么操作是因为passwd命令内部通过逻辑实现。\nset gid 设置在文件上的情况：\n该权限位仅对二进制文件有效 执行者对该程序需要具有x的权限 执行者在运行程序的过程中将会获得该执行群组的权限 设置在目录上的情况：\n用户对目录具有r和x的权限 用户在目录下的有效群组将会变成该目录的群组 若用户在此目录下具有w的权限，使用者建立的群组与该目录的群组相同 粘附位（sticky bit） 该权限位仅对目录有效，如果在目录上设置了粘附位，只有该目录的属主、该文件的属主或root用户可以删除或重命名该目录文件。\n/tmp文件夹的权限如下，其中的t位表示粘附位：\n1 2 [kuring@localhost /]$ ll -ad tmp/ drwxrwxrwt. 25 root root 4096 3月 1 14:51 tmp/ 如果在/tmp下kuring用户创建了自己的文件kuring_file，并设置权限为777，test用户并不能删除该文件\n1 2 3 4 5 6 7 8 9 10 11 12 [kuring@localhost tmp]$ touch kuring_file [kuring@localhost tmp]$ ll kuring_file -rw-rw-r--. 1 kuring kuring 0 3月 1 15:05 kuring_file [kuring@localhost tmp]$ chmod 777 kuring_file [kuring@localhost tmp]$ ll kuring_file -rwxrwxrwx. 1 kuring kuring 0 3月 1 15:05 kuring_file [kuring@localhost tmp]$ su - test 密码： [test@localhost ~]$ cd /tmp/ [test@localhost tmp]$ rm kuring_file rm: 无法删除\u0026#34;kuring_file\u0026#34;: 不允许的操作 表示方法 如果设置了setuid位，属主的执行权限中的x用s来代替； 如果设置了setgid位，组执行权限中的x用s来代替； 如果设置了sticky位，权限最后的那个字符被设置为t； 如果设置了setuid、setgid或sticky位中一个，又没有设置相应的执行位，这些位显示为S或T。\n权限设定方式 4为setuid位，2为setgid位，1为sticky位。具体参考《鸟哥的Linux私房菜》。\n参考文档 《鸟哥的Linux私房菜》 《UNIX/Linux系统管理技术手册》\n","date":"2014-03-08T00:00:00Z","permalink":"/post/linux_permission/","title":"Linux难点之权限类型"},{"content":"在使用SSH或者telent远程登录到Linux 服务器，经常运行一些需要很长时间才能完成的任务，比如系统备份、ftp 传输等等。通常情况下我们都是为每一个这样的任务开一个远程终端窗口，因为它们执行的时间太长了。必须等待它们执行完毕，在此期间不能关掉窗口或者断开连接，否则这个任务就会被杀掉，一切半途而废了。\n安装 CentOS下默认没有安装该命令，从screen的官方网站下载，下载地址：http://ftp.gnu.org/gnu/screen/。\n解压后执行./configure。 执行make命令。在执行make命令时会遇到错误pty.c:38:26: 错误：sys/stropts.h：没有那个文件或目录，在/usr/install/sys/目录下创建一个stropts.h的空文件即可。 执行make install，该命令并不会将screen命令复制到系统的PATH变量包含的路径下，即不能执行screen命令。 执行install -m 644 etc/etcscreenrc /etc/screenrc。 执行cp screen /bin/。 执行cp doc/man/man1/screen.1 /usr/share/man/man1/，即可以可使用man screen查看帮助。 实现原理 当关闭窗口或断开连接时，内核会将SIGHUP信号发送到会话期首进程，进程对SIGHUP的处理动作为终止。如果会话期首进程终止，则该信号发送到该会话期前台进程组。一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。\n为解决上述问题，Linux程序在设计时可设计成守护进程的方式启动。\n另外也可以通过nohup 命令 \u0026amp;的方式启动来解决问题。\n新建screen 直接输入screen命令即可使用。 输入screen -S kuring，这里给screen取了一个名字，方便辨认。 screen 命令直接在screen中执行命令，命令结束后screen退出。 分离与恢复 在screen窗口中执行ctrl + a d命令，screen会给出_[detached]_提示，并恢复到执行screen之前的bash。 查找之前的screen执行screen -ls，会列出当前打开的screen。\n1 2 3 4 There are screens on: 8576.pts-3.localhost (Attached) 8449.kuring (Detached) 2 Sockets in /tmp/uscreens/S-kuring. 这里系统中打开了两个screen，一个为Attached，另一个为Detached。\n重新连接执行screen -r kuring或screen -r 8449或screen -r，当系统中仅有一个处于Detached状态的screen时就可以直接执行screen -r命令。\n关闭窗口 在screen的shell中执行exit命令即可关闭screen。\n也可以执行ctrl + a k，会杀死当前screen中的所有运行进程。\n错误 在screen中执行vi命令时会提示_E437: terminal capability \u0026ldquo;cm\u0026rdquo; required_错误，执行echo $TERM查看发现打印值为_screen_，而未执行screen时的bash打印值为_xterm_，在screen中执行export TERM=xterm即可解决该问题。\n参考文档 更多高级使用方法请参考以下文档：\nlinux 技巧：使用 screen 管理你的远程会话 linux screen 命令详解\n","date":"2014-02-18T20:51:12Z","permalink":"/post/linux_screen/","title":"Linux下的screen命令"},{"content":"本文是VMWare下的CentOS操作系统将yum源更改为光盘的实例，光盘的iso文件存放在宿主机器上，通过VMWare的共享文件夹功能与CentOS系统共享文件。CentOS中共享文件夹的存放路径为/mnt/hgfs中。CentOS的光盘为两张，分别为CentOS-6.5-x86_64-bin-DVD1.iso、CentOS-6.5-x86_64-bin-DVD2.iso。注意LiveCD版的CentOS系统盘是不可以作为yum源的。\n挂载光盘 mkdir -p /media/cdrom;mkdir -p /media/CentOS，创建挂载两个挂载目录，分别挂载DVD1和DVD2。 执行mount /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD1.iso /media/cdrom/ -o loop;mount /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD2.iso /media/CentOS/ -o loop;，将iso挂载到创建的目录下。 执行df -h命令即可看到挂载的文件系统，输出如下： 1 2 /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD1.iso 4.2G 4.2G 0 100% /media/cdrom /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD2.iso 1.2G 1.2G 0 100% /media/CentOS 设置本地yum源 在/etc/yum.repos.d目录下CentOS-Base.repo记录着yum通过网络更新的源，CentOS-Media.repo记录着通过本地文件更新的源。其中CentOS-Media.repo文件的内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # CentOS-Media.repo # # This repo can be used with mounted DVD media, verify the mount point for # CentOS-6. You can use this repo and yum to install items directly off the # DVD ISO that we release. # # To use this repo, put in your DVD and use it with the other repos too: # yum --enablerepo=c6-media [command] # # or for ONLY the media repo, do this: # # yum --disablerepo=\\* --enablerepo=c6-media [command] [c6-media] name=CentOS-$releasever - Media baseurl=file:///media/CentOS/ file:///media/cdrom/ file:///media/cdrecorder/ gpgcheck=1 enabled=0 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 其中已经包含了/media/cdrom路径和/media/CentOS路径，至此配置已经完毕。\n安装软件需要通过命令yum --disablerepo=\\* --enablerepo=c6-media [command]，执行yum [command]命令时还是联网执行。\n设置开启启动自动挂载iso文件 在/etc/fstab文件中的末尾增加如下内容：\n1 2 /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD1.iso /media/cdrom/ iso9660 loop 0 0 /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD2.iso /media/CentOS/ iso9660 loop 0 0 ","date":"2014-02-18T00:00:00Z","permalink":"/post/centos_yum_iso/","title":"CentOS中将光盘作为安装源"},{"content":"trim函数在其他语言中比较常见，这里用C语言实现一个，不使用C语言的库函数。该例子中不需要额外的申请空间，算法的时间复杂度为O(1)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #include \u0026lt;stdio.h\u0026gt; char *trim(char * str) { char *buf1, *buf2; int i; if (str == NULL) { return NULL; } // 处理字符串前面的空格\tfor (buf1=str; *buf1 \u0026amp;\u0026amp; *buf1==\u0026#39; \u0026#39;; buf1++); // 将去掉前面空格的字符串向前复制 for (buf2=str, i=0; *buf1;) { *buf2++ = *buf1++; i++; } *buf2 = \u0026#39;\\0\u0026#39;; // 处理字符串后面的空格 while (*--buf2 == \u0026#39; \u0026#39;) { *buf2 = \u0026#39;\\0\u0026#39;; } return str;\t} int main(int argc, char *argv[]) { printf(\u0026#34;trim(\\\u0026#34;%s\\\u0026#34;) \u0026#34;, argv[1]); printf(\u0026#34;returned \\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, trim(argv[1])); return 0; } ","date":"2014-02-18T00:00:00Z","permalink":"/post/c_trim/","title":"用C语言实现的trim函数"},{"content":"X Window的实现机制还是比较难以理解的，尤其是跟软件开发中的客户端-服务器模式不太一样。涉及到概念也比较多，甚至很对教程对概念的理解不一。最近深入学习了下X Window的原理，在此做一下整理。先上一个摘自维基百科的图： 常用快捷键 ctrl+alt+fn：切换到相应的虚拟控制台，n为1-12。默认情况下，linux操作系统会在1-6上运行6个虚拟控制台。 ctrl+alt+退格键：关闭X window系统。 在vmware环境下，ctrl+alt快捷键跟vmware冲突，需要先按住ctrl+alt，然后按一下空格键并松开，再按下相应的fn键才能使用。\nX Server 负责硬件管理、屏幕绘制、字体，并接收输入设备（如键盘、鼠标等）的动作，并且告知X Client。 Linux下的X Server软件为Xorg，通过X（Xorg的链接文件）命令即可执行。 输入X命令后，会在第7个控制台启动X Server，将会出现一个什么都没有的漆黑界面，这是由于没有任何X client程序输入的原因。 可以在Linux下启动多个X Server软件，从0开始编号。如果再执行X:2命令会启动第二个X Server，此时X Server会在第8个控制台运行。如果第一次执行的是X:2命令则X Server会在第7个控制台运行。 在Windows操作系统下的Xming、Xmanager等可以远程连接Linux界面的软件其实就是X Server。\nX Client 即X应用程序，运行在X Window下的窗口程序都属于X client。比如firefox就是一个X Client。接收来自X Server的处理动作，将动作处理成为绘图数据，并将绘图数据传回给X Server。X Client与X Server之间通过X Window System Core Protocol协议进行通讯。 xclock是一个简易的X Client的时钟程序，在:1上启动X Server后，执行xclock –display :1\u0026amp;命令将xclock输出到X Server后的画面如下： 该程序可以在X Server上执行，但是画面非常简陋，甚至没有窗口的菜单栏和最大化等按钮。\nWindow Manager 一种特殊的X Client，提供了窗口的样式。常用的Window Manager包括GNOME默认的metacity、twm等。 将metacity输出到:1上的X Server的命令为metacity –display=:1 \u0026amp;，效果如下： 可以看到窗口多出了最小化、最大化、关闭按钮，并且窗口可以移动和缩放等操作。\nDisplay Manager 提供用户登录画面、帮助X Server建立Session。 gnome采用的Display Manager为gdm，KDE采用kdm，还有tdm、xdm等。\nDesktop Manager X Server、X Client、Window Manager的一个集合。常用的Desktop Manager包括：KDE、GNOME等。\nstartx启动流程 在命令行下执行startx命令后，系统直接进入了桌面环境，并未出现登录界面。进程树如下： startx会调用xinit命令，xinit命令的主要是启动一个X Server软件。 接着xinit命令会调用gnome-session启动gnome的环境所需要的软件。 init 5启动流程 在命令行下执行init 5，首先出现的画面为登录信息。进程树如下： 执行/etc/rc.d/rc5.d中的daemon。 执行/etc/X11/prefdm文件，会选择启动gdm、kdm、xdm、tdm。 这里以gdm为例，gdm是一个shell脚本，会启动gdm-binary命令。 实战 Windows主机连接Linux的教程参见我的另外一篇文章《Redhat安装完成之后的设置》中的相关部分。 两台Linux机器之间通过XWindow实现连接的用法比较少见，通常情况下可以通过vnc代替。 Linux主机连接Windows的工具为rdesktop。\n参考文章 《鸟哥的Linux私房菜》 《Linux操作系统之奥秘》 视频：RH033-ULE112-16-linux下X图形显示体系 视频：Xwindow详解\n","date":"2014-02-17T00:00:00Z","permalink":"/post/x_window/","title":"X Window学习"},{"content":"本文对Linux常用的网络命令进行整理和总结。\n连通性测试 ping命令 最常用的网络诊断命令。\n1 2 3 4 5 6 7 8 [kuring@localhost ~]$ ping www.baidu.com PING www.a.shifen.com (61.135.169.125) 56(84) bytes of data. 64 bytes from 61.135.169.125: icmp_seq=1 ttl=128 time=25.5 ms 64 bytes from 61.135.169.125: icmp_seq=2 ttl=128 time=20.3 ms 64 bytes from 61.135.169.125: icmp_seq=3 ttl=128 time=25.0 ms 64 bytes from 61.135.169.125: icmp_seq=4 ttl=128 time=21.7 ms 64 bytes from 61.135.169.125: icmp_seq=5 ttl=128 time=23.4 ms 64 bytes from 61.135.169.125: icmp_seq=6 ttl=128 time=21.9 ms 通过上述输出可以看出，ping命令可得到DNS对应的IP信息、ping的数据包大小、网络延迟信息。\n另外，可以通过-s参数指定ping的数据包大小。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 kuring@ubuntu:~$ ping www.baidu.com -s 1024 PING www.a.shifen.com (61.135.169.125) 1024(1052) bytes of data. 1032 bytes from 61.135.169.125: icmp_req=1 ttl=55 time=22.6 ms 1032 bytes from 61.135.169.125: icmp_req=2 ttl=55 time=22.9 ms 1032 bytes from 61.135.169.125: icmp_req=3 ttl=55 time=53.0 ms 1032 bytes from 61.135.169.125: icmp_req=4 ttl=55 time=28.0 ms 1032 bytes from 61.135.169.125: icmp_req=5 ttl=55 time=54.7 ms 1032 bytes from 61.135.169.125: icmp_req=6 ttl=55 time=93.1 ms 1032 bytes from 61.135.169.125: icmp_req=7 ttl=55 time=26.9 ms 1032 bytes from 61.135.169.125: icmp_req=8 ttl=55 time=25.2 ms 1032 bytes from 61.135.169.125: icmp_req=9 ttl=55 time=25.4 ms ^C1032 bytes from 61.135.169.125: icmp_req=10 ttl=55 time=21.2 ms --- www.a.shifen.com ping statistics --- 10 packets transmitted, 10 received, 0% packet loss, time 45436ms rtt min/avg/max/mdev = 21.225/37.350/93.149/21.966 ms 会发现ping命令的响应时间变长了，这正是由于ping发送数据包变大了。\ntraceroute 可以显示路由信息。\nmtr traceroute命令的升级版，可以动态刷新路由信息，可以显示路由上每个节点的丢包率和时间等信息，信息比较全面和直观。\narp相关 arping 可以通过该命令查看IP地址对应的mac地址。arping IP地址会立即发送一个arp广播，可以根据收到的arp回应的多少看局域网内是否中arp病毒、IP地址冲突等情况。\narp 跟arp协议相关，可以设置arp表、读取arp表等。\n端口相关 telnet 可以利用该命令来测试某个端口是否打开。例如执行telnet localhost 881其中881为本机的未打开端口，会产生如下输出：\n1 2 Trying 127.0.0.1... telnet: connect to address 127.0.0.1: Connection refused 执行telnet localhost 22，其中22端口为本机的ssh服务端口且已经打开，会产生如下输出：\n1 2 3 4 Trying 127.0.0.1... Connected to localhost. Escape character is \u0026#39;^]\u0026#39;. SSH-2.0-OpenSSH_5.3 则表示本机的22端口已经打开。\nnetstat 查看本机网络端口命令，常用netstat -aunp。\n#DNS相关\nhost 1 2 3 4 kuring@ubuntu:~$ host www.google.com.hk www.google.com.hk is an alias for www-wide.l.google.com. www-wide.l.google.com has address 74.125.128.199 www-wide.l.google.com has IPv6 address 2404:6800:4005:c00::c7 nslookup 1 2 3 4 5 6 7 8 kuring@ubuntu:~$ nslookup www.google.com.hk Server:\t127.0.0.1 Address:\t127.0.0.1#53 Non-authoritative answer: www.google.com.hk\tcanonical name = www-wide.l.google.com. Name:\twww-wide.l.google.com Address: 74.125.128.199 dig 可以代替nslookup的命令，显示的域名信息更为详细。\n其他 ab Linux下的压力测试工具，可以模拟多个客户端发送多个请求。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 kuring@ubuntu:~$ ab -c 100 -n 100 https://kuring.github.io/ This is ApacheBench, Version 2.3 \u0026lt;$Revision: 655654 $\u0026gt; Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/ Licensed to The Apache Software Foundation, http://www.apache.org/ Benchmarking kuring.github.io (be patient).....done Server Software: Tengine/2.0.0 Server Hostname: kuring.github.io Server Port: 80 Document Path: / Document Length: 707 bytes Concurrency Level: 100 Time taken for tests: 1.547 seconds Complete requests: 100 Failed requests: 10 (Connect: 0, Receive: 0, Length: 10, Exceptions: 0) Write errors: 0 Non-2xx responses: 90 Total transferred: 145330 bytes HTML transferred: 126570 bytes Requests per second: 64.64 [#/sec] (mean) Time per request: 1546.990 [ms] (mean) Time per request: 15.470 [ms] (mean, across all concurrent requests) Transfer rate: 91.74 [Kbytes/sec] received Connection Times (ms) min mean[+/-sd] median max Connect: 173 223 43.8 200 289 Processing: 167 340 301.5 236 1368 Waiting: 166 339 300.5 235 1364 Total: 345 563 295.4 438 1546 Percentage of the requests served within a certain time (ms) 50% 438 66% 555 75% 562 80% 597 90% 1108 95% 1474 98% 1484 99% 1546 100% 1546 (longest request) 参考 解决Linux服务器访问比较慢的问题-网络测试命令讲解 《鸟哥的Linux私房菜-服务器架设篇》 ","date":"2014-02-08T00:00:00Z","permalink":"/post/linux_net_tools/","title":"Linux常用网络诊断工具整理"},{"content":"长期以来在使用gdb调试代码的时候都会因为调试代码的时候查看代码不方便而烦恼，gdb的list命令不够好用。而且网上的教程中也确实不容易发现可以替代gdb的好的终端下的调试工具，对于图形界面的集成开发环境（比如Eclipse CDT）和图形界面的调试工具（比如DDD）不在本文讨论的范围内，毕竟很多时候连接linux的时候还是终端方式的居多。\ntui模式 直到后来偶然间发现了gdb的-tui参数，该参数通过文本用户界面模式进行调试代码，使用起来确实方便了许多，再也不用边调试边list代码了，该模式已经满足了我的边调试边查看代码的需求。另外，gdbtui命令也可完成相同的功能。一个tui调试模式的界面如下：\n虽然，tui模式大大的提供了调试的友好性，但是仍然有一些缺点。比如显示的代码无法语法高亮，虽然会很影响用户体验，但是我毕竟是一名后台开发的程序员，这点可以忽略不计。源码布局和gdb命令布局之间切换不够方便，这点也不要紧，毕竟可以切换，只是需要输入两个单词就可以切换了。最最有问题的就是，该命令使用的时curses库，当用ssh通过windows下的SecureCRT或者putty连接进行调试时，源码布局部分往往不能够自动刷新，需要手工输入CTRL+L来刷新，具体的原理我没有去深究。\ncgdb 主要是本着解决gdb tui中的源码布局不能自动刷新的问题，本文的重点cgdb命令终于闪亮登场了。该命令不仅解决了源码布局自动刷新问题，同时也支持了语法高亮，同时源码查看支持vi的部分命令。功能基本跟vimgdb相近，但是安装更容易，在ubuntu下只需执行sudo apt-get install cgdb即可。\n下面是一些常用命令：\nESC：切换焦点到源码模式，在该界面中可以使用vi的常用命令\ni：切换焦点到gdb模式\no：打开文件对话框，选择要显示的代码文件，按ESC取消\n空格：在当前行设置一个断点\n参考文章 Beej\u0026rsquo;s Quick Guide to GDB cgdb\n","date":"2014-02-02T00:00:00Z","permalink":"/post/cgdb/","title":"cgdb的使用"},{"content":"最近买了本新书《深度探索Linux操作系统》，在按照书中步骤学习的过程中，无奈在安装步骤中出错，于是只能到网上找书评。在浏览书评的过程中偶然看到LFS三个名词，google之，发现是手动安装Linux的官方学习手册，学习之。\n解压命令 一直对解压命令的参数记不清楚，记录一下： tar -Jvxf *.tar.xz tar -zvxf *.tar.gz tar -xvf *.tar.bz2\n将一般用户可以使用sudo命令执行命令的方法 执行visudo命令来修改文件内容，将本用户添加到文本文件中。修改的文件为/etc/sudoers，该文件默认为只读的，但是可以通过visudo命令来修改。\nlogin shell和non-login shell login shell：shell会重新读取/etc/profile和~/.bash_profile来应用新的环境变量。通过su - 用户名的方式登录为login shell方式。 non-login shell：此时shell不会读取/etc/profile和~/.bash_profile，而是读取~/.bashrc来应用新的环境变量。通过su 用户名登录的方式为non-login shell方式。\n文中lfs用户下的~/.bash_profile的内容如下：\n1 exec env -i HOME=$HOME TERM=$TERM PS1=\u0026#39;\\u:\\w\\$ \u0026#39; /bin/bash 而lfs用户下的~/.bashrc文件的内容如下：\n1 2 3 4 5 6 7 set +h umask 022 LFS=/mnt/lfs LC_ALL=POSIX LFS_TGT=$(uname -m)-lfs-linux-gnu PATH=/tools/bin:/bin:/usr/bin export LFS LC_ALL LFS_TGT PATH 而最令人奇怪的是即使通过su - lfs命令登录也会执行到.bashrc文件的内容，不信可以通过在.bash_profile和.bashrc文件的开始地方打印内容来验证。之所以出现如此奇怪的问题，原因在于~/.bash_profile中的命令。其中exec命令和linux系统中的exec系列函数的含义是一致的，即在当前bash中直接执行exec后面的命令，而不用fork一个新的进程来执行。env命令会通过当前用户的HOME和TERM环境变量及自定义的PS1环境变量来执行新的/bin/bash，而新执行的bash为non-login shell方式，因此会执行lfs用户下的.bahsrc文件。\n总结一下，就是~/.bash_profile文件中的env命令通过non-login shell方式执行了新的bash，exec命令的作用是不在当前bash中执行新的bash，而不是通过产生一个新进程的方式来执行bash。\nPOSIX \u0026amp;\u0026amp; FHS \u0026amp;\u0026amp; LSB POSIX.1-2008，通过该网站来查询系统函数等非常方便。 Filesystem Hierarchy Standard (FHS) Filesystem Hierarchy Standard (FHS)，可以通过此标准来学习Linux的目录含义。 Linux Standard Base (LSB) Specifications\nset +h The set +h command turns off bash\u0026rsquo;s hash function. Hashing is ordinarily a useful feature—bash uses a hash table to remember the full path of executable files to avoid searching the PATH time and again to find the same executable.\n虚拟终端PTYs PTY 设备与终端设备（terminal device）相类似——它接受来自键盘的输入，并将文字传递给运行在其上的程序以备输出。PTY 被依次编号，且每个 PTY 的编号就是它在 /dev/pts 目录中对应设备文件的文件名。\ndevpts file system 远程登陆(telnet,ssh等)后创建的控制台设备文件所在的目录。\nspecs文件 gcc 是一个驱动式的程序. 它调用其它程序来依次进行编译, 汇编和链接. GCC 分析命令行参数, 然后决定该调用哪一个子程序, 哪些参数应该传递给子程序. 所有这些行为都是由 SPEC 字符串(spec strings)来控制的. 通常情况下, 每一个 GCC 可以调用的子程序都对应着一个 SPEC 字符串, 不过有少数的子程序需要多个 SPEC 字符串来控制他们的行为.\n查看当前shell 查看默认shell可以用命令：echo $SHELL。 查看当前shell：ps | grep $$ | awk '{print $4}' 。 通过输入一个不存在的命令来查看，如输入tom，可显示bash: tom: command not found，说明当前的shell为bash。 expect 一种提供自动交互的脚本语言。\ntee命令 重定向输出到多个文本文件命令。\npkg-config configure脚本在检查相应依赖环境时(例：所依赖软件的版本、相应库版本等)，通常会通过pkg-config的工具来检测相应依赖环境。\n详细内容见：简述configure、pkg-config、pkg_config_path三者的关系\n","date":"2014-01-20T00:00:00Z","permalink":"/post/lfs/","title":"Linux From Scratch学习笔记"},{"content":"2013年在不经意间过去了，总结起来收获还是不小的。我的大部分业余时间用在了学习上，看书看书再看书。每个周五或周六晚上会玩上一晚上dota以放松，周末会抽一天时间出去散散心。\n学习 学习上进步还算比较大，学习方式以看书为主，尤其是对Linux有了较深入了解。在年初的时候制定了深入学习Linux的计划，包括Linux内核，大约从五月下旬开始学习。\n信息系统项目管理师 当时抱着该考试可以挂靠的心态来学习PM，从春节后开始学习，五月末考试，大约花了三个月时间。学习思路为从淘宝上买的YY讲课的培训视频，价格还算便宜，隔一天晚上上一课，一次讲课时间在一个半小时左右。听了几次课之后就发现老师废话挺多，但是如果自己看书复习又抓不住重点，因此还是硬着头皮听老师讲课，就当作休息了。参考图书共三本，加起来足够2000页的样子，还做了历年的部分真题。由于自己手懒，写作在考试之前没有写过一篇，只是在考试之前背了两篇范文，结果考试的时候写作没有压中，离分数线差了两分。\n下半年考试没有报名，经过了一次考试后看清了PM的真面目，纯属搞理论，距离实际太远。如果为了挂靠去花费宝贵的业余时间来学习实在有点不得不偿失。现在回想起来多少有些后悔，但是不尝试怎么能知道PM到底是个啥东西呢。\nLinux Linux的系统知识今年进步挺大的，无论是在编码、对系统的理解还是内核方面都有非常大的进步。\n《鸟哥的Linux私房菜\u0026ndash;基础学习篇》 两年多前看过一次，利用工作中的空闲时间重新回顾了一边，收获还是挺大的。\n《鸟哥的Linux私房菜\u0026ndash;服务器架设篇》 两年前买的书，当时没有通读，感觉含金量跟基础学习篇太大。利用工作之余通读一遍，收获也还算可以。\n《Unix环境高级编程》 Linux编程圣经，花了接近两个月时间看完，看完后收获很大。\n《Unix网络编程》 Linux网络编程圣经，花了一个月时间看完，看完后对写TCP/IP通信的程序很有帮助，作为《Unix环境高级编程》的很好补充。\n《Linux内核完全剖析》 该书花了我接近半年的时间来学习，现在看完了五分之四左右，还有文件系统章节没有学习。学习下来收获很大，后续会结合《Linux内核图解》一书来补充学习和归纳。\n《程序员的自我修养》 通过学习对Linux的elf和windows的pe结构都有了较深入的了解，不过事隔半年之后已经全部忘记了，需要再温习一遍。\n微信公共帐号 从年初开始，每天会花一定时间来阅读微信公共帐号的文章，初期订阅的文章基本都会阅读，后期订阅量大了有些就仅浏览标题了。我订阅的公共帐号里比较好的有：鬼脚七、道哥的黑板报、小道消息、Mac Talk等。鬼脚七写人生类的文章我比较喜欢看，也比较赞同作者的观点。\n其他 ###《30天自制操作系统》 从2012年年底开始读的图书，花了接近三个月的时间通读，读到中间部分还反过头来重读一遍。学习到三分之二的时候感觉对我用处不大，因为不会对我理解Linux或Windows操作系统有太大帮助，放弃之。\n《Hadoop权威指南》 项目需要，对Hadoop没有足够的兴趣，理解了是怎么回事对我就足够了。同时还研究了Solr、HBase、MonogoDB等大数据软件。\n《Git权威指南》 向对Git有进一步的理解，目前仅看完了一半，分支管理部分还没有学习。\n###《浪潮之巅》 吴军博士的经典，我主要拿来在看书看累的时候消遣用，和《黑客与画家》的效果相当，很抱歉看了大半年了到现在仍然在看。\n《深度探索C++对象模型》 之前曾一度要学习，无奈一直没有看进去，lippman写的和侯捷翻译的实在是诡异。这次边写博客记录边看，居然效果很不错。最后时间有限，留了个小尾巴没有看完。\n工作 由于之前的公司经营不善，加上今年全国经济形式不好，公司持续几个月发不下工资来，考虑到个人经济状况换了一份工作。在济南这种小地方，找个理想的工作实在是不容易，甚至今年很多公司都不再招聘。我找工作以技术为导向，家附近一堆做电信BOSS的外包公司，一概不考虑。新工作还算可以，依旧加班较少，只是周六是间歇性上班，这个可以忍，还算有较充足的业余时间来学习。\n工作中用到的技术以C、C++为主，兼涉及到Android、PHP等。从元旦之后上班第一天，自己单打独斗从零开始做了一个短信管家类项目，到五月份后终止，结果跟我预想的完全一致，以失败而告终。回想当初真该将我当时对项目的看法跟领导沟通一下，而不是盲目的接下公司安排的任务。当时领导对互联网、APP的理解完全是幼稚园水平，一家传统行业的软件企业想来玩转互联网行业困难可想而知，我作为唯一的项目成员在项目的开始已经看到了没有任何意外的结局。如果当初将这五个多月的时间用来做些Linux的东西，对我个人的成长将是相当巨大的。\n生活 2013经济上不是很宽裕，没有旅游，仅有的一次市外旅游是南部山区的红叶谷。在济南这种地方，工资压得低，技术牛人又少，实在悲剧。生活虽然对我少了些许精彩，但过的还算比较充实。\n博客 为了能够更好的总结学习经验，年初准备有写博客的计划。之前零零散散的也写过一些，没有坚持住，现在看来我的博客写作计划坚持的还算不错，从6月份到年底共写了40篇博客，大都是自己的学习经验总结。前期博客写了一些非技术类博文，后期觉得没有这个必要不再写非技术类博客。\n为了搭建博客，又不想花钱买个不稳定的空间。我研究了Github上的jekyll、基于网盘（DropBox、Google Driver）的Farbox等搭建博客技术，参照了大量的jekyll主题，几乎将jekyll wiki上列出的主题都看了一个遍，也没有发现一个令我非常满意的主题，后挑选一个主题在Github上实验，不知是我markdown语法写的问题还是Github对markdown的解析问题，解析效果不够理想，但是我写的markdown在Farbox上却没有问题。最后采用了Farbox作为博客平台，并对Farbox的默认主题进行了少量修改。并从Godaddy上购买了kuring.me域名。\n2014年计划 2014，除了工作之外，我也业余时间仍然会大部分奉献给阅读书籍，以Linux内核方向为主。趁着年轻，多读些书，永远是正确的。\n","date":"2014-01-07T00:00:00Z","permalink":"/post/2013_summary/","title":"2013年总结"},{"content":"在Virbutal Box下安装CentOS虚拟机，在安装完Virtual Box增强功能后在分辨率列表中，仍然没有适合屏幕的分辨率1600*900。本文将通过xrandr命令来修改当前屏幕的分辨率。\n一. 执行xrand -q列出当前系统中已有的分辨率。显示内容如下：\n1 2 3 4 5 6 7 8 Screen 0: minimum 64 x 64, current 1024 x 768, maximum 32000 x 32000 VBOX0 connected 1024x768+0+0 0mm x 0mm 1024x768 60.0*+ 60.0* 1600x1200 60.0 1440x1050 60.0 1280x960 60.0 800x600 60.0 640x480 60.0 需要特别注意的是VBOX0，代表显示器的名字，下面会用到。\n二. 执行cvt 1600 900命令列出分辨率1600*900需要的参数，后面会用到。显示内容如下:\n1 2 # 1600x900 59.95 Hz (CVT 1.44M9) hsync: 55.99 kHz; pclk: 118.25 MHz Modeline \u0026#34;1600x900_60.00\u0026#34; 118.25 1600 1696 1856 2112 900 903 908 934 -hsync +vsync 该命令列出的内容下文会用到。\n三. 执行xrandr --newmode \u0026quot;1600x900_60.00\u0026quot; 118.25 1600 1696 1856 2112 900 903 908 934 -hsync +vsync命令，该命令中的参数是参考步骤2中的输出信息。\n四. 执行xrandr --addmode VBOX0 1600x900_60.00来向系统分辨率组中添加分辨率1600*900。其中VBOX0为步骤1获取的显示器名字，1600x900_60.00为步骤3添加的分辨率模式。\n五. 执行xrandr --output VBOX0 1600x900_60.00来应用刚才添加的分辨率。这样屏幕就可以更改为正确的分辨率了。\n","date":"2013-11-15T00:00:00Z","permalink":"/post/xrandr/","title":"Linux下更改屏幕分辨率"},{"content":"网桥工作在数据链路层，将两个LAN连起来，根据MAC地址来转发帧。Linux下要配置网桥的方法有两种，一种是通过修改配置文件，另外一种是通过brctl工具。修改配置文件的方式是通过修改/etc/sysconfig/network-scripts/ifcgg-eth*文件来完成的，这种方式没有仔细研究。本文将编写两个脚本来完成网桥的创建和删除，脚本的功能为将机器上的网卡eth1和eth2桥接，而网桥本身未设置ip。\n网桥创建脚本 本脚本利用brctl命令将网卡eth1和eth2桥接，可以通过brctl show命令查看结果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/bash # 脚本作用为将两个网卡桥接 # 检测brctl命令是否存在 brctl \u0026gt; /dev/null if [ $? != 1 ]; then echo Command brctl not exist, please setup it. The setup execute command is \\\u0026#34;yum install bridge-utils\\\u0026#34; exit 0 fi # 检测网桥br0是否存在，如果存在首先删除 declare -i result=$(brctl show | grep eth0 | wc -l) if [ $result \u0026gt; 0 ]; then echo detect the bridge br0 have already exist, first delete it\tifconfig br0 down brctl delbr br0 fi ifconfig eth1 0.0.0.0 ifconfig eth2 0.0.0.0 brctl addbr br0 brctl addif br0 eth1 brctl addif br0 eth2 ifconfig br0 up echo create bridge br0 success, you can use command : \\\u0026#34;brctl show\\\u0026#34; to check 网桥删除脚本 本脚本将桥接网卡br0删除\n1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash # 检测是否存在网桥br0 declare -i result=$(brctl show | grep br0 | wc -l) if [ $result == 0 ]; then echo \u0026#34;bridge br0 not exists, exit immediately\u0026#34; exit 0 fi # 删除网桥br0 ifconfig br0 down brctl delbr br0 echo \u0026#34;delete bridge br0 success\u0026#34; 注意事项 brctl命令创建的桥接网卡在机器重启后会删除，最好将创建桥接网卡的命令放入到linux的开机启动脚本中，这样每次开机的时候都可以自动创建桥接网卡了。\n参考网址 bridge命令介绍\n","date":"2013-11-14T00:00:00Z","permalink":"/post/linux_bridge/","title":"Linux下搭建网桥及脚本编写"},{"content":"本文提供简易shell脚本来更改mac地址，在其他linux发行版中去掉sudo即可。脚本内容如下：\n1 2 3 4 #!/bin/sh sudo ifconfig eth0 down sudo ifconfig eth0 hw ether 08:00:27:DF:B3:7B sudo ifconfig eth0 up ","date":"2013-11-12T00:00:00Z","permalink":"/post/ubuntu_change_mac/","title":"在ubuntu中更改mac地址的方法"},{"content":"[TOC]\nVNC（Virtual Network Computing）是一套由AT\u0026amp;T实验室所开发的可操控远程的计算机的软件，其采用了GPL授权条款，任何人都可免费取得该软件。VNC软件主要由两个部分组成：VNC server及VNC viewer。用户需先将VNC server安装在被控端的计算机上后，才能在主控端执行VNC viewer控制被控端。VNC与操作系统无关，因此可跨平台使用，例如可用Windows连接到某Linux的电脑，反之亦同。\n该软件在RedHat或CentOS中默认是安装的，但是没有启用，一可以通过which vncserver命令来查看该命令是否安装。本文讲解在Linux下的搭建server，在Windows下搭建client的步骤。\n设置vncserver的密码 vncserver需要设置一个密码，该密码并不等同于系统帐号的密码，而是vnc客户端登录的时候输入的密码。执行vncpasswd命令来创建密码。\n修改vncserver配置文件 修改文件/etc/sysconfig/vncservers，在该文件末尾添加如下内容：\n1 2 VNCSERVERS=\u0026#34;1:root\u0026#34; VNCSERVERARGS[1]=\u0026#34;-geometry 1024x768 -alwaysshared -depth 24\u0026#34; 启动vncserver 执行service vncserver start命令来开启vncserver服务。\n客户端连接到服务端 这里采用windows下的vnc viewer工具来连接到vncserver。在输入IP的地方输入：IP地址:1来连接到vncserver端，其中:1要跟/etc/sysconfig/vncservers文件中的对应标号一致。这样就可以连接上vncserver，但是连接后界面非常简单，跟命令行界面类似。还需要对vncserver进一步配置。\n配置vncserver vncserver的配置文件在~/.vnc/xstartup文件中，该文件默认创建的内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh # Uncomment the following two lines for normal desktop: # unset SESSION_MANAGER # exec /etc/X11/xinit/xinitrc [ -x /etc/vnc/xstartup ] \u0026amp;\u0026amp; exec /etc/vnc/xstartup [ -r $HOME/.Xresources ] \u0026amp;\u0026amp; xrdb $HOME/.Xresources xsetroot -solid grey vncconfig -iconic \u0026amp; xterm -geometry 80x24+10+10 -ls -title \u0026#34;$VNCDESKTOP Desktop\u0026#34; \u0026amp; twm \u0026amp; 将其中的注释打开，即文件内容如下：\n1 2 3 4 5 6 7 8 9 10 11 #!/bin/sh unset SESSION_MANAGER exec /etc/X11/xinit/xinitrc [ -x /etc/vnc/xstartup ] \u0026amp;\u0026amp; exec /etc/vnc/xstartup [ -r $HOME/.Xresources ] \u0026amp;\u0026amp; xrdb $HOME/.Xresources xsetroot -solid grey vncconfig -iconic \u0026amp; xterm -geometry 80x24+10+10 -ls -title \u0026#34;$VNCDESKTOP Desktop\u0026#34; \u0026amp; twm \u0026amp; 然后执行service vncserver restart重新启动vncserver服务。客户端再重新连接vncserver既可以看到正常的界面了。\n参考文章 CentOS Linux下VNC Server远程桌面配置详解\n","date":"2013-11-10T00:00:00Z","permalink":"/post/linux_vnc/","title":"Linux下vnc的配置"},{"content":"上次文章《Windows和Linux之间的中文编码问题》中提到的在Windows下的源代码程序放到Linux下出现中文编码问题，解决方法为通过iconv工具转换源代码文件的编码为UTF8格式。最近多学习了些字符编码的知识，发现了解决此问题的另外一种办法。\n基础知识 我们在编译程序的时候会涉及到几个编码问题，包括C++源文件的编码、C++程序的内码和运行环境编码，其中C++程序的内码较难理解。\nC++程序的内码是指在可执行文件中字符串常量是以什么编码形式存放的，其中字符串常量为窄字符形式。在Windows系统中C++的内码通常为GB18030，在Linux下的gcc/g++使用的内码默认为utf8，可以通过fexec-charset参数来修改。\n运行环境编码即为操作系统的编码，通常情况下，简体中文Windows操作系统编码为GB18030，而Linux下默认为UTF8。\ngcc命令的参数 gcc有两个参数可以用来解决编码问题。 -finput-charset：用来指定源文件的编码。 -fexec-charset：用来指定生成的可执行文件的编码。\n如果这两个参数均未指定，则GCC不会对编码进行转换。 以上这两个参数就可以用来在不修改源文件编码的基础上来达到正确的效果，达到和上篇文章中解决问题同样的效果。\n关于Unicode编码 一直对Unicode编码比较糊涂，Unicode只是编码方法规范，而不是具体的存储方法。 常用的Unicode又分为UCS-2和UCS-两种编码，其中UCS-2采用固定的2个字节存储，UCS-4采用固定的4个字节存储。 通常情况下提到的Unicode编码即为UCS-2编码，比如Windows记事本中的保存为Unicode编码，其实就是保存为了UCS-2编码，由于每个字符均为2个字节，所以下次读取的时候仍然可以通过存储格式还原出来。\n参考文章 字符编码笔记：ASCII，Unicode和UTF-8 字符编码详解 关于c++的一些编码问题\n","date":"2013-11-10T00:00:00Z","permalink":"/post/windows_linux_code_2/","title":"再谈Windows和Linux之间的中文编码问题"},{"content":"最近设置一个Linux下的截屏程序的开机自启动，Linux的XWindow系统为gnome。\n最先想到的方式是修改/etc/inittab文件，因为Linux在开机自启动的时候会执行该文件，该文件的读取时机是在界面启动之前。我写了一个在main函数中睡眠10分钟的小程序，然后将程序添加到/etc/inittab文件中，结果开不了机了，因为需要睡眠10分钟后才能往下执行程序。幸好我用的虚拟机做的测试，并且在做测试之前备份了虚拟机。\n仔细想一下在/etc/rc.d/rc.local文件中启动截屏程序肯定是不合适的，因为如果用户默认是以运行级别非5启动，则程序仍然会被调用，但是没有XWindow，谈何截屏。\n接下来考虑将程序放到/etc/rc5.d目录下，这样就可以保证程序在XWindow环境下运行了。仔细一想也不合适，rc5.d仅在系统启动时运行，而Linux系统是多用户系统，允许多个用户同时登陆，多个用户登陆时截屏程序会怎样处理呢？这样显然不合适。\n然后想到程序既然为截屏程序，多个用户登陆的时候应该有几个用户就跑几个程序，这样才能保证每个用户的屏幕都能截取到。因此应该放到用户登录后的启动程序列表中。类似于windows系统中的开机启动项。我用的桌面为gnome，找到了gnome-session-properties命令来启动添加程序启动的界面，然后将我的程序添加到界面中即可。\n如果桌面系统为KDE，则应该也可以找到相关的设置界面。\n题外：利用/etc/inittab和rc5.d目录下添加脚本的方式来启动程序的用户为root，很多程序未了避免root权限带来的安全问题，程序内部采用了su - 用户名的方式切换到一半用户执行代码。程序还可以通过chroot的方式更改根目录的路径达到保护系统的目的。\n","date":"2013-11-09T00:00:00Z","permalink":"/post/linux_ui_autorun/","title":"Linux上一个界面程序的开机自启动设置"},{"content":"在开发Linux程序的时候通常会在Windows下编码，然后拿到Linux下编译调试。而两个操作系统之间的默认编码往往有差别。\n文件编码问题 在Windows下查看文件编码可以使用记事本打开文件，然后点击“另存为”在右下角即可看到当前文件的编码方式。如果显示为ANSI编码，在简体中文系统下，ANSI 编码代表 GB2312 编码。不同 ANSI 编码之间互不兼容，ANSI是American National Standards Institute的缩写， 记事本默认是以ANSI编码保存文本文档的。\n在Linux可以通过vi命令查看文件编码，用vi打开文件，然后输入:set encoding即可显示文件编码。\n在VS2008中创建文件的默认编码是根据当前系统的编码格式确定的。VS2008编译器可以同时支持GB2312和UTF-8两种编码。\n为了解决在Linux下的乱码问题，Linux下的编码格式为utf8编码，这里采用在Windows下将gb2312编码更改为utf8的方式来解决。iconv是一个可以转换文件编码的工具，编写一个批处理脚本来实现批量转换文件编码的功能。批处理文件的内容如下：\n1 2 3 4 5 6 7 @ECHO OFF FOR /R %%F IN (*.h,*.cpp) DO ( echo %%~nxF iconv.exe -f GB2312 -t UTF-8 %%F \u0026gt; %%F.utf8 move %%F.utf8 %%F \u0026gt;nul ) PAUSE 本脚本来自网络，不是我自己写的。 注意：在使用文件编码之前一定要备份文件，防止意外发生，否则后果自负。\n文件名编码问题 Windows的中文系统下文件名的编码默认为gbk，在Linux默认编码为UTF-8。如果将Windows下的中文文件名的文件复制到Linux下肯定会出现乱码的问题。可以利用convmv工具来解决编码的问题。\n具体执行操作为：在Linux系统下的要转换编码的目录下执行命令：convmv -f GBk -t UTF-8 --notest -r *，这样就会将该文件夹下的所有文件递归的转换编码为UTF-8。\nconvmv的帮助文档点这里。\n相关下载 脚本和iconv程序下载链接\n","date":"2013-09-30T21:50:16Z","permalink":"/post/windows_linux_code/","title":"Windows和Linux之间的中文编码问题"},{"content":"无继承情况下的对象构造 在《Unix环境高级编程》的7.6节中提到C程序的内存空间可以分为正文段、初始化数据段、非初始化数据段、栈、堆。其中初始化数据段包含程序中需明确赋初值的变量，如C语言中的全局变量int maxcount = 99;。非初始化数据段又称为bss（block started by symbol）段，在程序开始之前，内核将此段初始化为0或空指针，如出现在函数外面的long sum[1000];，该变量没有明确赋初值，因此放到了bss段中。 而在C++语言中，将所有的全局对象当做初始化过的数据来对待，因此不会将全局变量放到bss段中。\nPOD数据类型 书中提到Plain ol\u0026rsquo; data，查了下应该叫Plain Old Data,简称POD，指C风格的struct结构体定义的数据结构，其中struct结构体中只能定义常规数据类型，不可以含有自定义数据类型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 typedef struct Point { float x, y, z; } Point; Point global; Point foobar() { Point local; Point *heap = new Point(); *heap = local; delete heap; return local; } 首先看全局变量global，按照常规的理解，在程序启动的时候编译器会调用Point的合成的默认构造函数来初始化global变量，在程序退出时会调用Point的合成的析构函数来销毁global变量。实际上，C++编译器会将Point看成是一个POD对象，既不会调用合成的构造函数也不会调用合成的析构函数，但C++编译器会将global当成初始化过的数据来对待，不放入BSS段。\nfoobar函数中的local局部变量不会自动初始化，意味着local.x中的值是不可控的，但是local变量分配了栈空间。\n*heap = local;执行时仅简单执行按字节复制操作，不会产生赋值操作符，因为Point是一个POD类型。\nreturn local;同样仅通过字节复制操作产生一个临时对象。\n抽象数据类型 这次将上面的Point类型从struct变换为class\n1 2 3 4 5 6 7 8 class Point { public: Point(float x=0.0, float y=0.0, float z=0.0) : _x(x), _y(y), _z(z){} private: float _x, _y, _z; }; 在上节中的foobar函数中，各个对象的默认复制构造函数、赋值操作符和析构函数仍然不会调用，因为调用是没有意义的，因此编译器干脆就不产生。\n为继承做准备 再次更改Point类，引入虚函数。\n1 2 3 4 5 6 7 8 class Point { public: Point(float x=0.0, float y=0.0) : _x(x), _y(y) {} virtual float z(); private: float _x, _y; }; 引入虚函数后，类对象就需要一个vtbl来存放虚函数的地址，类对象中需要添加vptr指针。而vptr的初始化是在对象构造的时候，因此对象初始化的时候需要调用构造函数，同时默认构造函数和赋值构造函数会自动在构造函数的最前面插入初始化vptr的代码。\n继承体系下的对象构造 C++时会自动扩充类的每一个构造函数。扩充步骤如下：\n如果类含有虚基类，则所有虚基类的构造函数被调用，调用顺序为从左到右，从最深到最浅。 如果类含有基类，则基类构造函数会被调用，以基类的声明顺序为顺序。 如果类对象中含有vptr，必须在初始化类的成员变量之前为vptr指定初值，使其指向vtbl。 将成员初始化列表中数据成员的初始化操作放入构造函数内部，并且按照成员在类中的声明顺序。 如果类成员变量不在构造函数的初始化列表中，但是成员变量含有默认构造函数，则默认构造函数必须被调用。 虚拟继承 本小节将学习一下引入了虚继承机制之后构造函数的生成是什么样子的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Point { public: Point(float x=0.0, float y=0.0) : _x(x), _y(y) {} virtual float z(); private: float _x, _y; }; class Point3d : public virtual Point { public: Point3d(float x=0.0, float y=0.0, float z=0.0) : Point(x, y), _z(z) {} ~Point3d(); virtual float z() {return _z;} protected: float _z; }; class Vertex : virtual public Point { // 不是重点忽略 }; class Vertex3d : public Point3d, public Vertex { // 不是重点忽略 }; class PVertex : public Vertex3d { // 不是重点忽略 }; 类之间的继承关系如下图所示，已经属于最复杂的继承模型了。 如果要构造Vertex3d的实例，在内存中必须仅能有一个Point类型的对象，而如果在Point3d和Vertex基类中都构造一个Point实例显然是不合适的。答案是编译器会在Vertex3d的构造函数中生成Point的对象，在Point3d和Vertex的构造函数中均不会生成Point的对象。Vertex3d和Point3d的构造函数伪码如下面所示，Vertex构造函数的伪码和Point3d类似，这里就不再列出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Point3d* Point3d::Point3d(Point3d *this, bool __most_derived, float x, float y, float z) { // 如果子类初始化基类则本构造函数不需要初始化基类 if (__most_derived != false) { this-\u0026gt;Point::Point(x, y); } this-\u0026gt;__vptr_Point3d = __vtbl_Point3d;\t// 初始化指向本类的vptr this-\u0026gt;__vptr_Point3d_Point = __vtbl_Point3d_Point;\t// 初始化指向基类的vptr this-\u0026gt;_z = z; return *this; } Vertex3d* Vertex3d::Vertex3d(Vertex3d *this, bool __most_derived, float x, float y, float z) { if (__most_derived != false) { this-\u0026gt;Point::Point(x, y); } this-\u0026gt;Point3d::Point3d(false, x, y, z); this-\u0026gt;Vertex::Vertex(false, x, y); // 初始化vptr // 用户代码 return this; } 编译器在类的构造函数中增加了一个bool变量来判断本类是否需要初始化基类，虚基类的初始化始终在继承最底层的类构造函数中初始化。对于PVertex类来说，Point类的构造函数在该类的构造函数中调用。\nvptr初始化语意学 vptr的在构造函数中的初始化时机为：在基类构造函数调用操作之后，在成员初始化列表和构造函数中显式代码之前。 构造函数的执行先后顺序为：\n所有虚基类、基类的构造函数会被调用。 对象的vptr初始化，指向相关的vtbl。 在构造函数内展开成员的初始化列表。 执行显式代码。 对象复制语意学 ","date":"2013-09-23T00:00:00Z","permalink":"/post/inside_the_c-_object_model_chapter_5/","title":"深度探索C++对象模型读书笔记_第五章：构造、析构、复制语意学"},{"content":"没有一成不坏的硬件，尤其是数据放到物理硬盘中，说不定哪天硬盘闹脾气就崩掉了，硬盘不值钱，可是里面的数据值钱。下面分享下我的数据备份方案，我的原则是数据无论何时都至少留有一个备份。\n博客 我的博客是放到Dropbox中的，在云端和本地均有备份，确保了博客数据的绝对安全，即使云端坏掉还有本地，本地丢了还有云端。\n个人照片 由于照片都较大，放到本地硬盘很容易占满空间，而且还不经常用。除了在自己电脑上留有照片之外，选择将照片压缩并加密后按照年份放到百度云上。\n代码 工作几年了，已经积攒了一些代码，有些代码时不时的会查看到。对于可以公开的自己写的代码我以后打算放到我的Github上，一方面是由于Github上可以在线浏览代码，另一方面可以向别人分享我的代码。 对于私有的代码，暂时放到了金山快盘上，没有找到可以方便浏览代码的云端。\n文档 由于文档之类的资料也是经常用到，我选择了金山快盘。\n","date":"2013-09-16T18:33:32Z","permalink":"/post/my_data_bak/","title":"我的个人数据备份方案"},{"content":"大小端问题跟CPU的架构直接相关，我们常见的80x86系列CPU采用小端字节序模式。Windows平台就采用的80x86系列CPU，因此为小端字节序。 而主机之间进行网络通信时往往采用大端字节序，因此小端字节序机器在发送数据前需要进行字节序转换，在接收到数据处理处理数据之前要将网络字节序转换成本地字节序。\n在Linux平台下提供了四个函数用来字节序转换：\n1 2 3 4 5 #include \u0026lt;arpa/inet.h\u0026gt; uint32_t htonl(uint32_t hostlong); uint16_t htons(uint16_t hostshort); uint32_t ntohl(uint32_t netlong); uint16_t ntohs(uint16_t netshort); Windows平台下也提供了相关的自己序转换函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 #include \u0026lt;WinSock2.h\u0026gt; unsigned __int64 __inline htond( double value ); unsigned __int32 __inline htonf( float value ); u_long WSAAPI htonl( _In_ u_long hostlong ); unsigned __int64 __inline htonll( unsigned __int64 value ); u_short WSAAPI htons( _In_ u_short hostshort ); double __inline ntohd( unsigned __int64 value ); float __inline ntohf( unsigned __int32 value ); u_long WSAAPI ntohl( _In_ u_long netlong ); u_long __inline ntohll( unsigned __int64 value ); u_short WSAAPI ntohs( _In_ u_short netshort ); 这里有个技巧需要说明以下，比如要发送如下的结构体：\n1 2 3 4 5 struct foo { int a; long b; }; 为了避免每个成员都调用字节序转换函数，可以在结构体的内部定义两个方法用于转换字节序，添加字节序后的foo如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 struct foo { int a; long b; void ntoh() { a = ntohl(a); b = ntohl(b); } void hton() { a = htonl(a); b = htonl(b); } }; 需要特别注意的是，在发送结构体类型的数据时要注意字节对齐的问题，这里不再展开讨论，不同的平台有不同的解决办法。大体分为Winodws平台、AIX平台和GNU类平台。\n","date":"2013-09-15T10:41:25Z","permalink":"/post/linux_windows_network/","title":"Linux和Windows平台下的网络通信问题"},{"content":"近段时间写了两个通过http协议来获取指定网页的内容并将内容解析出来的程序。程序一可以解析出目前本博客的内容页面的内容、时间、访问次数参数，采用Qt类库实现；程序二可以解析出新浪博客页面的内容、时间等参数，采用Linux下的tcp相关API实现。均采用C++语言实现。\n程序一 该程序采用Qt类库实现，其中Http协议的发送和接收采用Qt类库封装的类，网页内容的解析采用Qt封装的解析XML的相关类。 该程序仅能解析标准的Html语言，对于网页中的所有\u0026quot;\u0026lt;\u0026gt;\u0026ldquo;标签必须有结尾才行。例如本页面源码中的\n1 \u0026lt;meta content=\u0026#34;black\u0026#34; name=\u0026#34;apple-mobile-web-app-status-bar-style\u0026#34; /\u0026gt; 必须是闭合的。如果是下面这样则无法正确解析网页内容，这是由于采用的Qt类库决定的。\n1 \u0026lt;meta content=\u0026#34;black\u0026#34; name=\u0026#34;apple-mobile-web-app-status-bar-style\u0026#34;\u0026gt; 程序二 该程序的Http协议部分采用Linux的tcp协议api实现，解析网页直接采用搜索字符串的方式实现，较上一种方式要底层，仅能运行在Linux系统下运行。\n相关下载 程序一和二的下载链接\n","date":"2013-09-02T19:03:43Z","permalink":"/post/parse_http/","title":"两个通过http获取指定网页内容并解析的简单程序"},{"content":"本文的安装环境为ubuntu13.04。为了以后便于查阅，本文将相关插件的使用放到了文章的开始部分。这里不作插件的相关介绍，相关介绍看文章底部的参考文章。\n插件使用 本插件快捷键会跟随下文安装内容一块同步。\nctags 在源码目录执行ctags -R可生成ctags文件。该文件在源码修改后并不会改变，需要重新生成ctags文件。 ctrl+]：转到函数定义处。 ctrl+T：回到执行ctrl+]的地方。\ntaglist :TlistOpen：打开taglist窗口 :TlistClose：关闭taglist窗口。 :TlistToggle：在打开和关闭间切换。\nNERD tree :NERDTree：打开窗口。\nwinmanager wm：打开和关闭taglist和NERD tree窗口。\na.vim :A：在新Buffer中切换到c/h文件 :AS：横向分割窗口并打开c/h文件 :AV：纵向分割窗口并打开c/h文件 :AT：新建一个标签页并打开c/h文件 F12：代替:A命令\nMiniBufExplorer \u0026lt;Tab\u0026gt;：向前循环切换到每个buffer名上 \u0026lt;S-Tab\u0026gt;：向后循环切换到每个buffer名上 \u0026lt;Enter\u0026gt;：在打开光标所在的buffer d：删除光标所在的buffer\n插件安装 安装ctags 执行： sudo apt-get install ctags。\n安装taglist 下载页面：http://www.vim.org/scripts/script.php?script_id=273。下载后得到taglist_46.zip文件。 执行unzip taglist_46.zip解压文件。 将解压出的文件复制到~/.vim目录下。sudo cp ~/tmp/ ~/.vim/。 在~/.vimrc文件中添加如下： 1 2 3 let Tlist_Show_One_File = 1 \u0026#34;不同时显示多个文件的tag，只显示当前文件的 let Tlist_Exit_OnlyWindow = 1 \u0026#34;如果taglist窗口是最后一个窗口，则退出vim let Tlist_Use_Right_Window = 1 \u0026#34;在右侧窗口中显示 参考网址：http://www.cnblogs.com/mo-beifeng/archive/2011/11/22/2259356.html\n安装文件浏览器NERD tree 下载页面：http://www.vim.org/scripts/script.php?script_id=1658。 将下载后的nerdtree.zip文件解压到~/.vim目录下。 安装winmanager 下载页面：http://www.vim.org/scripts/script.php?script_id=95 将下载后的winmanager.zip文件解压到~/.vim目录下 修改.vimrc文件，添加： 1 2 let g:winManagerWindowLayout=\u0026#39;FileExplorer|TagList\u0026#39; nmap wm :WMToggle\u0026lt;cr\u0026gt; 这样利用winmanager工具将taglist和NERD tree工具整合到了一个块，输入wm可以打开和关闭窗口。\n安装cscope 下载页面：http://cscope.sourceforge.net，下载后得到文件cscope-15.8a.tar.gz。 ./configure make。可能会出现错误，执行如下命令： 1 2 3 apt-get install libncurses-dev sudo apt-get install flex sudo apt-get install byacc 然后执行make clean后重新make。 4. sudo make install\n安装在h/c文件之间切换插件a.vim 下载页面：http://www.vim.org/scripts/script.php?script_id=31。 将下载的a.vim文件复制到~/.vim/plugin文件夹下。 在~/.vimrc文件中添加nnoremap \u0026lt;silent\u0026gt; \u0026lt;F12\u0026gt; :A\u0026lt;CR\u0026gt; 下面内容为快捷键列表： :A switches to the header file corresponding to the current file being edited (or vise versa) :AS splits and switches :AV vertical splits and switches :AT new tab and switches :AN cycles through matches :IH switches to file under cursor :IHS splits and switches :IHV vertical splits and switches :IHT new tab and switches :IHN cycles through matches ih switches to file under cursor is switches to the alternate file of file under cursor (e.g. on \u0026lt;foo.h\u0026gt; switches to foo.cpp) ihn cycles through matches 安装快速浏览和操作Buffer 下载页面：http://www.vim.org/scripts/script.php?script_id=159 将下载的 minibufexpl.vim文件丢到 ~/.vim/plugin 文件夹中即可 在~/.vimrc文件中增加如下行： 1 2 3 let g:miniBufExplMapCTabSwitchBufs = 1 let g:miniBufExplMapWindowNavVim = 1 let g:miniBufExplMapWindowNavArrows = 1 快捷键： 向前循环切换到每个buffer名上 向后循环切换到每个buffer名上 在打开光标所在的buffer d\t删除光标所在的buffer 参考文章 经典vim插件功能说明、安装方法和使用方法介绍 手把手教你把Vim改装成一个IDE编程环境 ","date":"2013-09-02T13:19:16Z","permalink":"/post/vim_plugin/","title":"vim插件安装"},{"content":"\n前段时间在家看书学习，难得的学习的好时机。\n楼下有一个卖饭的小摊及其猖狂，不仅占用了人行道来炒菜，而且还在马路上摆了一溜桌子供客人吃饭，不仅占用了人行道，连车行道都给占用了。这些也就罢了，对我影响都不算太大，更可气的是每天中午和晚上吃饭的时候会开着大音响放着恼人的音乐，我不想惹麻烦，我忍。\n今天中午我刚开始看书，看到难处需要精心思考，恼人的音乐又开始了，我实在忍不住了，TMD，维权。打市长热线12345投诉，市长热线让我打110投诉。继续打110投诉，然后跟110说了下具体情况后，说给相应的派出所去处理。派出所的小片警立刻就给我回电话了，说外放音乐正常经营范围，只要不在晚上或清晨放音乐就不算违规，他们管不着，建议我去下面跟卖饭的商量，好一个商量。然后我又说，他们非法占道经营，小片警又说这个归城管管，让我给城管打电话，好一个给城管打电话。好一个推卸责任，这些把我给惹毛了。\n挂断电话后，寻思这个理不太对，然后继续给市长热线12345打电话，告诉情况后，市长热线的妹子告诉我说这个事情我给你处理，好一个我给你处理，这才是为人民服务的态度，鼓掌。\n这是第二次机会接触小片警，每次都是让我失望，绝望，恨之入骨。第一次接触小片警我甚至kill him的心都有了。不一心想着为人民服务，却是一心想着推卸责任，处处刁难市民并从中谋取私利，对市民爱理不理，这就是小片警在我心中的形象，很难改变。越是权利小的小兵，架子越大，这也就决定了永远是个小兵的身份。\n如果没有市长热线那这件扰民的事情也就不了了之了，因为投诉110都不管用了，作为市民已经没有可以维权的机构了。还好有市长热线的存在给市民多了一个维权的途径。\n上周五打的电话，这个周一给我回复电话问我饭馆在哪一次，周二又打电话问我饭馆在哪，然后周三终于给处理了，下班途中派出所给我电话回复说：“已经处理好了，让小饭馆的音响声音调小了，以后如果再有这种情况可以继续打电话”。等我回家一看，果然音响不见了，世界一下子清净了，zf终于替我办事了。\n也许是因为我的事情不是很紧急的原因，整个处理流程过于慢了，等了足足五天的时间才处理好。\n当大家的权益受到损害时，请大家多给市长热线打电话维护自己的权益。\n","date":"2013-08-23T12:01:20Z","permalink":"/post/mayor_hotline/","title":"市长热线12345"},{"content":"\n得知老家三老爷家的三叔去世了，原本还在看代码的我收到消息之后立刻无法平静了，只好出去走走散散心，回家之后依然感觉莫名的胸闷，玩游戏分散分散精力，游戏过后依然胸闷。意料之中的失眠，中途醒了好几次。总感觉消息不是真实的，总感觉昨晚在梦中，真希望一觉醒来之后什么都没有发生。\n三叔42岁，正值壮年，在家附近的号称有一万员工的炼钢厂打工，在整个市也算是很大的企业了。工作中意外丧命。总觉得这样的事情不会发生在我身边，客观事实是发生了。\n听家人说，三叔小时候调皮爬到树上掏鸟窝从树上掉下来把一个肩膀都磕到身体里了，大家都觉得肯定好不了，在镇上医院住院打吊瓶打够了自己偷偷跑回学校，后来居然奇迹般的好了，而且还没留下任何痕迹。大家都说三叔命大，谁知三叔小时候躲过了一劫却没有躲过这一劫，这难道就是天命？三叔一生勤俭节约，人忠厚老实，到头来却落得如此下场，谁说上帝是公平的，谁说好人有好报，这都是胡扯。\n临近三叔出事的前天，我做了一个很不好的梦，梦的内容我已经记不起来了。回家后听家里人说很多人都做了不好的梦，甚至连平常不怎么做梦的都会被梦惊醒。这绝对不是巧合，很明显已经超出了当前科学的范畴。\n记得最后一次跟三叔接触还是在过年的时候，三叔到我家来转转，聊了几句，现在还记忆犹新。再上一次见面就是在去年夏天的一个下午，约着三婶去火车站接三叔家的弟弟和我爸，正巧在三叔家的门口碰到三叔，估计是要去上夜班。\n每年过年我们一大家20多人就会团聚在一起，男人一桌，女人一桌，还有我们小孩一桌，其乐融融。最近两年过年三叔是唯一缺席的，由于工作的原因，三叔正巧在过年的时候上夜班。总觉得少了三叔过年的时候是个遗憾，现在看来以后过年要永远遗憾下去了。\n企业在追求经济效益的同时，往往会忽略员工的安全。员工伤亡事件屡见不鲜，却很难得到企业的重视。相比人类的伤亡，企业的经济效益显得那么苍白无力。听说钢厂每年总会出些事故，但是事故的赔偿是从所有员工的工资中扣除的，而不是工厂承担，这也是工厂对安全问题不够重视的原因，反正出了事掉血的是员工。\n现在村中的人大部分出去在外面打工，农忙时回家忙几天。在此提醒相亲们一定要注意人身安全，没有了安全保障赚再多的钱都白搭。\n谨以此文献给为工作而献身的三叔。\n","date":"2013-08-05T09:11:24Z","permalink":"/post/weak_life/","title":"脆弱的生命"},{"content":"\n这段时间，中国大部分的地区都是高温不降，有些地区甚至出现了鸡蛋自然孵化的现象。媒体报道高温热死人的现象也是时有发生，虽说媒体的话不可信，但至少从一个侧面反映了天气的确是较往年同期高出一些，开始渐渐超出人类身体可以承受的温度，开始悄悄的打破往年同期的高温纪录。\n最近几年气候变化无常，跟人类的活动绝对脱不了干系。冬天雾霾可以持续一周不散，春天一个月干旱，夏季雨天可以持续一个月不变，秋季如蝉的生命般短暂。在北方已经生活了二十多年的我，这些现象在小时候是极少碰到的，现在却是极其频繁。记得小时候雨天过后时常会在天边挂上一道弯弯的彩虹，记得最后一次见彩虹是在小学一二年纪的时候，自此之后彩虹仅存在了我的脑海里。对于现在的大部分中国人而言，彩虹仅存在于永恒的记忆中和孩子们的画中。\n人类近几百年来正在肆无忌惮的向地球母亲索要不该属于人类自己的东西，人类已经占有了迄今为止地球上对人类有价值的且可以占有的所有资源，人类仍然在忘形的开发并破坏着地球上生物赖以生存的家园。\n拿中国的三峡大坝举例，从能源的角度考虑的确是有利的。但是从地球生态的角度考虑肯定是有害的。人类的存在时间相对地球是短暂的，地球每一处地形存在就有它存在的理由，已经经过了无数年的实践验证说明地形存在的正确性。可恶的中国ZF，可恨的脑残砖家居然能够利用理论来论证修建三峡的必要及正确性，TMD没学过实践是检验真理的唯一标准。在没有对地球有充足的了解之前不要利用有限的理论来推断并指导实践，因为往往实践之后就再也回不了头，就比如三峡大坝。谁敢说近几年的西南大旱、特大地震跟三峡拖得了干系，可以灾难发生了又有哪个砖家可以站出来声称我可以对这个灾难负责呢？\n我从小就一直在担忧一个问题如果再过几十年后几百年后地球上的煤炭、石油等不可再生资源被人类用过了人类该何去何从，我时长为此而忧虑不已。也许这有点杞人忧天，肯定有人会站出来说到那时候随着人类科技的发展早就发现了新能源了，这是谁给的自信？谁这么大胆敢预言人类几十年后一定可以发现新能源？何况不可再生资源中蕴藏着的价值肯定不仅是燃烧带来的能量这点价值，假如几十年后人类已经将石油资源消耗殆尽了，却发现石油中蕴藏着巨大的能量，估计那时候我们只有哭的份了，楚人一炬，可怜焦土。\n人类不过是地球上几百万种生物中的一种，如果硬要从广义公平的角度来考虑，人类在生物界占有的太多了，人类已经把该占有的不该占有的全部占为己有，贪婪的本性暴露无遗。很难想象几十年过后我们人类的家园已经成为了什么样子，四处可见的是拔地而起的高楼，柏油路横一条竖一条，无论在地球的哪个角落都能找到人类留下的痕迹。地球该随着人类的发展何去何从，我不敢想象，我能做到的仅仅是节约点力所能及的资源，仅此而已。\n","date":"2013-07-31T18:46:36Z","permalink":"/post/the_earth/","title":"地球家园"},{"content":"成员函数的各种调用方式 非静态成员函数的调用方式 在C++中必须要保证类的非静态成员函数必须和非成员函数的执行效率一致，在编译的过程中，编译期已经将类的非静态成员函数编译为了非成员函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Test { public: int sum() const { return a + b; } int add() { return a;\t} int add(int size) { return a + size; } public: int a; int b; }; 在上述Test类中，编译器会在编译阶段对类中的成员函数做一些转换。下面列出了编译器可能会做出的变换，不同的编译器实现不太一致。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 int sum_TestFv(const Test * const this) { return this-\u0026gt;a + this-\u0026gt;b; } int add_TestFv(Test * const this) { return this-\u0026gt;a; } int add_Testi(int size, Test * const this) { return this-\u0026gt;a + size; } 通过上面的变换可以总结出如下规律：\n将成员函数重新改写为一个外部函数，并且对函数的名字进行处理，使在程序中唯一。一种可能的处理办法就是将函数名更改为：函数名_类名_函数参数。这样即解决了类之间函数名相同的问题，又解决了类之间函数重载的问题。 在函数的参数末尾添加额外的this指针参数。对于const函数添加的this指针为双const类型，对于非const函数则添加的this指针为指向的内容可变的const指针。 在函数内对成员函数的存取采用this指针来实现。 虚成员函数 虚函数如果是通过指针类型访问，需要在运行时动态决定指针指向的类型，因此需要访问虚函数表才能够获取正确的虚函数地址。访问虚函数的方式为(*ptr-\u0026gt;vptr[i])(ptr)，其中i代表要调用的虚函数在虚函数表中的索引，最后一个ptr代表要调用虚函数的编译器添加的this指针参数。\n关于虚成员函数的更详细问题会在下一个节中进行讨论。\n静态成员函数 静态成员函数中没有this指针，可以理解成带类作用域的全局函数，执行效率跟全局函数一致。\n虚成员函数 这部分内容是本书的核心内容，可以参考陈皓的博客相关文章，已经对C++中的虚成员函数和虚成员变量进行了说明。\nC++ 虚函数表解析 C++ 对象的内存布局(上) C++ 对象的内存布局(下) 单一继承下的虚函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Point { public: virtual ~Point(){} virtual Point\u0026amp; mult(float) = 0; float x() const {return _x;} virtual float y() const {return 0;} virtual float z() const {return 0;} protected: Point(float x=0.0) {_x = x;} float _x; }; class Point2d : public Point { public: Point2d(float x=0.0, float y=0.0) : Point(x), _y(y) {} ~Point2d(){} Point2d\u0026amp; mult(float){return *this;} float y() const {return _y;} protected: float _y; }; class Point3d : public Point2d { public: Point3d(float x=0.0, float y=0.0, float z=0.0) : Point2d(x, y), _z(z) {} ~Point3d(){} Point3d \u0026amp; mult(float) {return *this;} float z() const {return _z;} protected: float _z; }; 三个类对应的虚函数表会转化成下图\n通过图中可以看出每个函数在虚函数表中的位置无论在基类还是在子类中位置总是固定的。图中的Point的实例应该是不存在的，因为类中含有纯虚函数mult。\n要想调用ptr-\u0026gt;z()就变得非常容易，可以在编译器就可以确定虚函数的调用。虽然ptr所指向的对象在编译器并不能确定，但是编译器可以将其转化成为(*ptr-\u0026gt;vptr[4])(ptr)。因为z()函数总是在虚函数表中的第四个位置，唯一需要在执行期确定的就是ptr所指的对象的实际类型。\n多重继承下的虚函数 避免重复造轮子，参考上面博文。\n虚拟继承下的虚函数 避免重复造轮子，参考上面博文。\n函数的效率 非成员函数、静态成员函数、非静态成员函数都被转换成为了完全相同的形式。 inline函数的执行效率最高。 虚函数的效率最低。\n指向成员函数的指针 这里学习到一个新的语法，之前没有接触过。即指向类成员函数的指针及使用方法。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #include \u0026lt;iostream\u0026gt; using namespace std; class Point { public: virtual ~Point() {} float x() {return _x;} public: Point(float x=0.0) { _x = x; } float _x; }; int main() { Point point(1.0); float (Point::*p)(); // 定义指向成员函数的指针 p = \u0026amp;Point::x; // 为指向成员函数的指针赋值 cout \u0026lt;\u0026lt; (point.*p)(); // 调用指向类成员函数的指针 return 1; } 如果成员函数的指针并不用于虚函数、多重继承、虚基类等情况，则成员函数的指针效率跟非成员函数指针的效率一致。\n指向虚成员函数的指针 书中对于函数取地址的语法在gcc和vs2008下我试验不成功，语法错误。\n多重继承下指向成员函数的指针 依赖于编译器的实现，用到的情况比较少，没仔细看。\n指向成员函数指针的效率 在引入了虚函数、多重继承、虚基类等情况后，指向成员函数的指针效率有所下降。\n内联函数 内联只是一个请求，编译器并不一定会将函数内联的展开。\n形式参数 内联时每一个形参都会被对应的实参取代。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 inline int min(int i, int j) { return i \u0026lt; j ? i : j; } inline int bar() { int minval; int val1 = 1024; int val2 = 2048; minval = min(val1, val2); minval = min(1024, 2048); minval = min(foo(), bar() + 1); } minval=min(val1, val2)会被内联展开成minval = val1 \u0026lt; val2 ? val1 : val2。 minval = min(1024, 2048)会被扩展为minval = 1024。 minval = min(foo(), bar() + 1)需要引入临时对象，被扩展为\n1 2 3 int t1; int t2; minval = (t1 = foo()) , (t2 = bar() + 1), t1 \u0026lt; t2 ? t1 : t2; 局部变量 1 2 3 4 5 6 7 8 9 10 11 12 13 inline int min(int i, int j) { int minval = i \u0026lt; j ? i : j; return minval; } inline int bar() { int minval; int val1 = 1024; int val2 = 2048; minval = min(val1, val2); } 在内联函数中引入局部变量，内联函数在内联的时候局部变量会拥有一个唯一的名称。代码中的minval = min(val1, val2)会被内联为：\n1 2 int _min_lv_minval; minval = (_min_lv_minval = val1 \u0026lt; val2 ? val1 : val2), _min_lv_minval; 内联函数可以代替C语言中的#define宏定义，但是当内联函数调用次数过多，会产生大量的扩展代码，使程序的大小变大。\n","date":"2013-07-25T00:00:00Z","permalink":"/post/inside_the_c-_object_model_chapter_4/","title":"深度探索C++对象模型读书笔记_第四章：函数语意学"},{"content":" 1 2 3 4 class X {}; class Y : public virtual X {}; class Z : public virtual X {}; class A : public Y, public Z {}; 对于上述代码我在vs2008中的实验结果，X的大小为1，Y和Z的大小为4，A的大小为8。X的大小为1是因为编译器给空类了1字节的空间。Y和Z的大小为4是因为内部包含一个vbptr（指向虚基类的指针）占用了4个字节。A的大小包含了两个vbptr，分别指向虚基类的指针X。利用cl main.cpp /d1reportSingleClassLayoutA 命令可以查看对象的内存布局，利用vs2008调试界面查看对象的内存布局往往是不全的，不推荐采用此种方式。下面为A的类布局。 在64位的linux的g++下测试X、Y、Z、A的大小分别为1、8、8、16，这是因为指针的大小为8个字节。\n一个类占用的空间比类本身非静态数据成员空间大的原因有如下两点：\n编译器自动加上额外的数据成员，用来支持某些语言特性，例如virtual特性。 内存边界调整的需要 3.1 数据成员的绑定 味同嚼蜡的章节。\n3.2 数据成员的布局 数据成员在内存中的布局顺序跟数据成员在类中的声明顺序是一致的，而且现在的编译器都不关心数据成员在类中是public、protected还是private的。\n为了内存对齐，编译器在变量之间插入了空白字节，不同的编译器内存对齐的原则并不一致。\n为了实现虚函数机制，编译器插入了vptr成员变量。\n以上这些内容，本章节并没有展开详解。\n3.3 数据成员变量的存取 数据成员包括静态数据成员和非静态数据成员。\n静态数据成员变量放在静态存储区，不会造成任何空间或执行时间上的浪费。\n对于非静态数据成员，无论成员变量是struct数据成员、类数据成员、单一继承、多重继承情况下执行效率完全一样。执行效率较静态数据成员变量稍低。\n1 2 3 4 5 6 7 8 class Test { public: int a; int b; int c; }; Test test; 在上述例子中要想读取test.c的位置，编译器需要执行类似这样的操作：\u0026amp;test + \u0026amp;Test::c，可以看出对类中变量的存取成本多了一个算数运算。\n对于虚拟继承的情况由于需要在运行期才能决定存取操作，需要一些额外的成本，在下文讨论。\n3.4 继承与数据成员 如果类中不包含继承机制，则数据成员的布局和struct中数据成员的布局是一致的。\n本节将从单一继承但不包含虚函数、单一继承包含虚函数、多重继承、虚拟继承四个方面讨论数据成员变量。陈浩有几篇博文对此进行了详细的解释，比书上内容要易懂和全面，这几篇文章必看。\nC++ 虚函数表解析 C++ 对象的内存布局(上) C++ 对象的内存布局(下) 单一继承且不包含虚函数 书中举例解释了为什么类继承时类成员之间的填补空白会比单个类时要多，下图的内存布局图中Concrete3继承自Concrete2，Concrete2继承自Concrete1。Concrete3类占用的空间大小为：bit1占用的1个字节+3个字节的空白，bit2占用的1字节+3字节的空白，bit3占用的1字节+3字节空白。如果Concrete3不继承自任何对象，而是包含bit1、bit2、bit3三个变量，占用的空间大小为1+1+1+1=4。\n之所以编译器在继承机制中会作如此处理，是为了在继承机制中对象之间的默认按比特复制操作更方便。例如：\n1 2 Concrete1 *p1 = new Concrete1, *p2 = new Concrete2; *p2 = *p1;\t// 此时编译器只需要按比特复制就可以了 单一继承包含虚函数 假设有如下类定义\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Base { public: Base() { printf(\u0026#34;Base\\n\u0026#34;); } virtual ~Base() { printf(\u0026#34;~Base\\n\u0026#34;); } virtual void foo() {} int base_x; }; class Derived : public Base { public: Derived() { printf(\u0026#34;Derived\\n\u0026#34;); } ~Derived() { printf(\u0026#34;~Derived\\n\u0026#34;); } void foo() {} int derived_y; }; 则Derived类的对象模型如下，通过图可以非常清晰的理解单一继承包含虚函数的情况：\n多重继承 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 class Base1 { public: Base1() { printf(\u0026#34;Base1\\n\u0026#34;); } virtual ~Base1() { printf(\u0026#34;~Base1\\n\u0026#34;); } virtual void base1_virtual_func() {} int base1_x; }; class Base2 { public: Base2() { printf(\u0026#34;Base2\\n\u0026#34;); } virtual ~Base2() { printf(\u0026#34;~Base2\\n\u0026#34;); } void base2_not_virtual_func() {} int base2_x; }; class Derived : public Base1, public Base2 { public: Derived() { printf(\u0026#34;Derived\\n\u0026#34;); } ~Derived() { printf(\u0026#34;~Derived\\n\u0026#34;); } void derived_func() {} void base1_virtual_func() {} int derived_y; }; 则Vertex3d类的对象模型如下，同样通过图可以非常清晰的理解多重继承的情况：\n重复继承 书中并没有涉及到重复继承，重复继承是指某个基类被间接重复继承了多次，属于重复继承和钻石级多重虚拟继承的过渡情况，有必要说明一下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class Base { public: virtual void base_virtual_func() {} void base_func() {} int base_x; }; class Base1 : public Base { public: Base1() { printf(\u0026#34;Base1\\n\u0026#34;); } virtual ~Base1() { printf(\u0026#34;~Base1\\n\u0026#34;); } virtual void base_virtual_func() {} virtual void base1_virtual_func() {} int base1_x; }; class Base2 : public Base { public: Base2() { printf(\u0026#34;Base2\\n\u0026#34;); } virtual ~Base2() { printf(\u0026#34;~Base2\\n\u0026#34;); } virtual void base_virtual_func() {} void base2_not_virtual_func() {} int base2_x; }; class Derived : public Base1, public Base2 { public: Derived() { printf(\u0026#34;Derived\\n\u0026#34;); } ~Derived() { printf(\u0026#34;~Derived\\n\u0026#34;); } void derived_func() {} void base1_virtual_func() {} int derived_y; }; 通过下图的对象模型可以看出，重复继承的类Base在Derived的实例中存在两份，要想直接更改Base类中的base_x变量的值，不能通过derived.base_x = 1直接赋值的方式，需要调用derived.Base1::base_x = 1的方式来更改，更改后的效果仅更改了Base1对象对应的Base类实例中的base_x的值。\n钻石型多重虚拟继承 该种方式的继承已经是所有继承中最为复杂的了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class Base { public: virtual void base_virtual_func() {}\t// 虚基类最好是不再提供虚函数 void base_func() {} int base_x; }; class Base1 : virtual public Base { public: Base1() { printf(\u0026#34;Base1\\n\u0026#34;); } virtual ~Base1() { printf(\u0026#34;~Base1\\n\u0026#34;); } virtual void base_virtual_func() {} virtual void base1_virtual_func() {} int base1_x; }; class Base2 : virtual public Base { public: Base2() { printf(\u0026#34;Base2\\n\u0026#34;); } virtual ~Base2() { printf(\u0026#34;~Base2\\n\u0026#34;); } //virtual void base_virtual_func() {}\t// 由于是虚拟继承，不再能重复重载父类的虚函数了 void base2_not_virtual_func() {} int base2_x; }; class Derived : public Base1, public Base2 { public: Derived() { printf(\u0026#34;Derived\\n\u0026#34;); } ~Derived() { printf(\u0026#34;~Derived\\n\u0026#34;); } void derived_func() {} void base1_virtual_func() {} int derived_y; }; 在下图标出的区域中，我认为Base应该是不存在的，这里只是vs2013为了展示的考虑而添加上的。虚拟继承基类Base位于Derived类对象的除该成员外的最后位置。\n对象成员的效率 作者经过试验测试，继承下的类成员读写效率跟读写普通变量效率相差不大，虚拟继承对程序的读写效率有影响。这跟理论上相差不大。\n指向数据成员的指针 小技巧：可以通过\u0026amp;类名::变量名的语法来获取类成员变量在类对象中的位置，即相对于类对象起始地址的偏移量。\n书中后面的内容个人感觉没有必要看了。\n","date":"2013-07-24T00:00:00Z","permalink":"/post/inside_the_c-_object_model_chapter_3/","title":"深度探索C++对象模型读书笔记_第三章：数据语意学"},{"content":"2.1 默认构造函数的生成 只有当编译器需要默认构造函数的时候才会合成默认构造函数，并不是类只要没有定义默认构造函数编译器就会合成默认构造函数，而是只有以下四种情况编译器会生成默认构造函数。编译器合成的默认构造函数仅会处理类的基类对象和类中的数据成员对象，对于类中的普通类型的非静态数据成员并不会作任何处理。比如类中一个指针类型的数据成员，编译器合成的默认构造函数不会对该指针作任何处理，该指针就是一个野指针。\n带有默认构造函数的类成员对象 一个类没有定义任何构造函数，该类中包含了一个带有默认构造函数（包括了合成的默认构造函数和定义的默认构造函数）的类成员对象，那么编译器需要为此类合成一个默认构造函数，合成默认构造函数的时机为该构造函数被调用时。合成的默认构造函数默认为内联函数，如果不适合使用内联函数，就合成explicit static的构造函数。\n默认构造函数、复制构造函数和赋值操作符的生成都是inline。inline函数有静态链接，不会被当前文件之外的文件看到。如果函数过于复杂不适合生成inline函数，会生成一个explicit non_inline static实体。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Dopey { public: Dopey(); }; class Sneezy { public: Sneezy(int); Sneezy(); }; class Bashful { public: Bashful(); }; class Snow_White { public: Dopey dopey; Sneezy sneezy; Bashful bashful; private: int mumble; }; void foo() { Snow_White snow_white; } 在上述例子中，foo()中需要调用Bashful的构造函数，编译器会为Bar类生成内联的默认构造函数。Bashful类会生成类似于下面的默认构造函数。\n1 2 3 4 5 6 inline Bar::Bar() { dopey.Dopey::Dopey(); sneezy.Sneezy::Sneezy(); bashful.Bashful::Bashful(); } 默认构造函数的生成原则为：如果类A中包含了一个或一个以上的类成员对象，那么类A的默认构造函数必须调用每一个类成员的默认构造函数。但是不会初始化普通类型的变量，因此在上例中必须手动初始化mumble变量。在编译器合成的默认构造函数中类成员变量的默认构造函数的调用次序为成员变量在类中的声明顺序，该顺序和类成员的构造函数初始化列表顺序是一致的。\n如果Snow_White类定义了如下的默认构造函数，则编译器会自动在定义的构造函数中增加调用类成员变量的代码，调用类成员变量相应构造函数的顺序仍然和类成员变量在类中的声明顺序一致。\n从中可以看出类成员变量的构造函数的调用要早于类构造函数的调用，这一点是在很多面试题中经常会见到的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 定义的默认构造函数，包含了类成员变量sneezy的初始化列表 Snow_White::Snow_White() : sneezy(1024) { mumble = 2048; } // 编译器扩张后的默认构造函数 Snow_White::Snow_White() : sneezy(1024) { dopey.Dopey::Dopey(); // 调用默认构造函数 sneezy.Sneezy::Sneezy(1024); // 自动调用合适的构造函数 bashful.Bashful::Bashful(); mumble = 2048; } 带有默认构造函数的基类 在继承机制中，一个没有构造函数的子类继承自带有默认构造函数的基类，则子类的构造函数会被合成，并且会调用基类的默认构造函数。若子类没有定义默认构造函数，却定义了多个带参数的构造函数，编译器会扩张所有自定义的构造函数，将调用基类默认构造函数的代码添加到子类的构造函数的最前面。\n从这里可以看出继承机制中，首先构造基类，后构造子类，这点也是面试题中经常遇到的。\n带有虚函数的类 为了实现虚函数或虚继承机制，编译器需要为每一个类对象设定vptr（指向虚函数表的指针）的初始值，使其指向一个vtbl（虚函数表）的地址。如果类包含构造函数则编译器会生成一些代码来完成此工作；如果类没有任何构造函数，则编译器会在合成的默认构造函数中添加代码来完成此工作。\n带有虚基类的类 需要维护内部指针来实现虚继承。\n2.2 复制构造函数的生成 复制构造函数被调用有三种情况：\n明确的一个对象的内容作为另外一个对象的初始值。如X xx = x或X xx(x)。 对象作为参数传递给函数时。 类对象作为函数返回值时。 合成复制构造函数的情况 如果一个类没有提供显式的复制构造函数，同默认构造函数一样，只有编译器认为需要合成复制构造函数时，编译器才会合成一个。那么问题来了，什么时候编译器才合成复制构造函数呢？书中给出的答案为当一个类不展现出_bitwise copy semantics_1的时候。具体来说有以下四种情况，跟类的默认构造函数的合成基本一致。\n当类内包含一个类成员变量且类成员变量声明了复制构造函数。 当类继承的基类有复制构造函数（复制构造函数可以是显示声明或合成的） 一个类中包含了一个或多个虚函数 类继承自一个或多个虚基类 其中前面两种情况必须将成员变量或基类的复制构造函数的调用插入到合成的复制构造函数中，因此不是按照按比特复制的。第三和第四点分别用下面两小节来说明。\n重新设定虚函数表的指针 当编译器需要在类对象中设定一个指向虚函数表的指针时，该类就不能再使用按位复制的复制构造函数了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class ZooAnimal { public: ZooAnimal(); virtual ~ZooAnimal(); virtual void animate(); virtual void draw(); }; class Bear : public ZooAnimal { public: Bear(); void animate(); void draw(); virtual void dance(); }; void foo() { // yogi的vptr指向Bear的虚函数表 Bear yogi; // franny的vptr指向ZooAnimal的虚函数表 ZooAnimal franny = yogi; draw(yogi);\t// 调用Bear::draw() draw(franny);\t// 调用ZooAnimal::draw() } 合成出来的ZooAnimal的复制构造函数会明确设定对象的vptr指向ZooAnimal的虚函数表，而不是从右值中复制过来的值。\n处理virtual base class subobjects 虚基类的存在需要特别处理，一个类对象如果以另外一个类对象作为初始值，而后者有一个virtual base class subobjects，也会使按比特复制的复制构造函数失效。\n每一个编译器都必须让派生的类对象的virtual base class subobjects位置在执行期准备完毕。按比特复制的复制构造函数可能会破坏virtual base class subobjects的位置，因此编译器必须在自己合成出来的复制构造函数中修改。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class ZooAnimal { public: ZooAnimal(); virtual ~ZooAnimal(); virtual void animate(); virtual void draw(); }; class Raccoon : public virtual ZooAnimal { public: Raccoon(); Raccoon(int val); }; class RedPanda : public Raccoon { public: RedPanda(); RedPanda(int val); }; 文章的内容没有完全理解，虚继承机制使用较少，可以暂时不用理解。\n2.3 程序转化语意学 本节涉及到了编译器优化的相关细节，由于较容易理解，可以直接看书上内容，对工作帮助不大。包括类对象的初始化优化，函数参数的初始化优化，函数返回值的初始化优化，使用者层面的优化和编译器层面的优化。\n如果不是上节指定的四种情况，不需要显示的声明复制构造函数，因为显示的声明的复制构造函数往往效率不如编译器合成的复制构造函数效率高。编译器合成的复制构造函数利用memcpy()或memset()函数来合成，效率最高。\n2.4 类成员的初始化列表 说到类成员的初始化列表必然想起一个经常出现的面试题：成员初始化列表的顺序是按照成员变量在类中声明的顺序。如果成员初始化列表的顺序和成员变量在类中声明的顺序不一致时某些编译器会提示警告。编译器将成员初始化列表的代码插入到构造函数的最开始位置，优先级跟调用类类型的成员变量的默认构造函数是一致的，都是跟类类型成员变量在类中的声明次序相关。\n类成员初始化必须使用成员初始化列表的四种方式：\n初始化一个引用类型的成员变量 初始化一个const的成员变量 调用基类的构造函数，且基类的构造函数采用成员初始化列表的方式 调用类成员的构造函数，且类成员的构造函数采用成员初始化列表的方式 1 2 3 4 5 6 7 8 9 10 11 12 13 class Word { public: Word() { _name = 0; _cnt = 0; } private: String _name; int _cnt; }; 此例子在构造函数中对成员变量进行测试，编译器对构造函数的扩张方式可能会生成如下的伪码：\n1 2 3 4 5 6 7 8 Word::Word() { _name.String::String(); String temp = String(0); _name.String::operator=(temp); temp.String::~String(); _cnt = 0; } 构造函数中生成了一个临时性的String对象，这浪费了一部分开销。如果将构造函数该成如下的定义方式：\n1 2 3 4 Word() : _name(0) { _cnt = 0; } 即将其中的类成员变量更改为成员初始化列表的方式来初始化，编译器会自动将构造函数扩张为如下方式，这样减少了临时对象，提供了程序效率。\n1 2 3 4 5 Word::Word() { _name.String::String(0); _cnt = 0; } 引申 下面例子是对本章内容的一个简单概况，也是面试题中经常碰到的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 class A { public: A() { printf(\u0026#34;A\\n\u0026#34;); } ~A() { printf(\u0026#34;~A\\n\u0026#34;); } }; class B { public: B(int n) { printf(\u0026#34;B_%d\\n\u0026#34;, n); } ~B() { printf(\u0026#34;~B\\n\u0026#34;); } }; class Base { public: Base() { printf(\u0026#34;Base\\n\u0026#34;); } virtual ~Base() { printf(\u0026#34;~Base\\n\u0026#34;); } }; class Derived : public Base { public: Derived() : _m(1), _b(_m) { printf(\u0026#34;Derived\\n\u0026#34;); } ~Derived() { printf(\u0026#34;~Derived\\n\u0026#34;); } int _m; // 下面两个类类型的成员遍历的构造函数的调用次序跟在类中的声明次序是相关的 B _b; // 类类型的类成员变量，初始化列表中包含该变量 A _a; // 类类型的类成员变量 }; int main() { // 调用基类的构造函数-\u0026gt;调用子类类类型成员变量的构造函数-\u0026gt;调用子类的构造函数 Derived derived; return 0; // 根据栈的特点，类析构的次序跟构造是相反的 } 上述代码执行的结果为：\n1 2 3 4 5 6 7 8 Base B_1 A Derived ~Derived ~A ~B ~Base 总结 本章讲述了合成的默认构造函数、合成的复制构造函数和构造函数的成员初始化列表。其中如果类没有定义默认构造函数，只有在文中提到的四种情况下编译器才会合成默认构造函数。合成的复制构造函数在需要的时候编译器就会生成，默认是按对象比特复制的方式实现，有四种情况下编译器是不按照比特复制的方式。\n[1] bitwise copy semantics书中翻译为“位逐次拷贝”，就是按照内存中的字节进行复制类，感觉翻译不如不翻译好理解。\n","date":"2013-07-22T00:00:00Z","permalink":"/post/inside_the_c-_object_model_chapter_2/","title":"深度探索C++对象模型读书笔记_第二章：构造函数语义学"},{"content":"C++对象模型是深入了解C++必须掌握知识，而《深度探索C++对象模型》一书基本是理解C++对象模型的必须之作。可惜本书看起来更像是作者Stanley B.Lippman的随笔，语言诙谐，跟作者的另外一本经典之作《C++ Primer》有着天壤之别，侯捷的翻译也是晦涩难懂，跟侯捷翻译的其他作品也有一定差距（这两位大师还真是凑到一块了），所以这本书看起来还是很吃力的。这里挑选文中重点记录笔记，忽略扯淡部分，以备忘。\nC++的额外开销 C++相比C语言多出了封装、继承和多态等特性，新特性的增加必然会以牺牲一部分性能为代价。额外开销主要是由virtual引起的，包括：\n虚函数机制：需要在运行期间动态绑定。 虚基类：多次出现在继承中的基类有一个单一且被共享的实体。 C++对象模型 C++类成员变量包括：静态成员变量和非静态成员变量。成员函数包括：静态成员函数、非静态成员函数和虚函数。\n单一类的对象模型 1 2 3 4 5 6 7 8 9 10 11 12 class Point { public: Point(float xval); virtual ~Point(); float x() const; /* 非静态成员函数 */ static int PointCount(); /* 静态成员函数 */ protected: virtual ostream\u0026amp; print(ostream \u0026amp;os) const; /* 虚函数 */ float _x; /* 非静态成员变量 */ static int _point_count; /* 静态成员函数 */ } 该类的c++对象模型如下图： 通过图中可以看出：\n非静态数据成员直接放到了类的对象里面。 静态数据成员放到所有的类对象的外面，即静态存储区。 静态和非静态的成员函数放在类对象之外，即代码区。 如果类中存在虚函数，则会产生一个虚函数表（vtbl），表中的表项存储了指向虚函数的指针。在类对象的开始位置添加一个指向虚函数表的指针（vptr）。vptr的赋值由类的构造函数和赋值运算符自动完成。 虚函数表的第一项指向用来作为动态类型识别用的type_info对象。 C++支持的编程范式(programming paradigms) 程序模型：通俗的理解成C语言的面向过程编程方式。 抽象数据类型模型：通过类封装成为了一种新的数据类型，该数据类型有别于基本数据类型。 面向对象模型：利用封装、继承和多态的特性。 C++支持多态的方式 隐含的转换操作，例如通过父类的指针指向子类的对象。shape *ps = new circle(); 通过虚函数机制。 通过dynamic_cast强制类型转换。如if (circle *pc = dynamic_cast\u0026lt;circle*\u0026gt;(ps))。 类对象的内存构成 非静态数据成员。 由于内存对齐而添加到非静态数据成员中的空白。 为了支持虚机制（包括：虚函数和虚继承）而额外占用的内存。 利用工具查看对象模型 查看C++类的对象模型有两种比较简便的方式，一种是使用Virtual Studio在调试模式下查看对象的组成，但是往往显示的对象不全;另外一种是通过Virtual Studio中cl命令来静态查看。\n本文选择使用cl工具的方式，cl命令位于C:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\VC\\bin目录下，为了方便使用可以将该变量添加到Path环境变量中。在命令行中执行cl命令，提示“计算机中丢失mspdb80.dll”，该文件位于C:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\Common7\\IDE目录下，将msobj80.dll,mspdb80.dll,mspdbcore.dll,mspdbsrv.exe四个文件复制到C:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\VC\\bin目录下即可。\n通过cl xxx.cpp /d1reportSingleClassLayoutXX命令即可查看文件中类的对象模型，其中该命令最后的XX需要更换为要查看的类名，中间没有空格。\n执行上述命令时提示：无法打开文件“LIBCMT.lib”。该文件位于C:\\Program Files (x86)\\Microsoft Visual Studio 9.0\\VC\\lib目录下，将该目录添加到环境变量lib中。重新打开命令行执行cl提示：无法打开文件“kernel32.lib”，将C:\\Program Files\\Microsoft SDKs\\Windows\\v6.0A\\Lib添加lib环境变量中。\n","date":"2013-07-20T00:00:00Z","permalink":"/post/inside_the_c-_object_model_chapter_1/","title":"深度探索C++对象模型读书笔记_第一章：关于对象"},{"content":"本程序将讲解java调用C语言写的二进制文件，并将二进制文件中的内容利用Java读出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; union data { int inter; char ch; }; struct Test { int length; char arr[20]; void toBigEndian() { union data c; c.inter = 1; if(c.ch == 1) { // 小端 unsigned char temp; unsigned char *tempData = (unsigned char *)\u0026amp;length; for (int i=0; i \u0026lt; sizeof(int) / 2; i++) { temp = tempData[i]; tempData[i] = tempData[sizeof(int) - i - 1]; tempData[sizeof(int) - i - 1] = temp; } } } }; int main() { Test test; memset(\u0026amp;test, 0, sizeof(Test)); test.length = 0x12345678; strcpy(test.arr, \u0026#34;hello world\u0026#34;); test.toBigEndian(); FILE *file = fopen(\u0026#34;test.txt\u0026#34;, \u0026#34;w+\u0026#34;); fwrite(\u0026amp;test, sizeof(Test), 1, file); fclose(file); return 1; } 本例子中的C程序将一个包含int变量和char数组的结构体写入文件中。\n其中需要考虑到机器的大小端问题，java程序采用的大端字节序，因此这里将C的结构体在写入文件时转换成大端字节序。在将结构体写入到文件时，将其中的int类型变量转换成大端字节序，如果机器本身即为大端字节序则不需要转换字节序。\njava端读取的文件代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import java.io.File; import java.io.FileInputStream; import java.io.FileNotFoundException; import java.io.IOException; public class Main { public static int byte2int(byte[] res) { int targets = (res[3] \u0026amp; 0xff) | (res[2] \u0026lt;\u0026lt; 8) | (res[1] \u0026lt;\u0026lt; 16) | (res[0] \u0026lt;\u0026lt; 24); return targets; } /** * @param args */ public static void main(String[] args) { File file = new File(\u0026#34;test.txt\u0026#34;); if (!file.exists()) { System.out.println(\u0026#34;文件不存在\u0026#34;); return; } byte[] data = new byte[50]; try { FileInputStream fis = new FileInputStream(file); int size = fis.read(data); System.out.println(\u0026#34;读取到\u0026#34; + size + \u0026#34;个字节的数据\u0026#34;); } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } // 转换完成的int值 int value = byte2int(data); System.out.printf(\u0026#34;%x\\n\u0026#34;, value); StringBuffer sb = new StringBuffer(); for (int i=4; i\u0026lt;24; i++) { System.out.print((char)data[i]); } } } ","date":"2013-07-19T15:47:40Z","permalink":"/post/java_call_c_bit_file/","title":"Java读取C语言写的二进制文件"},{"content":"\n药是什么？药是人类文明的发展过程中不断克服自身疾病的必然产物。谁让外星人造人的时候没有把人类制造的那么完美，要是人类除了生老死之外没有病这个状态，或许药物就不会产生。\n现实生活中，过度依赖药物的人比比皆是，尤其是在中国。在中国，药物已经用到了满天飞的程度了。得个小感冒会去药店买药，医生除了推荐你感冒药之外，肯定会推荐你消炎药。以至于大众普遍认为，有炎症必须吃消炎药。感冒烧到38度，医院医生会推荐你打针。感冒烧到38.5度，医生会推荐你挂上三天吊瓶，美其名曰挂吊瓶好的快，如果患者的心理能起到恢复快的作用的话，那么挂吊瓶应该是管用的。\n我没去过西方国家，但我猜测在西方国家，医药分离的占多数。得病了想吃个消炎药解解馋可没那么容易，消炎药岂能是想吃就吃，没有医生的处方药店怎敢卖给患者，又不是口香糖。想打个针过过瘾更是难了，喜欢花钱让针扎着屁股玩的活中国人比较喜欢。要想挂个吊瓶数滴答玩，除非烧到了40度一星期高烧不退，否则还是自己在家挂瓶自来水数着玩吧。以上有杜撰的成分，但是话激理不歪，你懂得。\n要知道中国较西方的文明程度还有差距。西方发明的西药在西方都谨慎使用了，反倒在中国大行其道。谁家没一抽屉瓶瓶罐罐，这可都是毒药啊。除了可以拉动GDP外，有理由怀疑中国担心未来人口老龄化加剧给经济发展造成压力而采取的措施。\n是时候阐述本文的观点了，药物不是万能的，能少用尽量少用。\n中药讲究是药三分毒，换成西药是药五分毒应该毫不夸张吧。当时治好了我们身体的病症，感觉一身轻松了，但是后患无穷。人类本身就具备一定的修复能力，而且修复能力很强。在人类自我修复能力许可的范围内非要硬用药物加速治疗，那造成的必然结果就是身体下次不干了，有药物可以对抗疾病，那用身体的修复能力干嘛。久而久之，身体就再也扛不住疾病了。\n吃西药多了往往治的病好了，却带来了其他疾病。比如感冒了吃消炎药，吃着吃着把胃给吃坏了。治感冒的医生才不会管你的其他身体部位情况，反正胃吃坏了找不到他。这也是医院分那么多科室的一个弊端，只从局部看问题，不能从整体上看问题，所以看到的问题总是片面的。\n解决药物依赖的问题从三方面着手解决。一方面要从心理上战胜自己，不要迷信药物，更不能依赖药物。另一方面，多运动，生命在于运动，真理中的真理。第三，眼光放长远，多看其他国家，多了解长寿的人是怎么生存的。\n真心希望过度依赖药物的人可以走出这个误区。\n","date":"2013-07-17T18:24:50Z","permalink":"/post/rely_on_drug_over/","title":"药物过度依赖"},{"content":"本文摘录《C/C++程序员面试宝典》一书中我认为需要注意的地方。\n数组指针与指针数组的区别 数组指针即指向数组的指针，定义数组指针的代码如int (*ap)[2]，定义了一个指向包含两个元素的数组的数组指针。 如果数组的每一个元素都是一个指针，则该数组为指针数组。实例代码如下：\n1 char *chararr[] = {\u0026#34;C\u0026#34;, \u0026#34;C++\u0026#34;, \u0026#34;Java\u0026#34;}; int (*ap)[2]和int *ap[2]的区别就是前一个是数组指针，后一个是指针数组。因为\u0026quot;[]\u0026ldquo;的优先级高于\u0026rdquo;*\u0026quot;，决定了这两个表达式的不同。\npublic、protected、private 修饰成员变量或成员函数 public：可以被该类中的函数、子类的函数、友元函数和该类的对象访问。 protected：可以被该类中的函数、子类的函数和友元函数访问，不能被该类的对象访问。 private：只能由该类中的函数或友元函数访问。 默认为private权限。 用在继承中 public：基类成员保持自己的访问级别，基类的public成员为派生类的public成员，基类的protected成员为派生类的protected成员。 protected：基类的public和protected成员在派生类中未protected成员。 private：基类的所有成员在派生类中为private成员。 ","date":"2013-07-16T11:30:56Z","permalink":"/post/c_c-_review/","title":"C/C++程序员面试宝典读书笔记"},{"content":"\n今天下午新办的工资银行卡办下来了，原先工资卡用的招商银行的金卡，现在新办了个农村信用社的普通卡，有种从城市的大马路走到了胡同里的城中村的感觉。于是大家高高兴兴的蹦蹦跳跳的屁颠屁颠的去领来的新的工资卡。为什么大家会高兴呢？这也是本文的因子，因为大家都已经有两个月没发工资了，新卡到说明工资在不久的将来也会到，大家当然高兴啦。\n可是深入想一下，大家真的应该高兴吗？工资本来就是公司单方面拖欠大家的，给大家造成的损失员工宽宏大量没有跟公司计较也就罢了。工资关系到民生，直接影响了员工每个人的生活。要是搁到法国估计一天不发工资，员工早就集体罢工了吧。换卡势必会造成大家的麻烦，原来的银行卡可能就要销户，新的银行是否足够方便。我们却是心宽体胖，公司的错既往不咎，只要看到发工资的希望我照样努力为公司奉献。\n这就好比是在中国假期几天高速路不收费，有车一族就喜出望外，“在中国真幸福，可以赚到国家便宜了”。殊不知，世界已建成14万公里收费公路，10万公里在我国。不知道了解这个之后，有车一族还笑得出来吗？我估计连哭的心态都有了。\n那大家应该是什么心态去领工资卡呢？平常心就好。至少不至于表现出屁颠屁颠的心态吧。\n中国两千多年的封建统治对人民的思想影响是深远的，人们心中早就建立起了人分三六九等的等级观念。而且这种等级观念影响是深远的，悄无声息的深入到我们所谓的“社会主义国家”内部的各个角落。电视里整天的宫廷剧，格格与皇阿玛齐飞，太监共丫鬟一色，这些可是典型的民权不公，民生不平。我们可是就靠这些文化垃圾养大的，心里怎么会没有了奴隶的思想呢？百姓心中的怕官、傍官的思想及考官的行为直接复制到了现在不曾改变，奴隶的心态也就不会改变。\n好在现代文明来了，互联网时代来了。西方的文明渐渐深入，我们有的救。打倒xx，打倒xx，期待来一场轰轰烈烈的革新吧。\n如果说奴隶的心态为“给你点阳光你就灿烂，给你点洪水你就泛滥”，期待我们的心态变成“没有阳光也要比比灿烂，没有洪水也要试图泛滥”。\n题图：《被释放的姜戈》电影海报\n","date":"2013-07-11T18:42:35Z","permalink":"/post/slave_mind/","title":"奴隶的心态"},{"content":"\n这是来自一个保守主义者的geeker的傻X的吐槽。\n最近股市暴跌，我却在思考为何有股票这个东西。我从未入市且对股市一点兴趣都没有甚至是反感。我所认识的股市游戏规则就是价位低的时候大股东入市，价位高的时候大股东抛售，大股东一吐一吞钱到手了，股民被掏空了。很多股民却用侥幸心理来入市，有赔有赚，出市的时候裤子都赔进去了。\n搞不懂那么多经济学专家、博士、硕士、学士，却搞不定一个经济学。若是真搞不懂，索性就不要去研究，反正学了也白学，浪费这个资源误人歧途干啥。一个砖家说熊，另一个砖家说牛，相互对掐有意思吗？\n为什么搞股市这个东西，这是嫌经济学不够复杂吗？股市难道能推动人类社会的进步？股市是企业融资的一种手段，而能上市的公司往往是相对不太缺钱的，而最缺钱的是小型创业公司。这也就早就了很多公司把上市套现作为了一个目标，这不明摆着投机取巧，一夜暴富。传销是集合了底层的力量资金而构成了金字塔，企业通过上市手段获取到了股民的资本来运作而发展自身，只不过金字塔只有两层罢了。\n假如没有股市，很多上班族就会坐在办公桌上安心工作，而不用时时刻刻关心着像过山车一样的死难看的折线，也不用设个老板键提心吊胆的担心自己的boss悄悄走到自己的面前。难道安安心心全身心投入工作不是更好？\n假如没有股市，或许就少了一个行业，一部分人就可以全身心投入到其他行业，带动其他行业的发展，推动历史进步，岂不快哉！\n假如没有股市，就不会有人赚发了之后，别人也总想着不劳而获，间接助长了人们投机取巧的气焰。\n假如没有股市，就不会因此而发家，当然这是少数。郭美美的母亲也不会用4万块钱赚到100万，也不会有郭美美的今天，也不会有红十字会的今天，当然这只是个笑话。没有股市郭照样可以炫富，因为人家本来就不是靠的股市。没有股市红十字会照样会没落，因为他们的本质就是那样。\n假如没有股市，就不会有人因为股市而跳楼的新闻，就不会有人将养老钱都搭进去了，就不会\u0026hellip;\n请不要职责我，因为我是一个傻X，永远不要和傻子讲道理，否则你也会变成一个傻X！\n","date":"2013-07-10T00:00:00Z","permalink":"/post/foolish_talk_stock/","title":"一个傻×对股市的吐槽"},{"content":"最近有同事想离职了，上午在跟领导一番谈话之后又有些犹豫了。大概领导会说些公司状况及个人发展之类的话语来说服同事不要离职，用大腿想想都知道的事。同事肯定会说自己的几个抱怨，领导肯定会一一还击，到最后领导领导一摊手“看没什么顾虑的吧，年轻人就是冲动，你再想想吧，先回去好好干活吧”。\n我在这里想说的事不要管别人怎么想，做自己，坚持自己的想法。\n一千个人眼里有一千个郭美美。有人说郭美美的自拍照真漂亮，有人说郭美美卸妆之后直接毁三观，有人说是郭美美毁掉了红十字会，有人说没有郭美美红十字会一样玩完，有人说郭美美炫富就是为了出名，有人说郭美美是偶像，有人说郭美美是呕像，有人说郭美美为咱们的游戏代言应该很棒，有人说\u0026hellip;\n同一件事情在不同人眼里就会有不同的看法。当然我的观点，在别人看来也许是那么的反感或不屑一顾。\n做技术的天天跟机器打交道，往往语言表达能力较弱，不善辩，更别提忽悠别人了。经过领导只言片语的轰炸，自己已经是茫然不知所措，自己最初的坚持在渐渐退去，别人的想法正在填充你的大脑，恭喜你被洗脑了。但是这种洗脑往往是暂时性的，往往当是生效，事后理性分析一下觉得“不对啊，我当时怎么了”。所以搞传销的一培训就是在一个鸟拉屎的地方培训上几个月来彻底洗脑。\n我还好，至少我会坚持自己的观点。记得我上次离职时，领导找我谈话，领导叽里呱啦说了很多，我明知道不对头，但却愣是无力反驳，那时候心里的就想说“求求你，别说了”。\n离职对同事自身而言是好事。同事工作两年，在公司工作一年多了，由于公司业绩差，工资非但没有涨过，最近甚至都拖欠了一个月的工资了，而且如果待在公司未来一年内都不涨工资的可能性极大。换个工作薪水可以提高，更可以接触到新鲜的技术，对自己无论在物质还是在技术上都是有利无害。我可以想到的唯一坏处就是舍不得公司，舍不得同事，舍不得那个靠窗的小办公桌。可是这些算个屁，公司的发展跟普通员工毛关系都没有，公司好坏员工拿到的都是稀薄的工资，何况公司已经发不下工资来了，是公司待你不厚道。感情在涉及到金钱后往往变得一文不值。\n就公司而言是坏事。公司恨不得不发你工资光给公司干活。领导之所以留你是因为你对公司的业务熟悉，公司如果再重新招人没个一个月的时间很难上手。假如换成了在公司里天天闲的蛋疼的人提离职，公司可能就巴不得立马卷铺盖走人。领导留人只能说明领导对你的肯定，其他的说明不了什么。公司的前途未卜，领导凭什么忽悠员工留下来？\n这是一个相互炒鱿鱼的时代。员工跟公司之间是对等的关系，公司对员工不满意可以解除合同，员工对公司不满意照样可以将公司炒掉换个新鲜的工作。\n做自己，坚持自己的观点，适当聆听别人的建议，才会赢！\n","date":"2013-07-05T19:08:58Z","permalink":"/post/do_your_self/","title":"做自己"},{"content":"本文是翻译的solrcloud的官方英文文档，本文仅将文中重点翻译，原文地址点这里。英文水平不咋地，翻译篇文章也算练练手。\nSolrCloud SolrCloud是Solr的分布式集群。可以通过集群来搭建一个高可用性，容错性的Solr服务。当想搭建一个大规模，容错性，分布式索引，查询性能好的Solr服务时可以采用SolrCloud。\n关于SolrCores和Collections的一点小知识 在单机运行时，单独的索引叫做SolrCore。如果想要创建多个索引，可以创建多个SolrCore。利用SolrCloud，一个索引可以存放在不同的Solr服务上。意味着一个单独的索引可以由不同的机器上的SolrCore组成。不同机器上的SolrCore组成了逻辑上的索引，这些SolrCore叫做collection。组成collection的SolrCore包括了数据索引和备份。\n例子A： 简单两个shard集群 这个例子简单创建了包含两个solr服务的集群，一个collection的数据分布到两个不同的shard上。 因为在这个例子中我们需要两个服务器，这里仅简单的复制example的数据作为第二个服务器，复制example目录之前需要确保里面没有索引数据。\n1 2 rm -r example/solr/collection1/data/* cp -r example example2 下面的命令会启动一个solr服务并启动一个新的solr集群。\n1 2 cd example java -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -DzkRun -DnumShards=2 -jar start.jar -DzkRun 参数会在solr服务中启动一个内置的zookeeper服务。 -Dbootstrap_confdir=./solr/collection1/conf 因为在zookeeper中没有solr配置信息，这一参数会将本地的./solr/conf目录下的配置信息上传到zookeeper中作为myconf配置参数。myconf是在下面的collection.configName参数中指定的。 -Dcollection.configName=myconf 为新的collection设置配置名称。如果不加这个参数配置默认名称为configuration1。 -DnumShards=2 划分索引到逻辑分区的个数。 浏览http://localhost(本地主机):8983/solr/#/~cloud可以看到集群的状态。\n通过目录树可以看到配置文件已经上传到了/configs/myconf/目录下，一个叫collection1的collection已经创建，在collection1下是shard的列表，这些shard组成了完整的collection。\n接下来准备启动第二个服务器，因为没有明确的设置shard的id，该服务会自动分配到shard2。\n启动第二个服务，并将其指向集群。\n1 2 cd example2 java -Djetty.port=7574 -DzkHost=localhost:9983 -jar start.jar -Djetty.port=7574来指定Jetty的端口号。 -DzkHost=localhost:9983用来指定Zookeeper集群。在本例中，在第一个Solr服务中运行了一个单独的Zookeeper服务。默认情况下，Zookeeper的端口号为Solr服务的端口号加上1000，即9983。 通过访问http://localhost(本地主机):8983/solr/#/~cloud，在collection1中就可以看到shard1和shard2。\n下面对一些文档建立索引。\n1 2 3 4 5 6 7 cd exampledocs java -Durl=http://localhost:8983/solr/collection1/update -jar post.jar ipod_video.xml java -Durl=http://localhost:8983/solr/collection1/update -jar post.jar monitor.xml java -Durl=http://localhost:8983/solr/collection1/update -jar post.jar mem.xml 无论是向集群中的任何一台服务器请求都会得到全部的collection：http://localhost:8983/solr/collection1/select?q=*:*。\n假如想更改配置，可以在关闭所有服务之后删除solr/zoo_data目录下的所有内容。\n实际测试插入速度要比单个服务慢。\n例子B：简单的两个shard重复的shard集群 本例子会通过复制shard1和shard2来创建上一个例子。额外的shard备份可以有高可用性和容错性，简单提升索引的查询能力。\n首先，在运行先前的例子中我们已经有了两个shard和一些索引文档。然后简单的复制这两个服务：\n1 2 cp -r example exampleB cp -r example2 example2B 然后，在不同的端口上启动两个新的服务：\n1 2 cd exampleB java -Djetty.port=8900 -DzkHost=localhost:9983 -jar start.jar 1 2 cd example2B java -Djetty.port=7500 -DzkHost=localhost:9983 -jar start.jar 重新浏览网址http://localhost(本地主机):8983/solr/#/~cloud，检查四个solr节点是否已经都启动。 因为我们已经告诉Solr我们需要两个逻辑上的shard，启动后的实例3和4会自动的成为原来shard的备份。\n向集群中的任意一个服务发起查询：http://localhost:7500/solr/collection1/select?q=*:*。多次发起这个查询并查看solr服务的日志。可以观察到Solr通过备份对请求做了平衡，通过不同的服务来处理请求。\n为了证明高可用性，在除了运行Zookeeper的服务上按下CTRL-C。（在例子C中将会讨论Zookeeper的冗余）当服务终止后，发送另外一个查询请求到其他服务，仍然能够看到所有的结果。\n在没一个shard至少还有一个服务时，SolrCloud仍然可以提供服务。可以通过关闭每一个实例来查看结果。假如关闭了一个shard的所有的服务，到其他服务的请求就会收到503错误。为了能够返回其他可用的shard中的文档，可以在请求中增加参数：shards.tolerant=true。\nSolrCloud用leaders和overseer来作为具体的实现。一些节点或备份将会扮演特殊的角色。不需要担心杀死了leader或overseer，假如杀死了其中的一个，集群会自动选择一个新的leader或overseer，并自动接管工作。任何的Solr实例都可以成为这种角色。\n例子C：两个shard集群，shard带备份和zookeeper集群 在例子B中问题是虽然有足够的Solr服务器可以避免集群挂掉，但是仅有一个zookeeper服务来维持集群的状态。假如zookeeper服务挂掉了，分布式的查询还是可以工作的，因为solr服务记录了zookeeper最后一次报告的状态。问题是没有新的服务器或客户端能发现集群的状态，集群的状态也不会改变。\n运行多个zookeeper服务可以保证zookeeper服务具有高可用性。每一个zookeeper服务需要知道集群中的其他服务，大部分服务需要提供服务。例如，一个含有三个zookeeper服务的集群允许其中一个失败剩余的两个仍然可以提供服务。五个zookeeper服务的集群可以允许一次失败两个。\n从产品角度考虑，推荐使用单独的zookeeper服务而不是solr服务中集成的zookeeper服务。你可以从这里读取到更多的zookeeper集群。在这个简单的例子中，我们仅简单的使用了集成的zookeeper。\n首先，停止四个服务，并清空zookeeper中的数据作为一个新的开始。\n1 rm -r example*/solr/zoo_data 我们仍然将服务分别运行在8983,7574,8900,7500端口。默认是在端口号+1000的端口上启动一个zookeeper服务，第一次运行的时候在另外三台服务器上zookeeper的地址分别为：localhost:9983,localhost:8574,localhost:9900。\n为了方便通过第一个服务上传solr的配置到zookeeper集群中。在第二个zookeeper服务启动之前程序会阻塞。这是因为zookeeper在工作的时候需要其他服务。\n1 2 3 cd example java -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -DzkRun -DzkHost=localhost:9983,localhost:8574,localhost:9900 -DnumShards=2 -jar start.jar 1 2 cd example2 java -Djetty.port=7574 -DzkRun -DzkHost=localhost:9983,localhost:8574,localhost:9900 -jar start.jar 1 2 cd exampleB java -Djetty.port=8900 -DzkRun -DzkHost=localhost:9983,localhost:8574,localhost:9900 -jar start.jar 1 2 cd example2B java -Djetty.port=7500 -DzkHost=localhost:9983,localhost:8574,localhost:9900 -jar start.jar 现在我们运行了三个内置的zookeeper服务，如果一个服务挂掉之后其他一切正常。为了证明，在exampleB上按下CTRL+C杀掉服务，然后浏览http://localhost:8983/solr/#/~cloud来核实zookeeper服务仍然可以工作。\n需要注意的是，当运行在多个机器上，需要在每一台机器上设置-DzkRun=hostname:port属性。\nZooKeeper 多个zookeeper服务同时运行来避免错误和高可用性叫做ensemble。从产品角度，推荐运行外部的zookeeper ensemble来代替solr集成的zookeeper。浏览zookeeper官方网站下载和运行一个zookeeper ensemble。可以参考Getting Started和ZooKeeper Admin。非常简单就可以运行。可以坚持使用solr来运行zookeeper集群，但是必须知道zookeeper集群不是非常容易动态改变的。除非solr增加对zookeeper更好的支持，重新开始是最好的改变方式。zookeeper和solr是两个不同的进程是最好的方式。\n当solr运行内置的zookeeper服务时，默认会使用solr服务的端口号加上1000作为zookeeper的客户端端口号。另外，默认会增加一个zookeeper的客户端端口号和两个zookeeper的选举端口号。所以在第一个例子中，solr运行在8983端口，内置的zookeeper服务运行在9983端口作为客户端端口，9984和9985作为服务端端口。\n当增加了更多zookeeper节点可以提高读性能，但是会稍微降低写性能。当集群状态稳定的时候，Solr用的Zookeeper非常少。下面有一些优化zookeeper的建议：\n最好的情况是zookeeper有一个专用的机器。zookeeper是一个准时的服务，专用的机器可以确保及时响应。当然专用的机器不是必须的。 当把事务日志和snap-shots放到不同的磁盘上可以提高性能。 假如zookeeper和solr运行在同一台机器上，利用不同的磁盘可以提高性能。 参考文档 https://wiki.apache.org/solr/SolrCloud\n","date":"2013-07-05T16:57:14Z","permalink":"/post/solrcloud_translate/","title":"SolrCloud官方文档翻译"},{"content":"前几天写了篇《在Linux上搭建solr环境》的博文，是基于solr3.6.2的安装。本文仅记录在tomcat7.0.41上搭建solr4.3.1搭建过程中需要注意的地方，其他地方可以参考上一篇博文。\n配置完成之后发现http://192.168.20.38:8090/solr无法访问，但是http://192.168.20.38:8090/却可以访问，通过查看tomcat的日志文件localhost.2013-07-03.log，发现里面有如下错误提示。\n1 2 严重: Exception starting filter SolrRequestFilter org.apache.solr.common.SolrException: Could not find necessary SLF4j logging jars. If using Jetty, the SLF4j logging jars need to go in the jetty lib/ext directory. For other containers, the corresponding directory should be used. For more information, see: http://wiki.apache.org/solr/SolrLogging 解决办法：将~/solr-4.3.1/example/lib/ext目录下的所有jar文件复制到~/apache-tomcat-7.0.41/lib目录下，然后重启tomcat即可。\n相关下载 用到的文件\n","date":"2013-07-03T10:00:30Z","permalink":"/post/solr4.3.1_setup/","title":"在tomcat7.0.41上搭建solr4.3.1"},{"content":"本文以《在Linux上搭建solr环境》为基础，假设已经在192.168.20.6和192.168.20.38上搭建了单机版solr环境。\n主服务器配置 找到solr的环境目录下的conf文件夹下的solrconfig.xml文件，我的是在/hadoop/solr/conf/solrconfig.xml目录下，打开后找到如下行\n1 \u0026lt;requestHandler name=\u0026#34;/replication\u0026#34; class=\u0026#34;solr.ReplicationHandler\u0026#34; \u0026gt; 默认是被注释的，将其修改为\n1 2 3 4 5 6 7 \u0026lt;requestHandler name=\u0026#34;/replication\u0026#34; class=\u0026#34;solr.ReplicationHandler\u0026#34; \u0026gt; \u0026lt;lst name=\u0026#34;master\u0026#34;\u0026gt; \u0026lt;str name=\u0026#34;replicateAfter\u0026#34;\u0026gt;commit\u0026lt;/str\u0026gt; \u0026lt;str name=\u0026#34;replicateAfter\u0026#34;\u0026gt;startup\u0026lt;/str\u0026gt; \u0026lt;str name=\u0026#34;confFiles\u0026#34;\u0026gt;schema.xml,stopwords.txt\u0026lt;/str\u0026gt; \u0026lt;/lst\u0026gt; \u0026lt;/requestHandler\u0026gt; replicateAfter表示solr会在什么情况下复制，可选项包括：commit、startup、optimize，这里保持默认。 confFiles表示要分发的配置文件。\n从服务器配置 在从服务器上，将/hadoop/solr/conf/solrconfig.xml文件相应的修改为\n1 2 3 4 5 6 \u0026lt;requestHandler name=\u0026#34;/replication\u0026#34; class=\u0026#34;solr.ReplicationHandler\u0026#34; \u0026gt; \u0026lt;lst name=\u0026#34;slave\u0026#34;\u0026gt; \u0026lt;str name=\u0026#34;masterUrl\u0026#34;\u0026gt;http://192.168.20.6:8080/solr/replication\u0026lt;/str\u0026gt; \u0026lt;str name=\u0026#34;pollInterval\u0026#34;\u0026gt;00:00:60\u0026lt;/str\u0026gt; \u0026lt;/lst\u0026gt; \u0026lt;/requestHandler\u0026gt; masterUrl为服务器的url地址。 pollInterval为从服务器的同步时间间隔。\n","date":"2013-07-02T14:29:23Z","permalink":"/post/solr_setup_distribute/","title":"搭建分布式的solr环境"},{"content":"本文采用Linux操作系统在hadoop用户下安装，solr采用3.x中的最新版本3.6.2，tomcat采用6.0.37版本，安装包可以从本文下方链接下载。 这里有两种安装方式，一种方式为利用solr自带的jetty来启动solr，默认端口为8983。另外一种方式为将solr集成到tomcat中。其中第一种方式较为简单，推荐新手采用。\n独立启动 将sorl的安装包解压到用户的根目录下，解压后文件夹为apache-solr-3.6.2。 进入到example目录下，执行java -jar start.jar命令，solr服务启动，端口为8983。 通过http://IP地址:8983/solr/来访问solr的web页面，进入admin页面后可以通过输入字符串来查找索引。查找索引默认显示的格式为xml格式，可以通过在url的后面加上参数wt=json来显示json格式的结果。 利用tomcat 安装tomcat 1. 将apache-tomcat-6.0.37.tar.gz解压到hadoop的跟目录下。 2. 修改hadoop用户的环境变量，执行vi ~/.bash_profile命令，添加如下：\n1 2 3 export CATALINA_HOME=/home/hadoop/apache-tomcat-6.0.37 export CLASSPATH=.:$JAVA_HOME/lib:$CATALINA_HOME/lib export PATH=$PATH:$CATALINA_HOME/bin 3. 执行source ~/.bash_profile 使修改的环境变量生效。 4. 执行tomcat的bin目录下的startup.bat脚本来启动tomcat。 5. 通过netstat -anp | grep 8080命令查看tomcat是否启动。\n安装solr 1. 将solr的dist/apache-solr-3.6.2.war文件复制到tomcat的webapps目录下，并将文件命名为solr.war。执行cp ~/apache-solr-3.6.2/dist/apache-solr-3.6.2.war ~/apache-tomcat-6.0.37/webapps/solr.war命令。WAR是一个完整的web应用程序，包括了Solr的jar文件和所有运行Solr所依赖的Jar文件，Jsp和很多的配置文件与资源文件。\n2. 修改~/apache-tomcat-6.0.37/conf/server.xml文件相应行的内容如下：\n1 2 3 4 \u0026lt;Connector port=\u0026#34;8080\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; URIEncoding=\u0026#34;UTF-8\u0026#34; redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; 增加URIEncoding=\u0026quot;UTF-8\u0026quot;来支持中文。这是因为solr基于xml，json，javabin，php，python等多种格式传输请求和返回结果。\n3.复制~/apache-solr-3.6.2/example/solr目录到/home/hadoop/solr位置。该位置为solr的应用环境目录。\n4. 修改/home/hadoop/solr/conf/solrconfig.xml文件中的dataDir一行内容为：\n1 \u0026lt;dataDir\u0026gt;${solr.data.dir:/home/hadoop/solr/data}\u0026lt;/dataDir\u0026gt; 目的是为了指定存放索引数据的路径。\n5. 在~/apache-tomcat-6.0.37/conf/Catalina/localhost目录下新建文件solr.xml。增加内容如下：\n1 2 3 \u0026lt;Context docBase=\u0026#34;/home/hadoop/apache-tomcat-6.0.37/webapps/solr.war\u0026#34; debug=\u0026#34;0\u0026#34; crossContext=\u0026#34;true\u0026#34; \u0026gt; \u0026lt;Environment name=\u0026#34;solr/home\u0026#34; type=\u0026#34;java.lang.String\u0026#34; value=\u0026#34;/home/hadoop/solr\u0026#34; override=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;/Context\u0026gt; 其中docBase为tomcat的webapps下的solr.war完整路径。Environment的value属性的值为存放solr索引的文件夹，即第三步中复制的文件夹。 需要注意的是：Catalina目录在首次启动tomcat时创建，因此在此步骤前需要启动过tomcat。\n6. 在tomcat的bin目录下通过startup.sh启动tomcat。\n7. 通过http://IP地址:8080/solr/来访问solr的web页面。\n相关命令 放入数据到solr中 在apache-solr-3.6.2/example/exampledocs目录下，执行java -jar post.jar 要存放的文件名。这里自己新建一个文件test.xml放入到solr中，文件内容如下：\n1 2 3 4 5 6 \u0026lt;add\u0026gt; \u0026lt;doc\u0026gt; \u0026lt;field name=\u0026#34;id\u0026#34;\u0026gt;company\u0026lt;/field\u0026gt; \u0026lt;field name=\u0026#34;text\u0026#34;\u0026gt;kaitone\u0026lt;/field\u0026gt; \u0026lt;/doc\u0026gt; \u0026lt;/add\u0026gt; 执行java -jar post.jar test.xml将数据放入solr中。\n删除数据 新建文本文件test_delete.xml，内容如下\n1 2 3 \u0026lt;delete\u0026gt; \u0026lt;id\u0026gt;company\u0026lt;/id\u0026gt; \u0026lt;/delete\u0026gt; 执行java -jar post.jar test_delete.xml将数据从solr中删除。 另外还可以通过命令行的方式来删除，命令为java -Ddate=args -jar post.jar '\u0026lt;delete\u0026gt;\u0026lt;id\u0026gt;company\u0026lt;/id\u0026gt;\u0026lt;/delete\u0026gt;'。\n在Eclipse中搭建环境操作Solr api 1. 新建一个java工程 2. 在工程中引入如下包：\ncommons-httpclient-3.1.jar commons-codec-1.6.jar apache-solr-solrj-3.6.2.jar slf4j-api-1.6.1.jar slf4j-log4j12-1.6.1.jar commons-logging-1.1.3.jar log4j-1.2.12.jar httpclient-4.2.5.jar httpcore-4.2.4.jar httpmime-4.2.5.jar 其中commons-httpclient-3.1.jar、commons-codec-1.6.jar、apache-solr-solrj-3.6.2.jar、slf4j-api-1.6.1.jar可以从solr的目录apache-solr-3.6.2中的dist目录下找到。\nslf4j-log4j12-1.6.1.jar可以从slf4j的压缩包中slf4j-1.6.1.tar.gz找到。\ncommons-logging-1.1.3.jar可以从slf4j的压缩包中commons-logging-1.1.3-bin.zip找到。\nlog4j-1.2.12.jar可以从log4j的压缩包中logging-log4j-1.2.12.tar.gz找到。\nhttpclient-4.2.5.jar、httpcore-4.2.4.jar、httpmime-4.2.5.jar在httpcomponents-client-4.2.5-bin.tar.gz文件中。\n具体的API编程可以参考Solr开发文档。\n在linux上编译并执行程序 1. 将工程中用到的jar包复制到Linux机器上，这里复制到/home/hadoop/test_solr/lib目录下。\n2. 将测试程序的源码放到Linux机器上，这里复制到/home/hadoop/test_solr目录下。其中源码包括三个文件：SolrTest.java、SolrClient.java、Index.java。该三个文件将会包含在下面相关下载中的Eclipse工程中。\n3. 在/home/hadoop/test_solr目录下执行\n1 javac -cp lib/apache-solr-solrj-3.6.2.jar:lib/commons-httpclient-3.1.jar:lib/log4j-1.2.12.jar:lib/commons-codec-1.6.jar:lib/commons-logging-1.1.3.jar:lib/slf4j-api-1.6.1.jar:lib/httpclient-4.2.5.jar:lib/httpcore-4.2.4.jar:lib/httpmime-4.2.5.jar:. SolrTest.java 其中-cp等同于-classpath参数，指定编译SolrTest.java文件需要的ClassPath路径，不要忘记路径后面的.表示当前路径，否则找不到当前目录下的其他java文件。 命令执行后会在/home/hadoop/test_solr目录下生成Index.class、SolrClient.class、SolrTest.class三个class文件。\n4. 在/home/hadoop/test_solr目录下执行\n1 java -cp lib/apache-solr-solrj-3.6.2.jar:lib/commons-httpclient-3.1.jar:lib/log4j-1.2.12.jar:lib/commons-codec-1.6.jar:lib/commons-logging-1.1.3.jar:lib/slf4j-api-1.6.1.jar:lib/httpclient-4.2.5.jar:lib/httpcore-4.2.4.jar:lib/:httpmime-4.2.5.jar:. SolrTest 来运行程序。\n在Linux上打包并执行 1. 在上面步骤基础上，为了方便执行，可以将class文件打成jar包来执行，这样在使用java命令执行的时候就不用指定classpath路径了，只需要在jar包的MANIFEST.MF文件中指定classpath。\n2. 在/home/hadoop/test_solr下新建一个文件，文件名可以随便，这里取名为MANIFEST.MF，与生成的jar包中的文件名一致，文件内容为\n1 2 3 4 5 6 7 Manifest-Version: 1.0 Created-By: 1.6.0_10 (Sun Microsystems Inc.) Main-Class: SolrTest Class-Path: /home/hadoop/test_solr/lib/apache-solr-solrj-3.6.2.jar /home/hadoop/test_solr/lib/commons-httpclient-3.1.jar /home/hadoop/test_solr/lib/log4j-1.2.12.jar /home/hadoop/test_solr/lib/commons-codec-1.6.jar /home/hadoop/test_solr/lib/commons-logging-1.1.3.jar /home/hadoop/test_solr/lib/slf4j-api-1.6.1.jar /home/hadoop/test_solr/lib/httpclient-4.2.5.jar /home/hadoop/test_solr/lib/httpcore-4.2.4.jar /home/hadoop/test_solr/lib/httpmime-4.2.5.jar 其中Main-Class指定main函数所在的类。 Class-Path指定用到的jar所在的路径。其中Class-Path的各个jar文件之间通过空格分隔而不是通过:分隔。\n3. 将class文件打包成jar文件。执行\n1 jar -cfm solrtest.jar MANIFEST.MF Index.class SolrClient.class SolrTest.class 会在此目录下生成solrtest.jar文件。jar命令会根据指定的MANIFEST.MF文件来产生jar包中的META-INF/MANIFEST.MF文件。两个文件内容并不完全一致，jar命令会根据格式对内容进行调整。\n4. 运行jar文件。通过java -jar solrtest.jar来执行。\n相关下载 本文中用到的安装包\n参考文档 简单的Solr安装配置 官方安装教程 Solr初体验系列讲的非常详细，适合初学者 Solr开发文档\n","date":"2013-07-02T00:00:00Z","permalink":"/post/solr_setup/","title":"在Linux上搭建solr环境"},{"content":"通过Eclipse编写java程序久了，发现已经不会用命令来编译java程序了。今天在windows下搭建了一个solr环境，想放到linux下去跑一下，在windows上打成jar包后放在linux下不能运行，是时候回顾一下java的编译命令了。而且网上的资料比较零散，没有特别系统的资料。\n本文在linux测试，同windows下的命令行工具差别不大。\n编译并执行单个文件 1. 在目录下~/test_java/com/kuring下新建HelloWorld.java的文件，文件内容为\n1 2 3 4 5 6 7 package com.kuring; public class HellowWorld { public static void main(String[] args) { System.out.println(\u0026#34;hello world\u0026#34;); } } 2. 在目录~/test_java下执行javac com/kuring/HelloWorld.java命令来编译文件。此时会在HelloWorld.java文件所在的目录下生成HelloWorld.class的二进制文件。\n3. 在目录~/test_java下执行java com.kuring.HelloWorld来执行HelloWorld.class。屏幕会输出hello world，说明文件执行成功。 也可以在任意路径下指定classpath路径来执行，命令为java -classpath ~/test_java com.kuring.HelloWorld，其中classpath指定了类的搜索路径。\n编译并执行多个文件 1. 在目录下~/test_java/com/kuring下新建HelloWorld2.java和Main.java的文件，HelloWorld2.java文件内容为\n1 2 3 4 5 6 7 package com.kuring; public class HellowWorld2 { public void print() { System.out.println(\u0026#34;hello world too\u0026#34;); } } Main.java的文件内容为\n1 2 3 4 5 6 7 8 package com.kuring; public class Main { public static void main(String[] args) { HelloWorld2 hello = new HelloWorld2(); hello.print(); } } 2. 在目录~/test_java下执行javac com/kuring/Main.java命令来编译文件。此时会在Main.java文件所在的目录下生成Main.class和HelloWorld2.class两个文件，可以看出javac有自动推导编译的功能。\n3. 在目录~/test_java下执行java com.kuring.Main。屏幕会输出hello world too，说明文件执行成功。\n打包 将上述例子中的程序打成jar包，可以在~/test_java目录下通过执行命令jar cvf my.jar com来生成jar文件。其中my.jar为要生成的jar文件的名字。 通过java -classpath my.jar com.kuring.Main来执行jar文件。 上述命令需要指定要执行的类名Main，如果想通过java -jar my.jar命令即可执行程序需要在jar包的META-INF/MANIFEST.MF文件中增加一行\n1 Main-Class: SolrTest 来执行含有main函数的类。然后通过jar -cfm my.jar MANIFEST.MF路径 要打包的目录或文件来重新生成jar包。这样就可以通过java -jar my.jar来执行jar包了。\n关于如何创建并执行引用了其他jar包的jar包，可以参考我的另外一篇博客《在Linux上搭建solr环境》，这里不再赘述。\n常用jar命令 功能 命令 用一个单独的文件创建一个 JAR 文件 jar cf jar-file input-file... 用一个目录创建一个 JAR 文件 jar cf jar-file dir-name 创建一个未压缩的 JAR 文件 jar cf0 jar-file dir-name 更新一个 JAR 文件 jar uf jar-file input-file... 查看一个 JAR 文件的内容 jar tf jar-file 提取一个 JAR 文件的内容 jar xf jar-file 从一个 JAR 文件中提取特定的文件 jar xf jar-file archived-file... 运行一个打包为可执行 JAR 文件的应用程序 java -jar app.jar 参考文档 JAR 文件揭密 Java程序的编译、执行和打包\n","date":"2013-07-01T11:04:01Z","permalink":"/post/compile_java_code/","title":"通过命令编译java程序"},{"content":"本文选择四台机器作为集群环境，hadoop采用0.20.2，HBase采用0.90.2，zookeeper采用独立安装的3.3.2稳定版。本文所采用的数据均为简单的测试数据，如果插入的数据量大可能会对结果产生影响。集群环境部署情况如下：\n机器名 IP地址 用途 Hadoop模块 HBase模块 ZooKeeper模块 server206 192.168.20.6 Master NameNode、JobTracker、SecondaryNameNode HMaster QuorumPeerMain ap1 192.168.20.36 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain ap2 192.168.20.38 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain ap2 192.168.20.8 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain 单线程插入100万行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 import java.io.IOException; import java.util.ArrayList; import java.util.List; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.HColumnDescriptor; import org.apache.hadoop.hbase.HTableDescriptor; import org.apache.hadoop.hbase.client.HBaseAdmin; import org.apache.hadoop.hbase.client.HTable; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.util.Bytes; public class InsertRowThreadTest { private static Configuration conf = null; private static String tableName = \u0026#34;blog\u0026#34;; static { Configuration conf1 = new Configuration(); conf1.set(\u0026#34;hbase.zookeeper.quorum\u0026#34;, \u0026#34;server206,ap1,ap2\u0026#34;); conf1.set(\u0026#34;hbase.zookeeper.property.clientPort\u0026#34;, \u0026#34;2181\u0026#34;); conf = HBaseConfiguration.create(conf1); } /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception { // 列族 String[] familys = {\u0026#34;article\u0026#34;, \u0026#34;author\u0026#34;}; // 创建表 try { HBaseAdmin admin = new HBaseAdmin(conf); if (admin.tableExists(tableName)) { System.out.println(\u0026#34;表已经存在，首先删除表\u0026#34;); admin.disableTable(tableName); admin.deleteTable(tableName); } HTableDescriptor tableDesc = new HTableDescriptor(tableName); for(int i=0; i\u0026lt;familys.length; i++){ HColumnDescriptor columnDescriptor = new HColumnDescriptor(familys[i]); tableDesc.addFamily(columnDescriptor); } admin.createTable(tableDesc); System.out.println(\u0026#34;创建表成功\u0026#34;); } catch (Exception e) { e.printStackTrace(); } // 向表中插入数据 long time1 = System.currentTimeMillis(); System.out.println(\u0026#34;开始向表中插入数据，当前时间为:\u0026#34; + time1); for (int i=0; i\u0026lt;1; i++) { InsertThread thread = new InsertThread(i * 1000000, 1000000, \u0026#34;thread\u0026#34; + i, time1); thread.start(); } } public static class InsertThread extends Thread { private int beginSite; private int insertCount; private String name; private long beginTime; public InsertThread(int beginSite, int insertCount, String name, long beginTime) { this.beginSite = beginSite; this.insertCount = insertCount; this.name = name; this.beginTime = beginTime; } @Override public void run() { HTable table = null; try { table = new HTable(conf, Bytes.toBytes(tableName)); table.setAutoFlush(false); table.setWriteBufferSize(1 * 1024 * 1024); } catch (IOException e1) { e1.printStackTrace(); } System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;从\u0026#34; + beginSite + \u0026#34;开始插入\u0026#34;); List\u0026lt;Put\u0026gt; putList = new ArrayList\u0026lt;Put\u0026gt;(); for (int i=beginSite; i\u0026lt;beginSite + insertCount; i++) { Put put = new Put(Bytes.toBytes(\u0026#34;\u0026#34; + i)); put.add(Bytes.toBytes(\u0026#34;article\u0026#34;), Bytes.toBytes(\u0026#34;tag\u0026#34;), Bytes.toBytes(\u0026#34;hadoop\u0026#34;)); putList.add(put); if (putList.size() \u0026gt; 10000) { try { table.put(putList); table.flushCommits(); } catch (IOException e) { e.printStackTrace(); } putList.clear(); try { Thread.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } } } try { table.put(putList); table.flushCommits(); table.close(); } catch (IOException e) { System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;失败\u0026#34;); e.printStackTrace(); } long currentTime = System.currentTimeMillis(); System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;结束，用时\u0026#34; + (currentTime - beginTime)); } } } 测试5次的结果分布图如下： 其中Y轴单位为毫秒。平均速度在1秒插入3万行记录。\n10个线程每个线程插入10万行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 import java.io.IOException; import java.util.ArrayList; import java.util.List; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.HColumnDescriptor; import org.apache.hadoop.hbase.HTableDescriptor; import org.apache.hadoop.hbase.client.HBaseAdmin; import org.apache.hadoop.hbase.client.HTable; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.util.Bytes; public class InsertRowThreadTest { private static Configuration conf = null; private static String tableName = \u0026#34;blog\u0026#34;; static { Configuration conf1 = new Configuration(); conf1.set(\u0026#34;hbase.zookeeper.quorum\u0026#34;, \u0026#34;server206,ap1,ap2\u0026#34;); conf1.set(\u0026#34;hbase.zookeeper.property.clientPort\u0026#34;, \u0026#34;2181\u0026#34;); conf = HBaseConfiguration.create(conf1); } /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception { // 列族 String[] familys = {\u0026#34;article\u0026#34;, \u0026#34;author\u0026#34;}; // 创建表 try { HBaseAdmin admin = new HBaseAdmin(conf); if (admin.tableExists(tableName)) { System.out.println(\u0026#34;表已经存在，首先删除表\u0026#34;); admin.disableTable(tableName); admin.deleteTable(tableName); } HTableDescriptor tableDesc = new HTableDescriptor(tableName); for(int i=0; i\u0026lt;familys.length; i++){ HColumnDescriptor columnDescriptor = new HColumnDescriptor(familys[i]); tableDesc.addFamily(columnDescriptor); } admin.createTable(tableDesc); System.out.println(\u0026#34;创建表成功\u0026#34;); } catch (Exception e) { e.printStackTrace(); } // 向表中插入数据 long time1 = System.currentTimeMillis(); System.out.println(\u0026#34;开始向表中插入数据，当前时间为:\u0026#34; + time1); for (int i=0; i\u0026lt;10; i++) { InsertThread thread = new InsertThread(i * 100000, 100000, \u0026#34;thread\u0026#34; + i, time1); thread.start(); } } public static class InsertThread extends Thread { private int beginSite; private int insertCount; private String name; private long beginTime; public InsertThread(int beginSite, int insertCount, String name, long beginTime) { this.beginSite = beginSite; this.insertCount = insertCount; this.name = name; this.beginTime = beginTime; } @Override public void run() { HTable table = null; try { table = new HTable(conf, Bytes.toBytes(tableName)); table.setAutoFlush(false); table.setWriteBufferSize(1 * 1024 * 1024); } catch (IOException e1) { e1.printStackTrace(); } System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;从\u0026#34; + beginSite + \u0026#34;开始插入\u0026#34;); List\u0026lt;Put\u0026gt; putList = new ArrayList\u0026lt;Put\u0026gt;(); for (int i=beginSite; i\u0026lt;beginSite + insertCount; i++) { Put put = new Put(Bytes.toBytes(\u0026#34;\u0026#34; + i)); put.add(Bytes.toBytes(\u0026#34;article\u0026#34;), Bytes.toBytes(\u0026#34;tag\u0026#34;), Bytes.toBytes(\u0026#34;hadoop\u0026#34;)); putList.add(put); if (putList.size() \u0026gt; 10000) { try { table.put(putList); table.flushCommits(); } catch (IOException e) { e.printStackTrace(); } putList.clear(); try { Thread.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } } } try { table.put(putList); table.flushCommits(); table.close(); } catch (IOException e) { System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;失败\u0026#34;); e.printStackTrace(); } long currentTime = System.currentTimeMillis(); System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;结束，用时\u0026#34; + (currentTime - beginTime)); } } } 耗时分布图为： 结果比单线程插入有提升。\n20个线程每个线程插入5万行 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 import java.io.IOException; import java.util.ArrayList; import java.util.List; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.HColumnDescriptor; import org.apache.hadoop.hbase.HTableDescriptor; import org.apache.hadoop.hbase.client.HBaseAdmin; import org.apache.hadoop.hbase.client.HTable; import org.apache.hadoop.hbase.client.Put; import org.apache.hadoop.hbase.util.Bytes; public class InsertRowThreadTest { private static Configuration conf = null; private static String tableName = \u0026#34;blog\u0026#34;; static { Configuration conf1 = new Configuration(); conf1.set(\u0026#34;hbase.zookeeper.quorum\u0026#34;, \u0026#34;server206,ap1,ap2\u0026#34;); conf1.set(\u0026#34;hbase.zookeeper.property.clientPort\u0026#34;, \u0026#34;2181\u0026#34;); conf = HBaseConfiguration.create(conf1); } /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception { // 列族 String[] familys = {\u0026#34;article\u0026#34;, \u0026#34;author\u0026#34;}; // 创建表 try { HBaseAdmin admin = new HBaseAdmin(conf); if (admin.tableExists(tableName)) { System.out.println(\u0026#34;表已经存在，首先删除表\u0026#34;); admin.disableTable(tableName); admin.deleteTable(tableName); } HTableDescriptor tableDesc = new HTableDescriptor(tableName); for(int i=0; i\u0026lt;familys.length; i++){ HColumnDescriptor columnDescriptor = new HColumnDescriptor(familys[i]); tableDesc.addFamily(columnDescriptor); } admin.createTable(tableDesc); System.out.println(\u0026#34;创建表成功\u0026#34;); } catch (Exception e) { e.printStackTrace(); } // 向表中插入数据 long time1 = System.currentTimeMillis(); System.out.println(\u0026#34;开始向表中插入数据，当前时间为:\u0026#34; + time1); for (int i=0; i\u0026lt;20; i++) { InsertThread thread = new InsertThread(i * 50000, 50000, \u0026#34;thread\u0026#34; + i, time1); thread.start(); } } public static class InsertThread extends Thread { private int beginSite; private int insertCount; private String name; private long beginTime; public InsertThread(int beginSite, int insertCount, String name, long beginTime) { this.beginSite = beginSite; this.insertCount = insertCount; this.name = name; this.beginTime = beginTime; } @Override public void run() { HTable table = null; try { table = new HTable(conf, Bytes.toBytes(tableName)); table.setAutoFlush(false); table.setWriteBufferSize(1 * 1024 * 1024); } catch (IOException e1) { e1.printStackTrace(); } System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;从\u0026#34; + beginSite + \u0026#34;开始插入\u0026#34;); List\u0026lt;Put\u0026gt; putList = new ArrayList\u0026lt;Put\u0026gt;(); for (int i=beginSite; i\u0026lt;beginSite + insertCount; i++) { Put put = new Put(Bytes.toBytes(\u0026#34;\u0026#34; + i)); put.add(Bytes.toBytes(\u0026#34;article\u0026#34;), Bytes.toBytes(\u0026#34;tag\u0026#34;), Bytes.toBytes(\u0026#34;hadoop\u0026#34;)); putList.add(put); if (putList.size() \u0026gt; 10000) { try { table.put(putList); table.flushCommits(); } catch (IOException e) { e.printStackTrace(); } putList.clear(); try { Thread.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } } } try { table.put(putList); table.flushCommits(); table.close(); } catch (IOException e) { System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;失败\u0026#34;); e.printStackTrace(); } long currentTime = System.currentTimeMillis(); System.out.println(\u0026#34;线程\u0026#34; + name + \u0026#34;结束，用时\u0026#34; + (currentTime - beginTime)); } } } 结果如下： 执行结果跟10个线程效果差不多。\n10个线程每个线程插入100万行 代码跟前面例子雷同，为节约篇幅未列出。 执行结果如下： 20个线程每个线程插入50万行 执行结果如下： 总结 多线程比单线程的插入效率有所提高，开10个线程与开20个线程的插入行效率差不多。 插入效率存在不稳定情况，通过折线图可以看出。 相关文章 在Linux上搭建Hadoop集群环境 在Linux上搭建HBase集群环境\n","date":"2013-06-26T00:00:00Z","permalink":"/post/hbase_test/","title":"HBase性能测试"},{"content":"本文选择安装的hadoop版本为网上资料较多的0.20.2，对于不懂的新技术要持保守态度。遇到问题解决问题的痛苦远比体会用不着功能的新版本的快感来的更猛烈。\n安装环境 本文选择了三台机器来搭建hadoop集群，1个Master和2个Slave。本文中的master主机即namenode所在的机器，slave即datanode所在的机器。节点的机器名和IP地址如下\n机器名 IP地址 用途 运行模块 server206 192.168.20.6 Master NameNode、JobTracker、SecondaryNameNode ap1 192.168.20.36 Slave DataNode、TaskTracker ap2 192.168.20.38 Slave DataNode、TaskTracker 安装Java 检查本机是否已安装Java 在命令行中输入java -version判断是否已经安装。如果已经安装检查Java的版本，某些操作系统在安装的时候会安装Jdk，但可能版本会太低。如果版本过低，需要将旧的版本删除。在Redhat操作系统中可以通过rpm命令来删除系统自带的Jdk。 安装java 本文选择jdk1.6安装，将解压出的文件夹jdk1.6.0_10复制到/usr/java目录下。 设置java的环境变量 添加系统环境变量，修改/etc/profile文件，在文件末尾添加如下内容： export JAVA_HOME=/usr/java/jdk1.6.0_10 export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar export PATH=$JAVA_HOME/bin:$PATH export JRE_HOME=$JAVA_HOME/jre\n修改完profile文件后要执行`source /etc/profile`命令才能使刚才的修改在该命令行环境下生效。 检查java是否安装成功 在命令行中输入java -version、javac命令来查看是否安装成功及安装版本。 配置hosts文件 本步骤必须操作，需要root用户来操作，修改完成之后立即生效。在三台机器的/etc/hosts文件末尾添加如下内容：\n192.168.20.6 server206 192.168.20.36 ap1 192.168.20.38 ap2\n修改完成之后可以通过ping 主机名的方式来测试hosts文件是否正确。\n新建hadoop用户 在三台机器上分别新建hadoop用户，该用户的目录为/home/hadoop。利用useradd命令来添加用户，利用passwd命令给用户添加密码。\n配置SSH免登录 该步骤非必须，推荐配置，否则在Master上执行start-all.sh命令来启动hadoop集群的时候需要手动输入ssh密码，非常麻烦。 原理：用户将自己的公钥储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回来。远程主机用事先储存的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求输入密码。 在本例中，需要实现的是192.168.20.6上的hadoop用户可以无密码登录自己、192.168.20.36和192.168.20.38的hadoop用户。需要将192.168.20.6上的ssh公钥复制到192.168.20.36和192.168.20.38机器上。\n在192.168.20.6上执行ssh-keygen –t rsa命令来生成ssh密钥对。会在/home/hadoop/.ssh目录下生成id_rsa.pub和id_rsa两个文件，其中id_rsa.pub为公钥文件，id_rsa为私钥文件。 在192.168.20.6上执行cat ~/.ssh/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys命令将公钥添加到授权的key里。 权限设置。在192.168.20.6上执行chmod 600 ~/.ssh/authorized_keys来修改authorized_keys文件的权限，执行chmod 700 ~/.ssh命令将.ssh文件夹的权限设置为700。如果权限不对无密码登录就配置不成功，而且没有错误提示，这一步特别注意。 在本机上测试是否设置无密码登录成功。在192.168.20.6上执行ssh -p 本机SSH服务端口号 localhost，如果不需要输入密码则登录成功。 利用scp命令将192.168.20.6上的公钥文件id_rsa.pub追加到192.168.20.36和192.168.20.38机器上的~/.ssh/authorized_keys文件中。scp命令的格式如下： 1 scp -P ssh端口号 ~/.ssh/id_rsa.pub hadoop@192.168.20.36:~/id_rsa.pub 在192.168.20.36和192.168.20.38机器上分别执行cat ~/id_rsa.pub \u0026gt;\u0026gt; ~/.ssh/authorized_keys命令将192.168.20.6机器上的公钥添加到authorized_keys文件的尾部。 6. 配置无密码登录完成，在Master机器上执行ssh -P 本机SSH服务端口号 要连接的服务器IP地址命令进行测试。\n搭建单机版hadoop 在192.168.20.6上首先搭建单机版hadoop进行测试。\n将hadoop-0.20.2.tar.gz文件解压到hadoop用户的目录下。 配置hadoop的环境变量。修改/etc/profile文件，在文件的下面加入如下： HADOOP_HOME=/home/hadoop/hadoop-0.20.2 export HADOOP_HOME export HADOOP=$HADOOP_HOME/bin export PATH=$HADOOP:$PATH\n修改完成之后执行source /etc/profile使修改的环境变量生效。 3. 配置hadoop用到的java环境变量 修改conf/hadoop-env.sh文件，添加export JAVA_HOME=/usr/java/jdk1.6.0_10。 4. 修改conf/core-site.xml的内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.default.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/home/hadoop/hadoop-0.20.2/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 修改/conf/mapred-site.xml的内容如下： 1 2 3 4 5 6 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.tracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;localhost:9001\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 至此单击版搭建完毕。可以通过hadoop自带的wordcount程序测试是否运行正常。下面为运行wordcount例子的步骤。 在hadoop目录下新建input文件夹。 将conf目录下的内容拷贝到input文件夹下，执行cp conf/* input。 通过start-all.sh脚本来启动单机版hadoop。 执行wordcount程序：hadoop jar hadoop-0.20.2-examples.jar wordcount input output。 通过stop-all.sh脚本来停止单机版hadoop。 搭建分布式hadoop 在上述基础之上，在192.168.20.6上执行如下操作。 1. 修改/home/hadoop/hadoop-0.20.2/conf目录下的core-site.xml文件。\n1 2 3 4 5 6 7 8 9 10 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.default.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://192.168.20.6:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.tmp.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/home/hadoop/hadoop-0.20.2/tmp\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop。而这个目录在每次重启后都会被干掉，必须重新执行format才行，否则会出错。 2. 修改/home/hadoop/hadoop-0.20.2/conf目录下的hdfs-site.xml文件。\n1 2 3 4 5 6 7 8 9 10 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.replication\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.support.append\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 3. 修改/home/hadoop/hadoop-0.20.2/conf目录下的mapred-site.xml文件。\n1 2 3 4 5 6 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapred.job.tracker\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;http://192.168.20.6:9001\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 4. 修改/home/hadoop/hadoop-0.20.2/conf目录下的masters文件。 将Master机器的IP地址或主机名添加进文件，如192.168.20.6。 5. 修改/home/hadoop/hadoop-0.20.2/conf目录下的slaves文件。Master主机特有 在其中将slave节点的Ip地址或主机名添加进文件中，本例中加入\n1 2 192.168.20.36 192.168.20.38 6. hadoop主机的master主机已经配置完毕，利用scp命令将hadoop-0.20.2目录复制到两台slave机器的hadoop目录下。命令为：scp -r /home/hadoop hadoop@服务器IP:/home/hadoop/。注意slaves文件在master和slave机器上是不同的。\n常用命令 hadoop dfsadmin -report 查看集群状态 http://192.168.20.6:50070/dfshealth.jsp 查看NameNode状态 http://192.168.20.6:50030/jobtracker.jsp Map/Reduce管理 hadoop fs -mkdir input 在HDFS上创建文件夹 hadoop fs -put ~/file/file*.txt input 将文件放入HDFS文件系统中 参考文档 细细品味Hadoop系列。超详细的hadoop教程，作者非常用心。 下载链接 http://pan.baidu.com/share/link?shareid=1235031445\u0026amp;uk=3506813023 提取码：v8ok\n","date":"2013-06-26T00:00:00Z","permalink":"/post/hadoop_setup/","title":"在Linux上搭建Hadoop集群环境"},{"content":"Linux处理ctrl+c信号的例子 当按下ctrl+c时如果代码正在执行sleep则会停止睡眠，调用信号处理函数。中断位置可能位于for循环代码段的任意位置，中断位置不可控。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; void h(int s) { printf(\u0026#34;抽空处理int信号\\n\u0026#34;); } main() { int sum=0; int i; signal(SIGINT,h); sigset_t sigs; for(i=1;i\u0026lt;=10;i++) { sum+=i; sleep(1); } printf(\u0026#34;sum=%d\\n\u0026#34;,sum); printf(\u0026#34;Over!\\n\u0026#34;); } 信号屏蔽的例子1 当按下ctrl+c时不会调用信号处理函数，当循环执行完毕后会调用信号处理函数，并且printf(\u0026ldquo;Over!\\n\u0026rdquo;)会被执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; void h(int s) { printf(\u0026#34;抽空处理int信号\\n\u0026#34;); } main() { int sum=0; int i; // 声明信号集合 sigset_t sigs; signal(SIGINT,h); // 清空集合 sigemptyset(\u0026amp;sigs); // 加入屏蔽信号 sigaddset(\u0026amp;sigs,SIGINT); // 屏蔽信号 sigprocmask(SIG_BLOCK,\u0026amp;sigs,0); for(i=1;i\u0026lt;=10;i++) { sum+=i; sleep(1); } printf(\u0026#34;sum=%d\\n\u0026#34;,sum); // 消除屏蔽信号 sigprocmask(SIG_UNBLOCK,\u0026amp;sigs,0); // 如果在上面按下ctrl+c，在此句不执行 printf(\u0026#34;Over!\\n\u0026#34;); } 当在循环中按下ctrl+c后，该函数输出结果为：\n1 2 3 sum=55 抽空处理int信号 Over! 信号屏蔽的例子2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;signal.h\u0026gt; // 信号处理函数 void h(int s) { printf(\u0026#34;抽空处理int信号\\n\u0026#34;); } main() { int sum=0; int i; signal(SIGINT,h); sigset_t sigs,sigp,sigq; sigemptyset(\u0026amp;sigs); sigemptyset(\u0026amp;sigp); sigemptyset(\u0026amp;sigq); sigaddset(\u0026amp;sigs,SIGINT); sigprocmask(SIG_BLOCK,\u0026amp;sigs,0); for(i=1;i\u0026lt;=10;i++) { sum+=i; sigpending(\u0026amp;sigp); if(sigismember(\u0026amp;sigp,SIGINT)) { printf(\u0026#34;SIGINT在排队!\\n\u0026#34;); // 是信号SIGINT有效 sigsuspend(\u0026amp;sigq); // 函数调用完毕后信号SIGINT无效 } sleep(1); } printf(\u0026#34;sum=%d\\n\u0026#34;,sum); // 消除屏蔽信号 sigprocmask(SIG_UNBLOCK,\u0026amp;sigs,0); printf(\u0026#34;Over!\\n\u0026#34;); } 该例子可以实现在指定的代码处处理信号。 其中sigsuspend函数原先如下：\n1 int sigsuspend(const sigset_t *mask); 函数解释：屏蔽新的信号，原来的屏蔽信号失效。是一个阻塞函数，该函数屏蔽mask信号；对非mask信号不屏蔽，信号处理函数调用完毕该函数返回；如果非mask信号没有信号处理函数，则此函数不返回。即返回条件：信号发生且信号为非屏蔽信号且信号必须要调用信号处理函数完毕。\n","date":"2013-06-14T23:10:36Z","permalink":"/post/linux_signal_deal_example/","title":"Linux中信号处理举例"},{"content":"\n我想拥有一只雌性袋鼠作为宠物，我会将其取名为\u0026quot;点点\u0026quot;。之所以是雌性是看中了袋鼠的温暖舒适的育儿袋。\n如果可以我会给育儿袋上面缝上个拉链，这样我就不会担心放在袋袋里面的东西会掉出来了。\n我可以领着我的点点去超市购物，将购买的东西放到袋袋里面，拉链一拉，然后蹦蹦跳跳的就回家了。\n晚上吃完饭，我可以领着点点去大街上走走，我可以将我的钱包、手机放到点点的袋袋里，拉链一拉，完全不用担心手机会摔坏。\n如果下起了雨或遇到了寒风，我可以钻到袋袋里，露个头在外面，一跳10米远，然后蹦蹦跳跳的就回家了。\n当然，这是不现实的，我在做梦，做一个好笑的梦。。。\n","date":"2013-06-12T12:10:56Z","permalink":"/post/i_want_to_have_one_kangaroos/","title":"我幻想拥有一只听话的袋鼠作为宠物"},{"content":"参加完姐姐的婚礼回到家后，静下心来之后内心莫名的感伤。也许是早晨三点醒四点起床导致身体在下午已经出现了疲惫。也许是生活本来就应该是平淡的，兴奋的多了，也自然疲惫的多，总之要保持一个符合每个人性格的平衡。也许是因为如女朋友同事的例子中一样，当丈夫娶妻过门的时候，丈夫的妹妹是哭的，因为妹妹有恋哥情节，难不成我也有恋姐情节的存在，还好我仅是有点伤感而已，不严重。也许是因为对时间流逝的无奈，总以为我们还没有长大，转眼间姐姐已经步入了婚姻的殿堂，成立了自己的新家庭，逝者如斯。趁着自己还有点文艺范的精神状态，写写这两天的一些感想，要不然明天一接触需要理性思考的计算机语言，这种感性点的状态又会归于平淡，又会在我鲜有的心路历程文章里少了一篇。\n小的时候总是羡慕姐姐在城里的生活，每年在仅有的几次见面中总能感受到一个不一样的姐姐。那时的我对于城市的大小、城市的好坏还没有概念，仅仅知道城市要比农村好不少倍，仅仅知道姐姐是城里的，是城市小朋友的代言人，是引以为荣的小榜样。也许下面这张小时候的珍贵照片最能反应出城市孩子和农村孩子之间的区别。 记得大约上八年级寒假的时候见过姐姐写的字，字迹铿锵有力且独特，不是常见的楷体字，由特定的字体加上自己的独到之处柔和而成。分明是经过训练方能造就，我从未在我认识的同学中见过能够写出如此华丽的字体，即使有书法天赋的同学由于没有经过特定的练习，字体往往都是课本上常见的楷体字。因为从上高中起，我已经开始渐渐地融入曾经向往的城市生活，直到如今我也算半个城市人。我和姐姐之间的差距在不断的缩短。当然会发现原来城市生活也有城市生活的弊端，如果让我选择一次我还会选择在农村度过我的童年，那种各种玩各种贴近大自然的生活是在高楼间无法体会到的。\n姐姐终于找到了可以寄托的归宿，姐夫是很优秀的。姐姐属于离不开爱情的类型，但身体却不是很好，也许是姐夫喊姐姐玉妹的缘由。期望在外来姐夫能够好好照顾姐姐，同时姐姐应该多注意身体，多吃粗粮。\n一个我想象不到的地方是随着现在人民文化水平的提高，婚礼的举办方式正在朝着复杂化的方向发展。现在的婚礼已经将旧有的当地风俗和现代的风俗结合在了一起，而且这两者是叠加的关系，意味着风俗越来越多，需要处理的事情越来越复杂。其实婚礼的作用无非就是热闹、喜庆， 我实在是搞不太明白复杂和热闹之间是一个什么样的关系。我期望我婚礼是简单的，简单的不能再简单，我甚至期望不需要亲朋好友的过多参与，我甚至不期望举办复杂的婚礼仪式，热闹的背后是家人的操劳，是长达数月的准备。我只希望能有一次难忘的旅行，两个人的旅行就足够幸福。\n祝福的含义我是不理解的，祝福原本是一个人向另外一个人的未来的美好祝愿，就是说些客套话，说些不切实际的话。其实一个人很难对另外一个人的外来向好的方面发展做出贡献，就如同大臣们天天喊着万岁万岁万万岁，却未见过哪个皇帝超过百岁一般。很多人都会向姐姐的婚礼说出祝福，但有多少是过后还记得自己曾经说过的。我在这里同样祝福姐姐和姐夫新婚快乐，生活幸福，和睦相处，恩恩爱爱，当然我是发自内心的，而且我的话是有文字可考究的。同样在给姐姐的红包里我写下了“原寻寻觅觅，现卿卿我我，年年岁岁情爱深，岁岁年年无不同”的祝福，这里同样做一个备忘。\n目前的婚庆已经完全市场化，只要是服务项没有不收费的可能性，而且往死里要。没有信仰的民族是个可怕的民族，如果马克思主义不能所谓一种信仰，那么中国人中的绝大多数是没有信仰的。即使大家上初中、高中、大学、研究生阶段都在马克思的理论，在当前的国情下估计也没有多少人能视马克思如珍宝。如果在国外我估计会出现在教堂举办婚礼非常廉价，甚至免费的可能性，因为能够为新郎新娘主持婚礼本身就是一件非常荣幸的事情，金钱不是最重要的，当然我不了解海外的真实情况。总感觉一件本来可以免费的事情只要跟金钱沾边总会变味。\n中国父母的典型形象是碎碎念，对于子女总会絮絮叨叨那么一箩筐。在不经意间，子女已经长大成人，可在父母的眼中子女永远是未长大的孩子。中国的父母在子女身上花费了太多的心血，以至于子女比自己更重要。\n一个活动的缺点是非常容易被人察觉的，而活动的优点往往不容易被人发觉。因此，要想顺利完成一个活动是不容易的。举办婚礼的时候我发现有很多事先未准备妥当的地方，而这些恰恰是最容易被发现的，而做得特别好的地方却是较难被人们发现。\n年龄之间的代沟还是比较明显的，这一点在酒桌上特别能够体现，在酒桌上话语最多的永远是上一辈人，因为年轻人跟上一辈人在思想上还是有较大的差距，这是好事。如果两代人之间差距过小，说明这个社会变化太慢。年轻人应该吸取上一代人的长处，去其槽粕，取其精华。我想未来的酒桌文化会大变样。\n我目前的理想生活状态是平淡充实的，我喜欢静下心来学习，我喜欢那种学习的充实感，我喜欢跟同龄人人心贴心的交谈，而不是在噪杂的环境中大家泛泛而谈。我的性格中腼腆的一面表现在我在与人沟通时内心的想法总会羞于表达，比如很难从我的嘴里向父母说出生日快乐之类的话语，向姐姐当面说出新婚快乐，白头偕老。一方面想着跟人沟通，另一方面却羞与吐露自己的内心想法，貌似有点矛盾的样子。\n人类本该是感性的，接触的外部环境多了之后会渐渐趋于理性，当然人类的科技进步需要理性。理性多了，自然感性就会少一些。这几年跟计算机打交道的时间成了生活、工作的最重要部分，我大部分生活状态处于理性状态，心情以平淡居多。包括我现在写这篇文章，也是在用理性的思维方式来写感性的文章，换成在高中时期写的文章却是用感性的思维方式来写感性的文章。\n我的性格未曾被工作改变，甚至是相貌在工作三年的时间里也未曾有大的变动。我现在仍然看上去像个学生，路人也经常会将我当成一名学生。女朋友说这是我不成熟的表现，我不知道成熟如何去界定，我认为我每天学到的东西都在助我向成熟迈进，直到我老去的那一刻我也不可能成熟，因为只要我还有进步我就不会成熟。我对于三年来的未改变我引以为豪，至少我没有被这个残缺的社会给同化掉，至少我还拥有自己的思想，同时也证明了一点我在大学期间思想就已经在保持了一种相对稳定的状态。我不认为工作后在官场上混的如鱼得水，在酒桌上的机智多变是一种成熟的表现，我对此持悲观态度。\n行行出状元。今天看到我姑在玩QQ农场、QQ牧场的游戏，熟练程度相当高。由于我姑空闲时间较多，常常一个人在家，已经玩了多年的游戏。我姑对电脑的使用还处在初级阶段，平常电脑的作用还定位在玩农场的阶段。只要做的次数多了，行行都可以出状元。\n本来想着把自己参加完姐姐婚礼之后的感想记录下来，结果写着写着开始有些怀旧，写自己的居多，困的不知所云，到此为止。不知等我老去的那天当我读到此文章时会是一种什么样的心情，难不成还会跟现在一样有点压抑的感觉？\n","date":"2013-06-11T22:57:07Z","permalink":"/post/my_cousin_wedding/","title":"参加完姐姐的婚礼之后"},{"content":"system函数 函数包含在C语言的标准库中，在头文件stdlib.h中声明如下：\n1 int system(const char *command); 该函数会创建一个独立的进程，该进程拥有独立的进程空间，为阻塞函数，只有当新进程执行完毕该函数才返回。\n返回值：可以通过返回值来获取新进程的main函数的返回值，返回值保存在int类型的第二个字节即8-15比特，可以通过向右移位或者利用宏WEXITSTATUS(status)来获取新进程的返回值。其中WEXITSTATUS(status)宏包含在头文件\u0026lt;sys/wait.h\u0026gt;中。\n这里以调用ls命令为例来展示用法：\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; int main() { int r=system(\u0026#34;ls\u0026#34;); // 用右移位的方式来获取新进程的返回值 //printf(\u0026#34;%d\\n\u0026#34;,r\u0026gt;\u0026gt;8\u0026amp;255); // 用WEXITSTATUS(status)宏的方式来获取新进程的返回值 printf(\u0026#34;%d\\n\u0026#34;,WEXITSTATUS(r)); } popen函数 函数包含在头文件stdio.h中，相关函数如下：\n1 2 FILE *popen(const char *command, const char *type); int pclose(FILE *stream); popen函数在父子进程之间建立一个管道，其中type指定管道的类型，可以为\u0026quot;r\u0026quot;或\u0026quot;w\u0026quot;即只读或可写。在shell中的管道符\u0026quot;|\u0026ldquo;即采用此函数来实现。popen函数为阻塞函数，函数的具体用法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; main() { char buf[1024]; FILE *f=popen(\u0026#34;ls\u0026#34;,\u0026#34;r\u0026#34;); // 根据管道获取文件描述符 int fd=fileno(f); int r; while((r=read(fd,buf,1024))\u0026gt;0) { buf[r]=0; printf(\u0026#34;%s\\n\u0026#34;,buf); } close(fd); // 关闭管道 pclose(f); } exec系列函数 该系列函数并不创建新的进程，而是将程序加载到当前进程的代码空间来执行并替换当前进程的代码空间，在exec*函数后面的代码将无法执行。\n1 2 3 4 5 int execl(const char *path, const char *arg, ...); int execlp(const char *file, const char *arg, ...); int execle(const char *path, const char *arg, ..., char * const envp[]); int execv(const char *path, char *const argv[]); int execvp(const char *file, char *const argv[]); fork函数 该函数非常常用。函数原型如下：\n1 pid_t fork(void); 调用该函数会产生一个子进程，该子进程不仅复制了父进程的代码空间、堆、栈，而且还复制了父进程的执行位置。之后父子进程同时执行，通常由于操作系统任务调度的原因，子进程会先执行。父进程和子进程之间的并不会共享堆、栈上的数据，可以通过文件或共享内存的方式来通讯。\n返回值：该函数父进程返回子进程的id，子进程返回0。通常在代码中通过返回值来判断是子进程还是父进程，用来执行不同的代码。\n如果父进程先结束，则子进程会成为孤儿进程，子进程仍然可以继续执行。进程数中的根进程init会成为该子进程的父进程。\n如果子进程先结束，则子进程会成为僵尸进程。僵尸进程并不再占用内存和CPU资源，但是会在进程数中看到僵尸进程。因此代码中必须对僵尸进程的情况做处理，通常的处理办法为采用wait函数和信号机制。子进程在结束的时候会向父进程发送一个信号SIGCHLD，整数值为17。父进程扑捉到该信号后通过调用wait函数来回收子进程的资源。其中wait函数为阻塞函数，会一直等待子进程结束。wait函数返回子进程退出时的状态码。下面通过实例演示一下当子进程先结束时父进程怎样回收子进程。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;signal.h\u0026gt; void processChildProcess() { printf(\u0026#34;receive the child process end\\n\u0026#34;); wait(); } int main() { int pid; pid = fork(); if (pid \u0026gt; 0) { // 父进程 signal(SIGCHLD, processChildProcess); while(1) { sleep(1); } } else { // 子进程 sleep(1); printf(\u0026#34;child process end\\n\u0026#34;); } } 关于fork的详细理解可以看这里：一个fork的面试题。\n","date":"2013-06-07T16:08:15Z","permalink":"/post/linux_create_process_methods/","title":"linux中创建新进程的方式"},{"content":"前段时间公司领导安排我完成一个android的小项目，功能为完成手机短信的分类功能。由于考虑到可能会和某运营商合作，项目需求多变，项目名到项目死掉的那一刻也没有想好，暂定叫短信管家。前段时间参加了今年软考中的高级项目管理师考试，对项目管理有一定的了解，由于在实际的工作中没有参与项目经理的角色，对项目管理的知识理论和实际结合还有一定差距。我一向是喜欢挑人毛病的，本文也不例外，将结合项目管理的知识领域来分析项目中存在的问题，并给出纠正的措施。本文并不想按照项目管理的44个过程组来写，不想受项目管理理论知识条条框框的约束。\n项目简介 在2013年元旦后上班第一天，经过领导简单介绍项目后，项目正式开始启动。项目参与人员包括我和一名美工。由于公司没有做android项目的经验，我是公司唯一懂点android项目的技术人员，我之前也仅仅是自学过一点。经过两个星期的学习之后开始了android项目的开发，春节前领导希望看到一个版本出来。离春节放假还有三天，美工搞出一个首界面来，我仿照首界面基本搞定。后来领导一看不符合需求，其实领导之前就看过首界面效果图，只是没有仔细看。当然领导也不知道要做出个什么样的东西，估计心想着技术人员弄出个什么样的东西来再修改。 年后，对需求开会进行了重新整理，通过思维导图的方式跟领导确定需求，就需求中存在的疑问进行确认。不过后来事实证明经过领导确认的需求也是在不断变更。这次的需求要远比上次复杂，将原来的短信分类从一级短信分类更改为了二级短信分类。公司没有互联网行业产品的经验，美工给出的效果图也仅是从桌面端来考虑移动产品的设计。我最终决定自己开始重新设计软件的界面，参考了众多同类软件，花费的时间比较多。大约到2013年4月底产品已经基本可用，经过我自己的测试和领导的使用效果还算可以。后来发现不支持彩信，经过我的一番研究后发现现有的产品功能要对彩信支持需要耗费非常大的工作量，因为软件本身的功能打破了android系统短信的设计，也难怪了android的应用中没有功能相似的产品出现。这也成为了该项目失败的直接原因。\n可行性研究 技术可行性 项目的起源来自领导的短信泛滥，领导想着能够搞一个拦截垃圾短信并对短信进行管理的软件将是酷的一件事情。领导接触过一些android，片面的认为这种方案在技术上可行。我也做了一个调研，发现现有的android软件项目中没有此功能的产品。\n经济可行性 支出分析：由于项目主要参与人员仅有我一人，支出的成本较小。收益分析：现在的互联网产品都是先圈用户再考虑盈利，我们这个软件产品也不例外。\n运行环境可行性 对于产品的运营存在两种方式：1、跟某地方运营商合作；2、自己单独发布。\n总体而言，由于项目较小，对可行性研究基本忽略，没有科学、可观、公证的对项目进行可行性分析。\n范围管理 在项目立项的时候仅有一个产品的大致方向，做一款android系统中的短信分类管理类应用。从始至终领导对于产品的理解在不断变化，每次变化都需要修改很多代码，跟领导对于导致产品研发过程做了很多无用功。 在项目进行到一个月后，我认识到了项目这样下去反复修改不可能有好的结果，因此跟领导召开了需求定义的会议，就产品的需求用思维导图的方式就项目的范围进行了定义，会后就需求中没有考虑到的问题跟领导进行了沟通确认。此次会议之后项目的需求基本敲定，虽然之后领导有需求变更的情况，但是相对较少。\n进度管理 由于公司初次开展android类项目，对项目的进度很难掌控。项目开始时负责市场部的领导给出了一个月之内完成的计划，我心想我自己加把劲应该能搞定吧，因为最初的项目需求还比较简单。我用了两个星期的时间来学习android知识，用两个星期的时间来开发，结果到最后我才刚根据美工提供的效果图完成了首页，我总是不能很好的估算自己的工作进度。这篇《为什么程序员总是不能准确预估工作量》的文章或许能替我解释些什么。 在项目范围确定之后我对自己工作的进度管理也不够好，第一次尝试做android项目，未知因素太多，编码中总能遇到这样那样的问题。\n成本管理 项目的整个成本应该我的人力成本占了绝大部分，本来软件项目中的人力成本就是占的比重较多的，何况主要是我一个人参与的项目，自然我那微薄的人力占了项目成本的大部分。至于后期项目的盈利并没有过多考虑，现在的互联网行业本来就是先圈用户后盈利。\n质量管理 我在项目的编码过程中，一般一个小功能完成之后都会进行详尽的功能测试。由于功能不负责，产品的质量只要多测试一般问题就不大了。\n人力资源管理 项目的初期我就感觉项目不可能成功，但是领导坚持要做，而且我一个做C++的程序员来写android的代码，我当时居然仅有听从领导的意识，没有适当的表达自己的想法。当时怀着一己之心认为可能一个月项目就完成了，完成之后工作依旧，还可以熟练熟练android的开发，谁知一练手就是做了五个月，而到最后项目仍然没有成型。 由于一直对项目持悲观态度，始终认为我自己开发出来的产品我都不想去用，用户怎么可能去用。长期自己独自做，兼任了产品经理、美工、码农的工作，做的过程中还要担心着需求的变更，在项目进行到中后期的时候， 缺少激励因素，对工作激情不够，这也影响了项目的进度。\n沟通管理 在项目的开发过程中基本能保持没完成一个关键功能跟领导汇报一次项目进展。在项目中沟通方面存在不少的问题，尤其以前期严重。前期由于跟领导的需求不一致，导致做了很多的重复性劳动。领导对项目的需求本来就不够明确，一旦有好的想法就加到项目中，这也导致了跟领导的沟通困难，因为这样的沟通效率太低。\n风险管理 项目可研阶段对技术可行性分析研究不够，导致了后期在对彩信进行分类处理的时候遇到了技术上的难题。当然前期的可研阶段做充分的技术可行性研究也是不太现实了，因为本来技术对公司而言就是未知的。最好的办法就是对自己不熟悉而且短期内不能实现盈利的项目不参与。\n文档管理 由于项目管理中要求每个阶段都有输出的文档，这么繁琐的文档我相信中国找不出几家软件公司能够办到，能办到的估计都已经死掉了。此项目中仅有范围定义的思维导图、我在编写某个具体的功能代码前绘制的流程图，应该说文档及其少。但我目前不认为这有什么不对，我还是坚信某些情况下看代码比看文档要来的更快，特别是看我这种android菜鸟写的代码。更何况此项目的需求在不断随着领导的想法而变化，文档都没法写，写文档的成本太高。\n我的一些反思： 在项目中我也存在不少问题。\n项目开始阶段应该向领导表达清楚自己的想法，阐明自己对项目的看法。 对项目的进度没有很好的把握。 对技术较为熟悉之后可以适当的学习后期开发功能需要技术，比如一直没有研究彩信的技术实现，导致后期在开发彩信功能时遇到技术问题。如果能够提前学习相关技术就能够提前预支项目的风险。 ","date":"2013-06-06T21:53:37Z","permalink":"/post/one_failed_project_in_pm/","title":"从项目管理的角度分析一个失败的软件项目"},{"content":"shell中的环境变量 查看环境变量 通过env命令可以查看所有的环境变量 通过echo $环境变量名方式来查看单个环境变量 设置环境变量 export命令来设置环境变量\n在程序中该如何获取和设置环境变量呢？\n通过main函数的第三个参数 通常大家接触比较多的是两个参数的main函数，实际上还有一个包含三个参数的main函数，第三个参数为包含了系统的环境变量的二级指针。 用法如下：\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;stdio.h\u0026gt; int main ( int argc, char *argv[], char *arge[]) { while (*arge) { printf(\u0026#34;%s\\n\u0026#34;, *arge); arge++; } return 0; } 实例将会输出该用户的所有环境变量。\n通过外部环境变量environ 该变量定义在“unistd.h”头文件中，定义为：extern char **environ; 用法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;stdio.h\u0026gt; extern char **environ; int main ( int argc, char *argv[] ) { char **env = environ; while (*env) { printf(\u0026#34;%s\\n\u0026#34;, *env); env++; } return 0; } 实例将会输出该用户的所有环境变量。\n通过系统函数 在C语言的头文件“stdlib.h”中定义了三个和环境变量相关的函数：\n1 2 3 char *getenv(const char *name); int setenv(const char *name, const char *value, int overwrite); int unsetenv(const char *name); 比较简单，不再举例。\n","date":"2013-06-04T21:59:37Z","permalink":"/post/linux_get_and_set_env_variable/","title":"在linux程序中获取和设置环境变量"},{"content":"在Linux的头文件sys/mman.h中提供了两个用来分配内存的函数：mmap和munmap，函数定义原型如下：\n1 2 void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset); int munmap(void *start, size_t length); mmap说明 返回值：内存映射后返回虚拟内存的首地址。 参数： start为指定的映射的首地址，该地址应该没有映射过，如果为0则有系统指定位置。 length为映射的空间大小，真正分配空间大小为(length/pagesize+1)。 prot为映射的权限，分为四种未指定（PROT_NONE）、读（PROT_READ）、写（PROT_WRITE）、执行（PROT_EXEC）。如果为PROT_WRITE，则直接可以PROT_READ。 flags：映射方式，分为内存映射和文件映射。内存映射：匿名映射。当值为文件映射是后面两个参数才有效。常用的值有：MAP_ANONYMOUS、MAP_SHARED、MAP_PRIVATE。 fd：映射的文件描述符。 offset为从文件的偏移位置开始映射。\nmunmap说明 从start位置开始释放length个字节的内存。\n应用举例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;sys/mman.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; main() { int *p = mmap(NULL, getpagesize(), PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_SHARED, 0, 0); munmap(p, getpagesize()); } 其中getpagesize()函数的作用为获取一个页的大小，系统默认为4K。\n","date":"2013-06-03T09:33:16Z","permalink":"/post/mmap_and_munmap/","title":"mmap和munmap函数的用法"},{"content":"当linux中的函数内部出错时通常函数会返回-1，并且将错误码保存到全局变量errno中，用来表示错误代码。errno全局变量包含在头文件errno.h文件中。下面给出三种打印错误信息的方法。\nperror函数 应用举例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;stdio.h\u0026gt; int main(void) { FILE *fp ; fp = fopen( \u0026#34;/root/noexitfile\u0026#34;, \u0026#34;r+\u0026#34; ); if ( NULL == fp ) { perror(\u0026#34;error : \u0026#34;); } return 0; } 输出如下： Permission denied\nstrerror函数 strerror函数原型为：char *strerror(int errnum);将参数errnum转换为对应的错误码。 应用举例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;stdio.h\u0026gt; int main(void) { FILE *fp ; fp = fopen( \u0026#34;/root/noexitfile\u0026#34;, \u0026#34;r+\u0026#34; ); if ( NULL == fp ) { printf(\u0026#34;%s\\n\u0026#34;, strerror(errno)); } return 0; } 输出如下： Permission denied\nprintf中的%m打印 应用举例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 #include \u0026lt;stdio.h\u0026gt; int main(void) { FILE *fp ; fp = fopen( \u0026#34;/root/noexitfile\u0026#34;, \u0026#34;r+\u0026#34; ); if ( NULL == fp ) { printf(\u0026#34;%m\\n\u0026#34;); } return 0; } 输出如下： Permission denied\n","date":"2013-06-02T15:04:33Z","permalink":"/post/linux_get_error_of_function/","title":"linux中获取错误信息的方式"},{"content":"Linux系统中提供了两个在堆中分配空间的底层函数，函数原型如下：\nvoid *sbrk(intptr_t increment); int brk(void *end_data_segment);\n两个函数的作用均为从堆中分配空间，并且在内部维护一个指针，指针的值默认为NULL。如果内部指针为NULL，则得到一页的空闲地址，系统默认为4K字节。指针向后移动即为分配空间，指针向前移动为释放空间。当内部指针的位置移动到一个页的开始位置时，整个页会被操作系统回收。brk为绝对改变位置，sbrk为相对改变位置。\nsbrk函数 在sbrk函数中，参数increment为要增加的字节数，increment可以为负数。当increment为负数时表示释放空间。当increment==0时，内部指针位置不动。函数调用成功返回内部指针改变前的值，失败返回(void *)-1。 函数使用举例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { int *p0 = sbrk(0); // 打印堆地址 printf(\u0026#34;this site of p0 is : %d\\n\u0026#34;, p0); int *p1 = sbrk(1000); // 这里仍然打印的是第一次的堆地址 printf(\u0026#34;this site of p1 is : %d\\n\u0026#34;, p1); int *p2 = sbrk(1); // 打印第一次堆地址+1000后的地址 printf(\u0026#34;this site of p2 is : %d\\n\u0026#34;, p2); // 回到初始堆地址，释放空间 sbrk(-1001); int *p3 = sbrk(0); // 检查是否回到初始地址 printf(\u0026#34;this site of p3 is : %d\\n\u0026#34;, p3); } 输出如下内容： this site of p0 is : 264622080 this site of p1 is : 264622080 this site of p2 is : 264623080 this site of p3 is : 264622080\nbrk函数 在brk函数中，函数作用为将当前的内部指针移动到end_data_segment位置。成功返回0，失败返回-1。 函数使用举例如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { int *p0 = sbrk(0); printf(\u0026#34;this site of p0 is : %d\\n\u0026#34;, p0); brk(p0 + 1000); printf(\u0026#34;this site is : %d\\n\u0026#34;, sbrk(0)); brk(p0 + 1001); printf(\u0026#34;this site is : %d\\n\u0026#34;, sbrk(0)); brk(p0); printf(\u0026#34;this site is : %d\\n\u0026#34;, sbrk(0)); } 函数输出如下： this site of p0 is : 206979072 this site is : 206983072 this site is : 206983076 this site is : 206979072\n适用场景： 内存的管理方式比malloc和free更加灵活，适合申请不确定的内存空间的情况，特别适合同类型的大块数据。如果用malloc则可能存在申请内存空间过多浪费的情况，过少时需要重新调用realloc来重新申请内存的情况。速度比malloc快。\n","date":"2013-06-02T13:39:54Z","permalink":"/post/sbrk_and_brk/","title":"sbrk和brk函数"},{"content":"前端时间折腾了一段时间的Github博客，终于搞明白了jekyll，开始写了一篇博客发现问题比较多。比如中文编码问题，Github对makedown的支持问题，Github的文章同步问题，网速问题。总体而言，感觉用Github来写博客还不是非常满意。\n今天下午偶然想起了前段时间看过faxbox的一篇文章，今天下载下来尝试，果然非常酷。优点如下：\n博客放在了Dropbox不会丢失。~~当然没有Github酷，Github可以通过版本管理来查看历史版本。~~貌似Dropbox本身有版本管理的功能。 写博客的方式简捷。Github的jekyll还需要利用rake命令来创建文章，这样才能保证文章的头部含有YAML标签。而faxbox直接编辑文本文件即可。 文本编辑器给力。可以通过实时预览功能查看makedown的解析情况，当然也可以通过在线的makedown编辑器stackedit来编写makedown。 网速比Github快。测试一下感觉网速还比较快。 可以修改模板，fork模板的方式简捷更酷。 对中文的支持好。因为本来就是本土化的软件当然对中文全力支持。 代码语法高亮。测试了一下默认的模板支持语法高亮。 自带的模板更漂亮。自带的模版虽然不多，但是有说明是适合博客还是相册。 网站自带网站分析工具。可以通过简单的浏览网页就可以知道网站的访问情况，虽然farbox在访问量达到1W之后要收费，按照目前的价格，但是我个人觉得比较值，希望farbox能够一直坚持做下去。做的好我愿意付费。 缺点：\n~~经过一下午的了解，暂时没有发现博客可以分类的功能，不过这个功能我暂时可以不需要。以后准备先写些博客在Github和Faxbox上同步更新，以便多比较比较这两种Geek的写博客方式。~~后来发现是有文章分类功能的，具体操作为将文章放入不同的文件夹中，文件夹的名字即为分类的名字，这种方式比jekyll的YAML语法方式要好用。 感觉评论功能有点鸡肋。博客还没写完有事工作，准备续写时发现有人留言指正错误，想回复谢谢，发现无法直接回复。用多说代替之，方法为直接将多说的js脚本存入新建的comment_js.md文件中即可，非常简洁。 Linux下的用户不太方便。由于farbox软件仅有windows版和mac版，dropbox在linux有相应版本，在linux下的用户不能享受到farbox编辑工具带来的方便。相反在linux下结合git和vim来编写博客却比较方便。 PS： Github博客地址：kuring.github.com Fixbox博客地址：kuring.faxbox.com\n","date":"2013-05-31T17:18:00Z","permalink":"/post/farbox%E4%B8%8Ejekyll%E5%AF%B9%E6%AF%94/","title":"farbox与jekyll对比"},{"content":"本文是在安装完成Hadoop的基础之上进行的，Hadoop的安装戳这里。 本文采用的Hadoop版本为0.20.2，HBase版本为0.90.6，ZooKeeper的版本为3.3.2（stable版）。 本文仍然采用了Hadoop安装的环境，机器如下：\n机器名 IP地址 用途 Hadoop模块 HBase模块 ZooKeeper模块 server206 192.168.20.6 Master NameNode、JobTracker、SecondaryNameNode HMaster QuorumPeerMain ap1 192.168.20.36 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain ap2 192.168.20.38 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain 安装ZooKeeper 由于HBase默认集成了ZooKeeper，可以不用单独安装ZooKeeper。本文采用独立安装ZooKeeper的方式。 1. 将zookeeper解压到/home/hadoop目录下。 2. 将/home/hadoop/zookeeper-3.3.2/conf目录下的zoo_sample.cfg文件拷贝一份，命名为为“zoo.cfg”。 3. 修改zoo.cfg文件，修改后内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take initLimit=10 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=5 # the directory where the snapshot is stored. dataDir=/home/hadoop/zookeeper-3.3.2/zookeeper_data # the port at which the clients will connect clientPort=2181 dataLogDir=/home/hadoop/zookeeper-3.3.2/logs server.1=192.168.20.6:2888:3888 server.2=192.168.20.36:2888:3888 server.3=192.168.20.38:2888:3888 其中，2888端口号是zookeeper服务之间通信的端口，而3888是zookeeper与其他应用程序通信的端口。 这里修改了dataDir和dataLogDir的值。 需要特别注意的是：如果要修改dataDir的值不能将原来的行在前面加个“#”注释掉后在后面再增加一行，这样是不起作用的。可以参考bin目录下的zkServer.sh文件中的72行，内容如下：\n1 ZOOPIDFILE=$(grep dataDir \u0026#34;$ZOOCFG\u0026#34; | sed -e \u0026#39;s/.*=//\u0026#39;)/zookeeper_server.pid 这里通过grep和sed命令来获取dataDir的值，对于行前面添加“#”注释是不起作用的。 4. 创建zoo.cfg文件中的dataDir和dataLogDir所指定的目录。 5. 在dataDir目录下创建文件myid。 6. 通过scp命令将zookeeper-3.3.2目录拷贝到其他节点机上。 7. 修改myid文件，在对应的IP的机器上输入对应的编号，该编号要和zoo.cfg中的一致。本例中在192.168.20.6上文件内容为1；在192.168.20.36上文件内容为2；在192.168.20.38上文件内容为3。至此ＺooＫeeper的安装已经完成。\n运行ZooKeeper 1. 在节点机上依次执行/home/hadoop/zookeeper-3.3.2/bin/zkServer.sh start脚本。运行第一个ZooKeeper的时候会因等待其他节点而出现刷屏现象，等启动起第二个节点上的ZooKeeper后就正常了。运行完成之后该脚本会出现刷屏现象，我这里没有理会。 2. 通过jps命令来查看各节点机上是否含有QuorumPeerMain进程。 3. 通过/home/hadoop/zookeeper-3.3.2/bin/zkServer.sh status命令来查看状态。本例中有三个节点机，其中必有一个leader，两个follower存在。 4. 在各节点机上依次执行/home/hadoop/zookeeper-3.3.2/bin/zkServer.sh stop来停止ZooKeeper服务。\n配置时间同步ntp服务 HBase在运行的时候各个节点之间时间不同步会存在莫名其妙的问题，这里选择以192.168.20.36机器作为时间同步服务器，其他机器从该机器同步时间。 在192.168.20.36上通过service ntpd start命令来启动ntp服务。 在其他机器上配置crontab命令，增加下面一行：\n1 0 */1 * * * /usr/sbin/ntpdate 192.168.20.36 \u0026amp;\u0026amp; /sbin/hwclock -w 这里采用一个小时同步一次时间的方式。\n安装HBase 1. 在HMaster机器上将HBase解压到/home/hadoop目录下。 2. 修改配置文件hbase-env.sh，使export HBASE_MANAGES_ZK=false。如果想要HBase使用自带的ZooKeeper则使用设置为true。使export JAVA_HOME=/usr/java/jdk1.6.0_10来指定java的安装路径。 3. 修改配置文件hbase-site.xml如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.rootdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://server206:9000/hbase\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.cluster.distributed\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.master.port\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;60000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.property.clientPort\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2181\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;server206,ap1,ap2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.zookeeper.property.dataDir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/home/hadoop/zookeeper-3.3.2/zookeeper_data\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;dfs.support.append\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hbase.regionserver.handler.count\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;100\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; 4. 修改regionservers，添加HRegionServer模块所运行机器的主机名。在本例中内容如下：\n1 2 ap1 ap2 5. 为了确保HBase和Hadoop的兼容性，这里将/home/hadoop/hadoop-0.20.2/hadoop-0.20.2-core.jar文件复制到/home/hadoop/hbase-0.90.6/lib目录下，并将原先的Hadoop的jar文件删掉或重命名为其他后缀的文件。 6. 将hbase-0.90.6文件夹通过scp命令复制到其他节点机上。\nHBase的启动 在Hadoop的NameNode所在的机器上使用start-all.sh脚本来启动Hadoop集群。 在各个节点机上调用zkServer.sh脚本来启动ZooKeeper。 在HMaster所在的机器上使用start-hbase.sh脚本来启动HBase集群。 HBase启动后会在HDFS自动创建/hbase的文件夹，可以通过hadoop fs -ls /hbase命令来查看，该目录不需要自动创建。如果在安装HBase的过程中失败需要重新启动，最好将此目录从集群中删除，通过命令hadoop fs -rmr /hbase来删除。\n需要特别注意的是在hadoop集群中hadoop fs -ls /hbase目录和hadoop fs -ls hbase目录并非一个目录，通过hadoop fs -ls hbase查看到的目录实际上为/user/hadoop/hbase目录。\nHBase管理界面 Master的界面：http://192.168.20.6:60010/master.jsp RegionServer的界面：http://192.168.20.36:60030/regionserver.jsp和http://192.168.20.38:60030/regionserver.jsp\n参考资料 http://blog.csdn.net/gudaoqianfu/article/details/7327191 http://thomas0988.iteye.com/blog/1309867 下载链接 http://pan.baidu.com/share/link?shareid=1235031445\u0026amp;uk=3506813023 提取码：v8ok\n","date":"2013-02-26T00:00:00Z","permalink":"/post/setup_hbase/","title":"在Linux上搭建HBase集群环境"}]