<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Github Kubernetes组织下开源项目（持续更新）]]></title>
      <url>%2Fpost%2Fkubernetes-opensource-project%2F</url>
      <content type="text"><![CDATA[在k8s官方的Github kubernetes group下除了k8s的核心组件外，还有很多开源项目，本文用来简要分析这些开源项目的用途。 node-problem-detector项目地址：https://github.com/kubernetes/node-problem-detector k8s的管控组件对于iaas层的node运行状态是完全不感知的，比如节点出现了ntp服务挂掉、硬件告警（cpu、内存、磁盘故障）、内核死锁。node-problem-detector旨在将node节点的问题通知给k8s组件，以DaemonSet的方式部署在所有的k8s节点上。 上报故障的方式支持如下两种方式： 对于永久性故障，通过修改node status中的condition上报给apiserver 对于临时性故障，通过Event的方式上报 node-problem-detector在将节点的故障信息上报给k8s后，通常会配合一些自愈系统搭配使用，比如Draino和Descheduler 。 Descheduler项目地址：https://github.com/kubernetes-sigs/descheduler k8s的pod调度完全动态的，kube-scheduler组件在调度pod的时候会根据当时k8s集群的运行时环境来决定pod可以调度的节点，可以保证pod的调度在当时是最优的。但是随着的推移，比如环境中增加了新的node、创建了一批亲和节点的pod，都有可能会导致如果相同的pod再重新调度会到其他的节点上。但k8s的设计是，一旦pod调度完成后，就不会再重新调度。 Descheduler组件用来解决pod的重新调度问题，可以根据配置的一系列的规则来触发驱逐pod，让pod可以重新调度，从而使k8s集群内的pod尽可能达到最优状态，有点类似于计算机在运行了一段时间后的磁盘脆片整理功能。Descheduler组件可以以job、cronjob或者deployment的方式运行在k8s集群中。 autoscaler项目地址：https://github.com/kubernetes/autoscaler]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[技术分享第14期]]></title>
      <url>%2Fpost%2Fknowledge-share-14%2F</url>
      <content type="text"><![CDATA[题图为一点资讯近期推出的一款定位线上社交的App啫喱，每个人可以给自己订制一个卡通形象，是国内厂商对于线上社交的一次新的尝试。 距离上期技术分享已经约有一年半的时间，这次不定期更新的时间有些久。2022年期望在博客方面增加时间投入，多关注开源技术，预计会大幅度提升更新的频率。 资源1. Katacoda 可以一键创建一个k8s集群的工具，甚至无需登录，比上期推荐的工具Play with Kubernetes更为方便。 2. OperatorHubk8s推出了CRD的机制后，大大增强了k8s的扩展能力，可以好不夸张的说，k8s之所以如此成功，跟CRD的扩展机制有很大关系。OperatorHub类似于DockerHub，收集了各种各样的operator实现。 3. lazykube 一款可以通过命令行终端来展示和管理k8s资源的工具。 4. LazyDocker 同上面的lazykube工具，LazyDocker是一个可以在命令行上查看本机docker的工具，可以查看docker上容器以及运行状态、本地的镜像以及分层信息、volume信息。 5. httpbin一个非常简单的http服务，可以用来在测试服务的连通性，尤其是可以用curl测试api层面的连通性，再也不用访问curl http://www.baidu.com了。比如执行`curl http://httpbin.org/headers`可以以json的形式返回request的http header信息，执行curl http://httpbin.org/status/200可以返回状态码为200。 不过，在国内访问该网站连通性并不是太好。还提供了镜像版本，可以直接在本地以docker的方式运行该服务。 6. Flux一款基于k8s的GitOps工具，采用k8s的声明式api基于operator实现。 7. OpenTelemetry云原生领域的可观测性工具，用来制定规范、提供sdk和采集agent实现，实现了可观察性中的tracing和metric，并没有实现logging部分。不负责底层的后端存储，可以跟负责metric的prometheus集成，负责tracing的jager集成。 8. kspan 该工具可以将k8s的event使用OpenTelemetry转换成tracing的形式，并将其存储在支持tracing的后端，比如jaeger中。 9. skopeoskopeo是一款镜像操作工具，用来解决常用的镜像操作，但这些功能docker命令却不太具备，或者需要调用docker registry的api才可以完成操作。比如镜像从一个镜像仓库迁移到另外一个镜像仓库，从镜像仓库中删除镜像等。 10. KubeOperator KubeOperator一个开源的轻量级 Kubernetes 发行版，提供可视化的 Web UI，可以用来在IaaS 平台上自动创建主机，通过 Ansible 完成自动化部署和变更操作。 另外，还提供了商业版本，支持更多的企业级特性，这种模式也是类似Redhat的最为常见的开源软件的商业变现方式。 11. Lens Kubernetes的客户端工具，非web端工具，可以通过本地访问远程的k8s集群，并且可以获取k8s集群的node、pod等信息，提供了mac、linux、windows三种客户端版本。 12. Caddy一款使用Golang编写的七层负载均衡软件，Github上有3万+的star数量，相比与nginx有两个明显的优势：提供了默认的https服务，支持证书的自签发；使用Golang开发，只需要一个二进制配合caddyfile配置文件即可运行，更轻量。 13. LVScaresealyun开源的一个轻量级的lvs管理工具，可以作为轻量级的负载均衡，基于Golang实现。输入这样一条指令 lvscare care --run-once --vs 10.103.97.12:6443 --rs 192.168.0.2:6443 --rs 192.168.0.3:6443 --rs 192.168.0.4:6443 即可在宿主机上创建对应的ipvs规则。自带了健康检查功能，支持四层和七层的健康检查，是该工具的最大优势。一般lvs会配合着keepalived来进行检查检查，一旦健康检查失败后会将rs的权重调整为0，但keepalived的配置相对复杂，该工具更轻量。 14. HUGO非常火爆的使用Golang开发的开源博客系统，目前Github上的Star数量已经超过5万+，更老牌的开源博客系统Hexo的Star数量才3万+，基于Vue开发的VuePress Star数量还不到2万。 15. sealer阿里巴巴开源的一款云原生的应用发布工具。该工具的设计思路非常先进，提出了集群镜像的概念，可以将k8s集群和应用build成为一个集群镜像，类似于docker镜像，一旦集群镜像构建完成后即可像容器一样运行在不同的硬件上。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[阿里巴巴开源云原生项目分析（持续更新）]]></title>
      <url>%2Fpost%2Falibaba_opensource_cloudnative%2F</url>
      <content type="text"><![CDATA[集群镜像sealer项目地址：https://github.com/alibaba/sealer 相关资料：集群镜像：实现高效的分布式应用交付 当前的应用发布经历了三个阶段： 阶段一 裸部署在物理机或者vm上。直接裸部署在机器上的进程，存在操作系统、软件包的依赖问题，比如要部署一个python应用，那么需要机器上必须要包含对应版本的python运行环境以及应用依赖的所有包。 阶段二 通过镜像的方式部署在宿主机上。docker通过镜像的方式将应用以及依赖打包到一起，解决了单个应用的依赖问题。 阶段三 通过k8s的方式来标准化部署。在k8s时代，可以将应用直接部署到k8s集群上，将应用的发布标准化，实现应用的跨机器部署。 在阶段三中，应用发布到k8s集群后，应用会对k8s集群有依赖，比如k8s管控组件的配置、使用的网络插件、应用的部署yaml文件，对镜像仓库和dockerd的配置也有所依赖。当前绝大多数应用发布是跟k8s集群部署完全独立的，即先部署k8s集群，然后再部署应用，跟阶段一的发布单机应用模式比较类似，先安装python运行环境，然后再启动应用。 sealer项目是个非常有意思的开源项目，旨在解决k8s集群连同应用发布的自动化问题，可以实现类似docker镜像的方式将整个k8s集群连同应用一起打包成集群镜像，有了集群镜像后既可以标准化的发布到应用到各个地方。sealer深受docker的启发，提出的很多概念跟docker非常类似，支持docker常见的子命令run、inspect、save、load、build、login、push、pull等。 Kubefile概念跟Dockerfile非常类似，且可以执行sealer build命令打包成集群镜像，语法也类似于Dockerfile。 CloudImage：集群镜像，将Kubefile打包后的产物，类比与dockerimage。基础集群镜像通常为裸k8s集群，跟docker基础镜像为裸操作系统一致。 Clusterfile：要想运行CloudImage，需要配合Clusterfile文件来启动，类似于Docker Compose。Clusterfile跟Docker Compose一致，并不是必须的，也可以通过sealer run的方式来启动集群镜像。 sealer要实现上述功能需要实现将k8s集群中的所有用到镜像全部打包到一个集群镜像中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[k8s在region和zone方面的支持情况]]></title>
      <url>%2Fpost%2Fk8s-support-zone-region%2F</url>
      <content type="text"><![CDATA[k8s虽然已经发展了多个版本，但在多region和多zone的场景下支持还是相对比较弱的，且很多的特性在alpha版本就已经废弃，说明k8s官方对于region和zone方面的支持情况有很大的不确定性。业界支持多reigon和容灾的特性更多是从上层的应用层来解决。本文主要是介绍k8s自身以及社区在region和zone方面的支持情况。 k8s标签https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone官方推荐的跟region和zone有关的标签： failure-domain.beta.kubernetes.io/region：1.17版本已经废弃，被topology.kubernetes.io/region取代 failure-domain.beta.kubernetes.io/zone：1.17版本已经废弃，被topology.kubernetes.io/zone取代 topology.kubernetes.io/region：1.17版本开始支持 topology.kubernetes.io/zone：1.17版本开始支持 服务拓扑ServiceTopology支持版本：在1.17版本引入，在1.21版本废弃 该特性在Service对象上增加了spec.topologyKeys字段，表示访问该Service的流量优先选用的拓扑域列表。访问Service流量的具体过程如下： 当访问该Service时，一定是从某个k8s的node节点上发起，会查看当前node的label topology.kubernetes.io/zone对应的value，如果发现有endpoint所在的node的lable topology.kubernetes.io/zone对应的value相同，那么会将流量转发到该拓扑域的endpoint上。 如果没有找到topology.kubernetes.io/zone相同拓扑域的endpoint，则尝试找topology.kubernetes.io/region相同拓扑域的endpoint。 如果没有找到任何拓扑域的endpoint，那么该service会转发失败。1234567891011121314apiVersion: v1kind: Servicemetadata: Name: my-app-webspec: selector: app: my-app ports: - protocol: TCP port: 80 targetPort: 8080 topologyKeys: - "topology.kubernetes.io/zone" - "topology.kubernetes.io/region" 在底层实现上，使用了1.16版本新引入的alpha版本特性 EndpointSlice，该特性主要是用来解决Endpoints对象过多时带来的性能问题，从而将Endpoints拆分为多个组，Service Topology特性恰好可以借助该特性来实现，本质上也是为了将Endpoints进行拆分。k8s会根据Service的topologyKeys来将Service拆分为多个EndpointSlice对象，kube-proxy根据EndpointSlice会将Service流量进行拆分。 拓扑感知提示Topology Aware Hints该特性在1.21版本引入，在1.23版本变为beta版本，用来取代Topology Key功能。该特性会启用两个组件：EndpointSlice控制器和kube-proxy。 在Service Topology功能中，需要给Service来指定topologyKeys字段。该特性会更自动化一些，只需要在Service上增加annotation service.kubernetes.io/topology-aware-hints:auto，EndpointSlice控制器会watch到Service，发现开启了拓扑感知功能，会自动向endpoints的hints中增加forZones字段，该字段的value会根据endpoint所在node的topology.kubernetes.io/zone来决定。 值得一提的是，当前的hints中并没有包含forRegions的字段。123456789101112131415161718192021apiVersion: discovery.k8s.io/v1kind: EndpointSlicemetadata: name: example-hints labels: kubernetes.io/service-name: example-svcaddressType: IPv4ports: - name: http protocol: TCP port: 80endpoints: - addresses: - "10.1.2.3" conditions: ready: true hostname: pod-1 zone: zone-a hints: forZones: - name: "zone-a" 参考 Well-Known Labels, Annotations and Taints Google Ingress for Anthos：https://cloud.google.com/kubernetes-engine/docs/concepts/ingress-for-anthos#architecture 阿里云ccm支持多个k8s集群的场景（手工指定lb id和server group方案）：https://help.aliyun.com/document_detail/335878.html#title-4wt-hc5-p2p https://tencentcloudcontainerteam.github.io/2019/11/26/service-topology/ https://kubernetes.io/zh/docs/concepts/services-networking/endpoint-slices/ 使用 EndpointSlice 扩展 Kubernetes 网络 Running in multiple zones]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下磁盘常用命令]]></title>
      <url>%2Fpost%2Fdisk-command%2F</url>
      <content type="text"><![CDATA[查看磁盘是否为ssd如果其中的rota值为1，说明为hdd磁盘；如果rota值为0，说明为ssd。 12345# lsblk -o name,size,type,rota,mountpointNAME SIZE TYPE ROTA MOUNTPOINTvdc 20G disk 1 /var/lib/kubelet/pods/eea3a54c-b211-4ee3-bcbe-70ba3fe84c05/volumes/kubernetes.io~csi/d-t4n36xdqey47v9e0ej8r/mountvda 120G disk 1└─vda1 120G part 1 / 但该规则在很多虚拟机的场景下并不成立，即使虚拟机的磁盘为ssd，但rota值仍然为1。可以通过修改rota的值的方式来标记磁盘的类型：echo ‘0’&gt; /sys/block/vdd/queue/rotational iostat只能观察磁盘整体性能，不能看到进程级别的 iops = r/s+w/s 吞吐量 = rkB/s + wkB/s 响应时间 = r_await + w_await iostat -xm $interval: interval为秒 123456789$ iostat -xm 1Linux 3.10.0-327.10.1.el7.x86_64 (103-17-52-sh-100-I03.yidian.com) 12/11/2016 _x86_64_ (32 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 4.99 0.00 1.46 0.01 0.00 93.54Device: rrqm/s wrqm/s r/s w/s rMB/s wMB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.26 0.02 1.62 0.00 0.02 29.31 0.00 0.89 3.37 0.87 0.05 0.01sdb 0.00 0.81 10.23 67.59 1.19 29.07 796.57 0.01 0.10 0.85 2.85 0.19 1.46 r/s: 每秒发送给磁盘的读请求数 w/s: 每秒发送给磁盘的写请求数 rMB/s: 每秒从磁盘读取的数据量 wMB/s: 每秒向磁盘写入的数据量 %iowait：cpu等待磁盘时间 %util：表示磁盘使用率，该值较大说明磁盘性能存在问题，io队列不为空的时间。由于存在并行io，100%不代表磁盘io饱和 r_await：读请求处理完成时间，包括队列中的等待时间和设备实际处理时间，单位为毫秒 w_await：写请求处理完成时间，包括队列中的等待时间和设备实际处理时间，单位为毫秒 svctm: 瓶颈每次io操作的时间，单位为毫秒，可以反映出磁盘的瓶颈 avgrq-sz：平均每次请求的数据大小 avgqu-sz：平均请求io队列长度 svctm：处理io请求所需要的平均时间，不包括等待时间，单位为毫秒 1234567891011121314151617$ iostat -d -x 1Linux 3.10.0-327.10.1.el7.x86_64 (103-17-42-sh-100-i08.yidian.com) 01/19/2019 _x86_64_ (24 CPU)Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 7.58 0.31 266.07 9.54 888.01 6.74 0.03 0.12 8.84 0.11 0.03 0.90sdb 0.00 0.67 4.39 40.68 386.20 9277.38 428.79 0.03 0.64 3.10 0.37 0.30 1.33sdd 0.00 0.82 5.08 66.78 451.99 15587.89 446.42 0.02 0.30 3.50 0.05 0.28 2.01sde 0.00 0.67 4.14 53.77 312.74 12121.84 429.43 0.00 0.03 4.00 1.44 0.30 1.72sdc 0.00 0.50 0.05 27.47 2.74 6098.56 443.28 0.03 0.95 12.42 0.93 0.21 0.58sdf 0.00 0.47 1.07 34.58 57.05 7504.68 424.24 0.00 0.05 2.60 2.63 0.25 0.88sdg 0.00 0.72 0.14 63.87 13.79 14876.74 465.22 0.03 0.42 10.57 0.40 0.19 1.24sdj 0.00 0.79 1.85 37.81 206.01 7881.89 407.90 0.03 0.67 2.27 0.59 0.36 1.42sdi 0.00 0.65 0.29 59.54 22.01 13501.71 452.05 0.01 0.25 5.27 0.22 0.19 1.14sdk 0.00 0.50 0.30 29.46 16.74 6330.68 426.57 0.05 1.58 4.03 1.55 0.24 0.70sdm 0.00 0.63 0.73 55.70 63.73 13008.16 463.34 0.02 0.35 5.98 0.28 0.23 1.29sdh 0.00 0.47 0.06 15.68 5.16 2970.09 378.01 0.03 1.82 7.22 1.80 0.27 0.42sdl 0.00 0.56 1.32 38.14 81.04 8945.16 457.50 0.09 2.22 4.30 2.15 0.19 0.76 wrqm: 每秒合并的写请求数 rrqm：每秒合并的读请求数]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kubectl常用命令]]></title>
      <url>%2Fpost%2Fkubectl-command%2F</url>
      <content type="text"><![CDATA[本文记录常用的kubectl命令，不定期更新。 统计k8s node上的污点信息 1kubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints --no-headers 查看不ready的pod 1kubectl get pod --all-namespaces -o wide -w | grep -vE &quot;Com|NAME|Running|1/1|2/2|3/3|4/4&quot; pod按照重启次数排序 1kubectl get pods -A --sort-by=&apos;.status.containerStatuses[0].restartCount&apos; | tail]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[k8s ipv4/ipv6双栈]]></title>
      <url>%2Fpost%2Fk8s-dual-stack%2F</url>
      <content type="text"><![CDATA[ipv6在k8s中的支持情况ipv6特性在k8s中作为一个特定的feature IPv6DualStack来管理。在1.15版本到1.20版本该feature为alpha版本，默认不开启。从1.21版本开始，该feature为beta版本，默认开启，支持pod和service网络的双栈。1.23版本变为稳定版本。 配置双栈alpha版本需要通过kube-apiserver、kube-controller-manager、kubelet和kube-proxy的–feature-gates=”IPv6DualStack=true”命令来开启该特性，beta版本该特性默认开启。​ kube-apiserver： --service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt; kube-controller-manager: --cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt; --service-cluster-ip-range=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt; --node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6 对于 IPv4 默认为 /24，对于 IPv6 默认为 /64 kube-proxy: --cluster-cidr=&lt;IPv4 CIDR&gt;,&lt;IPv6 CIDR&gt; 使用kind创建k8s集群由于ipv6的环境并不容易找到，可以使用kind快速在本地拉起一个k8s的单集群环境，同时kind支持ipv6的配置。 检查当前系统是否支持ipv6，如果输出结果为0，说明当前系统支持ipv6. 12$ sysctl -a | grep net.ipv6.conf.all.disable_ipv6net.ipv6.conf.all.disable_ipv6 = 0 创建ipv6单栈k8s集群执行如下命令：1234567891011cat &gt; kind-ipv6.conf &lt;&lt;EOFkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4name: ipv6-onlynetworking: ipFamily: ipv6# networking: # apiServerAddress: "127.0.0.1" # apiServerPort: 6443EOFkind create cluster --config kind-ipv6.conf 创建ipv4/ipv6双栈集群执行如下命令：1234567891011cat &gt; kind-dual-stack.conf &lt;&lt;EOFkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4name: dual-stacknetworking: ipFamily: dual# networking: # apiServerAddress: "127.0.0.1" # apiServerPort: 6443EOFkind create cluster --config kind-dual-stack.conf service网络spec定义k8s的service网络为了支持双栈，新增加了几个字段。 .spec.ipFamilies用来设置地址族，service一旦场景后该值不可认为修改，但可以通过修改.spec.ipFamilyPolicy来简介修改该字段的值。为数组格式，支持如下值： [“IPv4”] [“IPv6”] [“IPv4”,”IPv6”] （双栈） [“IPv6”,”IPv4”] （双栈） 上述数组中的第一个元素会决定.spec.ClusterIP中的字段。​ .spec.ClusterIPs：由于.spec.ClusterIP的value为字符串，不支持同时设置ipv4和ipv6两个ip地址。因此又扩展出来了一个.spec.ClusterIPs字段，该字段的value为宿主元祖。在Headless类型的Service情况下，该字段的值为None。​ .spec.ClusterIP：该字段的值跟.spec.ClusterIPs中的第一个元素的值保持一致。​ .spec.ipFamilyPolicy支持如下值： SingleStack：默认值。会使用.spec.ipFamilies数组中配置的第一个协议来分配cluster ip。如果没有指定.spec.ipFamilies，会使用service-cluster-ip-range配置中第一个cidr中来配置地址。 PreferDualStack：如果当前k8s集群配置为单栈，则仅分配单栈ip地址。如果k8s集群为双栈，则分配双栈ip地址，此时跟RequireDualStack的行为保持一致。 RequireDualStack：同时分配ipv4和ipv6地址。.spec.ClusterIP的值会从.spec.ClusterIPs选择第一个元素。 service配置场景先创建如下的deployment，以便于后面试验的service可以关联到pod。123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: MyAppspec: replicas: 3 selector: matchLabels: app: MyApp template: metadata: labels: app: MyApp spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 未指定任何协议栈信息在没有指定协议栈信息的Service，创建出来的service .spec.ipFamilyPolicy为SingleStack。同时会使用service-cluster-ip-range配置中第一个cidr中来配置地址。如果第一个cidr为ipv6，则service分配的clusterip为ipv6地址。创建如下的service 123456789101112apiVersion: v1kind: Servicemetadata: name: my-service labels: app: MyAppspec: selector: app: MyApp ports: - protocol: TCP port: 80 会生成如下的service123456789101112131415161718192021222324apiVersion: v1kind: Servicemetadata: labels: app: MyApp name: my-service namespace: defaultspec: clusterIP: 10.96.80.114 clusterIPs: - 10.96.80.114 ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: MyApp sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; 指定.spec.ipFamilyPolicy为PreferDualStack创建如下的service 12345678910111213apiVersion: v1kind: Servicemetadata: name: my-service-2 labels: app: MyAppspec: ipFamilyPolicy: PreferDualStack selector: app: MyApp ports: - protocol: TCP port: 80 提交到环境后会生成如下的service，可以看到.spec.clusterIPs中的ip地址跟ipFamilies中的顺序一致。 1234567891011121314151617181920212223242526apiVersion: v1kind: Servicemetadata: labels: app: MyApp name: my-service-2 namespace: defaultspec: clusterIP: 10.96.221.70 clusterIPs: - 10.96.221.70 - fd00:10:96::7d1 ipFamilies: - IPv4 - IPv6 ipFamilyPolicy: PreferDualStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: MyApp sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; 查看Endpoints，可以看到subsets中的地址为pod的ipv4协议地址。Endpoints中的地址跟service的.spec.ipFamilies数组中的第一个协议的值保持一致。123456789101112131415161718192021222324252627282930313233343536apiVersion: v1kind: Endpointsmetadata: labels: app: MyApp name: my-service-2 namespace: defaultsubsets:- addresses: - ip: 10.244.0.5 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-vrfps namespace: default resourceVersion: "16875" uid: 30a1d787-f799-4250-8c56-c96564ca9239 - ip: 10.244.0.6 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-wgz72 namespace: default resourceVersion: "16917" uid: 8166d43e-2702-45c6-839e-b3005f44f647 - ip: 10.244.0.7 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-x4lt5 namespace: default resourceVersion: "16896" uid: f9c2968f-ca59-4ba9-a69f-358c202a964b ports: - port: 80 protocol: TCP 接下来指定.spec.ipFamilies的顺序再看一下执行的结果，创建如下的service12345678910111213141516apiVersion: v1kind: Servicemetadata: name: my-service-3 labels: app: MyAppspec: ipFamilyPolicy: PreferDualStack ipFamilies: - IPv6 - IPv4 selector: app: MyApp ports: - protocol: TCP port: 80 在环境中生成的service如下，可以看到.spec.clusterIPs中的顺序第一个为ipv6地址，.spec.clusterIP同样为ipv6地址。1234567891011121314151617181920212223242526apiVersion: v1kind: Servicemetadata: labels: app: MyApp name: my-service-3 namespace: defaultspec: clusterIP: fd00:10:96::c306 clusterIPs: - fd00:10:96::c306 - 10.96.147.82 ipFamilies: - IPv6 - IPv4 ipFamilyPolicy: PreferDualStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: MyApp sessionAffinity: None type: ClusterIPstatus: loadBalancer: &#123;&#125; 查看Endpoints，可以看到subsets中的地址为pod的ipv6协议地址。Endpoints中的地址跟service的.spec.ipFamilies数组中的第一个协议的值保持一致。123456789101112131415161718192021222324252627282930313233343536apiVersion: v1kind: Endpointsmetadata: labels: app: MyApp name: my-service-3 namespace: defaultsubsets:- addresses: - ip: fd00:10:244::5 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-vrfps namespace: default resourceVersion: "16875" uid: 30a1d787-f799-4250-8c56-c96564ca9239 - ip: fd00:10:244::6 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-wgz72 namespace: default resourceVersion: "16917" uid: 8166d43e-2702-45c6-839e-b3005f44f647 - ip: fd00:10:244::7 nodeName: dual-stack-control-plane targetRef: kind: Pod name: nginx-deployment-65b5dd4c68-x4lt5 namespace: default resourceVersion: "16896" uid: f9c2968f-ca59-4ba9-a69f-358c202a964b ports: - port: 80 protocol: TCP 单栈和双栈之间的切换虽然.spec.ipFamilies字段不允许直接修改，但.spec.ipFamilyPolicy字段允许修改，但并不影响单栈和双栈之间的切换。单栈变双栈只需要修改.spec.ipFamilyPolicy从SingleStack变为PreferDualStack或者RequireDualStack即可。双栈同样可以变为单栈，只需要修改.spec.ipFamilyPolicy从PreferDualStack或者RequireDualStack变为SingleStack即可。此时.spec.ipFamilies会自动变更为一个元素，.spec.clusterIPs同样会变更为一个元素。 LoadBalancer类型的Service对于LoadBalancer类型的Service，单栈的情况下，会在.status.loadBalancer.ingress设置vip地址。如果是ingress中的ip地址为双栈，此时应该是将双栈的vip地址同时写到.status.loadBalancer.ingress中，并且要保证其顺序跟serivce的.spec.ipFamilies中的顺序一致。 pod网络pod网络要支持ipv6，需要容器网络插件的支持。为了支持ipv6特性，新增加了.status.podIPs字段，用来展示pod上分配的ipv4和ipv6的信息。.status.podIP字段的值跟.status.podIPs数组的第一个元素的值保持一致。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139apiVersion: v1kind: Podmetadata: labels: k8s-app: kube-dns pod-template-hash: 558bd4d5db name: coredns-558bd4d5db-b2zbj namespace: kube-systemspec: containers: - args: - -conf - /etc/coredns/Corefile image: k8s.gcr.io/coredns/coredns:v1.8.0 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 5 httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 name: coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /ready port: 8181 scheme: HTTP periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/coredns name: config-volume readOnly: true - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: kube-api-access-hgxnc readOnly: true dnsPolicy: Default enableServiceLinks: true nodeName: dual-stack-control-plane nodeSelector: kubernetes.io/os: linux preemptionPolicy: PreemptLowerPriority priority: 2000000000 priorityClassName: system-cluster-critical restartPolicy: Always schedulerName: default-scheduler securityContext: &#123;&#125; serviceAccount: coredns serviceAccountName: coredns terminationGracePeriodSeconds: 30 tolerations: - key: CriticalAddonsOnly operator: Exists ... volumes: - configMap: defaultMode: 420 items: - key: Corefile path: Corefile name: coredns name: config-volume - name: kube-api-access-hgxnc projected: defaultMode: 420 sources: - serviceAccountToken: expirationSeconds: 3607 path: token - configMap: items: - key: ca.crt path: ca.crt name: kube-root-ca.crt - downwardAPI: items: - fieldRef: apiVersion: v1 fieldPath: metadata.namespace path: namespacestatus: conditions: - lastProbeTime: null lastTransitionTime: "2022-01-16T12:51:24Z" status: "True" type: Initialized ... containerStatuses: - containerID: containerd://6da36ab908291ca1b4141a86d70f8c2bb150a933336d852bcabe2118aa1a3439 image: k8s.gcr.io/coredns/coredns:v1.8.0 imageID: sha256:296a6d5035e2d6919249e02709a488d680ddca91357602bd65e605eac967b899 lastState: &#123;&#125; name: coredns ready: true restartCount: 0 started: true state: running: startedAt: "2022-01-16T12:51:27Z" hostIP: 172.18.0.2 phase: Running podIP: 10.244.0.3 podIPs: - ip: 10.244.0.3 - ip: fd00:10:244::3 qosClass: Burstable startTime: "2022-01-16T12:51:24Z" pod中要想获取到ipv4、ipv6地址，可以通过downward api的形式将.status.podIPs以环境变量的形式传递到容器中，在pod中通过环境变量获取到的格式为: 10.244.1.4,a00:100::4。 k8s node在宿主机上可以通过ip addr show eth0的方式来查看网卡上的ip地址。123456789# ip addr show eth023: eth0@if24: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fc00:f853:ccd:e793::2/64 scope global nodad valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:2/64 scope link valid_lft forever preferred_lft forever 宿主机的网卡上有ipv6地址，k8s node上的.status.addresses中有所体现，type为InternalIP即包含了ipv4地址，又包含了ipv6地址，但此处并没有字段标识当前地址为ipv4，还是ipv6。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172apiVersion: v1kind: Nodemetadata: annotations: kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock node.alpha.kubernetes.io/ttl: "0" volumes.kubernetes.io/controller-managed-attach-detach: "true" labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: dual-stack-control-plane kubernetes.io/os: linux node-role.kubernetes.io/control-plane: "" node-role.kubernetes.io/master: "" node.kubernetes.io/exclude-from-external-load-balancers: "" name: dual-stack-control-planespec: podCIDR: 10.244.0.0/24 podCIDRs: - 10.244.0.0/24 - fd00:10:244::/64 providerID: kind://docker/dual-stack/dual-stack-control-planestatus: addresses: - address: 172.18.0.2 type: InternalIP - address: fc00:f853:ccd:e793::2 type: InternalIP - address: dual-stack-control-plane type: Hostname allocatable: cpu: "4" ephemeral-storage: 41152812Ki hugepages-1Gi: "0" hugepages-2Mi: "0" memory: 15936568Ki pods: "110" capacity: cpu: "4" ephemeral-storage: 41152812Ki hugepages-1Gi: "0" hugepages-2Mi: "0" memory: 15936568Ki pods: "110" conditions: - lastHeartbeatTime: "2022-01-16T15:01:48Z" lastTransitionTime: "2022-01-16T12:50:54Z" message: kubelet has sufficient memory available reason: KubeletHasSufficientMemory status: "False" type: MemoryPressure ... daemonEndpoints: kubeletEndpoint: Port: 10250 images: - names: - k8s.gcr.io/kube-proxy:v1.21.1 sizeBytes: 132714699 ... nodeInfo: architecture: amd64 bootID: 7f95abb9-7731-4a8c-9258-4a91cdcfb2ca containerRuntimeVersion: containerd://1.5.2 kernelVersion: 4.18.0-305.an8.x86_64 kubeProxyVersion: v1.21.1 kubeletVersion: v1.21.1 machineID: 8f6a98bffc184893ab6bc260e705421b operatingSystem: linux osImage: Ubuntu 21.04 systemUUID: f7928fdb-32be-4b6e-8dfd-260b6820f067 引用 https://kubernetes.io/docs/concepts/services-networking/dual-stack/ https://kubernetes.io/docs/tasks/network/validate-dual-stack/ ​https://kind.sigs.k8s.io/docs/user/configuration/ ​]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[k8s多集群管理方案 - KubeFed V2]]></title>
      <url>%2Fpost%2Fkubefedv2%2F</url>
      <content type="text"><![CDATA[背景Federation V1Federation v1 在 K8s v1.3 左右就已经着手设计（Design Proposal），并在后面几个版本中发布了相关的组件与命令行工具（kubefed），用于帮助使用者快速建立联邦集群，并在 v1.6 时，进入了 Beta 阶段；但 Federation v1 在进入 Beta 后，就没有更进一步的发展，由于灵活性和 API 成熟度的问题，在 K8s v1.11 左右正式被弃用。 Federation V2架构v2 版本利用 CRD 实现了整体功能，通过定义多种自定义资源（CR），从而省掉了 v1 中的 API Server；v2 版本由两个组件构成： admission-webhook 提供了准入控制 controller-manager 处理自定义资源以及协调不同集群间的状态 代码行数在2万行左右。 术语 Host Cluster：主集群/父集群 Member Cluster：子集群/成员集群 CRD扩展KubeFedCluster子集群抽象，通过kubefedctl join命令创建。后续controller会使用该信息来访问子k8s集群。123456789101112131415161718kind: KubeFedClustermetadata: name: kind-cluster1 namespace: kube-federation-systemspec: apiEndpoint: https://172.21.115.165:6443 caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1USXlOREUyTlRNME9Wb1hEVE14TVRJeU1qRTJOVE0wT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTmxjCnF3UHd5cHNFc2M0aXB3QnFuRndDN044RXo2Slhqajh3ZzRybnlXTDNSdlZ0cmVua1Nha1VyYlRjZWVIck9lQTUKWGlNUVo2T1FBY25tUGU0Q2NWSkFoL2ZLQzBkeU9uL0ZZeXgyQXppRjBCK1ZNaUFhK2dvME1VMmhMZ1R5eVFGdQpDbWFmUGtsNmJxZUFJNCtCajZJUWRqY3dVMHBjY3lrNGhSTUxnQmhnTUh4NWkzVkpQckQ2Y284dHcwVnllWncyCkdDUlh2ZzlSd0QweUs5eitOVS9LVS83QjBiMTBvekpNRlVJMktPZmI4N1RkQ0h2NmlBdlVRYVdKc1BqQ0M3MzQKcnBma1ZGZXB2S2liKy9lUVBFaDE4VE5qaitveEVGMnl0Vmo2ZWVKeFc3VVZrbit0T3BxdGpEeUtKVDllNnlwdAp6U1VDTnRZWTAzckdhNTlLczBVQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZOTnRQU3hsMlMxUldOb1BCeXNIcHFoRXJONlVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDME0vaktCL2dqcWRSNlRLNXJQVktSSFU2cy8xTTh2eTY2WFBiVEx4M0srazdicUFWdAoxVUFzcUZDZmNrVk91WHY3eFQ0ZHIzVzdtMHE1bDYzRGg3ZDJRZDNiQ00zY2FuclpNd01OM0lSMlpmYzR0VlBGCnRTMFFONElTa0hsYnBudXQxb0F3cy9CaXNwaXNRQ0VtbHF3Zy9xbmdPMStlWWRoWm5vRW40SEFDamF4Slc5MS8KNXlOR1pKRXdia2dYQTVCbSs3VEZRL2FiYnp5a1JvOWtTMnl5c29pMnVzcUg0ZnlVS0ZWK2RETnp3Ujh0ck16cgpjWkRBNHpaVFB1WGlYRkVsWlNRa2NJSGIyV0VsYmZNRGpKNjlyVG5wakJCOWNPQ25jaHVmK0xiOXQwN1lJQ01wCmNlK0prNVp2RElRUFlKK3hTeGdRaVJxMTFraWlKcFE4Wm1FWgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== proxyURL: "" secretRef: name: kind-cluster1-dbxnsstatus: conditions: - lastProbeTime: "2021-12-24T17:00:47Z" lastTransitionTime: "2021-12-24T17:00:07Z" message: /healthz responded with ok reason: ClusterReady status: "True" type: Ready Fedrated发布应用到子集群时手工创建。每一个要被联邦管理的资源都会对应一个Fedrated类型的资源，比如ConfigMap对应的是FederatedConfigMap。FederatedConfigMap包含了三部分的信息： template：联邦的资源详细spec信息，需要特别注意的是并没有包含metadata部分的信息。 placement：template中的spec信息要部署的k8s子集群信息 overrides：允许对部分k8s的部分资源进行自定义修改12345678910111213141516171819apiVersion: types.kubefed.io/v1beta1kind: FederatedConfigMapmetadata: name: test-configmap namespace: test-namespacespec: template: data: A: ala ma kota placement: clusters: - name: cluster2 - name: cluster1 overrides: - clusterName: cluster2 clusterOverrides: - path: /data value: foo: bar FederatedTypeConfig通过kubefedctl enable 创建。定义了哪些k8s api资源需要联邦，下面的例子描述了k8s ConfigMap要被联邦资源FederatedConfigMap所管理。1234567891011121314151617181920212223242526272829apiVersion: core.kubefed.io/v1beta1kind: FederatedTypeConfigmetadata: annotations: meta.helm.sh/release-name: kubefed meta.helm.sh/release-namespace: kube-federation-system finalizers: - core.kubefed.io/federated-type-config labels: app.kubernetes.io/managed-by: Helm name: configmaps namespace: kube-federation-systemspec: federatedType: group: types.kubefed.io kind: FederatedConfigMap pluralName: federatedconfigmaps scope: Namespaced version: v1beta1 propagation: Enabled # 是否启用该联邦对象 targetType: kind: ConfigMap pluralName: configmaps scope: Namespaced version: v1status: observedGeneration: 1 propagationController: Running statusController: NotRunning ReplicaSchedulingPreference用来管理相同名字的FederatedDeployment或FederatedReplicaset资源，保证所有子集群的副本数为spec.totalReplicas。1234567891011121314151617apiVersion: scheduling.kubefed.io/v1alpha1kind: ReplicaSchedulingPreferencemetadata: name: test-deployment namespace: test-nsspec: targetKind: FederatedDeployment totalReplicas: 9 clusters: A: minReplicas: 4 maxReplicas: 6 weight: 1 B: minReplicas: 4 maxReplicas: 8 weight: 2 安装使用kind来在本机创建多个k8s集群的方式测试。通过kind创建两个k8s集群 kind-cluster1和kind-cluster2。​ 安装kubefedctl1234wget https://github.com/kubernetes-sigs/kubefed/releases/download/v0.9.0/kubefedctl-0.9.0-linux-amd64.tgztar zvxf kubefedctl-0.9.0-linux-amd64.tgzchmod u+x kubefedctlmv kubefedctl /usr/local/bin/ 在第一个集群安装KubeFed，在第一个集群执行如下的命令123456789$ helm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts$ helm repo list$ helm --namespace kube-federation-system upgrade -i kubefed kubefed-charts/kubefed --version=0.9.0 --create-namespace# 会安装如下两个deployment，其中一个是controller，另外一个是webhook$ kubectl get deploy -n kube-federation-systemNAME READY UP-TO-DATE AVAILABLE AGEkubefed-admission-webhook 1/1 1 1 7m40skubefed-controller-manager 2/2 2 2 7m40s 子集群注册将cluster1和cluster2集群加入到cluster1中12kubefedctl join kind-cluster1 --cluster-context kind-cluster1 --host-cluster-context kind-cluster1 --v=2kubefedctl join kind-cluster2 --cluster-context kind-cluster2 --host-cluster-context kind-cluster1 --v=2 在第一个集群可以看到在kube-federation-system中创建出了两个新的kubefedcluster对象123456789101112131415161718192021222324$ kubectl -n kube-federation-system get kubefedclusterNAME AGE READYkind-cluster1 17s Truekind-cluster2 16s True$ apiVersion: core.kubefed.io/v1beta1kind: KubeFedClustermetadata: name: kind-cluster1 namespace: kube-federation-systemspec: apiEndpoint: https://172.21.115.165:6443 caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1USXlOREUyTlRNME9Wb1hEVE14TVRJeU1qRTJOVE0wT1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTmxjCnF3UHd5cHNFc2M0aXB3QnFuRndDN044RXo2Slhqajh3ZzRybnlXTDNSdlZ0cmVua1Nha1VyYlRjZWVIck9lQTUKWGlNUVo2T1FBY25tUGU0Q2NWSkFoL2ZLQzBkeU9uL0ZZeXgyQXppRjBCK1ZNaUFhK2dvME1VMmhMZ1R5eVFGdQpDbWFmUGtsNmJxZUFJNCtCajZJUWRqY3dVMHBjY3lrNGhSTUxnQmhnTUh4NWkzVkpQckQ2Y284dHcwVnllWncyCkdDUlh2ZzlSd0QweUs5eitOVS9LVS83QjBiMTBvekpNRlVJMktPZmI4N1RkQ0h2NmlBdlVRYVdKc1BqQ0M3MzQKcnBma1ZGZXB2S2liKy9lUVBFaDE4VE5qaitveEVGMnl0Vmo2ZWVKeFc3VVZrbit0T3BxdGpEeUtKVDllNnlwdAp6U1VDTnRZWTAzckdhNTlLczBVQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZOTnRQU3hsMlMxUldOb1BCeXNIcHFoRXJONlVNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFDME0vaktCL2dqcWRSNlRLNXJQVktSSFU2cy8xTTh2eTY2WFBiVEx4M0srazdicUFWdAoxVUFzcUZDZmNrVk91WHY3eFQ0ZHIzVzdtMHE1bDYzRGg3ZDJRZDNiQ00zY2FuclpNd01OM0lSMlpmYzR0VlBGCnRTMFFONElTa0hsYnBudXQxb0F3cy9CaXNwaXNRQ0VtbHF3Zy9xbmdPMStlWWRoWm5vRW40SEFDamF4Slc5MS8KNXlOR1pKRXdia2dYQTVCbSs3VEZRL2FiYnp5a1JvOWtTMnl5c29pMnVzcUg0ZnlVS0ZWK2RETnp3Ujh0ck16cgpjWkRBNHpaVFB1WGlYRkVsWlNRa2NJSGIyV0VsYmZNRGpKNjlyVG5wakJCOWNPQ25jaHVmK0xiOXQwN1lJQ01wCmNlK0prNVp2RElRUFlKK3hTeGdRaVJxMTFraWlKcFE4Wm1FWgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== proxyURL: "" secretRef: name: kind-cluster1-dbxnsstatus: conditions: - lastProbeTime: "2021-12-24T17:00:47Z" lastTransitionTime: "2021-12-24T17:00:07Z" message: /healthz responded with ok reason: ClusterReady status: "True" type: Ready 要想将cluster2从主集群中移除，可以执行 kubefedctl unjoin kind-cluster2 –cluster-context kind-cluster2 –host-cluster-context kind-cluster1 –v=2 应用发布集群联邦APIkubefed将资源分为普通k8s资源类型和联邦的资源类型。在默认的场景下，kubefed已经内置将很多资源类型做了集群联邦。12345678910111213$ k get FederatedTypeConfig -n kube-federation-systemNAME AGEclusterrolebindings.rbac.authorization.k8s.io 19hclusterroles.rbac.authorization.k8s.io 19hconfigmaps 19hdeployments.apps 19hingresses.extensions 19hjobs.batch 19hnamespaces 19hreplicasets.apps 19hsecrets 19hserviceaccounts 19hservices 19h 将crd资源类型集群联邦，执行 kubefedctl enable 命令1234$ kubefedctl enable customresourcedefinitionsI1224 20:32:54.537112 687543 util.go:141] Api resource found.customresourcedefinition.apiextensions.k8s.io/federatedcustomresourcedefinitions.types.kubefed.io createdfederatedtypeconfig.core.kubefed.io/customresourcedefinitions.apiextensions.k8s.io created in namespace kube-federation-system 可以看到会多出一个名字为federatedcustomresourcedefinitions.types.kubefed.io 的CRD 资源，同时会新创建一个FederatedTypeConfig类型的资源。当创建了FederatedTypeConfig后，就可以通过创建federatedcustomresourcedefinition类型的实例来向各个子集群发布CRD资源了。123456789101112131415161718192021222324252627282930$ k get crd federatedcustomresourcedefinitions.types.kubefed.ioNAME CREATED ATfederatedcustomresourcedefinitions.types.kubefed.io 2021-12-24T12:32:54Z$ k get FederatedTypeConfig -n kube-federation-system customresourcedefinitions.apiextensions.k8s.io -o yamlapiVersion: core.kubefed.io/v1beta1kind: FederatedTypeConfigmetadata: finalizers: - core.kubefed.io/federated-type-config name: customresourcedefinitions.apiextensions.k8s.io namespace: kube-federation-systemspec: federatedType: group: types.kubefed.io kind: FederatedCustomResourceDefinition pluralName: federatedcustomresourcedefinitions scope: Cluster version: v1beta1 propagation: Enabled targetType: group: apiextensions.k8s.io kind: CustomResourceDefinition pluralName: customresourcedefinitions scope: Cluster version: v1status: observedGeneration: 1 propagationController: Running statusController: NotRunning 在example/sample1下包含了很多例子，可以直接参考。接下来使用sample1中的例子进行试验。​ 在父集群创建namespace test-namespace12$ kubectl create -f namespace.yamlnamespace/test-namespace created 在主集群中创建federatednamespace资源12$ k apply -f federatednamespace.yamlfederatednamespace.types.kubefed.io/test-namespace created 在子集群中即可查询到新创建的namespace资源123$ k get ns test-namespaceNAME STATUS AGEtest-namespace Active 4m49s 其他的对象均可以按照跟上述namespace同样的方式来创建，比较特殊的对象为clusterrulebinding，该对象在默认情况下没有联邦api FederatedClusterRoleBinding，因此需要手工先创建FederatedClusterRoleBinding联邦api类型。1234$ kubefedctl enable clusterrolebindingI1225 01:46:42.779166 818254 util.go:141] Api resource found.customresourcedefinition.apiextensions.k8s.io/federatedclusterrolebindings.types.kubefed.io createdfederatedtypeconfig.core.kubefed.io/clusterrolebindings.rbac.authorization.k8s.io created in namespace kube-federation-system 在创建完成FederatedClusterRoleBinding联邦api类型后，即可以创建出FederatedClusterRoleBinding类型的对象。 子集群的个性化配置通过Fedrated中的spec.overrides来完成，可以覆盖spec.template中的内容12345678910111213141516171819202122232425262728293031323334353637383940apiVersion: types.kubefed.io/v1beta1kind: FederatedDeploymentmetadata: name: test-deployment namespace: test-namespacespec: template: metadata: labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx name: nginx placement: clusters: - name: cluster2 - name: cluster1 overrides: - clusterName: cluster2 clusterOverrides: - path: "/spec/replicas" value: 5 - path: "/spec/template/spec/containers/0/image" value: "nginx:1.17.0-alpine" - path: "/metadata/annotations" op: "add" value: foo: bar - path: "/metadata/annotations/foo" op: "remove" 多集群应用调度发布应用到特定的子集群在Fedrated的spec.placement.clusters中，定义了要发布到哪个子集群的信息。1234placement: clusters: - name: cluster2 - name: cluster1 不过上述方法需要指定特定的集群，为了有更丰富的灵活性，kubefed还提供了label selector的机制，可以提供spec.placement.clusterSelector来指定一组集群。123456spec: placement: clusters: [] clusterSelector: matchLabels: foo: bar 标签选择器通过跟kubefedclusters对象的label来进行匹配，可以执行下述命令给kubefedclusters来打标签。1kubectl label kubefedclusters -n kube-federation-system kind-cluster1 foo=bar 子集群差异化调度上述调度功能被选择的子集群为平等关系，如果要想子集群能够有所差异，可以使用ReplicaSchedulingPreference来完成，目前仅支持deployment和replicaset两种类型的资源，还不支持statefulset。1234567891011121314151617apiVersion: scheduling.kubefed.io/v1alpha1kind: ReplicaSchedulingPreferencemetadata: name: test-deployment namespace: test-nsspec: targetKind: FederatedDeployment totalReplicas: 9 clusters: A: minReplicas: 4 maxReplicas: 6 weight: 1 B: minReplicas: 4 maxReplicas: 8 weight: 2 spec.targetKind用来指定管理的类型，仅支持FederatedDeployment和FederatedReplicaset。spec.totalReplicas用来指定所有子集群的副本数总和。优先级要高于FederatedDeployment和FederatedReplicaset的spec.template中指定的副本数。spec.clusters用来指定每个子集群的最大、最小副本数和权重。spec.rebalance自动调整整个集群中的副本数，比如一个集群中的pod挂掉后，可以将pod迁移到另外的集群中。 子集群之间的交互无 缺点 父集群充当了所有集群的单一控制面 通过联邦CRD来管理资源，无法直接使用k8s原生的资源，集群间维护CRD版本和API版本一致性导致升级比较复杂。 参考资料 官方文档：https://github.com/kubernetes-sigs/kubefed/blob/master/docs/userguide.md#using-cluster-selector]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[证书技术]]></title>
      <url>%2Fpost%2Fcertificate%2F</url>
      <content type="text"><![CDATA[创建自签名证书创建CA123openssl genrsa -out root.key 4096openssl req -new -x509 -days 1000 -key root.key -out root.crtopenssl x509 -text -in root.crt -noout 创建私钥和公钥文件1234# 用来产生私钥文件server.keyopenssl genrsa -out server.key 2048# 产生公钥文件openssl rsa -in server.key -pubout -out server.pem 创建签名请求1234openssl req -new -key server.key -out server.csr# 查看创建的签名请求openssl req -noout -text -in server.csr 创建自签名证书新建自签名证书的附加信息server.ext，内容如下123456789authorityKeyIdentifier=keyid,issuerbasicConstraints=CA:FALSEkeyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = localhostIP.2 = 127.0.0.1IP.3 = 10.66.3.6 使用ca签发ssl证书，此时会产生server.crt文件，即为证书文件123openssl x509 -req -in server.csr -out server.crt -days 3650 \ -CAcreateserial -CA root.crt -CAkey root.key \ -CAserial serial -extfile server.ext 使用ca校验证书是否通过1openssl verify -CAfile root.pem server.crt 证书格式证书按照格式可以分为二进制和文本文件两种格式。 二进制格式分为： .der或者.cer：用来存放证书信息，不包含私钥。 文本格式分为： .pem：存放证书或者私钥。一般是.key文件存放私钥信息。对于pem或者key文件，如果存在——BEGIN CERTIFICATE——，则说明这是一个证书文件。如果存在—–BEGIN RSA PRIVATE KEY—–，则说明这是一个私钥文件。 *.key：用来存放私钥文件。 *.crt：证书请求文件，格式的开头为：—–BEGIN CERTIFICATE REQUEST—–证书格式的转换将cert证书转换为pem格式12openssl rsa -in server.key -text &gt; server-key.pemopenssl x509 -in server.crt -out server.pem 将pem格式转换为cert格式 证书的使用curl命令关于证书的用法： –cacert：指定ca来校验server端的证书合法性 –cert：指定客户端的证书文件，用在双向认证mTLS中 –key：私钥文件名，用在双向认证mTLS中 在mTLS的双向认证中，可以使用如下的命令： curl 参考链接 https://gist.github.com/liuguangw/4d4b87b750be8edb700ff94c783b1dd4 https://coolshell.cn/articles/21708.html https://help.aliyun.com/document_detail/160093.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[k8s多集群管理方案 - clusternet]]></title>
      <url>%2Fpost%2Fclusternet%2F</url>
      <content type="text"><![CDATA[简介腾讯云开源的k8s多集群管理方案，可发布应用到多个k8s集群。https://github.com/clusternet/clusternetGitHub Star：891 架构包含clusternet-hub和clusternet-agent两个组件。 clusternet-hub部署在父集群，用途： 接收子集群的注册请求，并为每个自己群创建namespace、serviceaccount等资源 提供父集群跟各个子集群的长连接 提供restful api来访问各个子集群，同时支持子集群的互访 clusternet-agent部署在子集群，用途： 自动注册子集群到父集群 心跳报告到中心集群，包括k8s版本、运行平台、健康状态等 通过websocket协议跟父集群通讯 CRD抽象ClusterRegistrationRequestclusternet-agent在父集群中创建的CR，一个k8s集群一个12345678910111213141516171819apiVersion: clusters.clusternet.io/v1beta1kind: ClusterRegistrationRequestmetadata: labels: clusters.clusternet.io/cluster-id: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusters.clusternet.io/cluster-name: clusternet-cluster-pmlbp clusters.clusternet.io/registered-by: clusternet-agent name: clusternet-2bcd48d2-0ead-45f7-938b-5af6af7da5fdspec: clusterId: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusterName: clusternet-cluster-pmlbp # 集群名称 clusterType: EdgeCluster # 集群类型 syncMode: Dualstatus: caCertificate: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1USXhNekV3TWpjeE5Wb1hEVE14TVRJeE1URXdNamN4TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTHVCCmJwdXNaZXM5RXFXaEw1WUVGb2c4eHRJai9jbFh2S1lTMnZYbGR4cUt1MUR3VHV3aUcxZUN4VGlHa2dLQXVXd1IKVFFheEh5aGY3U29lc0hqMG43NnFBKzhQT05lS1VGdERJOWVzejF2WTF5bXFoUHR0QVo0cGhkWmhmbXJjZTJLRQpuMS84MzNWbWlXd0pSZmNWcEJtTU52MjFYMVVwNWp6RGtncS9tY0JOOGN0ZU5PMEpKNkVVeTE2RXZLbjhyWG90ClErTW5PUHE4anFNMzJjRFFqYWVEdGxvM2kveXlRd1NMa2wzNFo4aElwZDZNWWVSTWpXcmhrazF4L0RYZjNJK3IKOGs5S1FBbEsvNWZRMXk5NHYwT25TK0FKZTliczZMT2srYVFWYm5SbExab2E2aVZWbUJNam03UjBjQ2w0Y1hpRwpyekRnN1ZLc3lsY1o3aGFRRTNFQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZPaGwxMHUwNnJvenZJUm9XVmpNYlVPMzFDbERNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBQytqMXpPQVZYVXpNUFJ0U29Kd3JhMU1FSkJOUTNMWitmWnZRYjdIbk53b00zaCthMgoyc25yUitSWTYrOFFDbXVKeis1eU5yYStEZDlnNzQ1Q0hDaFpVZzlsY3RjQTRzZVR5OXZqcUVQNVBNSzEvbi9PCnFEcDQyMUpqTjYvUXJmamIvbVM0elUvZXNGZGowQXRYQ0FLMWJsNmF0ai9jZXVBbzh1bTRPaUVlNnJhanBYTHUKWFlmQ1FVZFV5TWpQdEZHTDMwNytna0RpdlVsdXk3RkQ4aUpURTh2QWpxOVBXOW40SmxFMjdQWXR5QnNocy9XSApCZ2czQjZpTG10SjZlNzJiWnA3ZmptdmJWTC9VdkxzYXZqRXltZDhrMnN1bFFwQUpzeVJrMkEzM1g3NWJpS0RGCk1CU29DaHc3U2JMSkJrN0FNckRzTjd1Q2U3WWVIWGdZemdhRAotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== dedicatedNamespace: clusternet-sdqrh managedClusterName: clusternet-cluster-pmlbp result: Approved token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNkluZFpWMUV6UmxCbFdUUldWa1Y0UmxBeWMzaDJOV3RuUzBabVJsaG9jWG94TkhKM2JtUkxiRTlrYUZVaWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpqYkhWemRHVnlibVYwTFhOa2NYSm9JaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbU5zZFhOMFpYSnVaWFF0YmpneWFHc3RkRzlyWlc0dGN6SnpjRFVpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pWTJ4MWMzUmxjbTVsZEMxdU9ESm9heUlzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJamN3TWpRMk1tTTBMV0prTlRFdE5HVXpNeTA1WXpnekxUWmpPV1F6TUdGall6bG1aU0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwamJIVnpkR1Z5Ym1WMExYTmtjWEpvT21Oc2RYTjBaWEp1WlhRdGJqZ3lhR3NpZlEuUjJCdEQ1YzQxRm1seC1hTEFKSWQzbGxvOThSY25TakVUak5pSnVuTXg1US1jYXhwc3VkamUxekVtUF9mTHR6TjU4d21Dd3RXcHpoaXhSMWFTUHE5LXJTRlBIYzk3aDlTT3daZGdicC10alFBSjA4dllfYWdiVFRKLU1WN1dpN0xQVzRIcmt1U3RlemUzVHh2RW11NWwySlpzbG5UTXdkR3NGRVlVZzVfeWFoLUQwQ2pnTkZlR1ljUjJ1TlJBWjdkNW00Q1c5VmRkdkNsanNqRE5WX1k0RkFEbGo2cHgzSlh0SDQ4U1ZUd254TS1sNHl0eXBENjZFbG1PYXpUMmRwSWd4eWNSZ0tJSUlacWNQNnZVc0ZOM1Zka21ZZ29ydy1NSUcwc25YdzFaZ1lwRk9SUDJRa3hjd2NOUVVOTjh6VUZqSUZSSmdSSFdjNlo4aXd2eURnZTVn ClusterRole和RoleClusterRegistrationRequest被接收后会创建ClusterRole1234567891011121314151617181920212223242526apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: annotations: clusternet.io/autoupdate: "true" labels: clusternet.io/created-by: clusternet-hub clusters.clusternet.io/bootstrapping: rbac-defaults clusters.clusternet.io/cluster-id: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd name: clusternet-2bcd48d2-0ead-45f7-938b-5af6af7da5fdrules:- apiGroups: - clusters.clusternet.io resources: - clusterregistrationrequests verbs: - create - get- apiGroups: - proxies.clusternet.io resourceNames: - 2bcd48d2-0ead-45f7-938b-5af6af7da5fd resources: - sockets verbs: - '*' 并会在cluster对应的namespace下创建Role1234567891011121314151617apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: annotations: clusternet.io/autoupdate: "true" labels: clusternet.io/created-by: clusternet-hub clusters.clusternet.io/bootstrapping: rbac-defaults name: clusternet-managedcluster-role namespace: clusternet-sdqrhrules:- apiGroups: - '*' resources: - '*' verbs: - '*' ManagedClusterclusternet-hub在接受ClusterRegistrationRequest后，会创建一个ManagedCluster。一个k8s集群一个namespace1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: clusters.clusternet.io/v1beta1kind: ManagedClustermetadata: labels: clusternet.io/created-by: clusternet-agent clusters.clusternet.io/cluster-id: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusters.clusternet.io/cluster-name: clusternet-cluster-pmlbp name: clusternet-cluster-pmlbp namespace: clusternet-sdqrhspec: clusterId: 2bcd48d2-0ead-45f7-938b-5af6af7da5fd clusterType: EdgeCluster syncMode: Dualstatus: allocatable: cpu: 7600m memory: "30792789992" apiserverURL: https://10.233.0.1:443 # default/kubernetes的service clusterip appPusher: true capacity: cpu: "8" memory: 32192720Ki clusterCIDR: 10.233.0.0/18 conditions: - lastTransitionTime: "2021-12-15T07:47:10Z" message: managed cluster is ready. reason: ManagedClusterReady status: "True" type: Ready healthz: true heartbeatFrequencySeconds: 180 k8sVersion: v1.21.5 lastObservedTime: "2021-12-15T07:47:10Z" livez: true nodeStatistics: readyNodes: 1 parentAPIServerURL: https://172.21.115.160:6443 # 父集群地址 platform: linux/amd64 readyz: true serviceCIDR: 10.233.64.0/18 useSocket: true Subscription应用发布的抽象，人工提交到环境中。要发布的资源即可以是helm chart，也可以是原生的k8s对象。clusternet在部署的时候会查看部署的优先级，并且支持weight，优先部署cluster级别的资源，这点跟helm部署的逻辑一致。12345678910111213141516171819202122232425262728apiVersion: apps.clusternet.io/v1alpha1kind: Subscriptionmetadata: name: app-demo namespace: defaultspec: # 定义应用要发布到哪个k8s集群 subscribers: # defines the clusters to be distributed to - clusterAffinity: matchLabels: clusters.clusternet.io/cluster-id: dc91021d-2361-4f6d-a404-7c33b9e01118 # PLEASE UPDATE THIS CLUSTER-ID TO YOURS!!! # 定义了要发布哪些资源 feeds: # defines all the resources to be deployed with - apiVersion: apps.clusternet.io/v1alpha1 kind: HelmChart name: mysql namespace: default - apiVersion: v1 kind: Namespace name: foo - apiVersion: v1 kind: Service name: my-nginx-svc namespace: foo - apiVersion: apps/v1 kind: Deployment name: my-nginx namespace: foo HelmChartSubscriber发布的一个子资源之一，对应一个helm chart的完整描述12345678910apiVersion: apps.clusternet.io/v1alpha1kind: HelmChartmetadata: name: mysql namespace: defaultspec: repo: https://charts.bitnami.com/bitnami chart: mysql version: 8.6.2 targetNamespace: abc Localization差异化不同于其他集群的配置，用来描述namespace级别的差异化配置。 Globalization用来描述不同集群的cluster级别的差异化配置。 BaseDescription跟Base对象根据Localization和Globalization渲染得到的最终要发布到集群中的最终对象。 安装在父集群执行如下命令：123helm repo add clusternet https://clusternet.github.io/chartshelm install clusternet-hub -n clusternet-system --create-namespace clusternet/clusternet-hubkubectl apply -f https://raw.githubusercontent.com/clusternet/clusternet/main/manifests/samples/cluster_bootstrap_token.yaml 会在clusternet-system namespace下创建deployment clusternet-hub。 最后一条命令，会创建一个secret，其中的token信息为07401b.f395accd246ae52d，在子集群注册的时候需要用到。1234567891011121314151617181920212223242526apiVersion: v1kind: Secretmetadata: # Name MUST be of form "bootstrap-token-&lt;token id&gt;" name: bootstrap-token-07401b namespace: kube-system# Type MUST be 'bootstrap.kubernetes.io/token'type: bootstrap.kubernetes.io/tokenstringData: # Human readable description. Optional. description: "The bootstrap token used by clusternet cluster registration." # Token ID and secret. Required. token-id: 07401b token-secret: f395accd246ae52d # Expiration. Optional. expiration: 2025-05-10T03:22:11Z # Allowed usages. usage-bootstrap-authentication: "true" usage-bootstrap-signing: "true" # Extra groups to authenticate the token as. Must start with "system:bootstrappers:" auth-extra-groups: system:bootstrappers:clusternet:register-cluster-token 在子集群执行如下命令，其中parentURL要修改为父集群的apiserver地址12345helm repo add clusternet https://clusternet.github.io/chartshelm install clusternet-agent -n clusternet-system --create-namespace \ --set parentURL=https://10.0.248.96:8443 \ --set registrationToken=07401b.f395accd246ae52d \ clusternet/clusternet-agent 其中parentURL为父集群的apiserver地址，registrationToken为父集群注册的token信息。 访问子集群kubectl-clusternet命令行方式访问安装kubectl krew插件，参考 https://krew.sigs.k8s.io/docs/user-guide/setup/install/，执行如下命令：123456789101112yum install git -y( set -x; cd "$(mktemp -d)" &amp;&amp; OS="$(uname | tr '[:upper:]' '[:lower:]')" &amp;&amp; ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &amp;&amp; KREW="krew-$&#123;OS&#125;_$&#123;ARCH&#125;" &amp;&amp; curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/$&#123;KREW&#125;.tar.gz" &amp;&amp; tar zxvf "$&#123;KREW&#125;.tar.gz" &amp;&amp; ./"$&#123;KREW&#125;" install krew)echo 'export PATH="$&#123;PATH&#125;:$&#123;HOME&#125;/.krew/bin"' &gt;&gt; ~/.bashrcsource ~/.bashrc 安装kubectl插件clusternet：kubectl krew install clusternet 使用kubectl clusternet get可以看到发布的应用，非发布的应用看不到。 代码层面访问子集群可以参照例子：https://github.com/clusternet/clusternet/blob/main/examples/clientgo/demo.go#L42-L45 改动代码非常小，仅需要获取到对应集群的config信息即可。123456789101112131415161718config, err := rest.InClusterConfig() if err != nil &#123; klog.Error(err) os.Exit(1) &#125; // This is the ONLY place you need to wrap for Clusternet config.Wrap(func(rt http.RoundTripper) http.RoundTripper &#123; return clientgo.NewClusternetTransport(config.Host, rt) &#125;) // now we could create and visit all the resources client := kubernetes.NewForConfigOrDie(config) _, err = client.CoreV1().Namespaces().Create(context.TODO(), &amp;corev1.Namespace&#123; ObjectMeta: metav1.ObjectMeta&#123; Name: "demo", &#125;, &#125;, metav1.CreateOptions&#123;&#125;) 应用发布在主集群提交如下的yaml文件1234567891011121314151617181920212223242526apiVersion: apps.clusternet.io/v1alpha1kind: Subscriptionmetadata: name: app-demo namespace: defaultspec: subscribers: # defines the clusters to be distributed to - clusterAffinity: matchLabels: clusters.clusternet.io/cluster-id: dc91021d-2361-4f6d-a404-7c33b9e01118 # PLEASE UPDATE THIS CLUSTER-ID TO YOURS!!! feeds: # defines all the resources to be deployed with - apiVersion: apps.clusternet.io/v1alpha1 kind: HelmChart name: mysql namespace: default - apiVersion: v1 kind: Namespace name: foo - apiVersion: apps/v1 kind: Service name: my-nginx-svc namespace: foo - apiVersion: apps/v1 kind: Deployment name: my-nginx namespace: foo 实现分析在主集群实现了两个aggregated apiservice12345678910111213141516171819202122apiVersion: apiregistration.k8s.io/v1kind: APIServicemetadata: annotations: meta.helm.sh/release-name: clusternet-hub meta.helm.sh/release-namespace: clusternet-system labels: app.kubernetes.io/instance: clusternet-hub app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: clusternet-hub helm.sh/chart: clusternet-hub-0.2.1 name: v1alpha1.proxies.clusternet.iospec: group: proxies.clusternet.io groupPriorityMinimum: 1000 insecureSkipTLSVerify: true service: name: clusternet-hub namespace: clusternet-system port: 443 version: v1alpha1 versionPriority: 100 12345678910111213141516171819202122apiVersion: apiregistration.k8s.io/v1kind: APIServicemetadata: annotations: meta.helm.sh/release-name: clusternet-hub meta.helm.sh/release-namespace: clusternet-system labels: app.kubernetes.io/instance: clusternet-hub app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: clusternet-hub helm.sh/chart: clusternet-hub-0.2.1 name: v1alpha1.shadowspec: group: shadow groupPriorityMinimum: 1 insecureSkipTLSVerify: true service: name: clusternet-hub namespace: clusternet-system port: 443 version: v1alpha1 versionPriority: 1 相关链接 https://mp.weixin.qq.com/s/BSgb2uoAuHbxQOUvn8fEsA]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ecs的Linux主机上快速创建测试k8s集群]]></title>
      <url>%2Fpost%2Fcreate_test_k8s%2F</url>
      <content type="text"><![CDATA[经常有快速创建一个测试k8s集群的场景，为了能够快速完成，整理了如下的命令，即可在主机上快速启动一个k8s集群。部分命令需要外网访问，推荐直接使用海外的主机。 安装docker123456789101112131415yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engineyum install -y yum-utilsyum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repoyum install docker-ce docker-ce-cli containerd.io -ysystemctl enable docker &amp;&amp; systemctl start dockeryum install vim git make -y 安装kubectl kind helm12345678910111213141516171819202122232425262728293031# 安装kubectlcurl -LO &quot;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl&quot;chmod +x kubectlmv kubectl /usr/bin/yum install -y bash-completionecho -e &apos;\n# kubectl&apos; &gt;&gt; ~/.bash_profileecho &apos;source &lt;(kubectl completion bash)&apos; &gt;&gt;~/.bash_profileecho &apos;alias k=kubectl&apos; &gt;&gt;~/.bash_profileecho &apos;complete -F __start_kubectl k&apos; &gt;&gt;~/.bash_profilesource ~/.bash_profile# 安装helmwget https://get.helm.sh/helm-v3.7.2-linux-amd64.tar.gztar zvxf helm-v3.7.2-linux-amd64.tar.gzmv linux-amd64/helm /usr/bin/rm -rf linux-amd64# 安装kubectx kubensgit clone https://github.com/ahmetb/kubectx /tmp/kubectxcp /tmp/kubectx/kubens /usr/bin/knscp /tmp/kubectx/kubectx /usr/bin/kctxwget https://github.com/junegunn/fzf/releases/download/0.29.0/fzf-0.29.0-linux_amd64.tar.gz -P /tmptar zvxf /tmp/fzf-0.29.0-llinux_amd64.tar.gz -C /tmp/mv /tmp/fzf /usr/local/bin/# 安装kindcurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64chmod +x ./kindmv ./kind /usr/local/bin/echo -e &quot;\n# kind&quot; &gt;&gt; ~/.bash_profileecho &apos;source &lt;(kind completion bash)&apos; &gt;&gt;~/.bash_profile 创建集群其中将apiServerAddress指定为了本机，即创建出来的k8s集群仅允许本集群内访问。如果要是需要多个k8s集群之间的互访场景，由于kind拉起的k8s运行在docker容器中，而docker容器使用的是容器网络，此时如果设置apiserver地址为127.0.0.1，那么集群之间就没法直接通讯了，此时需要指定一个可以在docker容器中访问的宿主机ip地址。 123456789cat &gt; kind.conf &lt;&lt;EOFkind: ClusterapiVersion: kind.x-k8s.io/v1alpha4name: kindnetworking: apiServerAddress: &quot;127.0.0.1&quot; apiServerPort: 6443EOFkind create cluster --config kind.conf 其他周边工具12345678910111213141516# 安装kustomizecurl -s &quot;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh&quot; | bashmv kustomize /usr/local/bin/# 安装golangwget https://go.dev/dl/go1.17.5.linux-amd64.tar.gz -P /opttar zvxf /opt/go1.17.5.linux-amd64.tar.gz -C /opt/mkdir /opt/gopathecho -e &apos;\n# golang&apos; &gt;&gt; ~/.bash_profileecho &apos;export GOROOT=/opt/go&apos; &gt;&gt; ~/.bash_profileecho &apos;export GOPATH=/opt/gopath&apos; &gt;&gt; ~/.bash_profileecho &apos;export PATH=$PATH:$GOPATH/bin:$GOROOT/bin&apos; &gt;&gt; ~/.bash_profilesource ~/.bash_profile# 安装controller-gen，会将controller-gen命令安装到GOPATH/bin目录下go install sigs.k8s.io/controller-tools/cmd/controller-gen@latest 安装krew12345678910111213( set -x; cd &quot;$(mktemp -d)&quot; &amp;&amp; OS=&quot;$(uname | tr &apos;[:upper:]&apos; &apos;[:lower:]&apos;)&quot; &amp;&amp; ARCH=&quot;$(uname -m | sed -e &apos;s/x86_64/amd64/&apos; -e &apos;s/\(arm\)\(64\)\?.*/\1\2/&apos; -e &apos;s/aarch64$/arm64/&apos;)&quot; &amp;&amp; KREW=&quot;krew-$&#123;OS&#125;_$&#123;ARCH&#125;&quot; &amp;&amp; curl -fsSLO &quot;https://github.com/kubernetes-sigs/krew/releases/latest/download/$&#123;KREW&#125;.tar.gz&quot; &amp;&amp; tar zxvf &quot;$&#123;KREW&#125;.tar.gz&quot; &amp;&amp; ./&quot;$&#123;KREW&#125;&quot; install krew)echo -e &apos;\n# kubectl krew&apos; &gt;&gt; ~/.bash_profileecho &apos;export PATH=&quot;$&#123;KREW_ROOT:-$HOME/.krew&#125;/bin:$PATH&quot;&apos; &gt;&gt; ~/.bash_profilesource ~/.bash_profile]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[lvm]]></title>
      <url>%2Fpost%2Flvm%2F</url>
      <content type="text"><![CDATA[概念 物理卷PV(Physical Volume)：可以是整块物理磁盘或者物理磁盘上的分区 卷组VG(Volume Group)：由一个或多个物理卷PV组成，可在卷组上创建一个或者多个LV，可以动态增加pv到卷组 逻辑卷LV(Logical Volume)：类似于磁盘分区，建立在VG之上，在LV上可以创建文件系统 逻辑卷建立后可以动态的增加或缩小空间 PE(Physical Extent): PV可被划分为PE的基本单元，具有唯一编号的PE是可以被LVM寻址的最小单元。PE的大小是可以配置的，默认为4MB。 LE(Logical Extent): LV可被划分为LE的基本单元，LE跟PE是一对一的关系。 基本操作lvm相关的目录如果没有按照，在centos下使用yum install lvm2进行安装。 初始磁盘状态如下，/dev/sda上有40g磁盘空间未分配： 1234567891011121314151617181920# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda1 40G 2.8G 38G 7% /devtmpfs 236M 0 236M 0% /devtmpfs 244M 0 244M 0% /dev/shmtmpfs 244M 4.5M 240M 2% /runtmpfs 244M 0 244M 0% /sys/fs/cgrouptmpfs 49M 0 49M 0% /run/user/1000# fdisk -lDisk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000a05f8 Device Boot Start End Blocks Id System/dev/sda1 * 2048 83886079 41942016 83 Linux 对磁盘的操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# fdisk /dev/sdaWelcome to fdisk (util-linux 2.23.2).Changes will remain in memory only, until you decide to write them.Be careful before using the write command.# 创建主分区2，空间为1g，磁盘格式为lvmCommand (m for help): nPartition type: p primary (1 primary, 0 extended, 3 free) e extendedSelect (default p): pPartition number (2-4, default 2): 2First sector (83886080-167772159, default 83886080):Using default value 83886080Last sector, +sectors or +size&#123;K,M,G&#125; (83886080-167772159, default 167772159): +1GPartition 2 of type Linux and of size 1 GiB is setCommand (m for help): tPartition number (1,2, default 2): 2Hex code (type L to list all codes): 8eChanged type of partition &apos;Linux&apos; to &apos;Linux LVM&apos;# 创建主分区3，空间为5g，磁盘格式为lvmCommand (m for help): nPartition type: p primary (2 primary, 0 extended, 2 free) e extendedSelect (default p): pPartition number (3,4, default 3): 3First sector (85983232-167772159, default 85983232):Using default value 85983232Last sector, +sectors or +size&#123;K,M,G&#125; (85983232-167772159, default 167772159): +5GPartition 3 of type Linux and of size 5 GiB is setCommand (m for help): tPartition number (1-3, default 3): 3Hex code (type L to list all codes): 8eChanged type of partition &apos;Linux&apos; to &apos;Linux LVM&apos;# 保存上述操作Command (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy.The kernel still uses the old table. The new table will be used atthe next reboot or after you run partprobe(8) or kpartx(8)Syncing disks. 当前磁盘空间状态如下： 12345678910111213# fdisk -lDisk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000a05f8 Device Boot Start End Blocks Id System/dev/sda1 * 2048 83886079 41942016 83 Linux/dev/sda2 83886080 85983231 1048576 8e Linux LVM/dev/sda3 85983232 96468991 5242880 8e Linux LVM 将上述两个lvm磁盘分区创建pv，使用pvremove /dev/sda2可以删除pv 123456789101112131415161718192021222324252627# pvcreate /dev/sda2 /dev/sda3 Physical volume &quot;/dev/sda2&quot; successfully created. Physical volume &quot;/dev/sda3&quot; successfully created.[root@localhost vagrant]# pvdisplay &quot;/dev/sda2&quot; is a new physical volume of &quot;1.00 GiB&quot; --- NEW Physical volume --- PV Name /dev/sda2 VG Name PV Size 1.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID MgarF2-Ka6D-blDi-8ecd-1SEU-y2GD-JtiK2c &quot;/dev/sda3&quot; is a new physical volume of &quot;5.00 GiB&quot; --- NEW Physical volume --- PV Name /dev/sda3 VG Name PV Size 5.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID ToKp2T-30mS-0P8c-ZrcB-2lO4-ayo9-62fuDx 接下来创建vg，vg的名字可以随便定义，并将创建的两个pv都添加到vg中，可以看到vg的空间为两个pv之和 123456789101112131415161718192021222324252627282930313233# vgcreate vg1 /dev/sda1# vgdisplay -v --- Volume group --- VG Name vg1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 5.99 GiB PE Size 4.00 MiB Total PE 1534 Alloc PE / Size 0 / 0 Free PE / Size 1534 / 5.99 GiB VG UUID XI8Biv-JtUv-tsur-wuvm-IJQz-HLZu-6a2u5G --- Physical volumes --- PV Name /dev/sda2 PV UUID MgarF2-Ka6D-blDi-8ecd-1SEU-y2GD-JtiK2c PV Status allocatable Total PE / Free PE 255 / 255 PV Name /dev/sda3 PV UUID ToKp2T-30mS-0P8c-ZrcB-2lO4-ayo9-62fuDx PV Status allocatable Total PE / Free PE 1279 / 1279 接下来创建lv，并从vg1中分配空间2g 1234567891011121314151617181920# lvcreate -L 2G -n lv1 vg1 Logical volume &quot;lv1&quot; created.# lvdisplay --- Logical volume --- LV Path /dev/vg1/lv1 LV Name lv1 VG Name vg1 LV UUID EesY4i-lSqY-ef1R-599C-XTrZ-IcVL-P7W46Q LV Write Access read/write LV Creation host, time localhost.localdomain, 2019-06-16 07:29:38 +0000 LV Status available # open 0 LV Size 2.00 GiB Current LE 512 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 8192 Block device 253:0 接下来给lv1格式化磁盘格式为ext4，并将磁盘挂载到/tmp/lvm目录下 123456789101112131415161718192021222324252627282930313233# mkfs.ext4 /dev/vg1/lv1mke2fs 1.42.9 (28-Dec-2013)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks131072 inodes, 524288 blocks26214 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=53687091216 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Allocating group tables: doneWriting inode tables: doneCreating journal (16384 blocks): doneWriting superblocks and filesystem accounting information: done# mkdir /tmp/lvm[root@localhost vagrant]# mount /dev/vg1/lv1 /tmp/lvm[root@localhost vagrant]# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda1 40G 2.9G 38G 8% /devtmpfs 236M 0 236M 0% /devtmpfs 244M 0 244M 0% /dev/shmtmpfs 244M 4.5M 240M 2% /runtmpfs 244M 0 244M 0% /sys/fs/cgrouptmpfs 49M 0 49M 0% /run/user/1000/dev/mapper/vg1-lv1 2.0G 6.0M 1.8G 1% /tmp/lvm 接下来对lv的空间从2G扩展到3G，此时通过df查看分区空间大小仍然为2g，需要执行resize2fs命令 12345678910111213141516171819# lvextend -L +1G /dev/vg1/lv1 Size of logical volume vg1/lv1 changed from 2.00 GiB (512 extents) to 3.00 GiB (768 extents). Logical volume vg1/lv1 successfully resized.# resize2fs /dev/vg1/lv1resize2fs 1.42.9 (28-Dec-2013)Filesystem at /dev/vg1/lv1 is mounted on /tmp/lvm; on-line resizing requiredold_desc_blocks = 1, new_desc_blocks = 1The filesystem on /dev/vg1/lv1 is now 786432 blocks long.# df -hFilesystem Size Used Avail Use% Mounted on/dev/sda1 40G 2.9G 38G 8% /devtmpfs 236M 0 236M 0% /devtmpfs 244M 0 244M 0% /dev/shmtmpfs 244M 4.5M 240M 2% /runtmpfs 244M 0 244M 0% /sys/fs/cgrouptmpfs 49M 0 49M 0% /run/user/1000/dev/mapper/vg1-lv1 2.9G 6.0M 2.8G 1% /tmp/lvm 问题试验时操作失误，出现了先格式化磁盘，后发现pv找不到对应设备。vg删除不成功。 正常删除vg的方式，此时lv会自动消失 12vgreduce --removemissing vg1vgremove vg1 lvremove操作执行的时候经常会出现提示“Logical volume xx contains a filesystem in use.”的情况，该问题一般是由于有其他进程在使用该文件系统导致的。网络上经常看到的是通过fuser或者lsof命令来查找使用方，但偶尔该命令会失效，尤其在本机上有容器的场景下。另外一个可行的办法是通过 grep -nr &quot;/data&quot; /proc/*/mount 命令，可以找到挂载该目录的所有进程，简单有效。 三种Logic VolumeLVM的机制可以类比于RAID，RAID一个核心的机制是性能和数据冗余，并提供了多种数据的冗余模块可供配置。lvm在性能和数据冗余方面支持如下三种Logic Volume: 线性逻辑卷、条带化逻辑卷和镜像逻辑卷。上述几种模式是在vg已经创建完成后创建lv的时候指定的模式，会影响到lv中的pe分配。 下面使用如下的机器配置来进行各个模式的介绍和功能测试。12345678$ lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTvda 253:0 0 40G 0 disk└─vda1 253:1 0 40G 0 part /vdb 253:16 0 100G 0 diskvdc 253:32 0 200G 0 diskvdd 253:48 0 300G 0 diskvde 253:64 0 400G 0 disk 线性逻辑卷 Linear Logic Volume当一个VG中有两个或者多个磁盘的时候，LV分配磁盘容量的时候是按照VG中的PV安装顺序分配的，即一个PV用完后才会分配下一块PV。也可以在创建LV的通过指定PV中的PE段来将数据分散到多个PV上。该模式也是LVM的默认模式。 使用/dev/vdb和/dev/vdc两块磁盘来进行测试，先创建对应的PV。 12345678910111213141516171819202122232425262728$ pvcreate /dev/vdb /dev/vdc Physical volume &quot;/dev/vdb&quot; successfully created. Physical volume &quot;/dev/vdc&quot; successfully created.$ pvdisplay &quot;/dev/vdb&quot; is a new physical volume of &quot;100.00 GiB&quot; --- NEW Physical volume --- PV Name /dev/vdb VG Name PV Size 100.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID 8NnlYc-4f3f-fkeW-a3l3-LXoC-9UEH-fvpb5V &quot;/dev/vdc&quot; is a new physical volume of &quot;200.00 GiB&quot; --- NEW Physical volume --- PV Name /dev/vdc VG Name PV Size 200.00 GiB Allocatable NO PE Size 0 Total PE 0 Free PE 0 Allocated PE 0 PV UUID I9ffpN-c1vc-PQOB-yKyd-MdzO-Ngff-6e116t 创建VG vg1，该容量为上面两个磁盘空间之和123456789101112131415161718192021222324$ vgcreate vg1 /dev/vdb /dev/vdc Volume group &quot;vg1&quot; successfully created$ vgdisplay --- Volume group --- VG Name vg1 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 299.99 GiB PE Size 4.00 MiB Total PE 76798 Alloc PE / Size 0 / 0 Free PE / Size 76798 / 299.99 GiB VG UUID Gi0bJx-jqY8-YpSo-kB0l-9wdk-ZfCT-GpgFZY 从vg1中创建LV lv1，其大小为vg的全部大小 12345678910111213141516171819202122232425$ lvcreate --type linear -l 100%VG -n lv1 vg1 Logical volume &quot;lv1&quot; created.$ lvdisplay --- Logical volume --- LV Path /dev/vg1/lv1 LV Name lv1 VG Name vg1 LV UUID JQZ193-dz6A-I0Ue-rTKC-6XrQ-gb1F-Qy9kDl LV Write Access read/write LV Creation host, time iZt4nd6wiprf8foracovwqZ, 2022-01-08 23:28:45 +0800 LV Status available # open 0 LV Size 299.99 GiB Current LE 76798 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:0$ lvs -o lv_name,lv_attr,lv_size,seg_pe_ranges LV Attr LSize PE Ranges lv1 -wi-a----- 299.99g /dev/vdc:0-51198 lv1 -wi-a----- 299.99g /dev/vdb:0-25598 条带化逻辑卷 Striped Logic Volume类似于raid0模式，在该模式下，多块磁盘均会分配PE给LV。可以通过-i参数来指定可以使用VG中多少个PV。 优点：将数据的读写压力分散到了多个磁盘，可以提升读写性能。缺点：一个磁盘损坏后会导致数据丢失。 但在使用striped模式时，需要注意： 如果PV来自于同一个磁盘的不同分区，会导致更多的随机读写，不仅不能提升磁盘性能，反而会导致性能下降。 如果VG中某一个PV过小，则无法将所有的PV平均使用起来，存在木桶效应。 使用--type striped来指定为striped模式，--stripes来指定需要使用的pv数量，--stripesize指定写足够数量的数据后再更换为另外一个pv。在下面的创建命令中，可以看到创建出来了12800个LE，PE是平均分配到了两个pv上。 123456789101112131415161718192021222324$ lvcreate --type striped --stripes 2 --stripesize 32k -L 50G -n lv1 vg1 Logical volume &quot;lv1&quot; created.$ lvs -o lv_name,lv_attr,lv_size,seg_pe_ranges LV Attr LSize PE Ranges lv1 -wi-a----- 50.00g /dev/vdb:0-6399 /dev/vdc:0-6399$ lvdisplay --- Logical volume --- LV Path /dev/vg1/lv1 LV Name lv1 VG Name vg1 LV UUID FnOmCM-IkuE-3Rvv-fQqk-Cv1Q-lb8S-l5X7O3 LV Write Access read/write LV Creation host, time iZt4nd6wiprf8foracovwqZ, 2022-01-09 00:07:08 +0800 LV Status available # open 0 LV Size 50.00 GiB Current LE 12800 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 252:0 如果指定的lv大小为250G，由于分配到两块磁盘上，由于最小的磁盘只有100G，100G * 2无法满足250G的磁盘大小需求，此时会报错。 12lvcreate --type striped --stripes 2 --stripesize 32k -L 250G -n lv1 vg1 Insufficient suitable allocatable extents for logical volume lv1: 12802 more required 镜像逻辑卷 Mirror Logic Volume类似于raid1。可以解决磁盘的单点问题，一块磁盘挂掉后不至于丢失数据。 通过使用--type mirror来指定为镜像模式，-m参数来指定冗余的数量。 ref Linux LVM简明教程 初识LVM及ECS上LVM分区扩容 Linux LVM–三种Logic Volume]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[《金字塔原理》总结]]></title>
      <url>%2Fpost%2Fminto-pyramid%2F</url>
      <content type="text"><![CDATA[《金字塔原理》是一本教你如何清晰的思考问题，表达观点的畅销书籍，但其并非万能的，其层层架构建立在因果关系基础之上。但现实世界中的很多问题并非简单的因果关系，此时金字塔原理就用不上了。 什么是金字塔原理定义：任何一件事情都可以归纳出一个中心论点，而这个中心论点由3到7个论据进行支撑。每一个论据它本身又可以拆分成一个论点，该论点同样也可以拆分成3到7个论据，如此重复形状就像金字塔一样。 三个注意点： 结论先行 每层结论下面的论据不要超过7个 每一个论点要言之有物，有明确的思想 组织思想的方法在有了论点之后，论据该怎么拆分呢？可以按照下面四个原则来组织思想： 时间顺序 空间顺序 重要性顺序，比如按照老弱优先原则 逻辑演绎顺序。所以的逻辑演绎，就是三大段：大前提、小前提和结论。比如，凡人皆有一死，苏格拉底是人，所以苏格拉底也会死。但不推荐用此方法，因为对于听众的要求比较高，必须能够集中注意力才能听明白。 在思考的过程中，如果还没有论点，也可以使用上述方法先得出论据，然后推理出论点。梳理出的论据必须符合MECE法则：每一个论点下的论据，都应该相互独立，但又可以完全穷尽，即论据要做到不遗漏不重叠。 如何让别人对自己的观点感兴趣SCQ法则：Situation（设置背景）、Complication（冲突）和Question（疑问）。背景即先介绍大家都认同的背景信息从而引入话题。在SCQ都讲完后，就可以引入自己的论点了。 参考 得到 - 《金字塔原理》成甲解读]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[那些软件行业的定律]]></title>
      <url>%2Fpost%2Fit-law%2F</url>
      <content type="text"><![CDATA[康威定律任何一个组织在设计一个系统的时候，这个系统的结构与这个组织的沟通结构是一致的。工作了这么些年对此深有感触，即“组织架构决定软件架构”。 布鲁克定律在一个已经延期的项目中增加人手只会让项目延期更长。我个人不是特别认可此定律，该定律肯定是项目而定的，这要看项目的协作复杂程度，如果是体力劳动居多的项目，那么堆人还是特别好使的。 帕金森定律一项工作会占用掉所有用来完成它的时间。即如果不给一个项目设置截止日期，那么该项目就永远完成不了。 冰山谬论一款新软件的开发成本只占管理层预算的总成本的25%左右。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[演讲的思考系列文章 - 张一鸣字节跳动9周年]]></title>
      <url>%2Fpost%2Freview-zhangyiming%2F</url>
      <content type="text"><![CDATA[最近因为《致阿里》的缘故，读了不少阿里内网的热帖，很多都是洋洋洒洒几千字，而且说的有理有据。如果换做是我，很多帖子哪怕我憋上一天都是写不出来的。我一直在思考，我到底比别人差在了哪里。思来想去，其中一个原因是因为平时的思考总结不够，缺少积累。 恰巧看了张一鸣在字节跳动9周年演讲，三观跟我特别合，想以贴为例，一来分享一下我个人的内心想法加深对演讲内容的认识，二来可以依次来锻炼自己的逻辑归纳能力。后续如有特别值得学习的演讲，也会分享一下自己的学习和思考，比如2020年张小龙的微信公开课的演讲就特别值得学习。 文中提到最多的词莫过于“平常心”了，“平常心”是一个佛源词，看来张一鸣没少研究佛学，整篇演讲显得也比较佛系，对于平常心的最直白的解释就是： | 吃饭的时候好好吃饭，睡觉的时候好好睡觉 要想做到“好好吃饭，好好睡觉”对我来说是挺难的，我举一个简单的例子。平常周末来说，特别想睡一个懒觉，如果在睡觉前自己明确知道因为工作没有完成，第二天早上会有人找我，那么第二天早上一定会醒的比较早，想睡个懒觉都很难，即使第二天上午并不一定有人在我醒之前找到我。说明因为有事情的缘故，已经在无形中影响了睡眠质量。再举个例子，春节假期的睡眠质量明显会高于平常周末的睡眠质量，原因是心里总有各种工作的事情不能完全放下。如果将春节假期的心态为平常心，那么一年中的其他时间对我而言都不是平常心。 每家公司在年初的时候总会定义一些目标，比如全年营收目标为1个亿。一旦有了营收目标后，那么大家的工作重心一定会围绕的着目标展开。因为毕竟公司的资源是有限的，但是为了达成营收的目标，并不能保持一颗“平常心”来工作，焦虑的员工很难打磨出最好的产品，往往其他的地方就不会做的太好，比如用户体验、代码质量等等，一些本来很好的点子因为有营收目标的缘故，也很难展开实施，从而导致一些创新项目的流失。 接下来就是一些平常心的工作原则，总结下来有如下几点： 平常心对待自己，平常人做非常事 平常心对待预期，没有预期和标签的束缚会发挥的更好 平常心对待过去和未来，关注当下 平常心对待竞争对手 平常心对待业务 平常心对待成功和失败 文中特意提到了互联网八股文的一段话，这里就不再摘出来，我平时也没少见到类似的话术，看完后的感觉就是真牛逼，打死我都写不出来，但转头就忘记讲啥了，也许就是当时压根就看不懂，因为这些话太抽象了。互联网行业本身是一个特别务实接地气的行业，现在也渐渐在内卷严重，抽象的词汇也越来越多，期望下一个风口的到来，或许会将这股邪气吹走一些。 引用张一鸣演讲全文：外部波澜起伏，内心平静如常]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang中的异常处理机制]]></title>
      <url>%2Fpost%2Fgolang-error%2F</url>
      <content type="text"><![CDATA[异常处理分为error和defer和recover两类，其中error用来处理可预期的异常，recover用来处理意外的异常。 error支持多个返回值，可以将业务的返回值和错误的返回值分开，很多都会返回两个值。如果不使用error返回值，可以用_变量来忽略。12345678910111213141516171819202122// parseConfig returns a parsed configuration for an Azure cloudprovider config filefunc parseConfig(configReader io.Reader) (*Config, error) &#123; var config Config if configReader == nil &#123; return &amp;config, nil &#125; configContents, err := ioutil.ReadAll(configReader) if err != nil &#123; return nil, err &#125; err = yaml.Unmarshal(configContents, &amp;config) if err != nil &#123; return nil, err &#125; // The resource group name may be in different cases from different Azure APIs, hence it is converted to lower here. // See more context at https://github.com/kubernetes/kubernetes/issues/71994. config.ResourceGroup = strings.ToLower(config.ResourceGroup) return &amp;config, nil&#125; error的几种使用方式： 使用error的方式 说明 举例 errors.New 简单静态字符串的错误，没有额外的信息 errors.New(“shell not specified”) fmt.Errorf 用于格式化的错误字符串 fmt.Errorf(“failed to start kubernetes.io/kube-apiserver-client-kubelet certificate controller: %v”, err) 实现Error()方法的自定义类型 客户段需要检测并处理该错误时使用该方式 见下文自定义error Error wrapping Go 1.13支持的特性 errors.New原则： 不要在客户端判断error中的包含字符串信息。 BadGood1234567891011121314151617// package foofunc Open() error &#123; return errors.New("could not open")&#125;// package barfunc use() &#123; if err := foo.Open(); err != nil &#123; if err.Error() == "could not open" &#123; // handle &#125; else &#123; panic("unknown error") &#125; &#125;&#125;1234567891011121314151617// package foovar ErrCouldNotOpen = errors.New("could not open")func Open() error &#123; return ErrCouldNotOpen&#125;// package barif err := foo.Open(); err != nil &#123; if errors.Is(err, foo.ErrCouldNotOpen) &#123; // handle &#125; else &#123; panic("unknown error") &#125;&#125; 当然也可以使用自定义error类型，但此时由于要实现自定义error类型，代码量会增加。 自定义errorerror是个接口，可以用来扩展自定义的错误处理。123456789101112131415161718192021222324252627282930// file: k8s.io/kubernetes/pkg/volume/util/nestedpendingoperations/nestedpendingoperations.go// NewAlreadyExistsError returns a new instance of AlreadyExists error.func NewAlreadyExistsError(operationName string) error &#123; return alreadyExistsError&#123;operationName&#125;&#125;// IsAlreadyExists returns true if an error returned from// NestedPendingOperations indicates a new operation can not be started because// an operation with the same operation name is already executing.func IsAlreadyExists(err error) bool &#123; switch err.(type) &#123; case alreadyExistsError: return true default: return false &#125;&#125;type alreadyExistsError struct &#123; operationName string&#125;var _ error = alreadyExistsError&#123;&#125;func (err alreadyExistsError) Error() string &#123; return fmt.Sprintf( "Failed to create operation with name %q. An operation with that name is already executing.", err.operationName)&#125; 还可以延伸出更复杂一些的树形error体系： 1234567891011121314151617// package nettype Error interface &#123; error Timeout() bool // Is the error a timeout? Temporary() bool // Is the error temporary?&#125;type UnknownNetworkError stringfunc (e UnknownNetworkError) Error() stringfunc (e UnknownNetworkError) Temporary() boolfunc (e UnknownNetworkError) Timeout() bool Error Wrappingerror类型仅包含一个字符串类型的信息，如果函数的调用栈信息为A -&gt; B -&gt; C，如果函数C返回err，在函数A处打印err信息，那么很难判断出err的真正出错位置，不利于快速定位问题。我们期望的效果是在函数A出打印err，能够精确的找到err的源头。为了解决上述问题，需要error类型在函数调用栈之间传递，有如下解决方法： 使用fmt.Errorf()函数来增加额外的信息 使用Error Wrapping golang 1.13支持%w https://github.com/pkg/errors 使用fmt.Errorf()来封装error信息，基于已经存在的error再产生一个新的error类型，需要避免error中包含冗余信息。 BadGood123456// err: failed to call api: connection refuseds, err := store.New()if err != nil &#123; return fmt.Errorf( "failed to create new store: %s", err)&#125;123456// err: call api: connection refuseds, err := store.New()if err != nil &#123; return fmt.Errorf( "new store: %s", err)&#125;12failed to create new store: failed to call api: connection refusederror中会有很多的冗余信息12new store: call api: connection refusederror中没有冗余信息，同时包含了调用栈信息 但使用fmt.Errorf()来全新封装的error信息的缺点也非常明显，丢失了最初的err信息，已经在中间转换为了全新的err。 类型断言类型转换如果类型不正确，会导致程序crash，必须使用类型判断来判断类型的正确性。 BadGood1t := i.(string)1234t, ok := i.(string)if !ok &#123; // handle the error gracefully&#125; panic用于处理运行时的异常情况。使用原则 不要使用panic，在kubernetes项目中几乎没有使用panic的场景 即使使用panic后，一定要使用recover会捕获异常 在测试用例中可以使用panic BadGood12345678910func run(args []string) &#123; if len(args) == 0 &#123; panic("an argument is required") &#125; // ...&#125;func main() &#123; run(os.Args[1:])&#125;1234567891011121314func run(args []string) error &#123; if len(args) == 0 &#123; return errors.New("an argument is required") &#125; // ... return nil&#125;func main() &#123; if err := run(os.Args[1:]); err != nil &#123; fmt.Fprintln(os.Stderr, err) os.Exit(1) &#125;&#125; client-goclient-go利用队列来进行重试https://github.com/kubernetes/client-go/blob/master/examples/workqueue/main.go#L93 kube-builderkube-builder为client-go的更上次封装，本质上跟client-go利用队列来进行重试的机制完全一致。 发生了错误后该如何处理 打印错误日志 根据业务场景选择忽略或者自动重试 程序自己crash 如何避免 在编写代码时增加防御式编程意识，不能靠契约式编程。一个比较简单的判断错误处理情况的方法，看下代码中if语句占用的比例。https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet_volumes.go 需求的评估周期中，不仅要考虑到软件开发完成的时间，同时要考虑到单元测试（单元测试用例的编写需要较长的时间）和集成测试的时间 单元测试覆盖率提升，测试场景要考虑到各种异常场景]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网件R6400V2重新刷回官方系统教程]]></title>
      <url>%2Fpost%2Fr6400-reset%2F</url>
      <content type="text"><![CDATA[之前购买的网件R6400V2路由器刷到了梅林系统，但一直以来信号都特别差，甚至都比不过最便宜的水星路由器，想重新刷回官方系统看下是否是梅林系统的问题。本文记录下重新刷回梅林系统的操作步骤。 刷机之路从如下的地址下载固件。1链接：https://pan.baidu.com/s/1EBvUBlXozo_4zkXaeUMQng 密码：pqqv 在梅林系统的界面上找到固件升级的地方，将下载的固件上传。 结果悲剧的事情发生了，路由器出现了不断重启的状态，并且没有无线信号。猜测可能是因为固件用的是R6400，而非R6400V2导致的。或者是因为是用无线网络升级的原因，导致升级到一半网断掉了，从而导致失败了。 救砖之路按照网件通用救砖，超详细教程文档中的网件通用救砖方法，将路由器的一个LAN口跟电脑的网卡用网线直接相连，并设置网卡的ip地址为192.168.1.3，网关为255.255.255.0。 在路由器启动的状态下，在命令行执行ping -t 192.168.1.1来验证路由器是否可以ping通，如果不通，可能是因为路由器的LAN段不是192.168.1.0，可能是192.168.50.0的。 在windows电脑上下载对应的文件，并保存到本地的F盘下。 手工安装winpcap，主要是给nmrpflash来使用。 以管理员身份运行命令行工具，执行nmrpflash.exe -L后找到本机的网卡net1。执行nmrpflash命令后，立即重启路由器，如果第一次提示Timeout，可以立即执行该命令，如果出现下图的提示，说明命令执行成功。 参考文档 万能网件R6400刷回最近官方固件的方法（不适用于R6400V2） 网件通用救砖，超详细教程 Netgear 网件系列路由器救砖工具]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux网络接口特性]]></title>
      <url>%2Fpost%2Flinux-netcard-feature-md%2F</url>
      <content type="text"><![CDATA[MTUMTU是指一个以太网帧能够携带的最大数据部分的大小，并不包含以太网的头部部分。一般情况下MTU的值为1500字节。 当指定的数据包大小超过MTU值时，ip层会根据当前的mtu值对超过数据包进行分片，并会设置ip层头部的More Fragments标志位，并会设置Fragment offset属性，即分片的第二个以及后续的数据包会增加offset，第一个数据包的offset值为0。接收方会根据ip头部的More Fragment标志位和Fragment offset属性来进行切片的重组。 如果手工将发送方的MTU值设置为较大值，比如9000（巨型帧），如果发送方设置了不分片（ip头部的Don’t fragment），此时如果发送的链路上有地方不支持该MTU，报文就会被丢弃。 offload特性执行 ethtool -k ${device} 可以看到很多跟网络接口相关的特性，这些特性的目的是为了提升网络的收发性能。TSO、UFO和GSO是对应网络发送，LRO、GRO对应网络接收。 执行ethtool -K ${device} gro off/on 来开启或者关闭相关的特性。 LRO(Large Receive Offload)通过将接收的多个tcp segment聚合为一个大的tcp包，然后传送给网络协议栈处理，以减少上层网络协议栈的处理开销。 但由于tcp segment并不是在同一时刻到达网卡，因此组装起来就会变得比较困难。 由于LRO的一些局限性，在最新的网络上，该功能已经删除。 GRO(Generic Receive Offload)GRO是LRO的升级版，正在逐渐取代LRO。运行与内核态，不再依赖于硬件。 参考文章 关于MTU，这里也许有你不知道的地方 常见网络加速技术浅谈]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[技术分享第13期]]></title>
      <url>%2Fpost%2Fknowledge-share-13%2F</url>
      <content type="text"><![CDATA[又是很长的一段时间没有更新，果然又是不定期更新，文章的有些内容也是很久以前积累的，并不是因为太懒，而是确实没有太多的精力。 题图为雨中的望京SOHO，今年全国的雨水特别多，北京亦是如此。南方的鱼米之乡地区出现了严重的洪灾，不知道今年的粮食产量会受多大影响。我们的地球在人类翻天覆地的变更后实在经受不了太多的hack，愿雨季早日过去。 资源1.bocker bocker=bash + docker，其利用100多行bash代码实现的简易版的docker，使用到的底层技术跟docker是一致的，包括chroot、namespace、cgroup。 2.kubectx 天天操作k8s的工程师一定少不了使用kubectl命令，而用kubectl命令的工程师一定会特别烦天天输入-n ${namespace}这样的操作，该工具可以省去输入namespace的操作。刚开始的时候不是太习惯该工具，直到近期才感知到该工具的价值。🤦‍♂️ 3.KubeOperator k8s集群的安装操作基本上都是黑屏来完成的，同时集群规模较大时，还需要一些自动化的手段来解决安装和运维物理机的问题。KubeOperator提供了界面化的操作来完成k8s集群的配置、安装、升级等的操作，底层也是调用了ansible来作为自动化的工具。该项目已经加入CNCF，期望后面可以做的功能更加强大，给k8s集群的运维带来便利。 4.awesome-operators k8s生态的operator非常火爆，作为k8s扩展能力的一个重要组成部分，该项目汇总了常见的operator项目。 5.chaos-mesh pingcap开源的Kubernetes的混沌工程项目，可以使用CRD的方式来注入故障到Kubernetes集群中。 6.devops-exercises DevOps相关的一些面试题，涉及到的方面还是比较全的。 7.shell2http 可以将shell脚本放到业务页面上执行的工具，在web页面上点击按钮后，会执行shell脚本，shell脚本的输出会在web页面上显示。 8.Google Shell 风格指南 Google编程规范还是比较有权威性的，此为Shell的编码规范。 9.shellcheck Shell作为弱类型的编程语言，稍有不慎还是非常容易写错语法的，至少很多的语法我是记不住的，每次都是边查语法边写🤦‍。该项目为Shell的静态检查工具，用来检查可能的语法错误，在Github上的start数量还是非常高的。 不仅支持命令行工具检查，而且还可以跟常用的编辑器集成（比如vim、vscode），用来实现边写边检查的效果。还提供了web界面，可以将shell脚本输入到web界面上来在线检查。 10.teambition 阿里的一款的远程协作工具，类似于国外slack+trello的结合版，在产品设计上能看到太多地方借鉴了trello，非常像是trello的本土化版本，更贴近国人的使用习惯，可用于管理团队和个人的任务。 11.IcePanel IcePanel为vscode的一款插件，提供了k8s一些基础对象的编辑生成器，通过ui的界面即可生成k8s的ConfigMap、Deployment、Service等对象。 Play with Kubernetes 一个提供在线的kubernetes集群的工具，在界面上点一下按钮就可以创建一个k8s集群，不需要注册，非常方便，但创建的集群只有四个小时的使用时间。可以用来熟悉k8s的基本操作，或者试验一些功能。 精彩文章1.腾讯自研业务上云：优化Kubernetes集群负载的技术方案探讨 k8s虽然在服务器的资源利用率上比起传统的物理机或虚拟机部署服务方式有了非常大的提升，本文结合实践经验，从pod、node、hpa等多个维护来优化以便进一步的压榨服务器的资源。 书籍1.[Linux开源网络全栈详解：从DPDK到OpenFlow])(http://product.china-pub.com/8061094) 该书可以作为全面了解开源软件网络的相关技术，涉及到Linux虚拟网络、DPDK、OpenStack、容器相关网络等知识。 2.Kubernetes 网络权威指南：基础、原理与实践 该书可以作为全面了解k8s相关的容器网络的相关技术，如果对k8s周边的虚拟网络知识有所全面了解，该书籍还是比较适合的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[手动更新Custom Resource中的status部分]]></title>
      <url>%2Fpost%2Fupdate-cr-status%2F</url>
      <content type="text"><![CDATA[在k8s中的内置资源很多都有status部分，比如deployment，用来标识当前资源的状态信息。同样在CRD的体系中，也都有status部分。这些status部分信息，是由operator来负责维护的。 如果直接采用kubectl edit的方式来修改status部分信息，会发现是无法直接修改status部分的，因为status是无法修改成功的，因为status部分是CR的一个子资源。 可以通过如下的方式来完成修改 首先要准备一个完整的yaml文件，包含了status部分信息 这个的格式必须为json 1234567891011121314&#123; &quot;apiVersion&quot;: &quot;kuring.me/v1alpha1&quot;, &quot;kind&quot;: &quot;Certificate&quot;, &quot;metadata&quot;: &#123; &quot;name&quot;: &quot;test&quot;, &quot;namespace&quot;: &quot;default&quot; &#125;, &quot;spec&quot;: &#123; &quot;secretName&quot;: &quot;test&quot; &#125;, &quot;status&quot;: &#123; &quot;phase&quot;: &quot;pending&quot; &#125;&#125; 获取系统的TOKEN信息 通常在kube-system下会有admin的ServiceAccount，会有一个对应的Secret来存放该ServiceAccount的token信息。执行kubectl get secret -n kube-system admin-token-r2bvt -o yaml获取到token信息，并其中的token部分进行base64解码。 执行如下的脚本 需要将其中的变量信息修改一下 1234567891011#!/bin/bashobj_file=$1kind=certificatesAPISERVER=https://10.0.0.100:6443namespace=defaultTOKEN=eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1yMmJ2dCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImRiY2IyNzUzLWE5OGMtMTFlYS04NGVjLTAwMTYzZTAwOGU3MCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.tX3jyNh-GEuZQg-hmy7igqh9vpTAz8Jh9uEv-diZ5XWjX9JYhxwD9nxTQvCcvzY7iPIbvxQfW2GHDZISPoopX0vQy9mQ7npVitrOvFovk06plefI5Gxjdft6vdpt-ArsGTpm7-s9G-3aBg5x41h3Cdgyv-W-ypFlCr9dKu9K7BcRIXSq_GQlq5TBmd-LKFXoer4QGwkn7geq5-ziMk_lY21jIGVdIkq9IRiH8NWuCl7l8i6nQESQDUUpMyKDCqkJqUFV8UkrQL7TfqurFP36_TUAQTh2ZAE8nFnrKRoa09BnjT-FoPO6Jnq6COQjk3PGDHV8LKNDAjCCrs0A53IYGwobj=nginx-testecho &quot;begin to patch $obj the file &quot;$&#123;obj_file&#125;curl -XPATCH -H &quot;Accept: application/json&quot; -H &quot;Content-Type: application/merge-patch+json&quot; --header &quot;Authorization: Bearer $TOKEN&quot; --insecure -d @$&#123;obj_file&#125; $APISERVER/apis/kuring.me/v1alpha1/namespaces/$&#123;namespace&#125;/$&#123;kind&#125;/$obj/status]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux Bridge brctl命令]]></title>
      <url>%2Fpost%2Fbrctl%2F</url>
      <content type="text"><![CDATA[查看网桥设备以及端口使用brctl show可以查看本地上的所有的网桥设备以及接到网桥设备上的所有网络设备。 查看网桥设备的mac地址表执行brctl showmacs ${dev}，常用来排查一些包丢在网桥上的场景。其中port no为网桥通过mac地址学习到的某个mac地址所在的网桥端口号。12345678910$ brctl showmacs br0port no mac addr is local? ageing timer 1 02:50:89:59:ac:4b no 3.9669 02:e2:14:78:d7:92 no 0.57 1 0a:1e:01:dc:67:87 no 10.23 1 0a:60:3c:ca:a8:85 no 6.04 1 0e:01:ce:d6:fc:66 no 8.36 1 0e:0c:f8:6c:08:75 no 56.7358 0e:49:85:f6:a1:40 no 1.3022 0e:c0:99:b0:d9:f9 no 0.85 查看网桥设备的某个端口的挂载设备在上文中中可以获取到某个mac地址对应的网桥设备的端口号，要想知道某个网桥设备的端口号对应的设备可以使用brctl showstp ${dev}命令。 123456789101112131415161718192021222324252627brctl showstp br0br0 bridge id 8000.ae90501b5b47 designated root 8000.ae90501b5b47 root port 0 path cost 0 max age 20.00 bridge max age 20.00 hello time 2.00 bridge hello time 2.00 forward delay 15.00 bridge forward delay 15.00 ageing time 300.00 hello timer 0.03 tcn timer 0.00 topology change timer 0.00 gc timer 62.37 flagsbond0.11 (1) port id 8001 state forwarding designated root 8000.ae90501b5b47 path cost 100 designated bridge 8000.ae90501b5b47 message age timer 0.00 designated port 8001 forward delay timer 0.00 designated cost 0 hold timer 0.00 flagsveth02b41ce8 (20) port id 8014 state forwarding designated root 8000.ae90501b5b47 path cost 2 designated bridge 8000.ae90501b5b47 message age timer 0.00 designated port 8014 forward delay timer 0.00 designated cost 0 hold timer 0.00 flags hairpin mode 1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[本地连接远程的内网k8s集群]]></title>
      <url>%2Fpost%2Flocal-connect-remote-k8s%2F</url>
      <content type="text"><![CDATA[在日常开发的过程中，经常会需要在本地开发的程序需要在k8s中调试的场景，比如，写了一个operator。如果此时，本地又没有可以直接可达的k8s集群，比如k8s是在公有云的vpc环境内，外面无法直接访问。此时，ssh又是可以直接通过公网vip访问的vpc的网络内的。为了满足此类需要，可以采用ssh tunnel的方式来打通本地跟远程的k8s集群。 1. 本地建立ssh tunnel到远程集群网络mac用户可以通过SSH TUNNEL这个软件来在界面上自动化配置，具体的配置方式如下： 需要增加一个ssh的动态代理，监听本地的9909端口号。 2. 将远程集群的kubeconfig文件复制到本地将k8s集群中~/.kube/config文件复制到本地的~/.kube/config目录下 3. 在命令行中执行kubectl命令在命令行中配置http和https代理 12export http_proxy=127.0.0.1:9909export https_proxy=127.0.0.1:9909 然后至此就可以通过kubectl命令来访问远程的k8s集群了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[conntrack介绍]]></title>
      <url>%2Fpost%2Fconntrack%2F</url>
      <content type="text"><![CDATA[conntrack是netfilter提供的连接跟踪机制，允许内核识别出数据包属于哪个连接。是iptables实现状态匹配(-m state)以及nat的基础，由单独的内核模块nf_conntrack实现。 conntrack在图中有两处，一处位于prerouting，一处位于output。主机自身进程产生的数据包会经过output链的conntrack，主机的网络设备接收到的数据包会通过prerouting链的conntrack。 每个通过conntrack的数据包，内核会判断是否为新的连接。如果是新的连接，则在连接跟踪表中插入一条记录。如果是已有连接，会更新连接跟踪表中的记录。 需要特别注意的是，conntrack并不会修改数据包，如dnat、snat，而仅仅是维护连接跟踪表。 连接跟踪表的内容/proc/net/nf_conntrack可以看到连接跟踪表的所有内容，通过hash表来实现。 1ipv4 2 tcp 6 117 TIME_WAIT src=10.45.4.124 dst=10.45.8.10 sport=36903 dport=8080 src=10.45.8.10 dst=10.45.4.124 sport=8080 dport=36903 [ASSURED] mark=0 zone=0 use=2 117: 该连接的生存时间，每个连接都有一个timeout值，可以通过内核参数进行设置，如果超过该时间还没有报文到达，该连接将会删除。如果有新的数据到达，该计数会被重置。 TIME_WAIT: 当前该连接的最新状态 涉及到的内核参数 net.netfilter.nf_conntrack_buckets: 用来设置hash表的大小 net.netfilter.nf_conntrack_max: 用来设置连接跟踪表的数据条数上限 iptables与conntrack的关系iptables使用-m state模块来从连接跟踪表查找数据包的状态，上面例子中的TIME_WAIT即为连接跟踪表中的状态，但这些状态对应到iptable中就只有五种状态。特别需要注意的是，这五种状态是跟具体的协议是tcp、udp无关的。 状态 含义 NEW 匹配连接的第一个包 ESTABLISHED NEW状态后，如果对端有回复包，此时连接状态为NEW RELATED 不是太好理解，当已经有一个状态为ESTABLISHED连接后，如果又产生了一个新的连接并且跟此时关联的，那么该连接就是RELATED状态的。对于ftp协议而言，有控制连接和数据连接，控制连接要先建立为ESTABLISHED，数据连接就变为控制连接的RELATED。那么conntrack怎么能够识别到两个连接是有关联的呢，即能够识别出协议相关的内容，这就需要通过扩展模块来完成了，比如ftp就需要nf_conntrack_ftp INVALID 无法识别的或者有状态的数据包 UNTRACKED 匹配带有NOTRACK标签的数据包，raw表可以将数据包标记为NOTRACK，这种数据包的连接状态为NOTRACK conntrack-tools12# 查看连接跟踪表conntrack -L reference 云计算底层技术-netfilter框架研究]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ovs入门]]></title>
      <url>%2Fpost%2Fovs%2F</url>
      <content type="text"><![CDATA[控制器向上提供接口，用来供应用程序调用，此接口成为北向接口；控制器向下调用接口，控制网络设备，此接口成为南向接口。 OpenFlow是控制器和网络设备之间互通的南向协议，OpenvSwitch 用于创建软件的虚拟交换机。 原理 用户态进程 ovsdb：本地数据库，存储ovs的配置信息 vswitchd：ovs-ofctl用来跟该命令通讯，下发流表规则 install OpenvSwitch on CentOS 7123456789101112131415# 安装依赖yum install openssl-devel python-sphinx gcc make python-devel openssl-devel kernel-devel graphviz kernel-debug-devel autoconf automake rpm-build redhat-rpm-config libtool python-twisted-core python-zope-interface PyQt4 desktop-file-utils libcap-ng-devel groff checkpolicy selinux-policy-devel gcc-c++ python-six unbound unbound-devel -ymkdir -p ~/rpmbuild/SOURCES &amp;&amp; cd ~/rpmbuild/SOURCES# 下载ovs源码wget https://www.openvswitch.org/releases/openvswitch-2.12.0.tar.gztar zvxf openvswitch-2.12.0.tar.gz# 构建rpm包rpmbuild -bb --nocheck openvswitch-2.12.0/rhel/openvswitch-fedora.spec# 安装rpm包yum localinstall /root/rpmbuild/RPMS/x86_64/openvswitch-2.12.0-1.el7.x86_64.rpmsystemctl start openvswitch.service 在编译的时候有如下报错： 123 File &quot;/usr/lib64/python2.7/site-packages/jinja2/sandbox.py&quot;, line 22, in &lt;module&gt; from markupsafe import EscapeFormatterImportError: cannot import name EscapeFormatter 是因为markupsafe的版本不对导致的，解决方法为安装合适的版本： 12pip uninstall markupsafepip install markupsafe==0.23 vlan实验123# 创建虚拟交换机ovs_brovs-vsctl add-br ovs_brovs-vsctl add-port ovs_br first_port flow table试验 11234567891011121314151617181920212223242526272829303132# 创建namespaceip netns add ns1ip netns add ns2# 创建veth pair设备ip link add veth1 type veth peer name veth1_brip link add veth2 type veth peer name veth2_br# 设置veth pair设备的namespaceip link set veth1 netns ns1ip link set veth2 netns ns2# 创建OVS网桥ovs-vsctl add-br ovs1# 将veth pair设备另一端绑到网桥ovs1ovs-vsctl add-port ovs1 veth1_brovs-vsctl add-port ovs1 veth2_br# 启动veth pairip netns exec ns1 ip link set veth1 upip netns exec ns2 ip link set veth2 upip link set veth1_br upip link set veth2_br up# 设置veth1和veth2的ip地址ip netns exec ns1 ip addr add 192.168.1.100 dev veth1ip netns exec ns2 ip addr add 192.168.1.200 dev veth2# 配置路由ip netns exec ns1 route add -net 192.168.1.0 netmask 255.255.255.0 dev veth1ip netns exec ns2 route add -net 192.168.1.0 netmask 255.255.255.0 dev veth2 可以使用如下命令来查看刚才的操作： 1234567891011121314151617181920212223242526272829303132# 可以看到刚才创建的网桥$ ovs-vsctl list-brovs1# 查看网桥的端口$ ovs-vsctl list-ports ovs1veth1_brveth2_br# 查看网桥的状态$ ovs-vsctl show4ec35070-a763-4748-878a-c3784b5938a4 Bridge &quot;ovs1&quot; Port &quot;veth1_br&quot; Interface &quot;veth1_br&quot; Port &quot;ovs1&quot; Interface &quot;ovs1&quot; type: internal Port &quot;veth2_br&quot; Interface &quot;veth2_br&quot; ovs_version: &quot;2.12.0&quot;# 查看interface的状态，跟port是一一对应的$ ovs-vsctl list interface veth1_br...mac : []mac_in_use : &quot;32:13:6b:99:91:2b&quot;mtu : 1500mtu_request : []name : &quot;veth1_br&quot;ofport : 1 # ovs port编号... 接下来测试一下网络的连通性是没问题的。 1ip netns exec ns1 ping 192.168.1.200 查看当前流表，可以看到有一条默认的规则，该条规则用来实现交换机的基本动作。 12$ ovs-ofctl dump-flows ovs1 cookie=0x0, duration=1428.955s, table=0, n_packets=22, n_bytes=1676, priority=0 actions=NORMAL 将上述规则删除，再执行ping命令发现已经不通。说明该默认规则会将流量在端口之间进行转发。 1ovs-ofctl del-flows ovs1 新增加如下两条规则，用来表示将port 1的流量转发到port 3，将port 3的流量转发到port 1。其中的1和3分别为port编号，使用ovs-vsctl list interface veth1_br命令中的ofport可以看到。 123456ovs-ofctl add-flow ovs1 &quot;priority=1,in_port=1,actions=output:3&quot;ovs-ofctl add-flow ovs1 &quot;priority=2,in_port=3,actions=output:1&quot;$ ovs-ofctl dump-flows ovs1 cookie=0x0, duration=69.378s, table=0, n_packets=4, n_bytes=280, priority=1,in_port=&quot;veth1_br&quot; actions=output:&quot;veth2_br&quot; cookie=0x0, duration=69.063s, table=0, n_packets=4, n_bytes=280, priority=2,in_port=&quot;veth2_br&quot; actions=output:&quot;veth1_br&quot; 再执行ping命令，发现可以ping通了。 重新增加一条优先级更高的规则，将port 1的数据drop掉。此时再ping发现已经不通了。 1ovs-ofctl add-flow ovs1 &quot;priority=3,in_port=1,actions=drop&quot; 多table接下来清理掉规则，并将规则重新写入到table1中，默认规则是写入到table0中的 123ovs-ofctl del-flows ovs1ovs-ofctl add-flow ovs1 &quot;table=1,priority=1,in_port=1,actions=output:3&quot;ovs-ofctl add-flow ovs1 &quot;table=1,priority=2,in_port=3,actions=output:1&quot; 此时再执行ping命令，发现网络是不通的。因为table0中没有匹配成功，包被drop掉了。 再增加如下规则，即将table 0的规则发送到table 1处理，此时可以ping通。 1ovs-ofctl add-flow ovs1 &quot;table=0,actions=goto_table=1&quot; group table执行ovs-ofctl del-flows ovs1重新清理掉规则，执行下面命令查看group table内容，可以看到内容为空。 12# ovs-ofctl -O OpenFlow13 dump-groups ovs1OFPST_GROUP_DESC reply (OF1.3) (xid=0x2): 执行如下命令，完成数据包从table0 -&gt; group table -&gt; table1的过程，真正数据处理在table1中。 12345678910# 创建一个group table，其作用为将数据包发送到table 1ovs-ofctl add-group ovs1 &quot;group_id=1,type=select,bucket=resubmit(,1)&quot;# 将port 1和3 的数据发往group table 1ovs-ofctl add-flow ovs1 &quot;table=0,in_port=1,actions=group:1&quot;ovs-ofctl add-flow ovs1 &quot;table=0,in_port=3,actions=group:1&quot;# table 1为真正要处理数据的逻辑ovs-ofctl add-flow ovs1 &quot;table=1,priority=1,in_port=1,actions=output:3&quot;ovs-ofctl add-flow ovs1 &quot;table=1,priority=2,in_port=3,actions=output:1&quot; 此时再执行ping命令，发现是可以ping通的。 清理操作123456# 删除网桥ovs-vsctl del-br ovs1ip link delete veth1_brip link delete veth2_brip netns del ns1ip netns del ns2 常用操作 ovs-appctl fdb/show ovs1: 查看mac地址表 ovs-ofctl show ovs1: 可以查看网桥的端口号 ovs-vsctl set bridge ovs1 stp_enable=false: 开启网桥的生成树协议 ovs-appctl ofproto/trace ovs1 in_port=1,dl_dst=7a:42:0a:ca:04:65: 可用来验证一个包到达网桥后的处理流程 reference CentOS 7 安装 Open vSwitch OpenvSwitch初探 - FLOW篇 Open vSwitch (OVS) commands for troubleshooting]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kube-proxy iptables规则分析]]></title>
      <url>%2Fpost%2Fkube-proxy-iptables%2F</url>
      <content type="text"><![CDATA[kube-proxy默认使用iptables规则来做k8s集群内部的负载均衡，本文通过例子来分析创建的iptabels规则。 主要的自定义链涉及到： KUBE-SERVICES： 访问集群内服务的CLusterIP数据包入口，根据匹配到的目标ip+port将数据包分发到相应的KUBE-SVC-xxx链上。一个Service对应一条规则。由OUTPUT链调用。 KUBE-NODEPORTS: 用来匹配nodeport端口号，并将规则转发到KUBE-SVC-xxx。一个NodePort类型的Service一条。在KUBE-SERVICES链的最后被调用 KUBE-SVC-xxx：相当于是负载均衡，将流量利用random模块均分到KUBE-SEP-xxx链上。 KUBE-SEP-xxx：通过dnat规则将连接的目的地址和端口号做dnat，从Service的ClusterIP或者NodePort转换为后端的pod ip KUBE-MARK-MASQ: 使用mark命令，对数据包设置标记0x4000/0x4000。在KUBE-POSTROUTING链上有MARK标记的数据包进行一次MASQUERADE，即SNAT，会用节点ip替换源ip地址。 环境准备创建nginx deployment 123456789101112131415161718192021222324252627apiVersion: apps/v1kind: Deploymentmetadata: labels: app: nginx-svc version: nginx name: nginx namespace: defaultspec: replicas: 2 selector: matchLabels: app: nginx-svc template: metadata: labels: app: nginx-svc version: nginx spec: containers: - image: 'nginx:1.9.0' name: nginx ports: - containerPort: 443 protocol: TCP - containerPort: 80 protocol: TCP 创建service对象 12345678910111213141516apiVersion: v1kind: Servicemetadata: name: nginx-svc namespace: defaultspec: ports: - name: '80' port: 8000 protocol: TCP targetPort: 80 nodePort: 30080 selector: app: nginx-svc sessionAffinity: None type: NodePort 环境信息如下： 容器网段：172.20.0.0/16 Service ClusterIP cidr: 192.168.0.0/20 k8s版本： 提交后创建出来的信息如下： Service ClusterIP：192.168.103.148 nginx pod的两个ip地址：172.16.3.3 172.16.4.4 从宿主机上访问ClusterIP从本机请求ClusterIP的数据包会经过iptables的链：OUTPUT -&gt; POSTROUTING 要想详细知道iptabels的执行情况，可以通过iptables的trace功能。如何开启trace功能可以参考：http://kuring.me/post/iptables/。 执行 iptables -nvL OUTPUT -t nat 可以看到如下的iptables规则命令 12pkts bytes target prot opt in out source destination 17M 1150M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ 执行 iptables -nvL KUBE-SERVICES -t nat 可以查看自定义链的具体内容，里面包含了多条规则，其中跟当前Service相关的规则如下。 12pkts bytes target prot opt in out source destination1 60 KUBE-SVC-Y5VDFIEGM3DY2PZE tcp -- * * 0.0.0.0/0 192.168.103.148 /* default/nginx-svc:80 cluster IP */ tcp dpt:8000 执行 iptables -nvL KUBE-SVC-Y5VDFIEGM3DY2PZE -t nat 查看自定义链的具体规则 123pkts bytes target prot opt in out source destination0 0 KUBE-SEP-IFV44I3EMZAL3LH3 all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ statistic mode random probability 0.500000000001 60 KUBE-SEP-6PNQETFAD2JPG53P all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ 上述规则会按照特定的概率将流量均等的执行自定义链的规则，两个自定义的链的规则跟endpoint相关，执行 iptables -nvL KUBE-SEP-IFV44I3EMZAL3LH3 -t nat可查看endpoint级别的iptabels规则。dnat操作会修改数据包的目的地址和端口，从clusterip+service port修改为访问pod ip+pod端口。 123pkts bytes target prot opt in out source destination0 0 KUBE-MARK-MASQ all -- * * 172.16.3.3 0.0.0.0/0 /* default/nginx-svc:80 */0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp to:172.16.3.3:80 会在dnat操作之前为对数据包执行打标签操作。KUBE-MARK-MASQ 自定义链为对数据包打标记的自定义规则，执行 iptables -nvL KUBE-MARK-MASQ -t nat 12pkts bytes target prot opt in out source destination 1 60 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK or 0x4000 接下来看一下POSTROUTING链上的规则，iptables -nvL POSTROUTING -t nat。 12pkts bytes target prot opt in out source destination 205K 13M KUBE-POSTROUTING all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes postrouting rules */ 继续看一下KUBE-POSTROUTING链的内容，iptables -nvL KUBE-POSTROUTING -t nat，其中最后一条的MASQUERADE指令的操作实际上为SNAT操作。 12345Chain KUBE-POSTROUTING (1 references)pkts bytes target prot opt in out source destination 6499 398K RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 mark match ! 0x4000/0x4000 1 60 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK xor 0x4000 1 60 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service traffic requiring SNAT */ 即从本机访问service clusterip的数据包，在output链上经过了dnat操作，在postrouting链上经过了snat操作后，最终会发往目标pod。pod在处理完请求后，回的数据包最终会经过nat的逆过程返回到本机。 外部访问nodeport从外部访问本机的nodeport数据包会经过iptables的链：PREROUTING -&gt; FORWARD -&gt; POSTROUTING nodeport都是被外部访问的情况，入口位于PREROUTING链上。执行 iptables -nvL PREROUTING -t nat： 12pkts bytes target prot opt in out source destination 349K 21M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ 在KUBE-SERVICES链的最后一条规则为跳转到KUBE-NODEPORTS链 14079 246K KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL 执行iptables -nvL KUBE-NODEPORTS -t nat， 查看KUBE-NODEPORTS链123pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp dpt:300800 0 KUBE-SVC-Y5VDFIEGM3DY2PZE tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp dpt:30080 其中KUBE-MARK-MASQ链只有一条规则，即打上0x4000的标签。 12pkts bytes target prot opt in out source destination 0 0 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK or 0x4000 自定义链KUBE-SVC-Y5VDFIEGM3DY2PZE的内容如下，跟clusterip的规则是重叠的：123pkts bytes target prot opt in out source destination 0 0 KUBE-SEP-IFV44I3EMZAL3LH3 all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ statistic mode random probability 0.500000000000 0 KUBE-SEP-6PNQETFAD2JPG53P all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ KUBE-SEP-IFV44I3EMZAL3LH3的内容为，会经过一次DNAT操作:123pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 172.16.3.3 0.0.0.0/0 /* default/nginx-svc:80 */0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/nginx-svc:80 */ tcp to:172.16.3.3:80 在经过了PREROUTING链后，接下来会判断目的ip地址不是本机的ip地址，接下来会经过FORWARD链。在FORWARD链上，仅做了一件事情，就是将前面大了0x4000的数据包允许转发。 1234pkts bytes target prot opt in out source destination 0 0 KUBE-FORWARD all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding rules */0 0 KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate NEW /* kubernetes service portals */0 0 KUBE-EXTERNAL-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate NEW /* kubernetes externally-visible service portals */ KUBE-FORWARD的内容如下： 12345pkts bytes target prot opt in out source destination 0 0 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 ctstate INVALID0 0 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding rules */ mark match 0x4000/0x40000 0 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding conntrack pod source rule */ ctstate RELATED,ESTABLISHED0 0 ACCEPT all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes forwarding conntrack pod destination rule */ ctstate RELATED,ESTABLISHED 跟clusterip一样，会在POSTROUTING阶段匹配mark为0x4000/0x4000的数据包，并进行一次MASQUERADE转换，将ip包替换为宿主上的ip地址。 加入这里不做MASQUERADE，流量发到目的的pod后，pod回包时目的地址为发起端的源地址，而发起端的源地址很可能是在k8s集群外部的，此时pod发回的包是不能回到发起端的。NodePort跟ClusterIP的最大不同就是NodePort的发起端很可能是在集群外部的，从而这里必须做一层SNAT转换。 在上述分析中，访问NodePort类型的Service会经过snat，从而服务端的pod不能获取到正确的客户端ip。可以设置Service的spec.externalTrafficPolicy为Local，此时iptables规则只会将ip包转发给运行在这台宿主机上的pod，而不需要经过snat。pod回包时，直接回复源ip地址即可，此时源ip地址是可达的，因为源ip地址跟宿主机是可达的。如果所在的宿主机上没有pod，那么此时流量就不可以转发，此为限制。 使用LoadBalancer类型访问的情况externalTrafficPolicy为local123456789101112131415-A KUBE-SERVICES -d 10.149.30.186/32 -p tcp -m comment --comment &quot;acs-system/nginx-ingress-lb-cloudbiz:http loadbalancer IP&quot; -m tcp --dport 80 -j KUBE-FW-76HLDRT5IPNSMPF5-A KUBE-FW-76HLDRT5IPNSMPF5 -m comment --comment &quot;acs-system/nginx-ingress-lb-cloudbiz:http loadbalancer IP&quot; -j KUBE-XLB-76HLDRT5IPNSMPF5-A KUBE-FW-76HLDRT5IPNSMPF5 -m comment --comment &quot;acs-system/nginx-ingress-lb-cloudbiz:http loadbalancer IP&quot; -j KUBE-MARK-DROP# 10.149.112.0/23为pod网段-A KUBE-XLB-76HLDRT5IPNSMPF5 -s 10.149.112.0/23 -m comment --comment &quot;Redirect pods trying to reach external loadbalancer VIP to clusterIP&quot; -j KUBE-SVC-76HLDRT5IPNSMPF5-A KUBE-XLB-76HLDRT5IPNSMPF5 -m comment --comment &quot;Balancing rule 0 for acs-system/nginx-ingress-lb-cloudbiz:http&quot; -j KUBE-SEP-XZXLBWOKJBSJBGVU-A KUBE-SVC-76HLDRT5IPNSMPF5 -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-XZXLBWOKJBSJBGVU-A KUBE-SVC-76HLDRT5IPNSMPF5 -j KUBE-SEP-GP4UCOZEF3X7PGLR-A KUBE-SEP-XZXLBWOKJBSJBGVU -s 10.149.112.45/32 -j KUBE-MARK-MASQ-A KUBE-SEP-XZXLBWOKJBSJBGVU -p tcp -m tcp -j DNAT --to-destination 10.149.112.45:80-A KUBE-SEP-GP4UCOZEF3X7PGLR -s 10.149.112.46/32 -j KUBE-MARK-MASQ-A KUBE-SEP-GP4UCOZEF3X7PGLR -p tcp -m tcp -j DNAT --to-destination 10.149.112.46:80 缺点 iptables规则特别乱，一旦出现问题非常难以排查 由于iptables规则是串行执行，算法复杂度为O(n)，一旦iptables规则多了后，性能将非常差。 iptables规则提供的负载均衡功能非常有限，不支持较为复杂的负载均衡算法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker bridge network]]></title>
      <url>%2Fpost%2Fdocker-bridge-network%2F</url>
      <content type="text"><![CDATA[docker bridge是默认的网络模式 容器访问外网 默认情况下，容器即可访问外网。 启动一个容器：docker run -d nginx 容器中访问外网的请求如www.baidu.com，内核协议栈根据路由信息，会选择默认路由，将请求发送到容器中的eth0网卡，目的mac地址为网关172.17.0.1的mac地址。 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0 eth0网卡接收到数据包后，会将数据包转发到veth pair的另外一端，即宿主机网络中的veth6b173fd设备。 veth6b173fd设备是挂在网桥上的，会将数据包转发到网桥br0，br0即为网关172.17.0.1。 br0接收到数据包后，会将数据包转发给内核协议栈。 宿主机上的/proc/sys/net/ipv4/ip_forward为1，表示转发功能开启，即目的ip不是本机的会根据路由规则进行转发，而不是丢弃。 仅在宿主机上开启了ip_forward后，包即使转发了，还是无法回来的，因为包中的源ip地址为172.17.0.1，是私有网段的ip地址。需要做一次SNAT才可以，docker会在iptabels的nat表中的postrouting链中增加SNAT规则，下面规则的意思是源地址为172.17.0.0/16的会做一次地址伪装，即SNAT。 12345# iptables -nL -t natChain POSTROUTING (policy ACCEPT)target prot opt source destinationMASQUERADE all -- 172.17.0.0/16 0.0.0.0/0 在eth0网卡上抓包，可以发现源ip已经是eth0的网卡ip地址。 端口映射命令格式：-p ${host_port}:${container_port} 启动一个容器：docker run -d -p 8080:80 nginx 查看本地的iptables规则 12345678910111213141516171819202122$ iptables -nL -t natChain PREROUTING (policy ACCEPT)target prot opt source destinationDOCKER all -- 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT)target prot opt source destinationChain OUTPUT (policy ACCEPT)target prot opt source destinationDOCKER all -- 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCALChain POSTROUTING (policy ACCEPT)target prot opt source destinationMASQUERADE all -- 172.17.0.0/16 0.0.0.0/0MASQUERADE tcp -- 172.17.0.2 172.17.0.2 tcp dpt:80Chain DOCKER (2 references)target prot opt source destinationRETURN all -- 0.0.0.0/0 0.0.0.0/0## prerouting链引用，外面发往本机的8080端口的数据包，会dnat为172.17.0.2:80DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 to:172.17.0.2:80 丢包问题SNAT在并发比较高的情况下，会存在少量的丢包现象，具体原因跟conntrack模式的实现有关。conntrack在SNAT端口的分配和插入conntrack表之间有个延时，如果在这中间存在冲突的话会导致插入失败，从而出现丢包的问题。 该问题没有根治的解决办法，能大大缓解的解决办法为使用iptabels的–random-fully选项，SNAT选择端口为随机，大大降低出现冲突的概率。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux IPIP隧道协议]]></title>
      <url>%2Fpost%2Fipip%2F</url>
      <content type="text"><![CDATA[ipip协议为在ip协议报文的基础上继续封装ip报文，基于tun设备实现，是一种点对点的通讯技术。 installipip需要内核模块ipip的支持 12345$ modprobe ipip$ lsmod | grep ipipipip 13465 0tunnel4 13252 1 ipipip_tunnel 25163 1 ipip 实战 两台主机：172.16.5.126(host1)和172.16.5.127(host2) 在host1上创建tun1设备，执行如下命令： 12345678910111213141516171819202122# 用来创建tun1设备，并ipip协议的外层ip，目的ip为172.16.5.127， 源ip为172.16.5.126ip tunnel add tun1 mode ipip remote 172.16.5.127 local 172.16.5.126# 给tun1设备增加ip地址，并设置tun1设备的对端ip地址为10.10.200.10ip addr add 10.10.100.10 peer 10.10.200.10 dev tun1ip link set tun1 up$ ifconfig tun1tun1: flags=209&lt;UP,POINTOPOINT,RUNNING,NOARP&gt; mtu 1480 inet 10.10.100.10 netmask 255.255.255.255 destination 10.10.200.10 tunnel txqueuelen 1000 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 增加一条路由，所有到达10.10.200.10的请求会经过设备tun1$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface0.0.0.0 172.16.7.253 0.0.0.0 UG 0 0 0 eth010.10.200.10 0.0.0.0 255.255.255.255 UH 0 0 0 tun1172.16.0.0 0.0.0.0 255.255.248.0 U 0 0 0 eth0 同样在host2上创建tun1设备： 123ip tunnel add tun1 mode ipip remote 172.16.5.126 local 172.16.5.127ip addr add 10.10.200.10 peer 10.10.100.10 dev tun1ip link set tun1 up 并分别在host1和host2上打开ip_forward功能 1echo 1 &gt; /proc/sys/net/ipv4/ip_forward 然后在host1上ping 10.10.200.10，可以ping通。 在host1的tun1上抓包，可以看到正常的ping包。 在host1的eth1上抓包，可以看到已经是ipip的数据包了。 tun1.pcap 清理现场分别在两台主机上执行 1ip link delete tun0 ref 什么是 IP 隧道，Linux 怎么实现隧道通信？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux中的tap/tun设备]]></title>
      <url>%2Fpost%2Ftap-tun%2F</url>
      <content type="text"><![CDATA[Linux虚拟网络设备tap/tuntap/tun常用于隧道通讯，通过一个字符设备来实现用户态和内核态的通讯，字符设备一端连接着用户空间，一端连接着内核空间。 与物理网卡的最大不同是，tap/tun的数据源来自于用户态的程序，而物理网卡的数据源来自于物理链路。 对应的字符设备文件位置： tap: /dev/tap0 tun: /dev/net/tun 当应用程序打开字符设备文件时，驱动程序会创建并注册相应的虚拟设备接口，以tunX或tapX命名。应用程序关闭设备文件时，驱动程序会删除tunX和tapX网络虚拟设备，并删除建立起来的路由信息。 两个设备的不同点： tap是一个二层网络设备，只能处理二层的以太网帧，可以与物理网卡做桥接 tun是一个点对点的三层网络设备，只能处理处理三层的IP数据包，无法与物理网卡做桥接，可以通过三层交换方式与物理网卡连通。Linux下的隧道协议基于该tun设备实现，如ipip、gre。 1234567891011121314151617181920212223242526 ┌──────────────┐ │ │ │ APP │ │ │ └───────┬──────┘ │ │ │ │ │ │ ┌────────────▼──────────┐ │ │─ ─ ─ ─ ─ ─│ /dev/net/tun ├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ └────────────┬──────────┘ │ │ │ │ │ ┌───────▼──────┐ ┌──────────────┐ │ │ │ │ │ tunX ├────────────────▶│Network Stack │ │ │ │ │ └──────────────┘ └──────────────┘ tun设备应用举例123456789101112131415161718192021222324252627282930313233 ┌──────────────┐ ┌──────────────┐ │ │ │ │ │ APP A │ │ APP B │◀┐ │ │ │ │ │ └───────┬──────┘ └───────┬──────┘ │ │ │ │ │ │ │ 1│ │ │ │ 5│ │ │ │ │ ─ ─ ─ ─ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─│─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ │ │ │ │ │ 4│ │ │ │ ┌────────────▼────────────────────────▼─────┐ │ │ │ │ │ Network Stack │ │ │ │ │ └────────────┬───────────────────────┬──────┘ │ │ │ │ 6│ 3│ │ │ │ │ ┌───────▼──────┐ ┌─▼─────────┴──┐ │ │ │ │10.1.1.11 │ eth0 │ │ tun0 │ 192.168.1.11 │ │ │ │ └───────┬──────┘ └──────────────┘ │ 7│ │ ▼ 10.1.1.100 / 192.168.1.100 应用程序A要发送数据到其他物理机192.168.1.100，由于物理网络环境下只有10.1.1.11和10.1.1.100是相互连通的，192.168.1.11和192.168.1.100是不通的，为了192.168.1.11和192.168.1.100能够进行通讯，需要将数据包进行一次封装。 应用程序B是通过打开字符设备文件/dev/net/tun0的方式来打开网络设备 流程如下： A构造数据包，目的ip为192.168.1.100，并发送给协议栈 协议栈根据数据包中的ip地址，匹配路由规则，要从tun0出去 内核协议栈将数据包发送给tun0网络设备 tun0发送应用程序B打开，于是将数据发送给应用程序B B收到数据包后，在用户态构造一个新的数据包，源IP为eth0的IP 10.1.1.11，目的IP为配置的对端10.1.1.100，并封装原来的数据包 协议栈根据当前数据包的IP地址选择路由，将数据包发送给eth0 reference 详解云计算网络底层技术——虚拟网络设备 tap/tun 原理解析 Linux 网络工具详解之 ip tuntap 和 tunctl 创建 tap/tun 设备 Linux虚拟网络设备之tun/tap]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下网卡混杂模式]]></title>
      <url>%2Fpost%2Fif-promiscuous%2F</url>
      <content type="text"><![CDATA[网卡的混杂模式是指网卡将其接收的所有流量都交给cpu。非混杂模式下，网卡仅接收目的mac地址是自己mac地址的单播，以及多播和广播包，可以看出混杂模式是工作在二层的。 通过ifconfig eth0的方式，如果输出中包含了PROMISC字段，说明网卡处于混杂模式。但是如果ifconfig 命令的输出中未包含PROMISC字段，并不能说明网卡处于非混杂模式下。 可以通过查看 cat /sys/class/net/bond0/flags 的输出得知，如果置位了0x100，说明处于混杂模式。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux中的veth pair设备]]></title>
      <url>%2Fpost%2Fveth-pair%2F</url>
      <content type="text"><![CDATA[veth pair是一对虚拟的网络设备，两个网络设备彼此连接。常用于两个network namespace之间的连接，如果在同一个命名空间下有很多的限制。 12345678910111213141516171819202122232425┌──────────────────────────────────────────────────────────────────────────────┐│ ││ ││ network protocol ││ ││ │└────────────────────▲─────────────────────────▲──────────────────────▲────────┘ │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ ┌─────▼────┐ ┌─────▼────┐ ┌─────▼────┐ │ │ │ │ │ │ │ eth0 │ │ veth0 ◀───────────▶ veth1 │ │ │ │ │ │ │ └─────▲────┘ └──────────┘ └──────────┘ │ │ │ │ │ ▼ physical network 实战veth设备的ping测试1. 只给一个veth设备配置ip的情况测试给veth0配置ip 192.168.100.10，可以看到主机的路由表中增加了目的地为192.168.100.0的记录 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@localhost vagrant]# ip link add veth0 type veth peer name veth1[root@localhost vagrant]# ip addr add 192.168.100.10/24 dev veth0[root@localhost vagrant]# ip addr add 192.168.100.11/24 dev veth1## 因为veth创建完后默认不启用，此时还没有路由[root@localhost vagrant]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault gateway 0.0.0.0 UG 100 0 0 eth010.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0192.168.33.0 0.0.0.0 255.255.255.0 U 101 0 0 eth1## 启用veth0后增加路由[root@localhost vagrant]# ip link set veth0 up[root@localhost vagrant]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault gateway 0.0.0.0 UG 100 0 0 eth010.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0192.168.33.0 0.0.0.0 255.255.255.0 U 101 0 0 eth1192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0## 启用veth1后居然又增加了一条路由信息[root@localhost vagrant]# ip link set veth1 up[root@localhost vagrant]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 52:54:00:26:10:60 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global noprefixroute dynamic eth0 valid_lft 86214sec preferred_lft 86214sec inet6 fe80::5054:ff:fe26:1060/64 scope link valid_lft forever preferred_lft forever3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 08:00:27:98:06:20 brd ff:ff:ff:ff:ff:ff inet 192.168.33.11/24 brd 192.168.33.255 scope global noprefixroute eth1 valid_lft forever preferred_lft forever inet6 fe80::a00:27ff:fe98:620/64 scope link valid_lft forever preferred_lft forever4: veth1@veth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether e2:15:95:0a:1f:da brd ff:ff:ff:ff:ff:ff inet 192.168.100.11/24 scope global veth1 valid_lft forever preferred_lft forever inet6 fe80::e015:95ff:fe0a:1fda/64 scope link valid_lft forever preferred_lft forever5: veth0@veth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether b2:2c:f6:e4:74:c5 brd ff:ff:ff:ff:ff:ff inet 192.168.100.10/24 scope global veth0 valid_lft forever preferred_lft forever inet6 fe80::b02c:f6ff:fee4:74c5/64 scope link valid_lft forever preferred_lft forever[root@localhost vagrant]# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault gateway 0.0.0.0 UG 100 0 0 eth010.0.2.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0192.168.33.0 0.0.0.0 255.255.255.0 U 101 0 0 eth1192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0192.168.100.0 0.0.0.0 255.255.255.0 U 0 0 0 veth1 默认情况下arp表如下： 123456# arpAddress HWtype HWaddress Flags Mask Ifacelocalhost.localdomain (incomplete) veth0192.168.33.1 ether 0a:00:27:00:00:00 C eth1gateway ether 52:54:00:12:35:02 C eth010.0.2.3 ether 52:54:00:12:35:03 C eth0 使用ping命令ping -I veth0 192.168.100.11 -c 2，默认情况下veth1和veth0会接收到arp报文，但并没有arp的响应报文。这是因为默认情况下有些arp内核参数的限制。执行如下命令解决arp的限制。 12345echo 1 &gt; /proc/sys/net/ipv4/conf/veth1/accept_localecho 1 &gt; /proc/sys/net/ipv4/conf/veth0/accept_localecho 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filterecho 0 &gt; /proc/sys/net/ipv4/conf/veth0/rp_filterecho 0 &gt; /proc/sys/net/ipv4/conf/veth1/rp_filter veth pair设备的删除12# 删除veth0后会自动删除veth1$ ip link delete veth0 container与host veth pair的关系veth pair的其中一个设备位于container中备位于container中，另外一个设备位于host network namespace中，如何知道container中的eth0和host network namesapce中的veth设备的对应关系呢？ 原理为veth pair设备都有一个ifindex和iflink值，，容器中的eth0设备的ifindex值跟host network namespace中的对应veth pair设备的iflink值相等，反之亦然。 在容器中找到eth0的iflink方法一 获取iflink值：cat /sys/class/net/eth0/iflink 也可用此方法获取ifindex值：cat /sys/class/net/eth0/ifindex 方法二 123$ ip link show eth03: eth0@if18: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1500 qdisc noqueue state UP link/ether 96:5f:80:a3:a3:01 brd ff:ff:ff:ff:ff:ff 其中的3为eth0的ifindex。18为eth0的iflink，即对应的veth pair的另外一个设备的ifindex。 host network namespace中找到对应ifindex值的veth pair设备12345$ ip addr 18: veth0e09999e@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master br0 state UP group default link/ether de:b0:74:89:e8:3e brd ff:ff:ff:ff:ff:ff link-netnsid 4 inet6 fe80::dcb0:74ff:fe89:e83e/64 scope link valid_lft forever preferred_lft forever 其中的18为ifindex，3为对应的veth pair的ifindex。 reference Linux 虚拟网络设备 veth-pair 详解，看这一篇就够了 Linux虚拟网络设备之veth]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[技术分享第12期]]></title>
      <url>%2Fpost%2Fknowledge-share-12%2F</url>
      <content type="text"><![CDATA[距离上次的知识分享系列已经过去了半年之久，难以想象。在该系列的开始我就说过该系列是不定期的，果不食言，只是这次的不定期有些久😓。该系列会继续下去，但节奏仍然是不定期的，但应该不会间隔半年之巨了。 题图来自首钢工业遗址公园，首钢于2010年完成了搬迁到唐山市的工作，位于石景山的工厂被废弃。2019年国庆节前夕以公园的形式部分对外开放，跟普通公园不同的地方在于保留了很多之前的工厂建筑及大型生产机械，足够硬核，非常原汁原味。 2022年要举办的冬奥会也非常明智的将场地选择在了该公园，预计将来会有很多的场馆位于该公园内且对外开放。 国内很多的地市都有一些类似的建筑，比如90年代下岗潮之前的一些国企工厂，我知道的济南的机床厂就有很多个，但很多这些倒闭关门的工厂后来的建筑及地皮都给卖掉了，建筑物也直接给拆掉了，殊不知其衍生价值也是有不少的。位于北京酒仙桥的798就是个非常好的改造案例，798园区的建筑稍加改造，给很多艺术工作室提供了非常好的办公场地，也是城市中的一个亮眼的名片。 资源1.MessagePack json作为一种常见的数据序列化方案，存在占用空间过多、反序列化过于消耗cpu的问题。MessagePack是一种基于二进制的高效轻量的数据序列化方案，支持数据的压缩，支持丰富的编程语言。在上图中，可以看出原27字节的json数据转换为MessagePack后仅占用了18字节。 另外，据今日头条的压测，要比Thrift的二进制序列化方案更高效一些。 2.GoAst Viewer Golang中的ast、parser、token包可用来对golang的源码进行语法分析，并构建出AST树。GoAst Viewer支持在线输入Golang源码来构建AST树。 3.KubeEdge IoT目前正在大力发展，边缘设备的计算能力在逐渐增强，同时处理的数据量的需求正在快速增加，而数据中心的数据处理能力、网络带宽、扩展能力并没有太多的增强，未来势必会将部分计算能力下放到边缘设备，以降低数据中心的成本。 华为开源的KubeEdge为基于Kubernetes的边缘计算平台，支持边缘集群的编排和管理。 4.Mycat 国内开源的关系型数据库中间件，支持MySQL、Oracle等常见的关系型数据库。关系型数据库单表过大导致性能下降后的解决思路往往是分库分表，分库分表后需要增加中间件层来解决多个数据库多张表的数据增删改查问题，而Mycat是一个不错的解决方案。 当然Mycat的功能不仅限于此，比如支持跨库的两张表join、Mycat-eye可以来监控Mycat等，更多功能请参照官方网站。 5.WeChat Format 一款转换markdown格式的文档为微信公众号排版的工具，排版比较精美，推荐一试。 6.GitNote 一款基于git的笔记管理软件，可以将笔记存放到Github中，支持多种图床插件，支持多个平台（还没有移动端），支持富文本编辑和Markdown编辑。由于笔记是可以同步Github上的，可以做到永久保存和版本控制，而且笔记的存放目录和格式不受该工具的影响，可以说是完全没有侵入性，脱离了该工具仍然可以通过直接编辑git项目的方式来发布笔记。 7.Vlang开源啦 V语言宣布开源，从V语言的特性上看到了很多Golang的影子，并未看到耳目一新的特性。 8.krew 一款kubectl的plunin管理工具，mac平台下有brew包管理工具，随着kubectl的plugin机制的成熟，plugin管理工具应运而生。 9.cert-manager 运行在k8s上的证书管理工具，可以签发证书，基于CRD实现。 Let’s Encrypt提供了免费的tls证书，但证书有有效期限制，过期后需要手工重新申请证书，cert-manager可以做到从Let’s Encrypt自动申请证书，并过期后重新申请证书。 10.cmatrix 一款黑客帝国效果的命令行工具，除了炫酷也没啥其他用途了。 11.boxes boxes为一款有趣的命令行工具，可以显示很多神奇效果。 1234567891011121314 __ _,--=&quot;=--,_ __ / \.&quot; .-. &quot;./ \ / ,/ _ : : _ \/` \ \ `| /o\ :_: /o\ |\__/ `-&apos;| :=&quot;~` _ `~&quot;=: | \` (_) `/ .-&quot;-. \ | / .-&quot;-..---&#123; &#125;--| /,.-&apos;-.,\ |--&#123; &#125;---. ) (_)_)_) \_/`~-===-~`\_/ (_(_(_) (( Different all twisty a ) ) of in maze are you, (( passages little. ) ) (&apos;---------------------------------------&apos; 12.rally 一款elasticsearch的压测工具。 13.tekton Google开源的一款基于Kubernetes的应用发布框架，Google在云原生生态中出品一般质量都比较高，主要用来做CI/CD。 14.kubectl-debug 一款基于kubectl插件的debug工具，基础镜像使用nicolaka/netshoot(内置了大量的网络排查工具)，可用于kubernetes集群中快速定位问题。值得一提的是，该工具的初版是作者在参加pingcap面试时的小作业。 15.Monocular Rancher出品的一款基于管理helm chart的ui工具。 16.gitmoji github上的开源项目中经常会看到一些git commit message中包含了moji表情，而且有越来越多的趋势，这些moji表情不紧紧是好玩，而且还非常生动形象的表达了commit message的含义，并且非常醒目，但这些moji表情可不应该是滥用的。该网站记录了一些常用的moji表情在git commit中的含义。 精彩文章1.Monitoring and Tuning the Linux Networking Stack: Receiving Data 本文讲解了一个数据包到达网卡后是怎么一步步从网卡 -&gt; 操作系统 -&gt; 应用程序，并讲解了Linux中的实现方式。 绝大多数的工程师对于这一块的知识是较为模糊的，建议一读。 视频https://mp.weixin.qq.com/s?__biz=MzU3OTc1Njk4MQ==&amp;mid=2247486851&amp;idx=1&amp;sn=d0322f6d1a59c21e977488d9701d0476&amp;chksm=fd607b59ca17f24fb07f60b8f488e602ac7e75302c999c206938682387e1d3cac44feb67b208&amp;mpshare=1&amp;scene=1&amp;srcid=%23rd 半年多前比较火的视频，但我还是经常会想起来，给大家重温一下。 视频中为杭州一小伙深夜骑车逆行被交警拦下后情绪崩溃，失声痛哭。小伙每晚加班到十一二点，一方面女朋友在催着给送药匙，另一方面公司还在催着赶回公司，再加上被交警拦下，最终来自三方面的催促导致积压在小伙内心长久以来的压力爆发而情绪失控。隔着屏幕都能感受到小伙长期以来的压力，我猜想如果给他一些自由的时间，他一定会选择独自一人到一个安静的地方过上一段时间与世隔绝的生活。 生活本不易，在觉大多数的成年人生活中没有简单二字，祝愿各位生活如意！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下debug内核coredump]]></title>
      <url>%2Fpost%2Flinux-debug-crash%2F</url>
      <content type="text"><![CDATA[Linux内核会存在一些严重的bug，导致内核crash，会在/var/crash目录下产生类似”127.0.0.1-2019-09-30-21:33:38“这种的文件夹，里面包含了vmcore文件，该文件对于debug 内核crash的原因非常有帮助。 本文在CentOS 7下操作。 执行yum install crash来安装crash 另外还需要两个rpm包：kernel-debuginfo-3.10.0-957.el7.x86_64.rpm 和 kernel-debuginfo-common-x86_64-3.10.0-957.el7.x86_64.rpm，需要关注下操作系统的内核版本，这两个rpm包可以通过搜索引擎找到。 下到包后即可执行rpm -ivh *.rpm的方式来安装rpm包。 在机器上执行crash /usr/lib/debug/lib/modules/3.10.0-957.el7.x86_64/vmlinux /var/crash/xx/vmcore进行debug，可以输入bt命令来查看栈信息。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[go mod使用]]></title>
      <url>%2Fpost%2Fgo-mod%2F</url>
      <content type="text"><![CDATA[go mod从1.11开始已经成为了go的默认包管理工具，本文记录go mod的一些使用经验。 要想使用go mod，需要将go升级到1.11或者更高版本。 在没有go mod之前，项目源码必须是放在GOPATH目录下的，有了go mod之后项目即可以放在GOPATH目录下，也可以放在非GOPATH的目录下，在GOPATH目录下在执行时需要指定环境变量GO111MODULE=on,具体的写法可以是GO111MODULE=on go mod init 由于众所周知的原因，go的包相对还是比较难下载的，很多情况下还是需要vendor目录存在的，并将vendor目录中的包一并提交到代码库中。可以使用go mod vendor命令来完成，执行该命令后会将本地下载的包copy到vendor目录下。 坑1 提示unknown revision1234# GO111MODULE=on go get gitlab.aa-inc.com/bb@v2go: finding gitlab.aa-inc.com/bb v2go: finding gitlab.aa-inc.com v2go get gitlab.aa-inc.com/bb@v2: unknown revision v2 在获取单个包的时候提示unknown revision错误，后发现是go get默认是使用的https协议，而不是git协议，而git仓库的https协议不支持导致的，解决办法为: 1git config --global url.&quot;git@gitlab.aa-inc.com:&quot;.insteadOf &quot;https://gitlab.aa-inc.com/&quot; 参考文档 Go 每日漫谈——Go Module 的一些坑]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[日常工作中经常用到的命令]]></title>
      <url>%2Fpost%2Foften-command%2F</url>
      <content type="text"><![CDATA[shell的命令千千万，工作中总有些命令是经常使用到的，本文记录一些常用到的命令，用于提高效率。 python快速开启一个http server python -m SimpleHTTPServer 8080 awk按照,打印出一每一列 awk -F, &#39;{for(i=1;i&lt;=NF;i++){print $i;}}&#39; docker registry 列出镜像：curl http://127.0.0.1:5000/v2/_catalog?n=1000 查询镜像的tag: curl http://127.0.0.1:5000/v2/nginx/tags/list，如果遇到镜像名类似aa/bb的情况，需要转移一下 curl http://127.0.0.1:5000/v2/aa\/bb/tags/list]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux虚拟网络设备 - tap/tun]]></title>
      <url>%2Fpost%2Flinux-network-tap-tun%2F</url>
      <content type="text"><![CDATA[Linux虚拟网络设备 - tap/tuntap/tun常用于隧道通讯，通过一个字符设备来实现用户态和内核态的通讯，字符设备一端连接着用户空间，一端连接着内核空间。 对应的字符设备文件位置： tap: /dev/tap0 tun: /dev/net/tun 当应用程序打开字符设备文件时，驱动程序会创建并注册相应的虚拟设备接口，以tunX或tapX命名。应用程序关闭设备文件时，驱动程序会删除tunX和tapX网络虚拟设备，并删除建立起来的路由信息。 两个设备的不同点： tap是一个二层网络设备，只能处理二层的以太网帧 tun是一个点对点的三层网络设备，只能处理处理三层的IP数据包 1234567891011121314151617181920212223242526 ┌──────────────┐ │ │ │ APP │ │ │ └───────┬──────┘ │ │ │ │ │ │ ┌────────────▼──────────┐ │ │─ ─ ─ ─ ─ ─│ /dev/net/tun ├ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ │ │ └────────────┬──────────┘ │ │ │ │ │ ┌───────▼──────┐ ┌──────────────┐ │ │ │ │ │ tunX ├────────────────▶│Network Stack │ │ │ │ │ └──────────────┘ └──────────────┘ tun设备应用举例123456789101112131415161718192021222324252627282930313233 ┌──────────────┐ ┌──────────────┐ │ │ │ │ │ APP A │ │ APP B │◀┐ │ │ │ │ │ └───────┬──────┘ └───────┬──────┘ │ │ │ │ │ │ │ 1│ │ │ │ 5│ │ │ │ │ ─ ─ ─ ─ ─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─ ─│─ ─ ─ ─ ┼ ─ ─ ─ ─ ─ │ │ │ │ │ 4│ │ │ │ ┌────────────▼────────────────────────▼─────┐ │ │ │ │ │ Network Stack │ │ │ │ │ └────────────┬───────────────────────┬──────┘ │ │ │ │ 6│ 3│ │ │ │ │ ┌───────▼──────┐ ┌─▼─────────┴──┐ │ │ │ │10.1.1.11 │ eth0 │ │ tun0 │ 192.168.1.11 │ │ │ │ └───────┬──────┘ └──────────────┘ │ 7│ │ ▼ 10.1.1.100 / 192.168.1.100 应用程序A要发送数据到其他物理机192.168.1.100，由于物理网络环境下只有10.1.1.11和10.1.1.100是相互连通的，192.168.1.11和192.168.1.100是不通的，为了192.168.1.11和192.168.1.100能够进行通讯，需要将数据包进行一次封装。 应用程序B是通过打开字符设备文件/dev/net/tun0的方式来打开网络设备 流程如下： A构造数据包，目的ip为192.168.1.100，并发送给协议栈 协议栈根据数据包中的ip地址，匹配路由规则，要从tun0出去 内核协议栈将数据包发送给tun0网络设备 tun0发送应用程序B打开，于是将数据发送给应用程序B B收到数据包后，在用户态构造一个新的数据包，源IP为eth0的IP 10.1.1.11，目的IP为配置的对端10.1.1.100，并封装原来的数据包 协议栈根据当前数据包的IP地址选择路由，将数据包发送给eth0 reference 详解云计算网络底层技术——虚拟网络设备 tap/tun 原理解析 Linux 网络工具详解之 ip tuntap 和 tunctl 创建 tap/tun 设备 Linux虚拟网络设备之tun/tap]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[北京自驾青海甘肃大环线系列 - 5]]></title>
      <url>%2Fpost%2Fqinghai-5%2F</url>
      <content type="text"><![CDATA[行程：海南藏族自治州黑马河乡（9:30） -&gt; 茶卡盐湖（11:30 - 15:00）-&gt; 柴达木盆地 -&gt; 可鲁克湖（18:00 - 18:30） -&gt; 大柴旦镇（21:00） 路程：500+公里 日期：2019.4.22 今天是整个行程的第五天，第四天晚上到达了青海湖西端的黑马河乡，今天的主要行程是茶卡盐湖，以及横穿几乎整个柴达木盆地。 黑马河日出 黑马河地处青海湖最西端，往东边看去是平静的湖水，完全没有任何遮挡，加上高原的空气通透性特别好，况且地处干旱地带，以晴天为主，是个看日出的绝佳地点。 从黑马河赶往茶卡镇需要经过橡皮山，橡皮山口的海拔在3800米+，是整个行程中的海拔最高点。此时的橡皮山还完全是一座雪山，是整个行程中的第二次到达雪山。同时也是由于这些山脉的原因，才阻隔了青海湖和茶卡盐湖，使其分别成为了只进不出的咸水湖。 茶卡盐湖 茶卡盐湖几乎是青海旅游的毕竟之地，无论是环青海湖小环线，还是青海甘肃大环线，同时也是网红景点，网络上有太多穿着红色裙子的漂亮的小姐姐的照片和视频。 茶卡盐湖地处柴达木盆地，周边地势较低，在离盐湖十几公里的地方湖泊就清晰可见，天上的白云倒影在湖中，宛如一面镜子，因此素有“天空之境”之称。远处的昆仑山脉和祁连山脉的雪山清晰可见，由于天空太过透彻，使常年生活在华北地区的我很难判断出雪山的距离。 (图为采盐船) 茶卡盐湖的开发利用相当充分，一方面是用来开发采盐，并建有盐厂，湖面上可以看到开采盐矿的船只。另一方面，通过旅游来进一步最大化其价值，其实整个柴达木盆地有很多的盐湖，但最为大众所熟知的仅此一家。而且旅游跟采盐是完全分离的，互不相干，毕竟大部分游客都是来拍照的，旅游占用的盐湖面积比重相对较小。 茶卡盐湖的含盐量实在太高，大部分地方湖面的水都较浅，仅有薄薄的一层，下面全是沉积的盐矿，我此行并没有看到有数米深的湖水。很多地方是可以穿着靴子进入到里面的，但要小心盐坑，看似平坦的盐矿其实有很多的暗洞，很容易就陷进去，好在游客能去的地方盐洞都给堵住了，没啥安全问题。上图中黄色的网状结构为防止游客陷入的盖子。 死海的含盐量肯定没有盐湖的含量高的，尚且人体可以漂浮在表面，我想茶卡盐湖如果有较深的地方，人体一定是可以漂浮在表面的，可惜盐湖并未提供该项旅游体验服务，要不还真想一试。 茶卡盐湖的最佳拍照时机是清晨和傍晚，中午烈日当头的时候晒得人很不舒服，紫外线相当强烈，尤其是对于我这种没有墨镜装备的近视眼游客。在烈日下，去拍人像也很难拍的漂亮，眼睛是很难睁开的，手机在拍人像时，由于背景实在太亮，人像总是会偏黑一些。对于我而言，是很难做到早起的，也就赶不上清晨的茶卡盐湖，实际上到达盐湖景区的时候已经是中午了。 (盐湖的路都是用盐结晶铺成的，好不奢侈) (在湖中的沉淀物捞出来随手一攥都能成为一个大疙瘩) 柴达木盆地游览完茶卡盐湖已经是下午三点钟了，接下来还有400多公里的路程要走，好在高原的天黑时间要到晚上八点半。离开茶卡镇后，就进入了漫长的柴达木盆地。 我原以为柴达木盆地是属于新疆地区的，后来一看地图才发现彻头彻尾的属于青海省，夹在昆仑山、祁连山和阿尔金山三大山脉之间，而今天的行程几乎就是横穿整个柴达木盆地。 刚离开茶卡镇，还能看到一些牧民养的成群的牛羊，地上也长有一些枯草，但后来地上随着枯草的减少，牛羊也就看不到了。到后面几乎就是戈壁，看不到任何的动物，完全的荒原，让我想起了《北方的空地》中描述的羌塘无人区中的画面，我感觉无人区中也不过如此罢了。地理课本上说柴达木盆地是“聚宝盆”，矿产资源异常丰富，抛开这些埋藏在地底下的矿产不说，但就感官而言，是极其的荒凉。好在一路上，总有雪山陪伴，也不至于太孤单。一路上有很多动物出没的指示牌，好想看到一个活物，但都以失望告终。 由于地广人稀，这段高速公路的管理也相对简陋，曾看到了一辆当地的三轮车在高速的快车道上逆行，吓得我一脸懵，估计是要按正常的路线行驶要多走出好多路。 可鲁克湖 可鲁克湖位于高速公路旁的大约两公里处，甚至都不用下高速就可以到达，属于3A级景区。到达可鲁克湖时已经是下午6点钟，对于进入景区本不报太大的希望，经询问景区居然营业到晚上八点钟，高原地区果然天黑的晚。景区主要是一个偌大的可鲁克湖，可供游玩的地方特别少，简单拍照走人。如果在这里拍张照，拿给别人看，说是青海湖拍的，没有人能够看得出破绽。如果时间紧迫，该景区完全可以不去参观。 可鲁克湖景区旁边还有个比其更大的托素湖，托素湖是可鲁克湖的三倍大小，可鲁克湖的湖水最终会流入托素湖中。比较有意思的是，可鲁克湖的湖水属于淡水湖，而托素湖却属于典型的内陆咸水湖。咸水湖的形成离不开封闭的环境，托素湖由于没有排水渠道，只能靠蒸发来消耗水分，水分被蒸发后，水分中的盐分却不会被蒸发，日积月累就会形成咸水湖。而可鲁克湖由于会流向托素湖，湖水是流动的，因此没有形成咸水湖。 托素湖离高速较远，要开车过去需要沿着可鲁克湖的湖边，来回至少还得要一个半小时的时间，时间不太允许，且可玩性较差，就直接忽略了该景区。 托素湖旁边还有个外星人遗址，单看景区的名字是我特别感兴趣的类型，网上一搜，景区居然暂未开放! 当天终点 - 大柴旦又是在晚上的时候到达了当天的终点站大柴旦县。 12345678天空之镜湖白雪皑皑山漫漫柴达木渺渺无人烟盆地全是宝可惜只见草戈壁连成片夜宿大柴旦 未完待续…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[北京自驾青海甘肃大环线系列 - 4]]></title>
      <url>%2Fpost%2Fqinghai-4%2F</url>
      <content type="text"><![CDATA[第四天 行程：西宁市 -&gt; 海南藏族自治州黑马河镇 路程：250公里 日期：2019.4.21 经过了前三天的长途跋涉，今天开始了正式的旅行。早上在西宁市的华润万家采购了一些食物，因为还不太清楚接下来的大环线旅行中沿途的住宿饮食条件是什么样子的，也不太清楚很多情况下是否有足够的时间来饱餐一顿，毕竟每天都要赶很多的路。 第一站 塔尔寺 塔尔寺通常都是大家青海旅游的第一站，因为距离西宁市区比较近，大概有几十公里的路程，开车一个小时即可到达。 塔尔寺属于典型的藏传佛教圣地，藏传佛教是受印度佛教、青藏高原本土的苯波教和汉传佛教影响的共同产物，因此有很多区别于其他佛教分支的特点。唐朝时期的文成公主是信奉佛教的，远嫁松赞干布后，为汉族的佛教向青藏地区的渗透起到了非常大的作用。 塔尔寺在藏传佛教中的地位应该跟五台山差不多，但同时也吸收了一些中原道教的文化，比如寺内能看到有西王母的塑像。寺内建筑宏伟，院落也较多，比起五台山而言，除了整体面积小点，其它方面并不逊色。 未能领悟佛教的我，草草的参观塔尔寺后，明显发现了藏传佛教的很多特色，也带给了我不少的震撼。 好几个大殿的院内或者周围都有虔诚的佛教徒在做周而复始的朝拜，口中还振振有词，木质结构的地板早已磨的光滑无比。那种虔诚、执着非常值得敬佩，虽然以我现在的阅历还很难理解这种宗教行为。 第二站 日月山从塔尔寺出来后，已经是下午两点了，接下来就要往青海湖进发了。日月山是此行的必经之路，属于祁连山脉的支脉，也是此行中看到的第一个雪山，沿途海拔在3000米以上。 进入日月山之前，阳光普照，高原的阳光相当强烈，隔着车玻璃都能感觉到。一开进日月山，雪山清晰可见，突然下起了冰雹，紧接着又下起了雨夹雪，原本没有积雪覆盖的山上也披上了一层薄薄的银装。这一切仿佛在预示着什么，仿佛在迎接着远道而来的客人，又仿佛是在告诫我不要打扰到了雪山上的神灵。 因为日月山的风景太美，全然没有了赶路的想法，就想驻足体验一下日月山的神奇。主干道是京藏高速，有条公路可以通往日月山风景区，索性直接开往日月山风景区。到了景区门口，雨夹雪渐小，一下车发现空气凉了好多，原本仅穿了单件衣服，感觉立马要穿羽绒服的节奏，穿了件外套仍冻得直打哆嗦。高原的天气就是这般神奇，有阳光和没有阳光完全就是两个世界。 日月山风景区并没有必要进入参观，因为景观在景区外一览无余。 景区旁边能看到很多高原鼠兔，发出吱吱的响声，这玩意特别爱打洞，鼠兔的旁边可以看到几个洞。 之后的行程中在公路边上看到了一只死去的马匹，马的头颅已不知所踪。我想放到东部地区，这种现象是肯定不会出现的，死去的马儿肯定还有它的利用价值。 第三站 青海湖 从日月山出来沿着G109国道继续前行，中间经过了倒淌河风景区，倒淌河水源自日月山，自东向西注入青海湖，故名倒淌河。倒淌河的水量不大，加上本来也很难分清东南西北，就很难体会到倒淌河的精妙。看评价景点比较坑，并没有进入参观。 过不了多久，青海湖就会出现在了眼前，而且接下来的两个多小时会一直沿着青海湖南沿的公路前进，足足有一百公里的距离，不愧为中国最大的湖泊。青海湖有个游客去的较多的二郎剑景区，并没有进入，据说性价比依旧很低，无非就是看湖，青海湖的美从哪个角度都能感受到。 G109国道跟青海湖稍微有一段距离，最近处差不多有一里路的样子，中间都是草原，被牧民给围成了一块一块，山羊绵羊在悠闲的吃着草。找了一处牧民的区域，开车过去，一人10元的门票，即可到青海湖边拍照。 身为游客一定觉得湖边的牧民是真正的逍遥快活，每天在自己的牧场上骑马放牧，住在青海湖畔，远处有雪山作伴，呼吸着几乎没有污染的空气。可是一旦天天都是这样的生活，又有几个游客可以放下尘世间的所有诱惑来到这近乎纯净的地方呢？牧民们也有自己的烦恼，牧民的孩子上学怎么办，牧民们生病了怎么办，各种生活的不便利。虽呼吸着最新鲜的空气，却也承受着杀伤力很强的紫外线。冬天的高原夜晚想想都会瑟瑟发抖，但牧民们也要忍受。 我天真的以为，青海湖最初是真正的大海，后来由于地壳运动，地壳上升，而青海湖变成了一个内陆湖。哈哈，不过这个来源忽悠人还是蛮不错的。后来经查阅资料，青海湖的最初是一个内陆淡水湖，并且注入黄河。后来由于地壳运动，青海湖流入黄河的入口被切断，从而青海湖没有了任何出水口。再加上天气较为干旱，湖水的蒸发量大于湖水的注入量，导致湖水的盐分浓度逐渐增加，从而变成了一个咸水湖。青海境内的咸水湖成因都是因为蒸发量大于注入量，包括比较出名的茶卡盐湖。青海湖的湖水浓度并没有海水的浓度高，湖水帮大家尝过了，确实挺咸的。 青海湖的湖水非常清澈，虽湖水很宽，但空气透彻，一眼可以望到河对面远处的雪山。远处的云朵看起来非常低，跟湖水连在一起，微风吹过，湖水上一层层的波澜。 湖边的小水坑里已经有了青蛙，晚上的青海湖畔温度还是非常低的，不知道这些青蛙夜晚是怎么度过的。当然这些小水坑的水是纯淡水，亲尝无误。 终点站 黑马河镇黑马河位于青海湖的西端，因为是去往茶卡盐湖的必经之地，且可以看到青海湖，因此顺其自然靠着旅游而生存。早晨起来看日出的是个不错的地方，可以想象一下朝霞倒映在湖水中的场景，很多游客都会选择在黑马河观日出。 到达黑马河已经是晚上八点多了，温度较低，索性换上羽绒服，一点也不觉得热。住宿条件比较一般，但价格却不便宜，是整个行程住的性价比最低的宾馆了。找了一家餐馆就餐，全都是组团的游客，非常不错的食材却做的难吃的不行，上等的牦牛肉放到餐桌上后却嚼也嚼不烂，看着放弃的牦牛肉甚是可惜。这大概就是国内很多因旅游而起的餐馆的真实写照，反正能坑一个是一个，不要回头客，回回都是新客人。做的烂对自己的生意影响不大，但游客在心中对于当地的评价确实难以磨灭的。 未完待续…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[北京自驾青海甘肃大环线系列 - 3]]></title>
      <url>%2Fpost%2Fqinghai-3%2F</url>
      <content type="text"><![CDATA[第三天 行程：宁夏回族自治区中卫市中宁县 -&gt; 青海省西宁市 路程：500公里 日期：2019.4.20 昨晚到酒店比较晚，早上拉开窗帘，一所学校的操场映入眼帘，令我眼前一亮。离开校园已经太久了，看到同学们身穿运动装在操场上洋溢着青春即亲切又陌生。 第一站 黄河畔从地图上看中宁县城离黄河比较近，自然要去黄河边上走一走。但与保德县和府谷县不同的是，中宁县并不傍黄河而建，主城区而是离黄河有一公里的距离。我猜测保德县和府谷县边的黄河在地势低洼处，即使河水泛滥也不会殃及县城。但中宁县地势却是非常平坦，不能依靠地势优势阻止泛滥的河水，因此县城的选址离黄河稍有距离。 远处的黄河边上有一个多层的塔，是中宁县枸杞博物馆，中宁县有“天下枸杞出宁夏,中宁枸杞甲天下”的说法，因此建个枸杞博物馆还是比较合适的。由于到枸杞博物馆的路并不好走，而且不能判断此刻博物馆是对外开放，并未前往参观。 黄河水比其下游的府谷县的黄河水要变黄了很多，这也是我一直比较奇怪的地方。河道非常宽，目测有至少1公里的样子，现在是枯水期，河水并不太多。河床上有各式各样的鹅卵石，不知经过了多少次的河水冲刷才形成了现在的形状，我们个体的生命相对鹅卵石的生命实在是太短，挑选了几个像模像样的留作纪念。 在路上 宁夏境内，一路上偶尔会看到种植了一些铺着地膜的农作物，我猜测是土豆的可能性比较大，原因是山东的种植土豆方法就是类似思路。但看下田地中的土质，大部分都是小石块，哪有多少土壤成分呀，倒是拿着石块压着地膜来防止大风非常的方便。 在甘肃和青海接壤地方，道路两侧多了很多树木，，山上的草有很多都绿了起来，生机勃勃的。两天之内满眼都是枯黄色的画面，突然出现鲜绿色还有点不适应。 查看青海省的地图会发现，有好几个地级市都是围绕着青海湖命名的，海东市、海南藏族自治州、海西蒙古族藏族自治州、海北藏族自治州。提到海南，不熟悉的人还以为是海南岛呢。 地图中还有一个比较有意思的地方，海西蒙古族藏族自治州居然有两块区域，中间被玉树给分割开了，完全不接壤。让我立马想到的是在中国版图中另外比较有意思的是河北省的三河市，三河市被北京市和天津市包围，像是河北省的一个孤岛。 青海省有藏族和回族比较容易理解，居然还有蒙古族？要不也就不会叫海西蒙古族藏族自治州了。实际上内蒙古的最西端跟青海省仅隔了甘肃省一段狭长的河西走廊，直线距离上还是特别近的。蒙古族迁徙到青海要从元朝说起，整个中国的版图都是元朝的天下，自然青海省有就了蒙古族，后续又陆陆续续的有蒙古族迁徙都青海省。但青海省的蒙古族还吸收了部分当地藏族的习俗，很难说是蒙古族入侵藏族，还是藏族同化了蒙古族。 青海境内的青藏高速施工，走了很长一段的非高速，跟青海的村庄和县城有了第一次近距离接触的机会。青海省的海东市相对于甘肃而言，植被还是比较茂盛的，树木挺多的，沿路很多梨树，开着洁白的梨花。经过村庄的房屋看起来也不差，很多都是二层的小楼，而且村庄还是比较密集的，大概青海人口都聚集在了东部地区。总体给我外观感受，海东地区要比甘肃东部地区富裕很多。 第二站 西宁市 西宁市属于中国的西部地区，如果放眼到中国的地图中，相同纬度上，西宁算是比较靠中间的城市了，如果不是西部人烟稀少，更像是个中部城市。 晚上八点钟到达西宁市，事先知道青海省的人口只有区区600万，省会应该不咋地，结果还挺另我刮目相看的，城区面积感官上还是挺大的，颇具省会城市的规模，各种品牌应有尽有，夜景也还不错。 晚上吃了一碗当地特色羊肠面，面相不咋地，吃起来味道还不错。 收个尾123456人人都说宁夏美砂石大漠看到尾甘东地区很荒凉似乎水源很紧张夏都街头走一走车水马龙大高楼 未完待续…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[北京自驾青海甘肃大环线系列 - 2]]></title>
      <url>%2Fpost%2Fqinghai-2%2F</url>
      <content type="text"><![CDATA[第二天 行程：山西省保德县 -&gt; 陕西省榆林市府谷县 -&gt; 宁夏回族自治区中宁县 路程：600公里 第一站 - 府谷县第一天留宿的保德县临近黄河，早上醒来后本想去黄河边看下景色，其实最关心的是中段黄河的河水是什么颜色的，可惜黄河边是一条公路沿着黄河蜿蜒，而且局部在修，因为黄河的地势较低，视线受阻严重，连黄河都没看到。 索性直接导航到了黄河对岸的府谷县，山西省保德县跟陕西省榆林市府谷县隔黄河相望，仅一河之遥，可以在跨河大桥上非常方便的在两个省之间通行。跨河大桥并不是特别宽，两车会车是没任何问题的，桥上还有一些行人在通行。上图为连接两省的大桥，在府谷县可以看到“秦晋之好“几个大字。 保德县虽紧挨着黄河，却没有依靠上黄河的任何优势，反而黄河成为了一大劣势。相反，府谷县却非常明智的，楼房离着黄河稍有一段距离，选择在黄河边修建了滨河公园供休闲娱乐。 在府谷县的滨河公园可以清晰看到和对面的山西省保德县城，建筑物的朝向大多都是面向黄河，或者面向对面的府谷县城的。 由于现在是枯水期，人行道离河水还有一段距离，但仍可以看出黄河水还是特别清澈的，跟黄河的淤泥还是有比较鲜明的区别。跟更下游那夹杂的泥沙的黄河水一对比，让人完全联想不到是一条河。后面的行程中在更上游看到了几次黄河，但河水都要比此段的黄河水要黄，至少兰州的黄河水夹杂的泥沙量已经非常大，也就是说此段黄河水是我见到最清澈的。不知道是否我的判断有误，从理论上讲应该越下游黄河水越黄才对的。 在路上下午一点从府谷县出发，直奔西宁方向而去，最终到达了宁夏回族自治州中卫市中宁县城，最初地貌还是黄土高坡，大概从榆林市之后逐渐沙漠化，地上草还没长出来，在高速上很难判断远处是草原，还是完全的荒漠。 上图为在高速上的简陋厕所，与其说是黄土高坡地貌，我感觉更像是荒漠。 本以为到达宁夏后，地貌会变成接近于草原，因为提到宁夏我最先想到的是黄河冲积而成的河套平原。一路上一直在期盼，但一直也没遇到梦中的草肥水美的河套平原。事后一查资料才发现，即使河套平原最南的“西套”地区也在银川市周边，而我走的高速在银川市的南边大约100多公里的距离，可以说是跟河套平原擦肩而过。从卫星地图上可以清晰的看出河套地区的植被还是比较茂密的。上图中绿色的线为我经过的路线。 终点站 - 中宁县 最终到达了宁夏回族自治州中卫市中宁县城，到达中宁县后，恰巧赶上了一次沙尘暴，停车吃饭的时候，车上已经积了厚厚的一层土。本以为沙尘暴在当地是家常便饭，问下了当地人，说是沙尘暴也不常有，沙尘是从北部的腾格里沙漠吹过来的。从上图中可以看出，中宁县离得腾格里沙漠还是特别近的，刮个北风，沙尘天气还是非常容易出现的。 比较巧合的是，中宁县也恰巧在黄河边上，一天之内，驱车600公里，从黄河“几”字形的右边到达了左边位置。 却道是 123456789101112日行千余里胜似的卢骋朝食保德县夕塌宁安镇黄土变沙漠高山转草原早在黄河畔夕亦黄河边自东向西行作何现此况若君心有疑一览地图觅 未完待续…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[北京自驾青海甘肃大环线系列 - 1]]></title>
      <url>%2Fpost%2Fqinghai-1%2F</url>
      <content type="text"><![CDATA[近期有一段较为充裕的假期，出去旅游对我而言是再合适不过的选择，带着对远方田野的那份向往。特别有意向的方向有两个：南下和西进。 南方的很多地方都未曾去过， 比如张家界、桂林等，可以从北京出发一路到海南三亚地区，而且当下正巧是烟花三月下江南的好季节。 对于西部地区向往已久，青海、西藏和新疆地区都未曾去过，当下四月份不是西部地区的最佳旅游季节，山基本上还都是枯黄色，草儿还在沉睡中。 但心想南方交通较为便利，后续会去的可能性比较大，而且景区之间跨度较大，可以分开旅游。而西部往往需要拿出一大把的时间来玩耍才能尽兴，很多地方的交通不是特别方便，往往都是需要多个景点一起才最划算。 因此，最终的决定是北京自驾甘肃-青海大环线。 此刻的我已经结束了整个行程，行程的时间段为2019.4.18 - 2019.4.30，共计13天时间。起点为北京市，终点为山东省，整个的行程达6000公里。 事先计划的时间为12天，留了一天的buffer，最终用时为13天，没有太大的偏差。由于一直是我一人在开车，相对而言还是比较累的，基本上一天得开500公里，尤其是前面和后面赶路的时间，基本上一天在600公里以上。大部分时间是上午在10点之后出发，下午开车的时间比较多，很多时候需要晚上开一段时间才能满足一天的行程目标。如果时间比较充裕，把整个行程放慢为20天，感觉会比较轻松，会对旅行有更深的体会，只是我时间不太允许。 一路上经过北京、河北、山西、陕西、宁夏、甘肃、青海、河南、山东这些省，地貌包括平原、山脉、黄土高原、戈壁、沙漠、黄河、湖泊、雪山、盆地等地貌。 这一路上遇到的很多地方，之前在历史和地理课本上曾经了解过，很多地方有比较深厚的历史和文化底蕴，自己的认识却是很浅薄，而旅行可以加深对一个地方的地理和历史的认识，正所谓：“纸上得来终觉浅，绝知此事要躬行”。 本系列文章并不打算写成单纯游记的形式，而是更多会将我个人的体会和相关的历史地理知识融合在其中，在这其中查阅相关的资料对我而言也是一种成长。由于才疏学浅，所查资料有限，很多观点不见得是正确的，欢迎指正。 第一天行程：北京市 -&gt; 山西省忻州市保德县 上午11点从北京出发，驱车700公里，晚上八点半到达山西省忻州市保德县城。 先后经过了华北平原、太行山脉、黄土高原，北京和河北境内大部分属于华北平原，河北和山西省的交界处为太行山脉，期间穿过了多个隧道，基本上是整个的行程中走过的最长的一段隧道了，后续除了祁连山脉、六盘山外隧道就比较少了。 到了山西，以黄土高坡地貌为主，黄土高原也是整个行程中看到最多的地貌了。黄土高原东起太行山脉，西至青海省日月山，南抵秦岭，北达长城，包括了山西、陕西、宁夏、甘肃、青海、河南、内蒙古7个省。比我印象中的要大得多，我印象中主要是在山西境内和陕西的北部地区，一下子刷新了我的认知。 大家都知道，黄土高原是水土流失导致现在的千沟万壑的地貌，也是因此而导致黄河水特别的黄，那么是什么时间段发生的水土流失呢？历史上的黄土高原又是什么样的呢？ 黄土高原最初是被植被茂密的森林所覆盖，而且土壤在有充足水源的情况下还是特别肥沃的，并非大家印象中的植被少的可怜，要不然也不可能孕育出华夏文明。相反，正是由于黄土土质疏松易开垦的特点，才容易诞生人类文明，因为最早的人类文明生产工具是特别简陋的，对大自然的改造能力特别有限。黄土高原被破坏的历史时期基本上伴随着华夏文明的发展，到明清时期达到顶峰，建国之后开始逐渐恢复。 很难想象，偌大的黄土高原，竟然因为农耕时代的人类影响变成了如今这幅模样。假如再给现代的人类一次利用自然的机会，我想当下的人类一定会善加利用这片广袤的土地，黄河也许会改成另外一个名字。也就是说，人类不同的文明程度决定了对自然的破坏程度。 但今天的我们殊不知也在对自然进行着更深层次的破坏，如果放到未来来审视我们人类今天的作为，我们今天对这个星球的破坏要大大高于农耕时代对地球的破坏。比如，人类在贪婪的消耗着地球上的石油、煤炭资源，也许在不久的未来，人类会发现在这些燃料资源中还蕴藏着无穷尽的价值，但是这些资源已经被当下愚蠢的人类当做燃料给烧掉了。 一路上雾霾还是比较严重的，尤其是河北保定境内。保定的空气质量是早有心理预期，印象中山西的整体空气还是非常不错的。去年去过一次五台山，穿过几个长长的隧道进入山西境内后，空气会一下子好转，山西省忻州市的空气跟河北省保定的空气质量有着强烈的反差。但这次结果却比较失望，在整个山西境内并未出现晴空万里，而且看上去更像是雾霾，而不是阴天。 山西省境内有那么几段高速，连一辆车都看不到，让我一度怀疑已经抵达大西北的感觉。 晚上到达保德县，县城傍黄河而建，位于河道东侧。离黄河稍远的地方是黄土高坡，大部分楼房都离建在离黄河较劲的地势平坦处，而黄土高坡上由于地势的原因房屋逐渐变少。一进入县城，道路上到处都是开着远光灯的，仿佛对面一颗颗行进中的太阳，闪的我实在有些难受。有点匪夷所思，明明有路灯，实在想不明白远光灯的用途。真期望每个人作为社会中的一份子，在做事情的时候要多为他人考虑一分，而不是仅顾及自己的感受。 保德县城到处打着扫黑除恶的标语，吓得我开车都格外小心。甚至连小区门口都打着口号，让市民天天回家都可以看到。我深知越是提倡什么，说明越是缺少什么。后来走过好多地方后，发现到处都打着该口号，并不是当地特色。 却道是： 123456驱车青甘大环线夜晚留宿保德县县城建在黄河畔黄土高坡把城建黄河对岸府谷县一桥之隔秦晋缘 未完待续…]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[最长公共子序列和最长公共子串问题]]></title>
      <url>%2Fpost%2Falgorithm-max-subarray-or-suborder%2F</url>
      <content type="text"><![CDATA[最长公共子串及公共子序列问题属于一类问题，都可以使用动态规划的算法来解析，且动态规划方式比较类似，比较容易混淆。 定义最长公共子串：两个字符串中，相同的最长子串，字符必须是连续的 最长公共子序列：两个字符串中，相同的最长序列，字符不一定是连续的 比如：a[] = “abcde” b[] = “bce” 那么：最长子串：”bc”最长子序列：”bce” 解法假设A、B分别表示两个字符串 最长公共子串dp[i][j]表示子串A[:i]、B[:j]必须以A[i]、B[j]为结尾的两个字符串的最大子串长度 12345if A[i] == B[j] &#123; dp[i][j] = dp[i-1][j-1]&#125; else &#123; dp[i][j] = 0&#125; 最终dp二维数组中的最大值即为结果 最长公共子序列dp[i][j]表示子串A[:i]、B[:j]的两个字符串的最大子序列 12345if A[i] == B[j] &#123; dp[i][j] = dp[i-1][j-1]&#125; else &#123; dp[i][j] = max(dp[i-1][j], dp[i][j-1])&#125; 最终dp[i-1][j-1]为结果 leetcode Delete Operation for Two Strings （最长公共子序列） Maximum Length of Repeated Subarray （最长公共子串）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[VirtualBox磁盘扩容]]></title>
      <url>%2Fpost%2Fvirtualbox-scaleup%2F</url>
      <content type="text"><![CDATA[今天发现vagrant的其中一个虚拟机磁盘空间不够了，需要对其进行磁盘扩容，但不期望是通过增加新硬盘的方式，而是直接增加原磁盘容量的方式来无缝扩容。 修改磁盘文件进入到vm磁盘文件所在的目录~/VirtualBox VMs/dev_default_1531796361866_92956下 1234567891011121314151617181920212223% vboxmanage showhdinfo centos-vm-disk1.vmdkUUID: acbb4ffc-0580-40d6-8627-3ed24cd0beffParent UUID: baseState: createdType: normal (base)Location: /Users/lvkai/VirtualBox VMs/dev_default_1531796361866_92956/centos-vm-disk1.vmdkStorage format: VMDKFormat variant: dynamic defaultCapacity: 10000 MBytesSize on disk: 9634 MBytesEncryption: disabledIn use by VMs: dev_default_1531796361866_92956 (UUID: a153957c-e43f-4dd2-8512-f51d42dee3d3)# 将之前存储的vmdk格式的文件复制一份vdi格式的文件，由于需要复制文件，该命令需要执行一段时间% vboxmanage clonehd centos-vm-disk1.vmdk new-centos-vm-disk1.vdi --format vdi# 将vdi格式的文件修改磁盘空间上限大小为80g，但实际占用磁盘空间仍然为之前的大小% vboxmanage modifyhd new-centos-vm-disk1.vdi --resize 81920# 将vdi格式的文件重新转换为vmdk格式，会产生一个新的uuid% vboxmanage clonehd new-centos-vm-disk1.vdi resized.vmdk --format vmdk0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%Clone medium created in format &apos;vmdk&apos;. UUID: 7e454b50-0681-494b-b9ca-81700d217c0a 新的硬盘创建完成后，在virtualbox的界面上将对应虚拟机的硬盘更换为resized.vmdk，并将之前旧的centos-vm-disk1.vmdk给删除掉。 使用fdisk创建新的磁盘分区以上命令执行完成后，开启虚拟机，进入系统，可以看到磁盘空间大小变更为85.9GB，但挂载的磁盘空间大小仍然为8.3G，新增加的磁盘空间仍然处于未分配状态。 1234567891011121314151617181920212223242526272829303132333435# fdisk -lDisk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x0000ca5e Device Boot Start End Blocks Id System/dev/sda1 * 2048 1026047 512000 83 Linux/dev/sda2 1026048 20479999 9726976 8e Linux LVMDisk /dev/mapper/centos-root: 8866 MB, 8866758656 bytes, 17317888 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/mapper/centos-swap: 1048 MB, 1048576000 bytes, 2048000 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/centos-root 8.3G 7.9G 386M 96% /devtmpfs 296M 0 296M 0% /devtmpfs 307M 0 307M 0% /dev/shmtmpfs 307M 4.5M 303M 2% /runtmpfs 307M 0 307M 0% /sys/fs/cgroup/dev/sda1 497M 195M 303M 40% /bootvagrant 466G 390G 77G 84% /vagrantvagrant_data 466G 390G 77G 84% /vagrant_datatmpfs 62M 0 62M 0% /run/user/1000 接下来需要将未分配的磁盘空间 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# fdisk /dev/sda# 依次输入可创建新的分区np回车回车# 继续输入p，可以看到磁盘的情况，多出了/dev/sda3# /dev/sda3的System为Linux，而/dev/sda2的System为Linux LVMCommand (m for help): pDisk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x0000ca5e Device Boot Start End Blocks Id System/dev/sda1 * 2048 1026047 512000 83 Linux/dev/sda2 1026048 20479999 9726976 8e Linux LVM/dev/sda3 20480000 167772159 73646080 83 Linux# 依次输入将/dev/sda3更改为LVM格式t38epDisk /dev/sda: 85.9 GB, 85899345920 bytes, 167772160 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x0000ca5e Device Boot Start End Blocks Id System/dev/sda1 * 2048 1026047 512000 83 Linux/dev/sda2 1026048 20479999 9726976 8e Linux LVM/dev/sda3 20480000 167772159 73646080 8e Linux LVM# 输入w后进行保存操作Command (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy.The kernel still uses the old table. The new table will be used atthe next reboot or after you run partprobe(8) or kpartx(8)Syncing disks. 将新创建的磁盘分区添加到LVM分区中将机器重启后，继续执行如下命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# vgdisplay --- Volume group --- VG Name centos System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size 9.27 GiB PE Size 4.00 MiB Total PE 2374 Alloc PE / Size 2364 / 9.23 GiB Free PE / Size 10 / 40.00 MiB VG UUID cpEmYK-XFew-6ZWT-GEeY-yEou-0vLq-OJiD08# lvscan ACTIVE &apos;/dev/centos/swap&apos; [1000.00 MiB] inherit ACTIVE &apos;/dev/centos/root&apos; [&lt;8.26 GiB] inherit# pvcreate /dev/sda3 Physical volume &quot;/dev/sda3&quot; successfully created.# vgextend centos /dev/sda3 Volume group &quot;centos&quot; successfully extended# lvextend /dev/centos/root /dev/sda3 Size of logical volume centos/root changed from &lt;8.26 GiB (2114 extents) to &lt;78.49 GiB (20093 extents). Logical volume centos/root successfully resized.# xfs_growfs /dev/centos/rootmeta-data=/dev/mapper/centos-root isize=256 agcount=4, agsize=541184 blks = sectsz=512 attr=2, projid32bit=1 = crc=0 finobt=0 spinodes=0data = bsize=4096 blocks=2164736, imaxpct=25 = sunit=0 swidth=0 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=0log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0data blocks changed from 2164736 to 20575232# 最后执行命令可以看到磁盘空间已经增加# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/centos-root 79G 7.9G 71G 11% /devtmpfs 296M 0 296M 0% /devtmpfs 307M 0 307M 0% /dev/shmtmpfs 307M 4.5M 303M 2% /runtmpfs 307M 0 307M 0% /sys/fs/cgroup/dev/sda1 497M 195M 303M 40% /bootvagrant 466G 390G 77G 84% /vagrantvagrant_data 466G 390G 77G 84% /vagrant_datatmpfs 62M 0 62M 0% /run/user/1000]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang GC]]></title>
      <url>%2Fpost%2Fgolang-gc%2F</url>
      <content type="text"><![CDATA[常用垃圾回收方法1.引用计数类似于C++中的智能指针，每个对象维护一个引用计数，当对象被销毁时引用计数减一，当引用计数为0时立即回收对象。 在PHP、Python中使用，适用于内存比较紧张和实时性比较高的系统。 缺点： 由于频繁更新引用计数，降低了性能。 循环引用问题。解决办法为避免循环引用。 2.标记(mark)-清除(sweep)分为标记和清除两个阶段，算法在70年代就提出了，非常古老的算法。 该算法有一个标记初始的root区域，和一个受控堆区。root区域主要是程序当前的栈和全局数据区域。 从root区域开始遍历所有被引用的对象，所有被访问到的对象被标记为“被引用”，标记完成后对未被引用的对象进行内存回收。 缺点： 标记阶段，都会Stop The World，会大大降低性能。 3.分代回收将堆划分为两个或者多个代空间，新创建的对象存放于新生代，随着垃圾回收的重复执行，生命周期较长的对象会被放到老年代中。新生代和老年代采用不同的垃圾回收策略。 Go垃圾回收机制Go中采用三色标记算法，本质上还是标记清除算法，但是对标记阶段有改进。 步骤如下： 开始时，所有的对象均为白色 从root开始扫描所有可达对象，标记为灰色，放入待处理队列。root包括了全局指针和goroutine栈上的指针。 从队列取出所有灰色对象，将其引用对象标记为灰色放入队列，自身标记为黑色。 重复3，直到灰色队列为空。 将白色对象进行回收 优点：用户程序和标记操作可以并行执行。 详细过程如下图： 通过上图可以看出，STW有两个过程： gc开始的时候，需要一些准备工作，如开启write barrier re-scan的过程 每个对象需要一个标记位，go中并未将标记位存放于对象的内存区域中，而是采用非侵入式的标记位。go单独使用了标记位图区域来对应内存中的堆区域。 write barrier: 在运行垃圾回收算法的同时，应用程序在一直执行，上文中提到了写屏障write barrier用于将这些内存的操作记录下来。 GC日志GC日志对于定位问题还是比较方便的 1.开启GC日志可以增加环境变量GODEBUG=gctrace=1来开启gc日志，gc日志会打印到标准错误中。例如有如下的程序: 12345678910111213package mainimport &quot;time&quot;func main() &#123; for &#123; s := make([]int, 10) for i := 0; i &lt; 10000; i++ &#123; s = append(s, i) &#125; time.Sleep(time.Nanosecond) &#125;&#125; 执行GODEBUG=gctrace=1 go run gc.go即可打印gc日志到标准错误中。 2.GC日志的含义可以参照runtime 123456789gc # @#s #%: #+#+# ms clock, #+#/#/#+# ms cpu, #-&gt;#-&gt;# MB, # MB goal, # Pwhere the fields are as follows: gc # the GC number, incremented at each GC @#s time in seconds since program start #% percentage of time spent in GC since program start #+...+# wall-clock/CPU times for the phases of the GC #-&gt;#-&gt;# MB heap size at GC start, at GC end, and live heap # MB goal goal heap size # P number of processors used 例子: gc 11557 @179.565s 2%: 0.018+1.2+0.049 ms clock, 0.14+1.1/2.3/0.90+0.39 ms cpu, 13-&gt;14-&gt;7 MB, 14 MB goal, 8 P 3.GC的监控对于线上GC的监控，基本上读取runtime.MemStats结构中的内容，然后存储到时序数据库中。具体有如下两种获取方式： 123456// 方式1memStats := &amp;runtime.MemStats&#123;&#125;runtime.ReadMemStats(memStats)// 方式2 json格式expvar.Get(&quot;memstats&quot;).String() references Getting to Go: The Journey of Go’s Garbage Collector Go: runtime]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第11期]]></title>
      <url>%2Fpost%2Fknowledge-share-11%2F</url>
      <content type="text"><![CDATA[古语有云，一年之计在于春，这句话对于很多植物而言再合适不过。在经历了寒冬之后，很多植物选择将最美好的一面在春天里绽放。 资源1.PagerDuty PagerDuty是一家Sass平台厂商，其产品为一款告警处理平台，提供了On-Call管理、告警收敛分组、告警时间报表，并集成了多种告警方式。 2.Fathom 一款开源的简易网站数据分析工具，类似于Google Analytics或者百度分析。 3.Data Structure Visualizations 该网站将常见的计算机数据结构以可视化的形式展示在了界面上，可以在界面上点击按钮完成插入元素、删除元素等操作，对应的数据结构展现会实时发生变化，非常直观。 4.snowflake Twitter开源的分布式算法，常用于分布式id的生成，使用毫秒时间戳、机器id、毫秒内的流水号来生成随机id。采用此种方法生成的id可以保证单机递增，但不能保证是全局递增的。 5.Netlify 很多人喜欢将自己的博客系统利用Github的Pages功能托管到Github上，但Github Pages并不支持对静态页面的构建，只能将构建完成后的页面推动到Github上。Netlify支持对静态网站的持续集成和持续部署，代码可以托管于Github上，Netlify会自动构建和发布，支持免费的https协议和CDN。 但可惜的是，实际测试下来，网站的访问速度在国内不是很理想。 6.996.ICU 全世界最大的同性交友社区Github上异常火爆的声讨996工作制度的项目，两天的时间内已经突破6万多Star，issue的数量也已经破万（截止到2019-03-28 19:55），要知道Github上Star数最多的项目freeCodeCamp也才接近30万Star，全世界运行设备最多的操作系统linux也才7万多Star，这简直创造了Github上Star数增长的奇迹。 很多人在issue中提到了加群交友吐槽、倾诉加班不满，甚至还有过来找男盆友的，活脱脱把issue玩成了贴吧，留言中清一色的汉字，说明基本是中国人在玩。 我个人对于强制996加班的事情不是很赞成，虽然过去三年中我的工作强度应该大于996，但更多的是出于个人自愿和对健康的无视，公司层面并没有强制要求。人生确实有非常多美好的事情可以参与和享受，对于程序员这个群体而言，电脑屏幕之外的世界还很大，还有太多的领域值得去探索和挖掘。但如果确实是因为个人的爱好，在工作中能够获得很好的成就感和满足感，996或者更高强度的加班，我个人觉得都是值得的。 说起ICU，程序员这个群的职业病是颈椎、腰椎、视力等，失眠多梦也是大有人在。我个人也确实身体出过一些问题而住过院，人往往都是在身体好的时候不懂得去重视自己的身体，当身体一旦出毛病的时候才懂得去珍惜。我曾经生病的时候也是鼓励自己要多锻炼和注意身体，但当身体好了之后，当时的愿望又抛到了脑后。 身体出现问题往往不是一朝一夕造成的，而是长期积累的结果，尤其是刚工作的前几年，趁着年轻确实能多加班熬夜，但30岁之后往往体力就跟不上了。还是奉劝各位，在工作的时候多注意休息和加强锻炼，无论是996，还是朝九晚五，都要多注意。 精彩文章1.互联网运维工作 滴滴运维总监来伟在2017年对运维工作范围的思考，公司处在不同的阶段，运维所能干的事情也有所不同。 2.早点懂这几个道理，就不害怕被裁员了 IT行业中的一些职场规则，程序员在中年时期如果不做好职场转型，会逐步被更有活力更有体力的年轻人给取代。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang中的panic和recover用法]]></title>
      <url>%2Fpost%2Fgolang-panic%2F</url>
      <content type="text"><![CDATA[golang中的panic用于异常处理，个人感觉没有try catch finally方式直观和易用。 func panic(v interface{})函数的作用为抛出一个错误信息，同时函数的执行流程会结束，但panic之前的defer语句会执行，之后该goroutine会立即停止执行，进而当前进程会退出执行。 func recover() interface{}定义在panic之前的defer语句中，用于将panic()进行捕获，这样触发panic时，当前gotoutine不会被退出。 recover所返回的内容为panic的函数参数，如果没有捕获到panic，则返回nil。 注意：recover仅能定义在defer中使用，在普通语句中无法捕获recover异常。recover可以不跟panic定义在同一个函数中使用。 example 1123456789import &quot;fmt&quot;func main() &#123; defer fmt.Println(1) fmt.Println(2) panic(3) fmt.Println(4) defer fmt.Println(5)&#125; panic()执行后，会先调用defer函数，然后打印panic: 3，当前goroutine退出，后续语句不再执行，程序输出： 1234567821panic: 3goroutine 1 [running]:main.main() /Users/lvkai/src_test/go/panic/panic.go:8 +0xd5exit status 2 example 212345678910111213141516171819package mainimport &quot;fmt&quot;func main() &#123; func() &#123; defer func() &#123; if r := recover(); r != nil &#123; fmt.Println(&quot;recover: &quot;, r) &#125; &#125;() defer fmt.Println(1) fmt.Println(2) panic(3) fmt.Println(4) defer fmt.Println(5) &#125;() fmt.Println(6)&#125; 在执行panic后，触发当前函数中的defer中的recover函数，此时panic后的当前函数中的语句同样是不再执行，但当前goroutine不会退出。也就是说panic被recover后，会影响到当前函数中的后续语句的执行，但不影响当前goroutine的继续执行，输出内容如下： 123421recover: 36 example 31234567891011121314151617181920package mainimport &quot;fmt&quot;func main() &#123; func() &#123; defer func() &#123; if r := recover(); r != nil &#123; fmt.Println(&quot;recover: &quot;, r) &#125; &#125;() func() &#123; defer fmt.Println(1) panic(2) &#125;() fmt.Println(3) defer fmt.Println(4) &#125;() fmt.Println(5)&#125; recover跟panic定义在不同的函数中，仍然可以发挥作用。 1231recover: 25]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第10期]]></title>
      <url>%2Fpost%2Fknowledge-share-10%2F</url>
      <content type="text"><![CDATA[春回大地，题图为即将融化的河面以及还在冰面上行走的路人。 资源1.fastThread 在线的JVM线程栈分析工具，通过上传JVM Dump文件，在线查看线程分析结果。 2.全球空气质量地图 可以在线查看全球的PM2.5情况，很多国家的PM2.5都超过了200，但并不包含中国的数据，不知道是不是怕数据把其他国家吓死的缘故。 3.Walle 使用Python3开发的CI/CD平台，有相对友好的界面，目前Github Star数在6000+。 4.Træfɪk 为微服务而生的HTTP协议反向代理，自带API接口、dashboard，并支持Kubernetes Ingress、Mesos等，可动态加载配置文件等诸多nginx不具备的特性。 5.kcptun 基于KCP协议的UDP隧道，KCP协议能以比 TCP浪费10%-20%的带宽的代价，换取平均延迟降低 30%-40%。 6.k3s - 5 less than k8s 有人搞了个k3s项目，作为轻量级的k8s，整个二进制包只有40M大小。项目定位为边缘计算、IoT、CI等，支持多种硬件平台，裁剪了k8s的很多功能，比如云依赖，存储插件等，甚至连k8s依赖的etcd存储都默认更换为了sqlite3。 7.Drone 基于Golang的Container Native的CD平台，Github上star 17000+，插件的安装也是基于容器的。 8.kaniko 通过Dockerfile来构建docker镜像，需要dockerd进程的支持，这在物理机上操作没有任何问题。而如果要想在容器中通过Dockerfile构建docker镜像却变得困难起来，因为dockerd的运行需要root权限，而容器为了安全是不建议开启root权限的。 该工具可以在容器中不运行dockerd的情况下通过Dockerfile构建出docker镜像。 9.Nacos 阿里巴巴开源的微服务框架，支持配置中心、动态服务发现、动态DNS。 10.PowerDNS Linux下除了bind外的另一个可选择的DNS服务器，数据存储在mysql中，还有一个可选择的漂亮UI。 精彩书籍 《激荡十年，水大鱼大》 要想回顾一下过去的十年中都发生了哪些大事，中国发生了哪些变化，经济领域里有哪些大起大落，本书可以拿来一读。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网件R6400V2刷梅林教程]]></title>
      <url>%2Fpost%2Fr6400-merlin%2F</url>
      <content type="text"><![CDATA[最近突发奇想，想折腾一下路由器。经过研究半天后，锁定了网件R6400这款路由器，原因是可选择的系统较多，跟华硕的路由器架构一致，且支持较为强大的梅林系统。 整个刷机的过程还是非常简单的，虽然花了我不少时间，本文简单记录一下。 拿到手后，R6400比我印象中的要大不少，可以通过下图中的苹果手机进行对比。 R6400有v1和v2两个版本，其中v1版本的CPU频率为800MHz，v2版本的CPU频率为1GHz。v1版本的刷梅林系统和v2版本刷系统有所区别，v2版本需要先刷DD-WRT固件作为过渡固件，然后再刷梅林固件。 整个的过程最好用有线连接路由器操作，用无线会频繁掉线。 .chk结尾的文件为过渡固件，.trx为最终固件。 注意：下载的固件文件最好检验一下md5，确保固件的正确性。 刷dd-wrt固件连接上路由器后，在chrome浏览器中输入http://www.routerlogin.net可跳转到网件管理系统，通过一堆路由器设置后会重启路由器，然后重新登录。 选择“是”后会下载新网件固件，其实该步骤可以选择“否”即可。固件下载完成后，会升级固件，路由器会自动重启。 下载DD-WRT固件文件“DD固件.chk”，并在路由器的管理界面“高级 -&gt; 管理 -&gt; 路由器升级”中上传固件。 这里选择是，然后开始升级固件，升级完成后路由器会重启。 重启完成后，Wifi信号变成dd-wrt，没有密码，可直接连接。 在浏览器中输入192.168.1.1，会出现dd-wrt的界面。用户名和密码可以直接输入admin，因为该系统仅为中间过度系统。在dd-wrt这个偏工程师化的系统中有非常详细的信息，包括路由器的CPU和Memory等的硬件信息，甚至还有load average，多么熟悉的指标。 升级梅林固件在dd-wrt的固件升级中选择“R6400_380.70_0-X7.9.1-koolshare.trx”，刷入梅林固件。待路由器重启完成后，即完成梅林固件的刷入。此时路由器的Wifi SSID变为“NETGEAR”。 访问192.168.1.1，会出现梅林系统的管理界面，依次设置即可。 题外话：无线的密码修改完后，悲剧的事情发生了，路由器重启后居然连不上wifi，提示密码错误。不得不找来一台带有网口的笔记本用有线连接。在梅林管理系统中查看，未发现密码输入错误，明明输入的密码是对的，但SSID换一个密码居然奇迹般的可以无线连接了，怀疑是一个bug。 设置完成后路由器会重启，此时管理系统地址变更为192.168.50.1。 在“高级设置 -&gt; 无线网络 -&gt; 专业设置”中，调整区域为“United States”，据说可以加快速度。 要想使用软件中心，需要在系统设置中开启下图选项，并重启路由器。重启后，Format JFFS partition at next boot会自动设置为false。 ASUS Router下载app “ASUS Router”，可以直接连接到路由器，这是因为网件的路由器架构跟华硕完全一致。 ref 网件Netgear R6400 v2 开箱 刷梅林固件 梅林系统官网 梅林固件下载地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kubernetes中pod无法删除的问题排查]]></title>
      <url>%2Fpost%2Fdocker-rm-failed%2F</url>
      <content type="text"><![CDATA[现象12345678$ cat /etc/redhat-releaseCentOS Linux release 7.2.1511 (Core)$ uname -aLinux c3-a05-136-45-10.com 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux$ docker info | grep &quot;Storage Driver&quot;Storage Driver: devicemapper 在CentOS7.2的系统上，发现有一部分pod在delete后一直处于Terminating状态 12345678$ kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEhttpserver-prod-1-6cb97dfbcc-25dsh 0/1 Terminating 0 55d &lt;none&gt; 10.136.45.6 &lt;none&gt;httpserver-prod-1-6cb97dfbcc-f9flb 0/1 Terminating 0 54d &lt;none&gt; 10.136.45.4 &lt;none&gt;httpserver-prod-1-6cb97dfbcc-m7sl4 0/1 Terminating 0 55d &lt;none&gt; 10.136.45.6 &lt;none&gt;httpserver-prod-1-6cb97dfbcc-pqpht 0/1 Terminating 0 55d &lt;none&gt; 10.136.45.6 &lt;none&gt;httpserver-prod-1-6cb97dfbcc-r987g 0/1 Terminating 0 55d &lt;none&gt; 10.136.45.4 &lt;none&gt;httpserver-prod-1-6cb97dfbcc-zghhr 0/1 Terminating 0 54d &lt;none&gt; 10.136.45.6 &lt;none&gt; 查看docker的日志发现有如下报错信息如下，含义为在删除pod时由于/var/lib/docker/overlay/*/merged目录被其他应用占用，从而导致容器无法清除。 1234Jan 30 14:57:47 c3-a05-136-45-4.com dockerd[1510]: time=&quot;2019-01-30T14:57:47.704641914+08:00&quot; level=error msg=&quot;Error removing mounted layer e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9: remove /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged: device or resource busy&quot;Jan 30 14:57:47 c3-a05-136-45-4.com dockerd[1510]: time=&quot;2019-01-30T14:57:47.704772288+08:00&quot; level=error msg=&quot;Handler for DELETE /v1.31/containers/e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9 returned error: driver \&quot;overlay\&quot; failed to remove root filesystem for e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9: remove /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged: device or resource busy&quot;Jan 30 14:57:48 c3-a05-136-45-4.com dockerd[1510]: time=&quot;2019-01-30T14:57:48.228837657+08:00&quot; level=error msg=&quot;Error removing mounted layer 2851b80d5c45d1cac3e7384116da0ad022af21701f9aa0d9ba3598efd5723030: remove /var/lib/docker/overlay/0ff0f98e1abf43c10711f2804cae3cf37efd597016d38b4753e2af19c2e27eb9/merged: device or resource busy&quot;Jan 30 14:57:48 c3-a05-136-45-4.com dockerd[1510]: time=&quot;2019-01-30T14:57:48.228953497+08:00&quot; level=error msg=&quot;Handler for DELETE /v1.31/containers/2851b80d5c45d1cac3e7384116da0ad022af21701f9aa0d9ba3598efd5723030 returned error: driver \&quot;overlay\&quot; failed to remove root filesystem for 2851b80d5c45d1cac3e7384116da0ad022af21701f9aa0d9ba3598efd5723030: remove /var/lib/docker/overlay/0ff0f98e1abf43c10711f2804cae3cf37efd597016d38b4753e2af19c2e27eb9/merged: device or resource busy&quot; 通过docker ps -a看到容器的状态为”Removal In Progress”。通过docker inspect可以看到容器的进程已经退出了。 123456# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe6b7378c58a3 golang-httpserver &quot;/bin/sh -c &apos;go ru...&quot; 7 weeks ago Removal In Progress k8s_golang-httpserver_httpserver-prod-1-6cb97dfbcc-f9flb_default_9e3d2cbb-f9d4-11e8-b61c-f01fafd10a1b_0# docker inspect e6b7378c58a3 --format &apos;&#123;&#123;.State.Pid&#125;&#125;&apos;0 使用docker rm命令删除容器会报错 12# docker rm e6b7378c58a3Error response from daemon: driver &quot;overlay&quot; failed to remove root filesystem for e6b7378c58a34cb42c6fa7924f7a52b7a19a64b2166d7a56f363e73ecba6e5a9: remove /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged: device or resource busy 通过kubectl delete pods命令虽然可以强制删除pod，但在宿主机上仍然能看到容器的状态为”Removal In Progress”。 123# kubectl delete pods httpserver-prod-1-6cb97dfbcc-f9flb --grace-period=0 --forcewarning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.pod &quot;httpserver-prod-1-6cb97dfbcc-f9flb&quot; force deleted 通过搜索挂载目录的信息，可以找到是哪个进程挂载了该目录。可以看到是ntpd服务挂载了该目录。 12345678910# grep -nr 98a56 /proc/*/mountinfo/proc/2725007/mountinfo:48:296 183 0:183 / /var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/merged rw,relatime shared:88 - overlay overlay rw,lowerdir=/var/lib/docker/overlay/5e2a5f7af24e555a5afacd6a8faa406b42c51d7f2bb4cde22adcea22e0153583/root,upperdir=/var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/upper,workdir=/var/lib/docker/overlay/98a56d695c9e3d0b6a9f3b5e0e60abf7cdb3ce73e976b00e36ca59028e585a36/work# ps -ef | grep 2725007ntp 2725007 1 0 Jan07 ? 00:00:02 /usr/sbin/ntpd -u ntp:ntp -g# ntpd进程的启动时间在容器启动之后# ps -ef | grep ntpdroot 1179644 18205 0 19:52 pts/1 00:00:00 grep --color=auto -d skip -i ntpdntp 3853149 1 0 Jan07 ? 00:00:02 /usr/sbin/ntpd -u ntp:ntp -g 查看ntpd.service文件内容如下，其中PrivateTmp=true，该选项用于控制服务是否使用单独的tmp目录： 123456789101112[Unit]Description=Network Time ServiceAfter=syslog.target ntpdate.service sntp.service[Service]Type=forkingEnvironmentFile=-/etc/sysconfig/ntpdExecStart=/usr/sbin/ntpd -u ntp:ntp $OPTIONSPrivateTmp=true[Install]WantedBy=multi-user.target 问题复现123456789101112131415161718192021222324252627282930313233# 在系统上启动一个容器，此时ntpd必须处于running状态$ docker run -d httpserver:1 /bin/sh -c &quot;while : ; do sleep 1000 ; done&quot;# 启动容器$ docker run -d httpserver:1 /bin/sh -c &quot;while : ; do sleep 1000 ; done&quot;200222b438aac43bbe32a6c54e31ced0848482b9dec3e519d2f847c70c1ce801# 重启ntpd$ systemctl restart ntpd$ docker stop 200222b438aa# 此时容器的相关信息还存在$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES200222b438aa httpserver:1 &quot;/bin/sh -c &apos;while...&quot; About a minute ago Exited (137) 7 seconds ago hardcore_yalow# 强制删除容器失败$ docker rm -f 200222b438aaError response from daemon: driver &quot;devicemapper&quot; failed to remove root filesystem for 200222b438aac43bbe32a6c54e31ced0848482b9dec3e519d2f847c70c1ce801: remove /var/lib/docker/devicemapper/mnt/e53342aa9cf5f43e73b6596f88939b8d3fdefaf1ca03ee95a24d867e1de6c522: device or resource busy# 此时容器处于Removal In Progress状态$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES200222b438aa httpserver:1 &quot;/bin/sh -c &apos;while...&quot; 2 minutes ago Removal In Progress hardcore_yalow# 再次重启ntpd进程$ systemctl restart ntpd# 强制删除成功$ docker rm 200222b438aa200222b438aa 经在如下版本的CentOS7系统实验，该问题不存在。 12345678$ uname -aLinux localhost.localdomain 3.10.0-862.9.1.el7.x86_64 #1 SMP Mon Jul 16 16:29:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux$ cat /etc/redhat-releaseCentOS Linux release 7.5.1804 (Core)$ docker info | grep &quot;Storage Driver&quot;Storage Driver: overlay2 问题产生原因此问题为Systemd启用PrivateTmp选项后，导致mount namespace的一处内核bug。 处理方式在/usr/lib/systemd/system/docker.service的[Service]中增加MountFlags=slave，并重新启动docker服务，注意重启docker后，容器会重启。 当然也可以通过重启ntpd服务的方式来临时解决问题，但当下次删除容器时还需要重启ntpd。 还有一种办法是修改ntpd.service中的PrivateTmp=true，然后重启ntpd服务。 refDocker 故障（device or resource busy）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux Buffer与Cache的含义]]></title>
      <url>%2Fpost%2Flinux-buffer-cache%2F</url>
      <content type="text"><![CDATA[Linux中的Buffer与Cache的含义通常非常容易混淆，两者翻译成中文都可以叫做缓存，都是数据在内存中的临时存储，而且网络上很多文章都是错误的。 1234$ free -h total used free shared buff/cache availableMem: 125G 12G 347M 9.3M 113G 113GSwap: 0B 0B 0B free命令直接将buff和cache写到了一块，说明两者有很多共同点。 12345$ vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 7 1 0 364076 18664 118624552 0 0 214 11198 106 118 6 4 89 1 013 1 0 349096 18664 118638192 0 0 0 1012404 171031 270124 20 13 66 2 0 而通过vmstat命令可以分别看到buffer和cache的大小，单位为KB。 使用man free命令看到的解释如下： 123buffers: Memory used by kernel buffers (Buffers in /proc/meminfo)cache: Memory used by the page cache and slabs (Cached and Slab in /proc/meminfo) 查看proc的man手册结果如下： 1234567891011Buffers %lu Relatively temporary storage for raw disk blocks that shouldn&apos;t get tremendously large (20MB or so).Cached %lu In-memory cache for files read from the disk (the pagecache). Doesn&apos;t include SwapCached.SReclaimable %lu (since Linux 2.6.19) Part of Slab, that might be reclaimed, such as caches.SUnreclaim %lu (since Linux 2.6.19) Part of Slab, that cannot be reclaimed on memory pressure. 上述信息，文档写的并不是非常明确。 可以看出buffers是磁盘数据的缓存，通常不会特别大，缓存的数据包括磁盘的写请求和读请求。内核用于将分散的写磁盘操作集中起来，批量写入磁盘。 Cached是文件数据的缓存，同样可以缓存读请求和写请求。 Slab包括了SReclaimalbe和Sunreclaim两部分信息，其中SReclaimable是可回收部分，SUnreclaim是不可回收部分。 关于文件和磁盘的区别如下： 磁盘是一个块设备，可以划分为多个分区，每个分区上可以构建不同的文件系统，文件系统挂载到目录上后，就可以对该文件系统进行读写文件操作了。 读写普通文件系统中的文件时，会经过文件系统，由文件系统跟磁盘进行交互，而文件系统的缓存为cache。读写磁盘或者分区时，会跳过文件系统，直接对磁盘进行操作，而操作系统对磁盘的缓存称之为buffer。 ref Linux Programmer’s Manual PROC[5]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第9期]]></title>
      <url>%2Fpost%2Fknowledge-share-9%2F</url>
      <content type="text"><![CDATA[富士山&amp;富士吉田市，富士山的海拔高达3776米，远在80公里外的东京都能够看到。令人称奇的是，富士山海拔3360米以上的土地并不是归日本政府所有，而是归富士山上的浅间寺所有，日本政府每年都要支付大量的租金给浅间寺。在富士山周边游览后，突然萌生了登顶富士山的想法，不知是否有志同道合的驴友，可以相约在某年的夏季去一起实现梦想。 资源1.CRIU Linux下的一款实现checkpoint/restore功能的软件，该软件可以冻结某个正在运行的应用程序，并将应用程序的当前状态作为checkpoint存放在磁盘上的文件中，此后正在运行的应用程序会被kill。 此后，可以通过读取磁盘上的文件，恢复之前冻结的应用程序继续执行，而不是从main函数开始执行。 2.bindfs 将一个目录mount到另外一个目录的工具，利用该命令可以将docker中的路径挂载到宿主机上。具体操作命令类似如下： 1234PID=$(docker inspect b991b7ad105f --format &#123;&#123;.State.Pid&#125;&#125;)bindfs /proc/$PID/root /tmp/root# 别忘了卸载目录umount /tmp/root 3.微软亚洲研究院-对联电脑 微软亚洲研究院的自动对对联系统，给出上联后，可以自动给出多个下联，最终生成横批。 4.bcc 基于Linux eBPF的一系列的性能分析工具，包括IO、网络等多个方面。 5.pcstat 基于golang开发的linux下的文件缓存统计工具。 6.Electron 利用前端技术（JavaScript、HTML、CSS）来构建桌面程序的框架，当前很多流行的桌面应用都是使用该技术来开发的，比如VSCode、Slack、Atom等技术。 得益于ES6、V8引擎和Node.js，JavaScript技术已经横跨前端、后端、桌面端的技术栈。 7.Reading-and-comprehense-linux-Kernel-network-protocol-stack 该项目包含了对Linux网络协议栈的源码中文注释，对阅读Linux网络协议栈的代码有一些帮助。 精彩文章 Kubernetes API 与 Operator：不为人知的开发者战争（一） Kubernetes API 与 Operator：不为人知的开发者战争（二） 精彩语句1. “不能用”“不好用”“需要定制开发”，这才是落地开源基础设施项目的三大常态。 – 张磊《深入剖析Kubernetes》 开源项目在落地到公司内部实际使用时，会发现有这样或者那样的问题。开源项目往往是个通用项目，公司在落地时，总有其特殊需求之处，开源软件无法面面俱到，往往只能覆盖一些通用的需求。再加上靠社区来驱动，在bug方面、功能方面跟商业软件也还有较大差距。 娱乐1.《塞尔达传说-旷野之息》 任天堂Switch上的游戏神作，历时四年时间，300人的团队开发，最近一直在玩，已经深深被游戏设计的海拉鲁大陆所折服，完全开放的世界，不同于传统的闯关类游戏，该游戏的自由度非常高，有时候就单纯的在地图中瞎逛都是一种享受，随时都会有惊喜发生。 曾天真的以为，一个单机游戏能好玩到哪里去，但在玩游戏的每一刻都能体会到制作团队的用心，心里总是念到这才是我想要玩的游戏。自从玩了该游戏后，手机上的游戏再也没有打开过。我甚至一度感叹，在国内快糙猛的环境下是产生不了如此细腻良心作品的。如果大家有机会，可以尝试下这款游戏，或许会发现单机游戏还可以做得如此出彩。 2.ZELDA MAPS 同样是跟《塞尔达传说-旷野之息》相关的，由于塞尔达传说的地图实在过于庞大，包含了神庙、驿站、村庄、回忆（没错主人公Link失忆了）、各种支线任务、装备、呀哈哈、各类大小boss、迷宫等等，有玩家制作了一款在线的地图，可以在线查询地图中的各类元素，使用体验类似Google Map。还包含了账号体系，可以在地图上标记自己已经完成的任务。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Dockerfile中的ENTRYPOINT与CMD]]></title>
      <url>%2Fpost%2Fdocker-entrypoint-cmd%2F</url>
      <content type="text"><![CDATA[在Dockerfile中ENTRYPOINT与CMD的功能类似，同时再加上docker run后面追加的容器启动参数，是极其容易混淆的。而且又掺杂着exec模式和shell模式。 这里先说几个结论，有了结论再跟进下面的例子来理解会更容易一些： 实际上docker容器进程的完整启动参数为ENTRYPOINT CMD，如果没有指定ENTRYPOINT，docker会提供一个隐式的值/bin/sh -c。 docker run后面跟的容器启动参数仅会覆盖CMD部分。 exec模式与shell模式CMD和ENTRYPOINT两个命令均支持exec模式和shell模式。 exec模式格式类似CMD [ &quot;top&quot; ]，当容器启动时，top命令的进程号为1。 为了能够获取到环境变量，通常的写法为CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]，此时1号进程为sh。 shell模式的写法为CMD top，docker会以/bin/sh -c top的方式来执行命令，此时容器的1号进程为sh。 如果需要容器进程处理外部信号的情况下，shell模式下信号实际上时发送给了sh，而不是容器中的应用进程。 因此比较推荐使用exec模式，shell模式实际使用较少。 CMD CMD [“param1”, “param2”] 为ENTRYPOINT提供默认参数，需要指定ENTRYPOINT CMD [“executable”,”param1”,”param2”] exec模式 CMD command param1 param2 shell模式 CMD为容器提供默认的启动命令，如果在启动容器时通过命令行指定了的启动参数，则该启动参数会覆盖CMD默认的启动参数。 ENTRYPOINT不能被docker run增加的参数覆盖，启动时要执行ENTRYPOINT的参数。 ENTRYPOINT [“executable”, “param1”, “param2”] exec模式 ENTRYPOINT command param1 param2 shell模式 exec模式当为exec模式时，容器启动时，在命令行上添加的参数会被追加到ENTRYPOINT的参数列表中。 例如： 12FROM ubuntu:latestENTRYPOINT [ &quot;echo&quot;, &quot;hello&quot; ] 执行docker run --rm 0d89e8d4425a world，会输出hello world shell模式当ENTRYPOINT为shell模式时，docker run启动后追加的参数会被忽略。 例如： 123FROM ubuntu:latestENTRYPOINT echo hello 执行docker run --rm 0841e19b4d2e world仅输出hello。 ENTRYPOINT命令的覆盖ENTRYPOINT的命令可以通过docker run中增加--entrypoint选项来使用命令行中指定的参数覆盖ENTRYPOINT的参数。 ENTRYPOINT与CMD的组合使用当同时指定CMD和ENTRYPOINT模式时，实际上为ENTRYPOINT CMD 1234FROM ubuntu:latestENTRYPOINT [ &quot;echo&quot;, &quot;hello&quot; ]CMD [ &quot;world&quot; ] docker run --rm 7edf658370d9会输出hello world，而docker run --rm 7edf658370d9 kitty会输出hello kitty。 更复杂的情况可以参照下图： 如何查看ENTRYPOINT和CMD可以通过docker history ${image} --no-trunc来查生成镜像的所有Dockerfile命令 ref Dockerfile reference Dockerfile 中的 CMD 与 ENTRYPOINT]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第8期]]></title>
      <url>%2Fpost%2Fknowledge-share-8%2F</url>
      <content type="text"><![CDATA[题图为中国铁道博物馆东郊馆中的毛泽东号列车 资源1.Hawkular Hawkular为RedHat开源的监控解决方案，实现语言为java，监控数据的底层存储引擎使用Cassandra，包含了告警功能。目前Github上的Star还较少。RedHat的OpenShift就使用了该监控方案。 2.Kong 基于Nginx OpenResty的API网关，支持自定义插件，支持比原生nginx更多的功能。 3.NuoDB 弹性可伸缩的关系型数据库，兼容SQL标准。将数据库中的事务和存储进行了分离，存储层支持多种存储系统，比如文件系统、Amazon S3和HDFS。因为存储层可以是外部的存储，意味着NuoDB的扩展性会大大增强，使其部署到Kubernetes成为了比较容易的事情。 4.Linux命令hping3 hping3是一个用于生成和解析tcp/ip协议的工具，能够对数据包进行定制，可用于端口扫描、DDOS攻击等，是一个比较常见的黑客工具。 5.Firecracker Amazon开源的轻量级的虚拟机软件，使用KVM来创建和管理虚拟机，整体架构类似Kata Container。容器采用cgroup和namespace来做资源隔离，但是在安全性方面却比较差，轻量级的虚拟机在做到隔离性的同时，又提供了不错的启动速度，是容器领域的一个发展方向。 6.NginxConfig.io NginxConfig.io是一款在线生成nginx配置文件的工具，可以通过点点鼠标，在文本框中内容的方式轻松生成nginx的配置文件。 7.Caddy 一款实用Go语言编写的负载均衡工具，默认启用HTTPS服务，可以使用Let’s Encrypt来自动签发证书。配置文件的写法也比nginx要简洁。 8.loki Grafana团队最新发布的基于Go语言开发的日志聚合系统，loki不会对日志进行全文索引，而是以压缩聚合的方式进行存储，可以对日志流通过打标签的方式进行分组，页面的展示直接使用grafana。对Kubernetes Pod中的log做了特别的支持，比较适合抓取和存储Kubernetes Pod中的log。 个人感觉该工具未来会很火爆，尤其是跟Grafana有着无缝的整合。很多公司会使用ES来作为日志中心的底层存储，但不见得所有的服务都有按照关键字进行匹配搜索的需求，ES作为日志中心就显得不够高效和经济。 9.JSON-RPC json-rpc是rpc通讯中的一种json格式标准，该协议要求request和response的内容必须为json格式，且json有固定的格式。 10.KSQL Apache Kafka的开源SQL引擎，可以使用SQL的形式查询kafka中的消息，该产品跟Kafka一样，同样为Confluent出品。 精彩文章1.北京五环外的真实中国 朋友圈刷屏文章，文章以gif动画的形式描述了社会底层人士的艰辛生活，他们背上扛起的不仅是压得直不起腰来的砖头，而是面对困难努力生活的勇气，有些时候为了生计确实没得选择。 当我们在抱怨生活的同时，可以想想比我们更苦更累却默默承受生活之重的人们，或许心里会好受些。 书籍1.《深入解析Go》 从底层角度分析go语言实现，推荐所有golang开发者一看。 2.深入浅出Serverless：技术原理与应用实践 要想能够对Serverless技术的概念和现状有所了解，该书还是挺合适的。 该书介绍了公有云上的Serverless产品AWS Lambda、Azure Functions，开源项目OpenWhisk、Kubeless、Fission和OpenFasS，提供对这些技术的一站式了解。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TCP TIME_WAIT]]></title>
      <url>%2Fpost%2Ftime-wait%2F</url>
      <content type="text"><![CDATA[time_wait客户端在收到服务器端发送的FIN报文后发送ACK报文，并进入TIME_WAIT状态，等待2MSL（最大报文生存时间）后才断开连接，MSL在Linux中值为30s。 之所以设计time_wait主要用来解决以下异常场景： 确保对端处于关闭状态。主动断开连接一段发送最后一个ack报文，如果丢失，被动断开连接一端会重新发送fin报文。如果主动断开连接一方直接关闭，被动方会一直处于last-ack状态。 防止上一个连接中的包影响新的连接，上一个连接中的包在2MSL中一定可以到达对端。 过多的危害：在客户端占用过多的端口号 解决思路 将net.ipv4.tcp_max_tw_buckets值调小，当TIME_WAIT的数量到达该值后，TIME_WAIT状态会被清除，相当于没有遵守tcp协议 修改TCP_TIMEWAIT_LEN的值，但需要重新编译内核，非常不建议修改 打开tcp_tw_recycle和tcp_timestamps 打开tcp_tw_reuse和tcp_timestamps 采用长连接 tcp有个tcp时间戳选项，第一个是发送方的当前时钟时间戳（4个字节），第二个4字节为从远程主机接收到的最新时间戳 tcp_tw_reusetcp_tw_reuse意思为主动关闭连接的一方可以复用之前的time_wait状态的连接。 复用连接后，这条连接的时间更改为当前时间，延迟数据到达时，延迟数据时间小于新连接时间。 需要连接双方都打开timestamp选项。 该选项适用的范围为作为客户端主动断开连接，复用客户端的time_wait的状态，对服务端无影响。 tcp_tw_recycle内核会在一个RTO的时间内快速销毁掉time_wait状态，RTO时间为数据包重传的超时时间，该时间通过RTT动态计算，远小于2MSL。 需要连接双方都打开timestamp选项。 适用场景为服务端主动断开连接，time_wait状态位于服务端，服务端适用该选项快速回收time_wait状态的连接。 弊端：如果客户端在NAT网络中，如果配置了tcp_tw_recycle，可能会出现在一个RTO的时间内，只有一个客户端和自己连接成功的情况。 4.10之后，Linux内核修改了时间戳生成机制，该选项已经抛弃。 In Action解决time_wait状态过多的比较好的思路为采用http的keepalive功能。 nginxnginx对于upstream，默认是使用http1.0协议的，要想启用keepalive，需要在location中增加 12proxy_http_version 1.1;proxy_set_header Connection &quot;&quot;; 在upstream中增加keepalive参数，这里的参数含义为每个nginx worker连接所有后端的最大连接数。 1keepalive 200; 如果keepalive连接过少，此时由于使用的是http1.1的协议，upstream端不会主动断开连接，nginx会主动断开连接，此时nginx端的time_wait就会过多，会占用端口号，导致nginx端没有端口号可以使用。 ref 关于 Nginx upstream keepalive 的说明 HAProxy and HTTP errors 408 in Chrome 被抛弃的tcp_recycle]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[流量控制算法]]></title>
      <url>%2Fpost%2Frate-limit%2F</url>
      <content type="text"><![CDATA[限流的方式有多种，每种都有其应用场景。 限制请求的方式包括： 丢弃请求 放在队列中，等有令牌后再请求 走降级逻辑 计数器我之前设计的流控系统，以每秒为单位，如果一秒内超过固定的QPS，则将请求进行降级处理。该算法已经在生产环境中平稳运行了很久，也确实满足了业务的需求。 计数器流控算法简单粗暴，有一个缺点，即流控的单位为秒，但一秒的请求很可能是不均匀的，不能进行更细粒度的控制，也不允许流量存在某种程度的突发。 漏桶算法请求先进入漏桶中，漏桶以一定的速度出水，当水流的速度过大时会直接溢出。 漏桶大小：起到缓冲的作用 漏桶的出水速度：该值固定 令牌桶算法令牌桶算法相比漏桶算法而言，允许请求存在某种程度的突发，常用于网络流量整形和速率限制。 系统会恒定的速度往令牌桶中注入令牌，如果令牌桶中的令牌满后就不再增加。新请求来临时，会拿走一个令牌，如果没有令牌就会限制该请求。 这里的请求可以代表一个网络请求，或者网络的一个字节。 涉及到的变量： 网络请求平均速率r：每隔1/r秒向令牌桶中放入一个令牌，1秒共放入r个令牌 令牌桶的最大大小：令牌桶慢后，再放入的令牌会直接丢弃 令牌相当于操作系统中信号量机制。 业界较为出名的流控工具当属Guava中的RateLimiter，基于令牌桶算法实现。 在实际的代码实现中，并不一定需要一个固定的线程来定期往令牌桶中放入令牌，而是在请求到来时，直接计算得出当前是否还有令牌。比如下面的python代码实现： 12345678910111213141516171819202122import timeclass TokenBucket(object): # rate是令牌发放速度，capacity是桶的大小 def __init__(self, rate, capacity): self._rate = rate self._capacity = capacity self._current_amount = 0 self._last_consume_time = int(time.time()) # token_amount是发送数据需要的令牌数 def consume(self, token_amount): increment = (int(time.time()) - self._last_consume_time) * self._rate # 计算从上次发送到这次发送，新发放的令牌数量 self._current_amount = min( increment + self._current_amount, self._capacity) # 令牌数量不能超过桶的容量 if token_amount &gt; self._current_amount: # 如果没有足够的令牌，则不能发送数据 return False self._last_consume_time = int(time.time()) self._current_amount -= token_amount return True ref15行Python代码，帮你理解令牌桶算法]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux iowait]]></title>
      <url>%2Fpost%2Fiowait%2F</url>
      <content type="text"><![CDATA[iowait和load一样，都是非常容易让人产生误解的系统指标。 iowait表示cpu空闲且有未完成的io请求的时间，iowait高并不能反映出磁盘是系统的性能瓶颈。iowait高的时候cpu正处于空闲状态，没有任务可以执行。此时存在已经发出的磁盘io，此时的cpu空闲状态称之为iowait。本质上，iowait是一种特殊的cpu空闲状态。 iowait状态的cpu是运行在pid为0的idle线程上。 cpu此时之所以进入睡眠状态，是因为进程处于睡眠状态，在等待某个特定的事件（比如网络数据，io操作完成等）。 iowait仅能反应磁盘io的指标，并不能反应其他io设备的指标，比如网络丢包。 在io wait的进程处于不可中断状态，通过top命令可以看到进程状态为 由此可见，iowait包含的信息量非常少，仅凭iowait升高不能判断出系统io有问题。要想判断系统io有问题，还需要使用iostat等命令来查看系统的svctm、util、avgqu-sz等指标。 case 1 仅cpu的繁忙程度变化的情况下，会影响到iowait的值。 case 2 在cpu繁忙程序不变的情况下，发起io请求的时间不同也会影响到iowait的值。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux Seccomp]]></title>
      <url>%2Fpost%2Flinux-seccomp%2F</url>
      <content type="text"><![CDATA[seccomp是secure computing mode的缩写，是Linux内核中的一个安全计算工具，机制用于限制应用程序可以使用的系统调用，增加系统的安全性。可以理解为系统调用的防火墙，利用BPF来规律系统调用。 在/proc/${pid}/status文件中的Seccomp字段可以看到进程的Seccomp。 prctl下面程序使用prctl来设置程序的seccomp为strict模式，仅允许read、write、_exit和sigreturn四个系统调用。当调用未在seccomp白名单中的系统调用后，应用程序会被kill。 12345678910111213141516171819#include &lt;stdio.h&gt; /* printf */#include &lt;sys/prctl.h&gt; /* prctl */#include &lt;linux/seccomp.h&gt; /* seccomp's constants */#include &lt;unistd.h&gt; /* dup2: just for test */int main() &#123; printf("step 1: unrestricted\n"); // Enable filtering prctl(PR_SET_SECCOMP, SECCOMP_MODE_STRICT); printf("step 2: only 'read', 'write', '_exit' and 'sigreturn' syscalls\n"); // Redirect stderr to stdout dup2(1, 2); printf("step 3: !! YOU SHOULD NOT SEE ME !!\n"); // Success (well, not so in this case...) return 0;&#125; 执行上述程序后会输出如下内容： 123step 1: unrestrictedstep 2: only 'read', 'write', '_exit' and 'sigreturn' syscallsKilled 基于BPF的seccomp上述基于prctl系统调用的seccomp机制不够灵活，在linux 3.5之后引入了基于BPF的可定制的系统调用过滤功能。 需要先安装依赖包：yum install libseccomp-dev 12345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt; /* printf */#include &lt;unistd.h&gt; /* dup2: just for test */#include &lt;seccomp.h&gt; /* libseccomp */int main() &#123; printf(&quot;step 1: unrestricted\n&quot;); // Init the filter scmp_filter_ctx ctx; ctx = seccomp_init(SCMP_ACT_KILL); // default action: kill // setup basic whitelist seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(rt_sigreturn), 0); seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(exit), 0); seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(read), 0); seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(write), 0); // setup our rule seccomp_rule_add(ctx, SCMP_ACT_ALLOW, SCMP_SYS(dup2), 2, SCMP_A0(SCMP_CMP_EQ, 1), SCMP_A1(SCMP_CMP_EQ, 2)); // build and load the filter seccomp_load(ctx); printf(&quot;step 2: only &apos;write&apos; and dup2(1, 2) syscalls\n&quot;); // Redirect stderr to stdout dup2(1, 2); printf(&quot;step 3: stderr redirected to stdout\n&quot;); // Duplicate stderr to arbitrary fd dup2(2, 42); printf(&quot;step 4: !! YOU SHOULD NOT SEE ME !!\n&quot;); // Success (well, not so in this case...) return 0;&#125; 输入如下内容： 1234step 1: unrestrictedstep 2: only &apos;write&apos; and dup2(1, 2) syscallsstep 3: stderr redirected to stdoutBad system call docker中的应用通过如下方式可以查看docker是否启用seccomp： 12# docker info --format &quot;&#123;&#123; .SecurityOptions &#125;&#125;&quot;[name=seccomp,profile=default] docker每个容器默认都设置了一个seccomp profile，启用的系统调用可以从default.json中看到。 docker会将seccomp传递给runc中的sepc.linux.seccomp。 可以通过—security-opt seccomp=xxx来设置docker的seccomp策略，xxx为json格式的文件，其中定义了seccomp规则。 也可以通过--security-opt seccomp=unconfined来关闭docker引入默认的seccomp规则的限制。 ref Introduction to seccomp: BPF linux syscall filter 如何在Docker内部使用gdb调试器]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第7期]]></title>
      <url>%2Fpost%2Fknowledge-share-7%2F</url>
      <content type="text"><![CDATA[题图为金山岭长城，明代著名抗倭名将戚继光从南方调任至此修筑，为明长城之精华， 资源1.GoAccess 一款开源的实时分析nginx日志的工具，并拥有一个比较强大的dashboard。 2.Wayne 360开源的kubernetes的多集群管理平台。 3.MacKey 一个分享KeyNote模版的网站，每个KeyNote模版都带有动画和图片截图。 4.Nomad Hashicorp公司开源的集群调度工具，该公司另一款较为出名的产品为Vagrant。 5.registrator 该服务部署在宿主机上，自动将docker的容器注册到服务注册中心中，如consul、etcd等。 6.CNI-Genie 华为开源的容器网络解决方案，CNI（Container Network Interface）仅支持加载一个插件，该插件可以同时一次加载多个网络插件，在容器中可以同时存在多个网络解决方案的ip。 7.stress-ng Linux下有一个命令行的压测测试工具stress，可以用来测试cpu、内存、io等，stress-ng提供了更丰富的选项。 8.Resilience4j java版的开源熔断工具Hystrix宣布停止开发，并推荐了Resilience4j工具，该工具灵感来自于Hystrix，主要为java 8和函数式编程设计的自动熔断工具。 9.Standard Go Project Layout 我刚开始写go的时候，一度被golang的源码目录结构所困惑，这个项目提供了一个标准的goalng目录结构的用法，很多开源项目都是按照这个标准组织的。 10.dive docker images不是一个单独的文件存储在宿主机上，而是采用分层设计，以便于多个镜像之间复用相同的层数据。dive可以用来分析docker image的每一层的具体组成。 11.Swoole php号称是世界上最好的编程语言之一，但最为人诟病的是其网络模型是同步模型，导致其性能一直上不去。Swoole可以实现类似于Golang中的goroutine同步编程模型来实现异步的功能。 精彩文章1.知乎社区核心业务 Golang 化实践 本文记录了知乎内部使用golang来重构python的实践经验，用来解决python编程语言的运行效率低和维护成本高的问题。 2.如何在Docker内部使用gdb调试器 本文记录了一些docker关于权限相关的技术实现。 3.ofo剧中人：我不愿谢幕 以记者的角度记录了OFO的发家、辉煌、衰败，曾有过彷徨与迷茫，曾有过野性与嚣张，但最终还是要倒在资本面前。 大家都在吐槽OFO押金退不了的事情，看到一个评论中的不错的点子，可以在OFO的退押金页面增加广告位，毕竟流量就是金钱，退押金页面的流量也是流量，反正押金也退不了，不如借此来一波，至少比在公众号中卖蜂蜜要好的多。 一个生动的细节是，有黑摩的司机不爽共享单车影响他们生意，砸ofo的车。ofo后期转化了一批相当数量的司机当修车师傅，化干戈为玉帛。 上述操作还是非常犀利的，说白了还是利益在作怪。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux TCP backlog]]></title>
      <url>%2Fpost%2Flinux-backlog%2F</url>
      <content type="text"><![CDATA[一个正常的tcp server在处理请求时会经过如下的系统调用：socket() bind() listen() accept() read() write() close()。一个请求在被应用程序读取之前，可能处于SYN_RCVD和ESTABLISHED两种状态。 SYN_RCVD状态是server端接收到了client端的SYN包，server端会将该连接放到半连接队列中，并向客户端发送SYN+ACK包，此时连接处于半连接状态。 ESTABLISHED状态为已经完成了三次握手，但是server端的应用程序还未调用accept系统调用的情况。 这两种情况下都需要操作系统提供相应队列来保存连接状态。 backlog用来设置这两个队列的最大值，但在不同的操作系统中有不同的含义，下面的说明以linux操作系统为准。 其中第一个维护SYN_RCVD状态的队列使用内核参数net.ipv4.tcp_max_syn_backlog来控制，如果队列超过这一阈值，连接会被拒绝。该值默认为1000. 第二个维护ESTABLISHED状态的队列，该队列的长度由应用程序调用listen系统调用时指定。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux中断]]></title>
      <url>%2Fpost%2Flinux-interrupt%2F</url>
      <content type="text"><![CDATA[概念中断由硬件产生，并发送到中断控制器，中断控制器再发送中断到CPU，CPU检测到中断信号后，会中断当前的工作，每个中断都有IRQ（中断请求），基于IRQ，CPU将中断请求分发到对应的硬件驱动上通知操作系统，操作系统会对中断进行处理。 中断控制器常见的中断控制器有两种：可编程中断控制器8259A和高级可编程中断控制器（APIC）。传统的8259A只适合单CPU的情况，现在都是多CPU、多核心的SMP体系，所以为了充分利用SMP体系结构，把中断传递给系统上的每个CPU以便更好实现并行和提高性能，Intel引入了高级可编程中断控制器（APIC）。 光有高级可编程中断控制器的硬件支持还不够，Linux内核还必须能利用这些硬件的特质，所以只有kernel 2.4以后的版本才支持把不同的硬件中断请求（IRQs）分配到特定的CPU核心上，这个绑定技术被称为SMP IRQ Affinity。 在设置网卡中断的cpu core时，有一个限制就是，IO-APIC 有两种工作模式：logic 和 physical，在 logic 模式下 IO-APIC 可以同时分布同一种 IO 中断到8颗 CPU (core) 上（受到 bitmask 寄存器的限制，因为 bitmask 只有8位长。）；在 physical 模式下不能同时分布同一中断到不同 CPU 上，比如，不能让 eth0 中断同时由 CPU0 和 CPU1 处理，这个时候只能定位 eth0 到 CPU0、eth1 到 CPU1，也就是说 eth0 中断不能像 logic 模式那样可以同时由多个 CPU 处理。 软中断和硬中断为了解决中断处理程序执行时间过长和中断丢失的问题，Linux系统将中断分为上半部和下半部。 上半部在中断禁止模式下运行，用来快速处理中断，主要用来处理跟硬件密切相关的工作。 下半部处理上半部未完成的工作，通常以内核线程的方式运行。 以Linux接收网卡数据包为例进行说明： 网卡接收到一个数据包后，会通过硬件中断的方式通知内核新的数据到了。内核会调用中断处理程序进行处理。 上半部将网卡中的数据写入到内存中，并更新一下硬件寄存器的状态，最后发送一个软中断信号，通知下半部进一步的处理。 下半部被软中断信号唤醒后，从内存中读到数据，按照网络协议栈对数据进行解析和处理，并发送给应用程序。 上面所说的上半部即硬中断，下半部即软中断，但一些内核自定义的事件也属于软中断，比如内核调度和RCU锁等。 硬中断：由外设产生，用来通知操作系统外设状态的变化。在处理中断的同时要关闭中断。特点为处理要尽可能的快。 软中断：为了满足实时性要求，硬中断处理时间都比较短，将时间比较长的中断放到软中断中来完成，称为下半部。int就是软中断指令，中断向量表是中断号和中断处理程序的对应表。每个CPU对应一个软中断内核线程ksoftirqd/cpu编号。 12345[root@120-14-29-SH-1037-B07 ~]# ps -ef | grep ksoftroot 3 2 0 2016 ? 01:47:12 [ksoftirqd/0]root 21 2 0 2016 ? 00:47:51 [ksoftirqd/1]root 26 2 0 2016 ? 00:47:34 [ksoftirqd/2]root 31 2 0 2016 ? 00:47:46 [ksoftirqd/3] 中断嵌套：硬中断可以嵌套，即新的硬中断可以打断正在执行的中断，但同种中断不可以。软中断不可嵌套，但相同类型中断可在不同的cpu上执行。 相关命令mpstat123456789101112131415161718192021222324252627282930313233# 显示cpu处理的中断数量[root@103-17-164-sh-100-k07 ~]# mpstat -I SUM 1Linux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU)05:27:59 PM CPU intr/s05:28:00 PM all 61274.0005:28:01 PM all 61712.0005:28:02 PM all 62315.0005:28:03 PM all 59280.00^CAverage: all 61145.25# 显示每个核处理的中断数量[root@103-17-164-sh-100-k07 ~]# mpstat -I SUM 1 -P ALLLinux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU)05:30:30 PM CPU intr/s05:30:31 PM all 61446.0005:30:31 PM 0 40489.0005:30:31 PM 1 6839.0005:30:31 PM 2 6935.0005:30:31 PM 3 7185.00# 显示更详细的信息[root@120-14-31-SH-1037-B07 ~]# mpstat -P ALL 1Linux 3.10.0-327.el7.x86_64 (120-14-31-SH-1037-B07.yidian.com) 09/10/2017 _x86_64_ (4 CPU)01:15:09 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01:15:10 AM all 6.35 0.00 11.64 0.00 0.00 7.67 0.00 0.00 0.00 74.3401:15:10 AM 0 5.05 0.00 11.11 0.00 0.00 0.00 0.00 0.00 0.00 83.8401:15:10 AM 1 6.38 0.00 12.77 0.00 0.00 9.57 0.00 0.00 0.00 71.2801:15:10 AM 2 8.70 0.00 10.87 0.00 0.00 14.13 0.00 0.00 0.00 66.3001:15:10 AM 3 6.32 0.00 12.63 0.00 0.00 7.37 0.00 0.00 0.00 73.68 lspci可以来查看网卡型号，驱动等信息，内容较多 12345678910111213141516171819202122232425262728293031323334353637[root@103-17-164-sh-100-k07 ~]# lspci -vvv | more00:00.0 Host bridge: Intel Corporation Xeon E7 v2/Xeon E5 v2/Core i7 DMI2 (rev 04) Subsystem: Super Micro Computer Inc Device 0668 Control: I/O- Mem- BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx- Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast &gt;TAbort- &lt;TAbort- &lt;MAbort- &gt;SERR- &lt;PERR- INTx- Interrupt: pin A routed to IRQ 0 Capabilities: [90] Express (v2) Root Port (Slot-), MSI 00 DevCap: MaxPayload 128 bytes, PhantFunc 0 ExtTag- RBE+ DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported- RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop- MaxPayload 128 bytes, MaxReadReq 128 bytes DevSta: CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend- LnkCap: Port #0, Speed 5GT/s, Width x4, ASPM not supported, Exit Latency L0s &lt;64ns, L1 &lt;16us ClockPM- Surprise+ LLActRep+ BwNot+ LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk- ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt- LnkSta: Speed unknown, Width x0, TrErr- Train- SlotClk- DLActive- BWMgmt- ABWMgmt- RootCtl: ErrCorrectable- ErrNon-Fatal- ErrFatal- PMEIntEna- CRSVisible- RootCap: CRSVisible- RootSta: PME ReqID 0000, PMEStatus- PMEPending- DevCap2: Completion Timeout: Range BCD, TimeoutDis+, LTR-, OBFF Not Supported ARIFwd- DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled ARIFwd- LnkCtl2: Target Link Speed: 2.5GT/s, EnterCompliance- SpeedDis- Transmit Margin: Normal Operating Range, EnterModifiedCompliance- ComplianceSOS- Compliance De-emphasis: -6dB LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete-, EqualizationPhase1- EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest- Capabilities: [e0] Power Management version 3 Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0+,D1-,D2-,D3hot+,D3cold+) Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME- Capabilities: [100 v1] Vendor Specific Information: ID=0002 Rev=0 Len=00c &lt;?&gt; Capabilities: [144 v1] Vendor Specific Information: ID=0004 Rev=1 Len=03c &lt;?&gt; Capabilities: [1d0 v1] Vendor Specific Information: ID=0003 Rev=1 Len=00a &lt;?&gt; Capabilities: [280 v1] Vendor Specific Information: ID=0005 Rev=3 Len=018 &lt;?&gt;... ethtool用来查看网卡信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[root@103-17-164-sh-100-k07 ~]# ethtool eth3Settings for eth3: Supported ports: [ FIBRE ] Supported link modes: 1000baseT/Full 10000baseT/Full Supported pause frame use: No Supports auto-negotiation: Yes Advertised link modes: 1000baseT/Full 10000baseT/Full Advertised pause frame use: No Advertised auto-negotiation: Yes Speed: 10000Mb/s Duplex: Full Port: FIBRE PHYAD: 0 Transceiver: external Auto-negotiation: on Supports Wake-on: d Wake-on: d Current message level: 0x00000007 (7) drv probe link Link detected: yes# 可查看Ring buffer的大小[root@103-17-164-sh-100-k07 ~]# ethtool -g eth3Ring parameters for eth3:Pre-set maximums:RX: 4096RX Mini: 0RX Jumbo: 0TX: 4096Current hardware settings:RX: 512RX Mini: 0RX Jumbo: 0TX: 512# 列出信息较多，包含网卡的统计信息，包括丢包量信息[root@103-17-164-sh-100-k07 ~]# ethtool -S eth3 | moreNIC statistics: rx_packets: 680955795162 tx_packets: 27260701850 rx_bytes: 248285654162670 tx_bytes: 195321924245892 rx_pkts_nic: 683081802539 tx_pkts_nic: 27260700665 rx_bytes_nic: 251132784690871 tx_bytes_nic: 195447730645152 lsc_int: 11 tx_busy: 0 non_eop_descs: 1811095697 rx_errors: 67381 tx_errors: 0 rx_dropped: 0 tx_dropped: 0 multicast: 1025690658 broadcast: 206937242 rx_no_buffer_count: 0 collisions: 0 rx_over_errors: 0 rx_crc_errors: 67302 rx_frame_errors: 0 hw_rsc_aggregated: 2358414213 hw_rsc_flushed: 232402327 fdir_match: 10634169417 fdir_miss: 669277016191 fdir_overflow: 3321 rx_fifo_errors: 0 rx_missed_errors: 2538 tx_aborted_errors: 0 tx_carrier_errors: 0 tx_fifo_errors: 0 tx_heartbeat_errors: 0 tx_timeout_count: 0 tx_restart_queue: 0 rx_long_length_errors: 80264 rx_short_length_errors: 0 tx_flow_control_xon: 1 rx_flow_control_xon: 0 tx_flow_control_xoff: 51 rx_flow_control_xoff: 0 rx_csum_offload_errors: 0 alloc_rx_page_failed: 0 alloc_rx_buff_failed: 0 rx_no_dma_resources: 0 os2bmc_rx_by_bmc: 0 os2bmc_tx_by_bmc: 0 os2bmc_tx_by_host: 0 os2bmc_rx_by_host: 0# 查看网卡多队列的支持情况，当前网卡支持8个队列，使用了8个队列[root@103-17-6-sh-100-j11 ~]# ethtool -l eth0Channel parameters for eth0:Pre-set maximums:RX: 0TX: 0Other: 1Combined: 8Current hardware settings:RX: 0TX: 0Other: 1Combined: 8# 设置网卡当前使用的多队列，当前使用的网卡数量不能超过最大值8，该值跟网卡的中断数量一一对应，即/proc/interrupts中看到的eth0的中断数量[root@103-17-6-sh-100-j11 ~]# ethtool -L eth0 combined 2[root@103-17-6-sh-100-j11 ~]# ethtool -l eth0Channel parameters for eth0:Pre-set maximums:RX: 0TX: 0Other: 1Combined: 8Current hardware settings:RX: 0TX: 0Other: 1Combined: 2 sar123456789101112131415161718192021# 列出网卡的接收包信息，比iftop更直观[root@103-17-164-sh-100-k07 ~]# sar -n DEV 1Linux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU)05:06:12 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s05:06:13 PM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.0005:06:13 PM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.0005:06:13 PM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.0005:06:13 PM eth3 77893.00 4328.00 31413.92 30134.16 0.00 0.00 46.0005:06:13 PM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00# 可列出错误包的相关信息[root@103-17-164-sh-100-k07 ~]# sar -n EDEV 1Linux 3.10.0-327.10.1.el7.x86_64 (103-17-164-sh-100-k07.yidian.com) 09/09/2017 _x86_64_ (4 CPU)05:07:13 PM IFACE rxerr/s txerr/s coll/s rxdrop/s txdrop/s txcarr/s rxfram/s rxfifo/s txfifo/s05:07:14 PM eth0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0005:07:14 PM eth1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0005:07:14 PM eth2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0005:07:14 PM eth3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0005:07:14 PM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 通过中断可以看到网卡包含四个中断55-58，均位于cpu0上。 12345678910111213141516171819202122232425262728293031323334353637383940[root@103-17-164-sh-100-k07 ~]# cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 0: 44 0 0 0 IR-IO-APIC-edge timer 1: 3 0 0 0 IR-IO-APIC-edge i8042 8: 42 0 0 0 IR-IO-APIC-edge rtc0 9: 1 0 0 0 IR-IO-APIC-fasteoi acpi 12: 4 0 0 0 IR-IO-APIC-edge i8042 16: 99 0 0 0 IR-IO-APIC-fasteoi ehci_hcd:usb1 18: 0 0 0 0 IR-IO-APIC-fasteoi i801_smbus 23: 83 0 0 0 IR-IO-APIC-fasteoi ehci_hcd:usb2 37: 12019917 0 0 0 IR-PCI-MSI-edge 0000:00:1f.2 48: 0 0 0 0 DMAR_MSI-edge dmar0 55: 746827548 0 0 0 IR-PCI-MSI-edge eth3-TxRx-0 56: 2674693551 0 0 0 IR-PCI-MSI-edge eth3-TxRx-1 57: 2341522223 0 0 0 IR-PCI-MSI-edge eth3-TxRx-2 58: 3587929355 0 0 0 IR-PCI-MSI-edge eth3-TxRx-3 59: 3334 0 0 0 IR-PCI-MSI-edge eth3 61: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 63: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 64: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 65: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 66: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 67: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 68: 2 0 0 0 IR-PCI-MSI-edge ioat-msix 69: 2 0 0 0 IR-PCI-MSI-edge ioat-msixNMI: 1100069 693493 635982 615953 Non-maskable interruptsLOC: 1120358899 146726541 4134846029 168005659 Local timer interruptsSPU: 0 0 0 0 Spurious interruptsPMI: 1100069 693493 635982 615953 Performance monitoring interruptsIWI: 57255892 115292160 113458706 112987848 IRQ work interruptsRTR: 0 0 0 0 APIC ICR read retriesRES: 525229423 2791640970 427214674 1986396041 Rescheduling interruptsCAL: 4294536344 4294488238 4294478163 4294552026 Function call interruptsTLB: 65239533 57937650 55104990 52690662 TLB shootdownsTRM: 0 0 0 0 Thermal event interruptsTHR: 0 0 0 0 Threshold APIC interruptsMCE: 0 0 0 0 Machine check exceptionsMCP: 160132 160132 160132 160132 Machine check pollsERR: 0MIS: 0 修改中断的cpu分配echo “2” &gt; /proc/irq/49/smp_affinity 其中2表示cpu1, 49表示中断号。 查看软中断123456789101112[root@120-14-31-SH-1037-B07 ~]# cat /proc/softirqs CPU0 CPU1 CPU2 CPU3 HI: 1 3 0 1 TIMER: 1795378091 3617740778 2674553229 1524071492 NET_TX: 202188392 22218135 17427628 17205883 NET_RX: 3388060179 60871361 68145291 38670323 BLOCK: 4741950 2422 1309 1489BLOCK_IOPOLL: 0 0 0 0 TASKLET: 2102131738 3720214 3104944 1942912 SCHED: 526046585 612421231 496815061 456047989 HRTIMER: 0 0 0 0 RCU: 3147020579 4237695975 3820083676 3267816268 软中断包括10个类别，NET_RX（网络接收中断）、NET_TX（网络发送中断） 查看数据包统计123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112[root@c1-g08-120-166-30 ~]# netstat -sIp: 7586513384 total packets received 0 forwarded 741 with unknown protocol 0 incoming packets discarded 7586512643 incoming packets delivered 7948370396 requests sent outIcmp: 23 ICMP messages received 0 input ICMP message failed. ICMP input histogram: destination unreachable: 1 echo requests: 19 echo replies: 3 46 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 9 echo request: 18 echo replies: 19IcmpMsg: InType0: 3 InType3: 1 InType8: 19 OutType0: 19 OutType3: 9 OutType8: 18Tcp: 561299810 active connections openings 2005002 passive connection openings 8 failed connection attempts 282644949 connection resets received 725 connections established 7585817181 segments received 13957880471 segments send out 1742807 segments retransmited 136 bad segments received. 523811266 resets sentUdp: 445457 packets received 3 packets to unknown port received. 0 packet receive errors 553840367 packets sent 0 receive buffer errors 0 send buffer errorsUdpLite: InErrors: 3TcpExt: 341304 invalid SYN cookies received 1 resets received for embryonic SYN_RECV sockets 1 ICMP packets dropped because they were out-of-window 1997421 TCP sockets finished time wait in fast timer 222787 delayed acks sent 1819 delayed acks further delayed because of locked socket Quick ack mode was activated 178186 times 13 packets directly queued to recvmsg prequeue. 3280604069 packet headers predicted 1972740996 acknowledgments not containing data payload received 798190042 predicted acknowledgments 642444 times recovered from packet loss by selective acknowledgements Detected reordering 23 times using FACK Detected reordering 1618 times using SACK 59 congestion windows fully recovered without slow start 16 congestion windows partially recovered using Hoe heuristic 17089 congestion windows recovered without slow start by DSACK 9594 congestion windows recovered without slow start after partial ack TCPLostRetransmit: 2849 4926 timeouts after SACK recovery 672451 fast retransmits 28946 forward retransmits 680 retransmits in slow start 5024 other TCP timeouts TCPLossProbes: 1390602 TCPLossProbeRecovery: 997235 4723 SACK retransmits failed 178189 DSACKs sent for old packets 979863 DSACKs received 62 DSACKs for out of order packets received 282645553 connections reset due to unexpected data 9 connections reset due to early user close TCPDSACKIgnoredNoUndo: 936061 TCPSpuriousRTOs: 5150 TCPSackShifted: 233309 TCPSackMerged: 733778 TCPSackShiftFallback: 1768422 IPReversePathFilter: 1 TCPRetransFail: 11 TCPRcvCoalesce: 1290965687 TCPOFOQueue: 1753072 TCPChallengeACK: 136 TCPSYNChallenge: 136 TCPSpuriousRtxHostQueues: 73 TCPAutoCorking: 272452106 TCPSynRetrans: 4538 TCPOrigDataSent: 9260789170 TCPHystartTrainDetect: 3721895 TCPHystartTrainCwnd: 64417412 TCPHystartDelayDetect: 80 TCPHystartDelayCwnd: 2579 TCPACKSkippedSynRecv: 4IpExt: InMcastPkts: 249744 OutMcastPkts: 83317 InBcastPkts: 226 InOctets: 12312144006244 OutOctets: 12449967070063 InMcastOctets: 22309104 OutMcastOctets: 9664620 InBcastOctets: 104240 InNoECTPkts: 7586513474 InECT0Pkts: 47 irqbalance查看是否运行：systemctl status irqbalance irqbalance根据系统中断负载的情况，自动迁移中断保持中断的平衡，同时会考虑到省电因素等等。 但是在实时系统中会导致中断自动漂移，对性能造成不稳定因素，在高性能的场合建议关闭。 irqbalance用于优化中断分配，它会自动收集系统数据以分析使用模式，并依据系统负载状况将工作状态置于 Performance mode 或 Power-save mode。处于Performance mode 时，irqbalance 会将中断尽可能均匀地分发给各个 CPU core，以充分利用 CPU 多核，提升性能。处于Power-save mode 时，irqbalance 会将中断集中分配给第一个 CPU，以保证其它空闲 CPU 的睡眠时间，降低能耗。 ref Linux 多核下绑定硬件中断到不同 CPU（IRQ Affinity） 深度剖析告诉你irqbalance有用吗？ 怎么理解Linux软中断？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从0开始学习微服务阅读笔记]]></title>
      <url>%2Fpost%2Fmicro-service%2F</url>
      <content type="text"><![CDATA[本文为极客时间专栏从0开始学习微服务的阅读笔记。 在了解微服务之前，先来了解一下单体应用。 在上学那会，做过一些企业站，技术往往是基于LAMP（Linux + Apache + MySQL + PHP），这种业务较为简单的企业站，就是单体应用。所有的业务代码都是放在一个PHP程序中，代码只需要我自己来维护就可以了，测试、上线、运维全部搞定。 但这种单体应用要是放到稍微有点规模的互联网公司中必然是行不通的。一个公司里有很多的人，不可能公司这么多技术人员共同维护一套代码，这样子团队的写作必然是个问题。系统的健壮性也会比较差，一旦单体应用中的一个模块出问题后往往会影响到其他的模块，导致整个系统不可用，比如PHP的服务业界通常使用php-fpm来管理，而php-fpm处理连接的方式一个请求一个线程，当一部分请求因为延时高时会消耗过多的线程资源，导致其他请求没有可用的线程可以处理。 单体应用的其他缺点不再罗列，比如代码膨胀过度、发布较慢、系统高可用性差、团队协作成本高等。 为了解决单体应用的这些缺点，方法只有一个就是将单体应用拆分为多个服务即服务化，服务之间通过RPC的方式相互调用。 即服务化后，业界又提出了微服务的概念。 维基百科中有如下定义： 2014年，Martin Fowler 与 James Lewis 共同提出了微服务的概念，定义了微服务是由以单一应用程序构成的小服务，自己拥有自己的行程与轻量化处理，服务依业务功能设计，以全自动的方式部署，与其他服务使用 HTTP API 通讯。同时服务会使用最小的规模的集中管理 (例如 Docker) 能力，服务可以用不同的编程语言与数据库等元件实作。 但看这个定义，看的我一脸懵逼，实在太过抽象。其实微服务也没有一个特别明确的定义。 那么服务化和微服务之间有什么不同之处呢？ 服务拆分粒度更细。 每个微服务都独立部署和维护。 微服务需要服务治理。由于微服务会将服务变多，势必需要一个服务管理平台来对微服务进行管理。 将单体应用向微服务拆分的方式可以分为纵向拆分和横向拆分。这里以一点资讯app为例来解释纵向拆分和横向拆分。 一点资讯分为信息流、正文页等模块，而这些功能都依赖于用户信息获取模块，纵向拆分即将信息流、正文页拆分为微服务，横向拆分即将信息流、正文页都依赖的用户信息获取模块拆分为单独的服务。 微服务架构采用微服务后会带来一系列复杂的问题，新技术在解决了一部分问题的同时，总会带来一些新的问题，比如服务怎么定义接口、服务的发布方式和服务发现、服务的监控、服务的治理（包括依赖关系梳理、熔断机制等）、故障快速定位等。 那么一个标准的微服务架构应该长什么样子呢？ 服务提供方在服务启动时向服务注册中心注册服务，声明自己能够提供的服务及当前服务的地址等信息。 服务调用者请求注册中心，查询所要调用服务的地址，并通过约定好的协议向服务提供者发起请求，获取到结果后按照数据协议格式将数据进行反序列化。 在整个服务的调用过程中，服务的请求耗时、调用量、调用成功与否等信息都会作为监控记录下来，服务的调用关系会通过trace系统记录下来，以便用于后续的故障定位和追踪。如果服务调用失败，则需要服务治理的方式来保证调用方的正常运行。 服务的接口定义需要解决的问题是服务的接口有哪些？每个接口的输入什么？每个接口的输出是什么？ RESTful APIHTTP协议的接口定义通常使用该协议，常见Wiki或者Swagger的方式来管理。 IDL文件IDL（interface description language）用于描述接口，使得不同的编程语言不同的平台服务之间可以相互通讯。常见实现包括Thrift和protobuf，protobuf作为序列化方式的一种，通常会使用gRPC进行通讯。 XML文件方式服务提供者将接口描述信息保存在xml配置文件，服务启动后会加载配置文件，将服务暴露出去。 服务消费者将要调用的接口信息写在xml配置文件中，进程启动后加载xml配置文件。 坑服务提供者通常需要提供服务的超时时间等参数，而消费者端也需要该信息，为了让消费者端能看到生产者端的配置，可以将信息放在配置中心中，但这却增大了配置中心的内容，当一旦配置变化需要同步时，同步的数据会变多。 注册中心要想实现服务之间的调用，通常会使用反向代理和服务注册中心的方法。 反向代理的方法业界一般使用较多的包括nginx、haproxy以及业界新秀envoy等。 在微服务架构中更多提及的方案为采用服务注册中心的方法，而注册中心的稳定性就显得尤其重要。 业界注册中心采用较多的方案为zookeeper、etcd、consul、eureka。这些注册中心的存储数结构要么是树状接口，要么是key-value形式，其中k-v形式的可以变更key的值以实现类似树状结构。 注册中心的设计注册中心API及功能注册中心需要提供以下api以供服务提供方和服务调用方使用。 服务注册接口，提供服务提供方使用 服务反注册接口，提供服务提供方以便销毁服务 心跳汇报接口，服务提供方通过心跳汇报服务是存活状态的。一旦当服务出现异常时，服务注册中心应该立即将服务从注册中心剔除。 服务订阅接口，服务调用方用于获取服务提供方的实例列表 服务变更接口，服务调用方用于获取最新可用服务。一旦注册中心探测到有新的服务实例或者实例减少，应该立即通知所有订阅该服务的服务调用者更新本地的节点信息。 注册中心存储哪些服务信息通常可以按照“服务名-分组-节点信息”三层结构来存储，其中节点信息包括：节点ip地址、节点端口号、请求失败时重试次数、请求结果是否压缩。 分组的划分原则包括： 按照业务的核心程度 按照机房维度 线上环境、测试环境 注册中心如何工作服务提供者注册节点 查看注册节点是否在白名单内，即是否可以向注册中心注册 查看注册的服务名、服务分组是否存在 将节点信息添加到对应的存储位置 服务提供者反注册 查看服务名、服务分组对应的服务是否存在 将节点删除 服务消费者查询节点信息 从本机内存中查找服务信息 如果有本地快照存储可以从中查找 服务消费者订阅服务变更 消费者获取到服务信息后，在本地保留cluster的sign值 每个一段时间从注册中心获取cluster的sign值，如果不一致，就从注册中心拉取服务节点，并更新内存环境和本地快照 服务框架完整的服务框架包括：通讯框架、通讯协议、序列化和反序列化。 开源RPC框架跟语言相关的框架：Dubbo、Motan（微博）、Tars（腾讯）、Spring Cloud 跨平台的开源RPC框架：gRPC、Thrift 服务监控常用的开源监控软件包括：ELK、Graphite、TICK、Prometheus 服务追踪使用分布式会话跟踪技术，利用traceid 开源方案包括OpenZipkin、jaeger等 服务治理通过一系列的手段保证在意外情况下，服务仍然能够正常运行。 节点管理服务调用失败可能是服务提供者自身出现了问题，也可能是网络问题导致。 1.注册中心自动摘除机制 服务提供者和注册中心之间保持心跳，当超时后注册中心自动摘除服务提供者。 2.服务消费者摘除 将服务提供者的探活机制放到消费者端，消费者在探测到服务提供者失败后自动摘除服务提供者。这种情况可以避免服务提供者和注册中心之间网络出现异常，但是服务提供者和服务消费者之间可以通讯的情况。 负载均衡常用的包括随机算法、轮询、加权轮询算法、最少活跃调用、一致性hash算法（可以配合着静态注册中心达到比较好的效果） 自适应最优选择算法：在客户端维护一份每一个服务节点的性能统计快照，每隔一段时间去更新快照。在发起请求时，根据二八原则，将服务节点分成两部分，找出20%的那部分响应最慢的节点并降低权重。也可称为动态加权轮询算法。1分钟的更新时间间隔是个不错的选择。 个人感觉自适应最优选择算是个不错的选择，但还可以针对业务场景继续优化，比如权重进行动态调整。 服务路由用于限定服务消费者可选择服务提供者节点的范围。 应用场景：分组调用（组的划分可以按照机房等维度）、灰度发布、流量切换（比如机房故障后，用于机房之间的流量调度）、读写分离（读接口部署在一起，写接口部署在一起）。 规则的写法A.条件路由 某个ip的消费者仅访问某个ip的服务提供者 排除某个服务节点（所有的服务消费者都不能访问某个服务提供者） 白名单和黑名单 机房级别的隔离（可以基于ip地址的规则来做） 读写分离（所有get类方法仅访问某些节点等） B.脚本路由 使用脚本语言的形式来描述 路由的获取方式 本地配置：存储在服务消费者本地 配置中心配置，可以修改规则后动态下发 服务容错手段包括超时、重试、双发、熔断等。 双发的思路为服务调用者在发起一次服务调用后，在给定的时间内（该时间要比服务超时的时间短）没有得到结果，再发起另外一个请求。 熔断的思路为在某一个时间内如果服务调用失败次数超过一定值，则触发熔断，不再向服务提供者发起请求。 断路器中的状态包括：Closed、Open、Half Open（半打开状态，用于探测后端服务是否已经正常） Hystrix是最出名的熔断器，计算服务调用的失败率是通过滑动窗口来实现，滑动窗口内包含10个桶，每个桶为1秒内的服务调用情况。 FailOver，失败自动切换。消费者调用失败后，自动从可以节点中选择下一个节点重新调用，可设置失败的次数。通常适合只读的场景。 FailBack，失败通知。调用失败后，不能重试，而是根据失败的信息来决定后续的执行策略。通常用于写场景。 FailCache，失败缓存。调用失败后，隔一段时间后再重试。 FailFast，快速失败。调用失败后不再重试。 如何识别服务节点是否存活如果注册中心为zk，在服务节点变化的时候，注册中心会向服务调用方推送服务节点变化的通知。但当网络抖动的时候，可能会存在节点的状态频繁变化的问题，导致服务消费者频繁收到节点变更通知，或者导致注册中心获取到的服务节点过少。可通过以下手段来避免该问题。 服务端故障时的应对策略故障包括：集群故障、单个idc故障、单机故障 集群故障比如触发bug、突发流量等 限流：限制超出接口阈值的部分请求 降级：一种思路为通过开关来实现。具体可以分为多种等级：一级降级对业务影响比较小，可以设置为自动降级；二级降级对业务有一定影响，设置为手工降级；三级降级需要谨慎操作。 单idc故障基于dns的流量切换：延时稍高，比较适合入口流量。 基于rpc的分组流量切换：将之前单个机房内访问的流量切换为多个机房访问。 单机故障可以通过自动重启服务的手段来解决。但要避免一次性重启服务过多的问题。 动态注册中心的保护机制 心跳开关保护机制，给注册中心设置一个开关，当开发打开时，即使网络频繁抖动，注册中心也不会通知消费者节点变更，后者设置一定的百分比打开该开关。正常情况下该开关可以不打开。 服务节点摘除保护机制，设定一个阈值比例，在出现网络抖动的情况下，注册中心也不会将超过这个阈值的节点给下掉，防止一下子下掉过多节点。该机制正常情况下，应该开启。 静态注册中心的保护机制在服务消费者端来判断是否服务提供者是否存活。服务消费者调用某一个节点失败超过一定次数就将节点标记为不可用。并隔一段时间后再去探测该节点是否存活。 注册中心的服务正常情况下不改变的，只有当服务在发布的时候才去修改注册中心中的节点。注册中心中的节点变化后仍然通知服务消费者，只是在网络出现抖动的时候，不再去通知。 个人感觉这个思路更靠谱。 服务治理平台1.服务管理 包括服务上下线、节点添加和删除、服务查询、服务节点查询等 2.服务治理 限流、降级、切流量 3.服务监控 可以包含服务的tracing监控等 4.问题定位 5.日志查询 6.服务运维 发布部署和扩缩容 配置中心业内采用的开源配置中心包括: Spring Cloud 功能相对较弱，变更配置需要通过git操作 Apollo 对Spring Boot的支持比较好 进阶内容做好容量规划包括容量评估和调度决策两方面。 压测服务的单机最大容量可以采用区间加权的方式来计算，比如0 ~ 10ms区间权重为1，10 ~ 50ms区间权重为2,500ms以上权重为32，通过累加的方式计算出单机的权重，从而评估出单机最大容量。(该评估容量的方式非常实用) 调度策略可以根据水位线来做决定，一条是安全线，一条是致命线。当水位线处于致命线需要立即扩容，当水位线回到安全线以上时可以进行缩容。（水位线的方式很赞） 缩容的思路是采用逐步缩容，每隔5分钟判断一次水位线是否在致命线以上，并按照10%、30%的比例进行缩容。 为了防止水位线的抖动，可以一分钟采集一个点，当5个点中的3个点都满足条件时才进行缩容。 多机房部署1.主从架构 所有的写请求都发给主机房，主机房更新本机房的缓存和数据库，其他机房的缓存和数据库从主机房同步。主机房出现问题后，就没法更新了。 2.独立机房架构 缓存层，每个机房都有写请求，每个机房的写请求通过消息同步组件将写请求同步给另外一个机房。数据库mysql只有一个主库，另外机房的mysql同步数据到该机房。 以上缓存消息同步组件的实现大概如下： 包括两个模块reship和collector，reship负责将本机房的写请求发一份给别的机房，collector负责从别的机房读取写请求，并发给本机房的处理服务器。 可以通过消息队列和rpc调用来实现reship和collector的通讯。 3.多机房的数据一致性 可通过消息对账机制来保证一致性，原理为通过一个单独的程序后面来验证是否一个写请求是否已经被所有的机房都处理，如果没有处理，则重新发送写请求。 混合云部署公有云上为了安全往往不部署数据库 DevOps实践持续集成：确保每一次代码的merge request都通过，分为四个阶段：build（开发分支代码的编译与单元测试）、package（开发分支代码打包成docker镜像）、deploy（开发分支代码部署到测试环境）、test（集成测试）。包括了代码检查和单元测试环节。 持续交付：代码merge request到develop分支后，develop分支的代码能够在生产环境中测试通过，并进行小流量灰度验证，可以随时上线。分为五个阶段：build（develop分支的代码编译与单元测试）、package（develop分支代码打包）、deploy、test、canary（develop分支代码的小流量灰度验证）。 持续部署：合并develop代码到master分支，并打包成docker镜像，并可随时上线。包括：build、package、clear、production。 Service MeshService Mesh技术以轻量级网络代理的方式与应用代码部署在一起，主要有两个关键点技术。 SideCar用于转发服务之间的调用，在服务消费者端SideCar用于将请求转发到服务提供者端的SideCar中，在服务提供者端的SideCar在接收到请求后转发给本机上的服务提供者。 SideCar实现方式有基于ipstables的网络拦截和直接转发请求两种方式。 Control plane用于基于SideCar的服务调用的治理，用来取代微服务中需要服务框架干的事情，包括服务发现、负载均衡、请求路由、故障处理、安全认证、监控上报、日志记录、配额限制等。 IstioPilot主要用于流量控制，包括Rules API（提供API，用于流量控制）、Envoy API（给Envoy提供API，获取服务注册信息、流量控制信息等）、抽象模型（对服务注册信息、流量控制进行抽象）、平台适配层（适配k8s、Mesos等多个平台，将平台特定的注册信息转换成平台无关的抽象模型）。 Pilot的流量控制功能包括服务发现和负载均衡、请求路由、超时重试、故障注入。 Mixer实现策略控制和监控日志收集等功能，理论上Envoy发起的每次请求都会发送到Mixer，可以异步发送。 Mixer的策略控制包括对服务的访问频率限制和访问控制。 Citadel用于保证服务之间的安全，需要Envoy的配合。Citadel中存储了秘钥和证书，通过Pilot将授权策略和安全命名信息分发给Envoy，Envoy和Envoy之间通过双向TLS证书来进行通讯，由Mixer来管理授权和审计。 ref微服务架构技术栈选型手册]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TCP协议中的Nagle算法]]></title>
      <url>%2Fpost%2Fnagle%2F</url>
      <content type="text"><![CDATA[Nagle算法为了避免网络中存在太多的小数据包，尽可能发送大的数据包。定义为在任意时刻，最多只有一个未被确认的小段。小段为小于MSS尺寸的数据块，未被确认是指数据发出去后未收到对端的ack。 Nagle算法是在网速较慢的时代的产物，目前的网络环境已经不太需要该机制，该算法在linux系统中默认关闭。 延时ACK机制: 在接收到对端的报文后，并不会立即发送ack，而是等待一段时间发送ack，以便将ack和要发送的数据一块发送。当然ack不能无限延长，否则对端会认为包超时而造成报文重传。linux采用动态调节算法来确定延时的时间。 可以举例来描述一下，client连续向server端发送两个小于MSS的数据包。client发送第一个数据包，根据Nagle算法，此时没有未确认的数据段，该数据包可以直接发送。server端接收到数据包后，由于延时ACK机制，并不会立即发送ack，而是需要等到延时ack机制超时后再发送第二个数据包。此时client端由于Nagle算法， 存在一个未被确认的数据包，不能向server端发送第二个数据包。 在延时要求尽量小的情况下，并不适合用Nagle算法，比如SSH会话。可以通过设置TCP_NODELAY来完成。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[部署java应用到容器]]></title>
      <url>%2Fpost%2Fcontainer-java%2F</url>
      <content type="text"><![CDATA[java 8u131之后的版本开始支持容器特性，之前的版本中并不支持容器相关的特性。 java基础知识JVM默认的最大堆内存大小为系统内存的1/4，可以使用参数-XX:MaxRAMFraction=1表示将所有可用内存作为最大堆。 cgroup的限制在docker中能够看到，通过查看/sys/fs/cgroup目录下的文件可以获取。 JVM的用户地址空间分为JVM数据区和direct memory。JVM数据区由heap、stack等组成，GC是操作的这一片内存。direct memory是额外划分出来的一片内存空间，需要手工管理内存的申请和释放。 direct memory使用Unsafe.allocateMemory和Unsafe.setMemory来申请和设置内存，是直接使用了C语言中的malloc来申请内存。由jvm参数MaxDirectMemorySize来限制direct memory可使用的内存大小。 java &lt; 8u131没有对容器的任何支持，对cpu和内存的限制需要通过jvm的参数来配置。 java中并不能看到内存资源的限制，会存在使用内存超过限制而被OOM的问题。可通过在程序中设置-Xmx来解决该问题。 JVM GC（垃圾对象回收）对Java程序执行性能有一定的影响。默认的JVM使用公式“ParallelGCThreads = (ncpus &lt;= 8) ? ncpus : 3 + ((ncpus * 5) / 8)” 来计算做并行GC的线程数，其中ncpus是JVM发现的系统CPU个数。一旦容器中JVM发现了宿主机的CPU个数（通常比容器实际CPU限制多很多），这就会导致JVM启动过多的GC线程，直接的结果就导致GC性能下降。Java服务的感受就是延时增加，TP监控曲线突刺增加，吞吐量下降。 显式的传递JVM启动参数-XX:ParallelGCThreads告诉JVM应该启动几个并行GC线程。它的缺点是需要业务感知，为不同配置的容器传不同的JVM参数。 java9 and java &gt;= 8u131增加了XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap参数来检查内存限制。JVM中可以看到cgroup中的内存限制。 可以根据容器中的cpu限制来动态设置GC线程数，不再需要单独设置-XX:ParallelGCThreads。 java10jvm参数XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap已经默认开启，但新增加-XX:-UseContainerSupport参数来更好支持容器，支持内存和cpu。 在开启-XX:-UseContainerSupport的同时，XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap会被关闭。 ref Java SE support for Docker CPU and memory limits 美团容器平台架构及容器技术实践]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第6期]]></title>
      <url>%2Fpost%2Fknowledge-share-6%2F</url>
      <content type="text"><![CDATA[题图为公司楼下公园的杨树林。时光易逝弹指间，又到一年叶落时。 资源1.runV 基于 hypervisor 的 OCI runtime 2.operator-sdk operator机制利用CRD机制增强了kubernetes的灵活性，但operator的编写代码很多模式都是固定的，该项目提供了更高层次的抽象。 3.orchestrator 用来管理mysql的集群拓扑和故障自动转移的工具。 4.Tars 腾讯开源的RPC框架，在腾讯内部已经有多年的使用历史，目前支持多种语言。 5.ngx_http_dyups_module nginx module，可以提供Restful API的形式来动态修改upstream，而不用重新reload nginx。 6.Dragonfly 阿里巴巴开源的基于P2P的容器镜像分发系统。 7.SonarQube 开源的代码检查和扫描工具，支持多种语言，并提供了友好的web界面用来查看分析结果。 8.QUIC QUIC是Google开发的基于UDP的传输层协议，提供了像TCP一样的数据可靠性，但降低了数据的传输延时，并具有灵活的拥塞控制和流量控制。 9.OpenMessaging 阿里巴巴发起的分布式消息的应用开发标准，目前github上的star数还较少。 10.nsenter nsenter是一个命令行工具，用来进入到进程的linux namespace中。 docker提供了exec命令可以进入到容器中，nsenter具有跟docker exec差不多的执行效果，但是更底层，特别是docker daemon进程异常的时候，nsenter的作用就显示出来了，因此可以用于排查线上的docker问题。 精彩文章1.为何程序员永远是高薪行业 从记者的视角来了解阿里云的历史。 2.Harbor传奇（1）- Harbor前世 3.蚂蚁金服 Service Mesh 实践探索 4.美团容器平台架构及容器技术实践 美团内部的容器平台HULK已经从第一代的自研升级为第二代的基于kubernetes的容器管理平台。由此可以反映出kubernetes在容器管理领域的地位。 5.Serverless：后端小程序的未来 Serverless是未来软件架构的一个演进方向，包括BasS（Backend as a Service，后端即服务）和FaaS（Functions as a Service，函数即服务）两个组成部分。 BaaS包括对象存储、数据库、消息队列等服务，并以API的形式提供应用依赖的后端服务。 FaaS中的运行是通过事件触发的方式，代码执行完成后即运行结束，因此代码必须是无状态的。FaaS平台负责服务的自动扩容，并可做到按照服务的使用资源付费，以节省大量开支。 Serverless给开发人员带来了非常大的便利性，但同时也软件跟云平台绑定特别紧密。 图书1.《奈飞文化手册:“硅谷重要文件”的深度解读》 Netflix公司的技术文化一直非常被业界推崇，可以从Netflix OSS已经开源的软件项目，很多的开源项目在社区也有不错的影响力，本书值得每一位技术从业者一读。 精彩句子 我们要求大家做出的任何举动，出发点都是以对客户和公司最有利为出发点，而不是试图证明自己正确。 - 奈飞文化手册:“硅谷重要文件”的深度解读]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[golang使用pprof分析程序性能瓶颈]]></title>
      <url>%2Fpost%2Fgolang-pprof%2F</url>
      <content type="text"><![CDATA[golang中的pprof工具可以分析系统问题，开启pprof功能非常简单，即在import中增加_ &quot;net/http/pprof&quot;的导入即可，然后通过http调用/debug/pprof接口即可在web界面上看到pprof的相关信息。 golang 1.11版本中已经自带了火焰图功能，火焰图为性能分析的利器，可以快速找到程序性能的瓶颈。不再需要使用go-torch项目。 查看火焰图需要用到Graphviz工具，该工具需要单独安装。 Graphviz工具运行的服务器系统为CentOS，使用下载源码包的方式进行安装。 依次执行下面命令： 123./configuremakemake install 例子本文的使用环境：服务器程序为transfer，运行在linux系统中。执行go tool pprof命令运行在linux服务器10.103.17.184上。 程序运行后调用程序的/debug/pprof/profilehttp接口，可获取到cpu profile数据文件。例如，在服务器上可以执行运行wget http://10.103.34.138:3300/debug/pprof/profile命令，将profile文件下载到另外一台分析的服务器上。 在分析服务器上执行go tool pprof -http=&quot;10.103.17.184:20000&quot; transfer profile 在浏览器中打开10.103.17.184:20000即可得到性能分析的结果。 同样也可以在本机访问远程的程序暴露的pprof数据，使用命令如：go tool pprof -http :9090 http://10.66.161.43:10245/debug/pprof/heap]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[nsenter的用法]]></title>
      <url>%2Fpost%2Fnsenter%2F</url>
      <content type="text"><![CDATA[nsenter是一个命令行工具，用来进入到进程的linux namespace中。 docker提供了exec命令可以进入到容器中，nsenter具有跟docker exec差不多的执行效果，但是更底层，特别是docker daemon进程异常的时候，nsenter的作用就显示出来了，因此可以用于排查线上的docker问题。 CentOS用户可以直接使用yum install util-linux来进行安装。 启动要进入的容器：docker run -d ubuntu /bin/bash -c &quot;sleep 1000&quot; 获取容器的pid可以使用` 要进入容器执行如下命令： 1234# 获取容器的piddocker inspect 9f7f7a7f0f26 -f &apos;&#123;&#123;.State.Pid&#125;&#125;&apos;# 进入pid对应的namespacesudo nsenter --target $PID --mount --uts --ipc --net --pid ref nsenter man page nsenter GitHub]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第5期]]></title>
      <url>%2Fpost%2Fknowledge-share-5%2F</url>
      <content type="text"><![CDATA[题图为北京城西部的潭柘寺，始建于西晋年间，有“先有潭柘寺，后有幽州城”的说法。 明朝燕王朱棣听取了重臣姚广孝的建议后，起兵“靖难”，并成功夺取皇位。朱棣继皇帝位后，姚广孝辞官到京西的潭柘寺隐居修行。据说当年修建北京城时，设计师就是姚广孝，他从潭柘寺的建筑和布局中获得了不少灵感。 资源1.virtual-kubelet 很多公有云厂商都提供了弹性容器服务实例，比如阿里云的ECI（Elastic Container Instance）、AWS Fargate、Azure Container Instances等，但这些平台都提供了私有的API，与kubernetes的API不兼容。该项目将公有云厂商的的容器组虚拟为kubernetes集群中的一个超级node，以便支持kubernetes的API，与此同时失去了很多kubernetes的特性。 2.Kata Containers 容器在部署服务方面有得天独厚的优势，但受限于内核特性，在隔离性和安全性方面仍然较弱。虚拟机（VM）在隔离性和安全性方面都比较好，但启动速度和占用资源方面却不如容器。Kata Containers项目作为轻量级的虚拟机，但提供了快速的启动速度。同时支持Docker容器的OCI标准和kubernetes的CRI。目前华为公有云已经将此技术用于生产环境中。 3.Knative 在今年的Google Cloud Next大会上，Google发布了Knative, 这是由Google、Pivotal、Redhat和IBM等云厂商共同推出的Serverless开源工具组件，它与Istio，Kubernetes一起，形成了开源Serverless服务的三驾马车。 4.naftis 小米信息部武汉研发中心开源的istio的dashboard。 5.kubespy 用来查看kubernetes中资源实时变化的命令行工具。 6.Md2All 如果你已经习惯了markdown写作，在微信公账号发文时，可以使用该工具渲染后，将文章复制到微信公众号后台。 精彩文章1.阿里云的这群疯子 从记者的视角来了解阿里云的历史。 2.王垠最近博客-更新一下 曾经以天才自居桀骜不驯傲视一切的垠神，突然变得温顺了许多，开始意识到自己的缺点，开始享受生活。 3.为何“秀恩爱，死得快”？我是认真的 用量子物理学的知识来解释为啥“秀恩爱，死得快”。 4.CTO、技术总监、首席架构师的区别 5.面对云厂商插管吸血，MongoDB使出绝杀 半年后看下MongoDB的修改开源协议的做法在国内奏效否。 6.终于明白了 K8S 亲和性调度 通过该文章，已经差不多可以了解kubernetes调度的亲和性、反亲和性、taint和toleration机制了。 7.微软资深工程师详解 K8S 容器运行时 图书1.鸟哥的Linux私房菜：基础学习篇（第四版） 鸟哥的linux私房菜终于出新版了，最新版本是基于CentOS7的。 电影1.嗝嗝老师 电影讲述了印度贫民窟中的孩子在学校上学总是遭受歧视不爱学习各种调皮捣蛋，在一位新老师来了后，将学生们带向正轨的故事。印度电影总能将平凡的电影演绎的很魔性，单就这些故事就已经足够了。偏偏这位老师还是抽动秽语综合征患者，在受到老师和学生们的双重歧视下，给故事情节增加了许多感人和励志色彩。 强迫症患者谨慎观看，看完电影后，总感觉得抽搐两下才舒服。 精彩句子 货币的贬值，是永恒的趋势。明天的物价，一定比今天的贵。你想要赚钱，就一定要把今天的钱，换成明天的物。而且时间越紧凑越好。 – 八年之后 房价多少？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第4期]]></title>
      <url>%2Fpost%2Fknowledge-share-4%2F</url>
      <content type="text"><![CDATA[今年以来，网络上一直在传言济南市要吞并莱芜市的消息，最近几天尤甚。市民们纷纷去市政府门前拍照留念，纪念莱芜市的最后一天，虽然到今天为止传言还未变成现实，但应该是迟早要到来的。 有趣的是，莱芜市是1993才从泰安市中独立出来，很多人都感慨道：出生是泰安人，长大是莱芜人，明天变成济南人。地理位置上而言，莱芜跟济南搭界，而且莱芜市是山东省17地市中面积最小的一个。 山东的发展策略一直是各地市全面发展，济南市作为省会，在中国城市中的存在感确实不够强，吞并莱芜后也一样不会变强太多，一个地市要想变强，要从多方面找原因。 资源1.k8s-deployment-strategies kubernetes内置的deployment和statefulset对象往往很难满足企业的部署需求，比如蓝绿发布、金丝雀发布等，Github上的项目介绍了其他部署方式在kubernetes上的具体实现方式。 2.Let’s Encrypt https越来越普及，通常CA颁发的证书都是收费的，Let’s Encrypt是一家非盈利的CA机构，为广大的小型站点和博客博主提供了非常大的帮助。Github pages中也是采用了Let’s Encrypt来提供自定义域名的https服务支持。 3.openmetrics 监控领域存在多款开源软件，比如premetheus、influxdb、opentsdb等，每种软件的写入数据格式都不一致，该开源项目旨在定义监控数据的标准格式，目前支持premetheus的文本格式和protobuf两种格式。该项目目前还在起步阶段，已经加入CNCF，期待后续一统行业标准。 4.NATS Go语言实现的消息队列，目前已经加入CNCF。 5.sequel fumpt sql的在线格式化工具。 6.The Linux Audit Project Linux下的日志审计工具，CentOS系统下默认安装，可以通过man auditd看到该工具的说明。 7.kafkabridge 360开源的kafka客户端库的封装，只需调用极少量的接口，就可完成消息的生产和消费。支持多种语言：c++/c、php、python、golang。 精彩文章1.Keyhole,Google Maps发展史 文章为微信公众号余晟以为的系列文章，大部分素材来源于《Never Lost Again》一书，该书作者为Bill Kilday，Keyhole和Google Maps团队的核心成员。文章介绍了Google Maps的前身Keyhole的创业史，后被Google收购后，又推出了基于web的Google Maps产品，继而开发出了Google Earth产品。即使在Google内部，也存在团队之间的孤立及不信任问题。 Keyhole，Google Maps前传 Google Maps，Keyhole后传 从Google Maps到Google Earth 2.Kubernetes 调度器介绍 文章对kubernetes的kube-scheduler的整体流程介绍的比较清晰。 3.A Brief History of Alibaba Founders 阿里巴巴的18罗汉介绍。 图片1.电传打字机设备(Teletype) 早期的计算机设备比较笨重，计算机放在单独的一个房间中，操作计算机的人坐在另外一个房间中，通过终端机设备来操作计算机。 早期的终端设备为电传打字机(Teletype)，该设备价格比较低廉，通过键盘输入，并将输出内容打印出来。图中的设备为ASR-33，在YouTube上可以可以看到视频。 有意思的是，实际上Teletype的出现要早于计算机，原本用于在电报线路上发送电报，但是后来计算机出现后直接拿来作为计算机的终端设备。 在linux操作系统中设计了tty子系统用于支持tty设备，并将具体的硬件设备放到/dev/tty*目录下，这里的tty设备即Teletype。不过后来随着其他终端设备的引入，tty这个名字仍旧保留了下来，tty目前已基本代表终端的总称。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第3期]]></title>
      <url>%2Fpost%2Fknowledge-share-3%2F</url>
      <content type="text"><![CDATA[题图为北京玉渡山风景区中的盘山公路，旁边有个观景台，在观景台上可以鸟瞰官厅水库。 资源1.Intel RDT Intel RDT(Resource Director Technology)资源调配技术框架，包括高速缓存监控技术（CMT）、高速缓存分配技术（CAT）、内存带宽监控（MBM）和代码和数据优先级（CDP），容器技术的runc项目中使用到了CAT技术来解决cgroup下的CPU的三级缓存隔离性问题。 在linux 4.10以上内核中通过资源控制文件系统的方式来提供给用户接口，类似cgroup的管理方式。 感兴趣的可以了解下runc项目源码。 2.Thanos Prometheus作为Google内部监控系统Borgmon的开源实现版本，存在高可用和历史数据存储两个致命的缺点，Thanos利用Sidecar等技术来解决Prometheus的缺点。 3.netshoot 123456 dP dP dP 88 88 8888d888b. .d8888b. d8888P .d8888b. 88d888b. .d8888b. .d8888b. d8888P88&apos; `88 88ooood8 88 Y8ooooo. 88&apos; `88 88&apos; `88 88&apos; `88 8888 88 88. ... 88 88 88 88 88. .88 88. .88 88dP dP `88888P&apos; dP `88888P&apos; dP dP `88888P&apos; `88888P&apos; dP 用于排查docker网络问题的工具，以容器的方式运行在跟要排查问题的容器同一个网络命名空间中，该容器中已经具备了较为丰富的网络命令行工具，用于排查容器中的网络问题。 4.bat 用来替代cat的命令行工具，支持语法高亮、自动分页。mac下可直接使用brew install bat来安装。 5.asciiflow 写博客的往往都比较痛恨图片的存储问题，尤其是使用markdown语法写作的，图片往往需要图床来存储，常常跟文章不在一起存储。asciiflow是较为小众的一款ascii图形工具，可以应付较为简单的图形绘制，直接以文字的形式呈现简单图形，省去了存储图片的繁琐。 6.processon 免费的在线图行绘制协作工具，支持流程图、思维导图等多种图形，有类似visio的使用体验，同时是web版的，支持多人协作。我目前在使用，不过免费版有使用限制。 精彩文章1.手把手教你打造高效的 Kubernetes 命令行终端 文中汇总了各种可以取代kubernetes的命令行kubectl的工具，以便提供更方便的操作，比如更完善的自动补全。 2.Understand Container - Index Page 学习容器的cgroup和namespace的系列文章。 3.gVisor是什么？可以解决什么问题？ docker容器技术基于cgroup和namespace来实现，但系统调用仍是调用宿主机的系统调用，比如在其中一个容器中通过系统调用修改了当前系统时间，在其他容器中看到的时间也已经修改过了，这显然不是符合期望的，通常可以通过Seccomp来限制容器中的系统调用。 gVisor为Google开源的容器Runtime，通过pstrace技术来截获系统调用，从而保证系统的安全。目前还不成熟，单就凭Google的开源项目，该项目还是非常值得关注的。 4.Use multi-stage builds Dockerfile的多阶段构建技术，对于解决编译型语言的发布非常有帮助，可以在其中一个image中编译源码，另外一个image用于将编译完成后的二进制文件复制过来后打包成单独的线上运行镜像。而这两部操作可以合并到一个Dockerfile中来完成。 5.唯品会Noah云平台实现内幕披露 唯品会内部云平台的实践，涉及到大量的干货，值的花时间一读。 App推荐1.Nike Training 健身类app我用过keep、火辣健身、FitTime（以收费课程居多），偶然间在AppStore上看到了Nike Training，如果厌倦了国内的健身类app，不防尝试一下。 新奇1.手机QQ扫一扫 用手机QQ扫一扫100元人民币正面，可以出现浮动的凤凰图案，并会跳转到人民币鉴别真伪的视频页面，视频效果确实不错，忍不住会多扫描几遍。 2.kubeadm kubernetes的组件非常多，部署起来非常复杂，因此社区就推出了kubeadm工具来简化集群的部署，将除了kubelet外的其他组件都部署在容器中。令人惊奇的是，kubeadm几乎完全是一个芬兰高中生Lucas KaIdstrom的作品，是他在17岁时利用业余时间完成的一个社区项目。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux capability]]></title>
      <url>%2Fpost%2Fcapability%2F</url>
      <content type="text"><![CDATA[传统的unix权限模型将进程分为root用户进程（有效用户id为0）和普通用户进程。普通用户需要root权限的某些功能，通常通过setuid系统调用实现。但普通用户并不需要root的所有权限，可能仅仅需要修改系统时间的权限而已。这种粗放的权限管理方式势必会带来一定的安全隐患。 linux内核中引入了capability，用于消除需要执行某些操作的程序对root权限的依赖。 capability用于分割root用户的权限，将root的权限分割为不同的能力，每一种能力代表一定的特权操作。例如，CAP_SYS_MODULE用于表示用户加载内核模块的特权操作。根据进程具有的能力来进行特权操作的访问控制。 只有进程和可执行文件才有能力，每个进程拥有以下几组能力集(set)。 cap_effective: 进程当前可用的能力集 cap_inheritable: 进程可以传递给子进程的能力集 cap_permitted: 进程可拥有的最大能力集 cap_ambient: Linux 4.3后引入的能力集， cap_bounding: 用于进一步限制能力的获取 可以通过/proc/${pid}/status文件中的CapInh CapPrm CapEff CapBnd CapAmb来表示，每个字段为8个字节即64bit，每个比特表示一种能力，这几个字段存放在进程的内核数据结构task_struct中，由此可见capability的最小单位为线程，而不是进程。 example 1 设置进程能力在执行下面程序之前需要安装yum install libcap-devel 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;#undef _POSIX_SOURCE#include &lt;sys/capability.h&gt;extern int errno;void whoami(void)&#123; printf("uid=%i euid=%i gid=%i\n", getuid(), geteuid(), getgid());&#125;void listCaps()&#123; cap_t caps = cap_get_proc(); ssize_t y = 0; printf("The process %d was give capabilities %s\n",(int) getpid(), cap_to_text(caps, &amp;y)); fflush(0); cap_free(caps);&#125;int main(int argc, char **argv)&#123; int stat; whoami(); stat = setuid(geteuid()); pid_t parentPid = getpid(); if(!parentPid) return 1; cap_t caps = cap_init(); // 给进程增加5中能力 cap_value_t capList[5] =&#123; CAP_NET_RAW, CAP_NET_BIND_SERVICE , CAP_SETUID, CAP_SETGID,CAP_SETPCAP &#125; ; unsigned num_caps = 5; cap_set_flag(caps, CAP_EFFECTIVE, num_caps, capList, CAP_SET); cap_set_flag(caps, CAP_INHERITABLE, num_caps, capList, CAP_SET); cap_set_flag(caps, CAP_PERMITTED, num_caps, capList, CAP_SET); if (cap_set_proc(caps)) &#123; perror("capset()"); return EXIT_FAILURE; &#125; listCaps(); // 将进程的能力清除 printf("dropping caps\n"); cap_clear(caps); // resetting caps storage if (cap_set_proc(caps)) &#123; perror("capset()"); return EXIT_FAILURE; &#125; listCaps(); cap_free(caps); return 0;&#125; 并执行如下操作： 12345678910gcc capability.c -lcap -o capability# 需要使用root执行，因为普通用户不能给进程设置能力sudo ./capability# 输出如下内容uid=0 euid=0 gid=0The process 5044 was give capabilities = cap_setgid,cap_setuid,cap_setpcap,cap_net_bind_service,cap_net_raw+eipdropping capsThe process 5044 was give capabilities = example 2 获取进程能力12345678910111213141516171819202122232425#undef _POSIX_SOURCE#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;#include &lt;linux/capability.h&gt;#include &lt;errno.h&gt;int main()&#123; struct __user_cap_header_struct cap_header_data; cap_user_header_t cap_header = &amp;cap_header_data; struct __user_cap_data_struct cap_data_data; cap_user_data_t cap_data = &amp;cap_data_data; cap_header-&gt;pid = getpid(); cap_header-&gt;version = _LINUX_CAPABILITY_VERSION_1; if (capget(cap_header, cap_data) &lt; 0) &#123; perror("Failed capget"); exit(1); &#125; printf("Cap data 0x%x, 0x%x, 0x%x\n", cap_data-&gt;effective,cap_data-&gt;permitted, cap_data-&gt;inheritable);&#125; 可以通过capget命令获取进程的能力 12345678[vagrant@localhost tmp]$ gcc get_capability.c -lcap -o get_capability# 普通用户默认情况下没有任何能力[vagrant@localhost tmp]$ ./get_capabilityCap data 0x0, 0x0, 0x0# root用户默认拥有所有的能力[vagrant@localhost tmp]$ sudo ./get_capabilityCap data 0xffffffff, 0xffffffff, 0x0 工具 getcap用于获取程序文件所具有的能力。 getpcaps用于获取进程所具有的能力。 setcap用于设置程序文件所具有的能力。 1234567891011121314# 将chown命令授权给普通用户也具备更改文件owner的能力# 其中eip分别代表cap_effective(e) cap_inheritable(i) cap_permitted(p)[vagrant@localhost tmp]$ sudo setcap cap_chown=eip /usr/bin/chown[vagrant@localhost tmp]$ getcap /usr/bin/chown/usr/bin/chown = cap_chown+eip# 使用root创建测试文件[vagrant@localhost tmp]$ sudo touch /tmp/aa# 普通用户也可以修改root用户创建文件的owner了[vagrant@localhost tmp]$ chown vagrant:vagrant /tmp/aa# 清除chown的能力[vagrant@localhost tmp]$ sudo setcap -r /usr/bin/chown[vagrant@localhost tmp]$ getcap /usr/bin/chown runc项目中的应用runc的容器配置文件spec.Process.Capabilities可以定义各个能力集的能力，用来限制容器的能力。 docker中的应用docker默认情况下给容器去掉了一些比较危险的capabilities，比如cap_sys_admin。 例如在docker中使用gdb命令默认是不允许的，这是因为docker已经将SYS_PTRACE相关的能力给去掉了。 在docker中使用--cap-add和--cap-drop命令来增加和删除capabilities， 可以使用--privileged赋予容器所有的capabilities，该操作谨慎使用。 ref Linux的capability深入分析(1) Linux的capability深入分析(2) Linux Programmer’s Manual CAPABILITIES 如何在Docker内部使用gdb调试器 docker run]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker基础知识之user namespace]]></title>
      <url>%2Fpost%2Fnamespace_user%2F</url>
      <content type="text"><![CDATA[user namespace是所有namespace中实现最复杂的一个，也是最晚引入的一个，是在linux2.6版本中才引入。因为涉及到权限机制，跟capability有着比较密切的关系。 创建user namespace在clone或者unshare系统调用使用CLONE_NEWUSER参数后，在子进程中看到的uid和gid跟父进程中的不一样，子进程中找不到uid时，会显示最大的uid 65534（在/proc/sys/kernel/overflowuid中设置）。 1234567891011121314151617181920212223vagrant@ubuntu-xenial:/tmp$ iduid=1000(vagrant) gid=1000(vagrant) groups=1000(vagrant)vagrant@ubuntu-xenial:/tmp$ readlink /proc/$$/ns/useruser:[4026531837]# 使用unshare命令创建新的user namespacevagrant@ubuntu-xenial:/tmp$ unshare --user /bin/bashnobody@ubuntu-xenial:/tmp$ readlink /proc/$$/ns/useruser:[4026532145]# 新的user namespace没有映射关系，默认使用/proc/sys/kernel/overflowuid中定义的user id和/proc/sys/kernel/overflowgid中的group idnobody@ubuntu-xenial:/tmp$ iduid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup)#--------------------------第二个shell窗口----------------------# 在父user namespace上创建文件夹，可以看到用户为vagrantvagrant@ubuntu-xenial:/tmp$ mkdir testvagrant@ubuntu-xenial:/tmp$ ll /tmp | grep testdrwxrwxr-x 2 vagrant vagrant 4096 Sep 23 17:11 test/#--------------------------第一个shell窗口----------------------# 在新创建的user namespace下用户显示为nobodynobody@ubuntu-xenial:/tmp$ ll /tmp | grep testdrwxrwxr-x 2 nobody nogroup 4096 Sep 23 17:11 test/ 映射user id和group id到新的user namespace创建完新的user namespace后，通常会先映射user id和group id，方法为添加映射关系到/proc/${pid}/uid_map和/proc/${pid}/gid_map中。 user namespace被创建以后，第一个进程被赋予了该namespace的所有权限，但该进程并不拥有父namespace的任何权限。利用该机制可以做到一个用户在父user namespace中是普通用户，在子user namespace中是超级用户的功能。 为了将容器中的uid和父user namespace上的uid和gid进行 关联起来，可通过/proc//uid_map和/proc//gid_map来进行映射的。这两个文件的格式为： 1ID-inside-ns ID-outside-ns length 第一个字段ID-inside-ns表示在容器显示的UID或GID，第二个字段ID-outside-ns表示容器外映射的真实的UID或GID。第三个字段表示映射的范围，一般填1，表示一一对应。 0 1000 256这个配置的含义为父user namespace的1000-1256映射到新user namespace的0-256. 创建子进程在没有指定CLONE_NEWUSER时文件内容如下，子进程跟父进程的用户完全一致： 123# 表示把namespace内部从0开始的uid映射到外部从0开始的uid，其最大范围是无符号32位整形[root@centos7 1325]# cat uid_map 0 0 4294967295 要想实现以普通用户运行程序，在子进程中以root用户执行，仅需要将uid_map文件修改为普通用户映射到子进程中的0即可，因为uid为0表示root用户。 那么谁拥有写入该文件的权限呢？ /proc/${pid}/[u|g]id的拥有者为创建新user namespace的用户，拥有map文件写入权限的仅有两个用户：和该用户在同一个user namespace中的root用户，创建新的user namespace的用户。创建新的user namespace有没有写入map文件的权限，还要取决于capability中的CAP_SETUID和CAP_SETGID两个权限。 为了方便写入/proc/${pid}/uid_map和/proc/${pid}/gid_map文件，可以使用newuidmap和newgidmap命令来完成。 继续上述例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#--------------------------第一个shell窗口----------------------# 在新user namespace中获取当前的进程号nobody@ubuntu-xenial:/tmp$ echo $$13226#--------------------------第二个shell窗口----------------------vagrant@ubuntu-xenial:/tmp$ ll /proc/13226/uid_map /proc/13226/gid_map-rw-r--r-- 1 vagrant vagrant 0 Sep 23 17:31 /proc/13226/gid_map-rw-r--r-- 1 vagrant vagrant 0 Sep 23 17:31 /proc/13226/uid_map# 提示当前进程没有权限写入vagrant@ubuntu-xenial:/tmp$ echo &apos;0 1000 100&apos; &gt; /proc/13226/uid_map-bash: echo: write error: Operation not permitted# 查看当前bash没有任何capabilityvagrant@ubuntu-xenial:/tmp$ cat /proc/$$/status | egrep &apos;Cap(Inh|Prm|Eff)&apos;CapInh: 0000000000000000CapPrm: 0000000000000000CapEff: 0000000000000000# 使用root权限给/bin/bash可执行文件增加cap_setgid和cap_setuidvagrant@ubuntu-xenial:/tmp$ sudo setcap cap_setgid,cap_setuid+ep /bin/bash# 启动新的bash后capability会生效vagrant@ubuntu-xenial:/tmp$ bashvagrant@ubuntu-xenial:/tmp$ cat /proc/$$/status | egrep &apos;Cap(Inh|Prm|Eff)&apos;CapInh: 0000000000000000CapPrm: 00000000000000c0CapEff: 00000000000000c0# 重新写入vagrant@ubuntu-xenial:/tmp$ echo &apos;0 1000 100&apos; &gt; /proc/13226/uid_mapvagrant@ubuntu-xenial:/tmp$ echo &apos;0 1000 100&apos; &gt; /proc/13226/gid_map# 第二次再写入会失败，仅允许写入一次vagrant@ubuntu-xenial:/tmp$ echo &apos;0 1000 100&apos; &gt; /proc/13226/uid_mapbash: echo: write error: Operation not permitted# 将刚才设置的capability取消vagrant@ubuntu-xenial:/tmp$ sudo setcap cap_setgid,cap_setuid-ep /bin/bashvagrant@ubuntu-xenial:/tmp$ getcap /bin/bash/bin/bash =vagrant@ubuntu-xenial:/tmp$ exitexit#--------------------------第一个shell窗口----------------------# 第一个窗口中userid已经变更为0了nobody@ubuntu-xenial:/tmp$ iduid=0(root) gid=0(root) groups=0(root)# 重新执行一个新的bash，会发现提示符已经变更为root了nobody@ubuntu-xenial:/tmp$ bashroot@ubuntu-xenial:/tmp## 可以看到新的bash已经拥有的所有的capability，但也仅限于当前的user namespace中root@ubuntu-xenial:/tmp# cat /proc/$$/status | egrep &apos;Cap(Inh|Prm|Eff)&apos;CapInh: 0000000000000000CapPrm: 0000003fffffffffCapEff: 0000003fffffffff 问题user namespace在linux3.8内核版本上才实现，存在一定的安全问题。在redhat和centos系统下，user namespace作为了一个实验feature，默认情况下未开启。 执行如下命令sudo grubby --args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;并重启系统后可以就可以打开user namespace feature了。执行grubby --remove-args=&quot;user_namespace.enable=1&quot; --update-kernel=&quot;$(grubby --default-kernel)&quot;可关闭user namespace feature。 经实验，上述操作未生效，后续待查该问题。 ref What’s Next for Containers? User Namespaces Linux Namespace系列（07）：user namespace (CLONE_NEWUSER) (第一部分)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker基础知识之mount namespace]]></title>
      <url>%2Fpost%2Fnamespace_mount%2F</url>
      <content type="text"><![CDATA[在使用CLONE_NEWNS来创建新的mount namespace时，子进程会共享父进程的文件系统，如果子进程执行了新的mount操作，仅会影响到子进程自身，不会对父进程造成影响。 12345678910111213141516171819202122232425262728293031323334353637#define _GNU_SOURCE#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;#include &lt;stdio.h&gt;#include &lt;sched.h&gt;#include &lt;signal.h&gt;#include &lt;unistd.h&gt;/* 定义一个给 clone 用的栈，栈大小1M */#define STACK_SIZE (1024 * 1024)static char container_stack[STACK_SIZE];char* const container_args[] = &#123; &quot;/bin/bash&quot;, NULL&#125;;int container_main(void* arg)&#123; printf(&quot;Container [%5d] - inside the container!\n&quot;, getpid()); sethostname(&quot;container&quot;,10); /* 重新mount proc文件系统到 /proc下 */ system(&quot;mount -t proc proc /proc&quot;); execv(container_args[0], container_args); printf(&quot;Something&apos;s wrong!\n&quot;); return 1;&#125;int main()&#123; printf(&quot;Parent - start a container!\n&quot;); int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWUTS | CLONE_NEWPID | CLONE_NEWNS | SIGCHLD, NULL); /*启用CLONE_NEWUTS Namespace隔离 */ waitpid(container_pid, NULL, 0); printf(&quot;Parent - container stopped!\n&quot;); return 0;&#125; 执行效果： 123456789[root@centos7 docker_learn]# ./mountParent - start a container!Container [ 1] - inside the container!# 由于ps是读取的/proc目录下的文件来显示当前系统系统的进程，新进程挂载/proc目录到新的proc文件系统，自然看不到父进程的/proc目录了[root@container docker_learn]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 22:20 pts/3 00:00:00 /bin/bashroot 30 1 0 22:20 pts/3 00:00:00 ps -ef 但是在容器技术中，在容器中不应该看到宿主机上挂载的目录。在Linux中使用chroot技术来实现，在容器中将/挂载到指定目录，这样在容器中就看不到宿主机上的其他挂载项。在容器技术中，chroot所使用的目录即容器镜像的目录。容器镜像中并不包含操作系统的内核。 mount namespace是Linux中第一个namespace，是基于chroot技术改良而来。 Docker Volume为了解决容器中能够访问宿主机上文件的问题，docker引入了Volume机制，将宿主机上指定的文件或者目录挂载到容器中。而整个的docker Volume机制跟mount namespace的关系不太大。 Volume用到的技术为Linux的绑定挂载机制，该机制将一个指定的目录或者文件挂载到一个指定的目录上。 容器启动顺序如下： 创建新的mount namespace dockerinit根据容器镜像准备好rootfs dockerinit使用绑定挂载机制将一个指定的目录挂载到rootfs的某个目录上 dockerinit调用chroot 容器启动时需要创建新的mount namespace，根据容器镜像准备好rootfs，调用chroot。docker volume的挂载时机是在rootfs准备好之后，调用chroot之前完成。 上文提到进入新的mount namespace后，mount namespace会继承父mount namespace的挂载, docker volume一定是在新的mount namespce中执行，否则会影响到宿主机上的mount。在调用chroot之后已经看不到宿主机上的文件系统，无法进行挂载。 执行这一操作的进程为docker的容器进程dockerinit，该进程会负责完成根目录的准备、挂载设备和目录、配置hostname等一系列需要在容器内进行的初始化操作。在初始化完成后，会调用execv()系统调用，用容器中的ENTRYPOINT进程取代，成为容器中的1号进程。 volume挂载的目录是挂载在读写层，由于使用了mount namespace，在宿主机上看不到挂载目录的信息，因此docker commit操作不会将挂载的目录提交。 下面使用例子来演示docker volume的用法 在宿主机上使用docker run -d -v /test ubuntu sleep 10000创建新的容器，并创建docker容器中的挂载点/test，该命令会自动在容器中创建目录，并将宿主机上指定目录下的随机目录挂载到容器中的/test目录下。 可在宿主机上通过如下命令查看到volume的情况 123456789101112131415161718# 列出当前docker在使用的所有volume[root@localhost vagrant]# docker volume lsDRIVER VOLUME NAMElocal 4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6# 查看volume在宿主机上的挂载点[root@localhost vagrant]# docker volume inspect 4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6[ &#123; &quot;CreatedAt&quot;: &quot;2018-09-15T23:36:09+08:00&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Labels&quot;: null, &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6/_data&quot;, &quot;Name&quot;: &quot;4ad97e6356707b66cd1cacc4a2e223d9c79d11eca26fe12b1becc9dd664fc5c6&quot;, &quot;Options&quot;: &#123;&#125;, &quot;Scope&quot;: &quot;local&quot; &#125;] docker文件系统 rootfs在最下层为docker镜像的只读层。 rootfs之上为dockerinit进程自己添加的init层，用来存放dockerinit添加或者修改的/etc/hostname等文件。 rootfs的最上层为可读写层，以Copy-On-Write的方式存放任何对只读层的修改，容器声明的volume挂载点也出现这一层。 ref极客时间-深入剖析Kubernetes-08]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker基础知识之network namespace]]></title>
      <url>%2Fpost%2Fnamespace_network%2F</url>
      <content type="text"><![CDATA[network namespace用来隔离Linux系统的网络设备、ip地址、端口号、路由表、防火墙等网络资源。用户可以随意将虚拟网络设备分配到自定义的networknamespace里，而连接真实硬件的物理设备则只能放在系统的根networknamesapce中。 一个物理的网络设备最多存在于一个network namespace，可以通过创建veth pair在不同的network namespace之间创建通道，来达到通讯的目的。 容器的bridge模式的实现思路为创建一个veth pair，一端放置在新的namespace，通常命名为eth0，另外一端放在原先的namespace中连接物理网络设备，以此实现网络通信。 docker daemon负责在宿主机上创建veth pair，把一端绑定到docker0网桥，另一端到新建的network namespace进程中。建立的过程中，docker daemon和dockerinit通过pipe进行通讯。 一、测试例子测试network namespace的过程比较复杂。 docker默认采用的为bridge模式，在容器所在的宿主机上看到的网卡情况如下： 1234567891011[root@localhost software]# ip link show1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:a5:78:ca brd ff:ff:ff:ff:ff:ff4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a3:75:00:16 brd ff:ff:ff:ff:ff:ff18: veth71f2650@if17: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether ca:05:f7:db:6f:4c brd ff:ff:ff:ff:ff:ff link-netnsid 0 其中的enp0s3和enp0s8可以忽略，为虚拟机使用的网卡。docker0和veth71f2650@if17是需要关注的网卡。 123[root@localhost software]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242a3750016 no veth71f2650 下面的操作为在已经运行docker的虚拟机上的，以便于跟docker进行比较。 以下命令根据coolshell中的步骤进行配置，并对执行命令的顺序进行了调整。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127# 增加network namespace ns1[root@localhost software]# ip netns add ns1[root@localhost software]# ip netnsns1# 激活namespace ns1中的lo设备[root@localhost software]# ip netns exec ns1 ip link set dev lo up# 创建veth pair[root@localhost software]# ip link add veth-ns1 type veth peer name lxcbr0.1# 多出了lxcbr0.1@veth-ns1和veth-ns1@lxcbr0.1两个设备# 后面的操作步骤中将lxcbr0.1位于主网络命名空间中，veth-ns1位于ns1命名空间中[root@localhost software]# ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:a5:78:ca brd ff:ff:ff:ff:ff:ff4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a3:75:00:16 brd ff:ff:ff:ff:ff:ff18: veth71f2650@if17: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether ca:05:f7:db:6f:4c brd ff:ff:ff:ff:ff:ff link-netnsid 019: lxcbr0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether c6:b7:4d:7f:f8:90 brd ff:ff:ff:ff:ff:ff20: lxcbr0.1@veth-ns1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether c6:8a:26:3d:ba:de brd ff:ff:ff:ff:ff:ff21: veth-ns1@lxcbr0.1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f2:03:22:93:d6:f4 brd ff:ff:ff:ff:ff:ff# 将设备veth-ns1放入到ns1命名空间中[root@localhost software]# ip link set veth-ns1 netns ns1# 可以看到veth-ns1设备在当前命名空间消失了[root@localhost software]# ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 08:00:27:a5:78:ca brd ff:ff:ff:ff:ff:ff4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:a3:75:00:16 brd ff:ff:ff:ff:ff:ff18: veth71f2650@if17: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default link/ether ca:05:f7:db:6f:4c brd ff:ff:ff:ff:ff:ff link-netnsid 019: lxcbr0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether c6:b7:4d:7f:f8:90 brd ff:ff:ff:ff:ff:ff20: lxcbr0.1@if21: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether c6:8a:26:3d:ba:de brd ff:ff:ff:ff:ff:ff link-netnsid 1# 同时在命名空间ns1中看到了设备veth-ns1，同时可以看到veth-ns1设备的状态为DOWN[root@localhost software]# ip netns exec ns1 ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0021: veth-ns1@if20: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f2:03:22:93:d6:f4 brd ff:ff:ff:ff:ff:ff link-netnsid 0# 将ns1中的veth-ns1设备更名为eth0[root@localhost software]# ip netns exec ns1 ip link set dev veth-ns1 name eth0[root@localhost software]# ip netns exec ns1 ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0021: eth0@if20: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f2:03:22:93:d6:f4 brd ff:ff:ff:ff:ff:ff link-netnsid 0# 为容器中的网卡分配一个IP地址，并激活它[root@localhost software]# ip netns exec ns1 ifconfig eth0 192.168.10.11/24 up# 可以看到eth0网卡上有ip地址[root@localhost software]# ip netns exec ns1 ifconfigeth0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 192.168.10.11 netmask 255.255.255.0 broadcast 192.168.10.255 ether f2:03:22:93:d6:f4 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 添加一个网桥lxcbr0，类似于docker中的docker0[root@localhost software]# brctl addbr lxcbr0[root@localhost software]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242a3750016 no veth71f2650lxcbr0 8000.000000000000 no# 关闭生成树协议，默认该协议为关闭状态[root@localhost software]# brctl stp lxcbr0 off[root@localhost software]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242a3750016 no veth71f2650lxcbr0 8000.000000000000 no# 为网桥配置ip地址ifconfig lxcbr0 192.168.10.1/24 up[root@localhost software]# ifconfig lxcbr0lxcbr0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.10.1 netmask 255.255.255.0 broadcast 192.168.10.255 inet6 fe80::c4b7:4dff:fe7f:f890 prefixlen 64 scopeid 0x20&lt;link&gt; ether c6:b7:4d:7f:f8:90 txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 648 (648.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# 将veth设备中的其中一个lxcbr0.1添加到网桥lxcbr0上[root@localhost software]# brctl addif lxcbr0 lxcbr0.1# 可以看到网桥lxcbr0中已经包含了设备lxcbr0.1[root@localhost software]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242a3750016 no veth71f2650lxcbr0 8000.c68a263dbade no lxcbr0.1# 为网络空间ns1增加默认路由规则，出口为网桥ip地址[root@localhost software]# ip netns exec ns1 ip route add default via 192.168.10.1[root@localhost software]# ip netns exec ns1 ip routedefault via 192.168.10.1 dev eth0192.168.10.0/24 dev eth0 proto kernel scope link src 192.168.10.11# 为ns1增加resolv.conf[root@localhost software]# mkdir -p /etc/netns/ns1[root@localhost software]# echo &quot;nameserver 8.8.8.8&quot; &gt; /etc/netns/ns1/resolv.conf 二、常用命令1. 列出当前的network namespace1.1 使用lsns命令lsns命令通过读取/proc/${pid}/ns目录下进程所属的命名空间来实现，如果是通过ip netns add场景的命名空间，但是没有使用该命名空间的进程，该命令是看不到的。 123456789101112# lsns -t net NS TYPE NPROCS PID USER COMMAND4026531956 net 383 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 214026532490 net 1 1026 rtkit /usr/libexec/rtkit-daemon4026532762 net 2 24872 root /pause4026532866 net 20 25817 root /pause4026532965 net 3 30763 root /pause4026533059 net 3 2794 root /bin/sh -c python /usr/src/app/clean.py &quot;$&#123;endpoints&#125;&quot; &quot;$&#123;expire&#125;&quot;4026533163 net 2 1122 102 /docker-java-home/jre/bin/java -Xms2g -Xmx2g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupan4026533266 net 4 13920 root /pause4026533371 net 2 1844 root /pause4026533559 net 3 1067 root sleep 4 1.2 通过ip netns命令该命令仅会列出有名字的namespace，对于未命名的不能显示。 ip netns identify ${pid} 可以找到进程所属的网络命名空间 ip netns list: 显示所有有名字的namespace 2. 通过pid进入具体的network namespace2.1 通过nsenter命令nsenter --target $PID --net可以进入到对应的命名空间 2.2 docker --net参数docker提供了--net参数用于加入另一个容器的网络命名空间docker run -it --net container:7835490487c1 busybox ifconfig。 2.3 setns系统调用一个进程可以通过setns()系统调用来进入到另外一个namespace中。 编写setns.c程序，该程序会进入到进程id所在的网络命令空间，并使用gcc setns.c -o setns进行编译，编译完成后执行./setns /proc/4913/ns/net ifconfig可以看到网卡的信息为容器中的网卡信息。 1234567891011121314#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main(int argc, char *argv[]) &#123; int fd = open(argv[1], O_RDONLY); if (setns(fd, 0) == -1) &#123; perror("setns"); exit(-1); &#125; execvp(argv[2], &amp;argv[2]); printf("execvp exit\n");&#125; 如果执行./setns /proc/4913/ns/net /bin/bash，在宿主机上查看docker进程和/bin/bash进程的网络命名空间/proc/${pid}/ns/net，会发现都指向lrwxrwxrwx 1 root root 0 Sep 14 14:42 net -&gt; net:[4026532133]同一个位置。 3. pid的获取方式最简单的方式上文第1点中的PID列 3.1 /proc/[pid]/ns可以使用如下命令查看当前容器在宿主机上的进程id。 1docker inspect --format &apos;&#123;&#123;.State.Pid&#125;&#125;&apos; a1bf0119d891 每个进程在/proc/${pid}/ns/目录下都会创建其对应的虚拟文件，并链接到一个真实的namespace文件上，如果两个进程下的链接文件链接到同一个地方，说明两个进程同属于一个namespace。 12345678[root@localhost runc]# ls -l /proc/4913/ns/total 0lrwxrwxrwx 1 root root 0 Sep 11 00:21 ipc -&gt; ipc:[4026532130]lrwxrwxrwx 1 root root 0 Sep 11 00:21 mnt -&gt; mnt:[4026532128]lrwxrwxrwx 1 root root 0 Sep 11 00:18 net -&gt; net:[4026532133]lrwxrwxrwx 1 root root 0 Sep 11 00:21 pid -&gt; pid:[4026532131]lrwxrwxrwx 1 root root 0 Sep 11 00:21 user -&gt; user:[4026531837]lrwxrwxrwx 1 root root 0 Sep 11 00:21 uts -&gt; uts:[4026532129] reference DOCKER基础技术：LINUX NAMESPACE（下） 极客时间-深入剖析Kubernetes 一文搞懂 Linux network namespace]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第2期]]></title>
      <url>%2Fpost%2Fknowledge-share-2%2F</url>
      <content type="text"><![CDATA[题图为北京灵山主峰，海拔2303米，北京最高峰。登上主峰的时候，恰巧一头牛就在山顶悠闲，拍照的时候，牛哥把我带上去的枣、葡萄、花生米全部吃光了，甚至连橘子皮都没剩下，吃完后牛哥又悠闲的去吃草了，让我见识了啥叫吃葡萄不吐葡萄皮。 教程 《Kubernetes权威指南 企业级容器云实战》 kubernetes的书籍并不多，该书八月份刚初版，内容较新，并不是一本kubernetes的入门书籍，而是讲解kubernetes在企业落地为PASS平台时需要做的工作，建议对kubernetes有一定了解后再看。 书的内容为HPE的多名工程师拼凑而成，有些部分的内容明显是没有经过实践验证的理论派想法，但总体来看值得一读。 书中提到了很多kubernetes较新版本才有的特性、微服务、service mesh、lstio，对于补充自己已经掌握的知识点有一定帮助。 书的后半部分反而显的干货少了非常多，我仅草草的过了一遍。 《Go语言高级编程》 Golang中相对进阶的中文教程，我还没来得及看。 Gorilla facebook发表的分布式的时序数据库论文，如果英文看起来吃力，可以看一下小米运维公众号中的翻译版本。facebook并未提供开源的实现，但在github上能找到一些开源的实现。 《深入剖析Kubernetes》 极客时间app的专栏，本来购买之前没有报特别大的预期，但读完头几篇文章后被作者的文字功底折服，将PASS、容器的来龙去脉、docker的发展讲解的很到位，超出了我的预期。期待后面更新的专栏能够保持搞水准。 SDN手册 一本介绍SDN相关知识的开源电子书。 资源 M3DB 监控领域还是比较缺少特别好用的分布式时间序列存储数据库，性能特别优异的数据库往往都是单机版的，缺少高可用的方案，比如rrdtool、influxdb、graphite等。OpenTSDB、KairosDB、Druid等虽为分布式的时序数据库，但使用或者运维起来总有各种不方便的地方。uber开源的m3在分布式时序数据库领域又多了一个方案，并可作为prometheus的远程存储。 cilium 使用BPF(Berkeley Packet Filter)和XDP(eXpress Data Path)内核技术来提供网络安全控制的高性能开源网络方案。 kubeless kubernetes平台上的Serverless项目，Faas（功能即服务）一定是云计算发展的一个趋势。目前CNCF中还没有Serverless项目，期待CNCF下能够孵化一个Serverless项目。 工具 vagrant 还在使用virturalbox的你，是时候使用vagrant了。vagrant作为对虚拟机的管理，虽然引入了一些概念带来了更大的复杂性。但同时功能上也更强大，比如对box的管理，可以将box理解为docker image，便于将虚拟机的环境在不同的主机上分发。 公众账号推荐 小米运维 开通时间不算特别长，但文章的质量不错，都是比较接地气的干货，看得出确实是在工作中遇到的问题或者是总结经验，值的一读。 开柒 曾经公众号的名字为开八，江湖人称八姐，忘记为何更改为开柒了，曾经的搜狐记者。总能非常及时的爆料很多互联网的内幕，消息来源往往非常准确，可见八姐在圈内的人脉非同一般。 毕导 打发时间非常好的公众号，用理科男的思维方式进行恶搞，是不是拿出冗长的数学公式来证明日常生活中的小尝试，语言诙谐幽默，绝对是公众号中的一股清流。可惜每篇文章都很长，我没有太多时间把每一篇文章都看一遍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[打场的记忆]]></title>
      <url>%2Fpost%2Fdachang%2F</url>
      <content type="text"><![CDATA[晚上在工位上安静地看着技术文档，不知为何脑中突然闪现了小时候打场的场景，那段记忆并无特殊之处，但大概是那片金黄色的背景和村民脸上丰收的喜悦以及村民之间的和谐相处的画面突然浮现在了我的眼前，让我不由得想写点什么以便回忆那段近乎忘却的回忆。 说起“打场”这个词，可能很多人都不太熟悉，其实这个词也已经很多年我都没有接触过了，我甚至都不知道现在农村老家的村民们还是否会时长提起这个词语。我甚至不知道这个词是不是我们那地域性的方言，这些都不重要了。以下内容摘自百度百科： 打场[dǎ cháng]，指把收割下来带壳的粮食平摊在场院里，用马拉磙子，或者用小型拖拉机，碾压这些粮食，使之脱去外壳，这一系列活动就叫打场。 我的家在山东，小的时候还没有那么多的农业经济作物，地里更多的农作物以小麦和玉米为主，一年两季。农作物的种植周期跟二十四节气是息息相关的，每年的二十四节气的芒种左右是小麦丰收的季节，时间大概在高考前后。比我大一点的应该还有麦假，就是在收麦子的时候不上学了回家帮着收麦子，我没怎么有印象我有过麦假。 小时候的机械化程度低，甚至谈不上机械化，收麦子这种活自然完全是人工完成的。在机械化程度还可以的今天，收麦子这种活在当时还是有着稍微繁琐的工序，也比较耗费人力，一到收麦子的时候，几乎家里所有的劳力都要出来干活了。 村里场地基本上都是挨着的，这应该是生产队时代的产物，那时候土地是公有的，场地自己要连成一片比较容易。后来土地开始分割给个人，这块场地仍然是保留的，只不过是场地被划分成了很多份，每家分上一点。 在打场那个年代，我是几乎很少参与过收麦子的活动，那时我还小，就是在场地里捣乱和瞎跑，还时不时让在场地里干活的村民调侃一番，毕竟小孩子活泼好动，村民们调侃一下解解乏也算是为他们做的一点点微不足道的贡献了。 收麦子的活第一步是割麦子，割麦子这个活我还正儿八经得干过几次，虽然每次干的时间都非常短。关于割麦子的画面，我脑海中浮现的是烈日当头，家人们带着帽子弓着背拿着镰刀一人一沟麦子往前赶，一会儿麦秸在怀里就抱不下了，然后将麦秸放下继续往前赶。那时候的帽子几乎全部是用草编织成的，两根绳子沿着帽檐搭下来系在下巴上，很是牢固，即使低着头帽子也不会掉下来。我干活的时候每次负责的那一沟麦子总是割的最慢的，几乎要慢一倍的样子，而且是干的最差的，背后总会留下一些麦子没有割下来，到现在我也仍然好奇，他们是怎么干的这么快的，我感觉当时已经尽最大的努力了，而这仅是他们的正常速度。 在麦子割完后，就需要将麦子放到集中的晒干了。之所以要晒干，是因为要是麦秸不晒干，麦粒是很难从麦秸上拖不下来的。而要晒干，那么多麦秸就需要一片地方来晾晒麦秸和麦粒。那时候还没怎么有柏油路，家里也很少有水泥的房顶，为了能够有晾晒的地方，村里有一片地就是场地，在收麦子的时候专门用来收麦子的，收完麦子后再种其他作物。之所以场地是集中式的，这还要从生产队时代说起，那时候土地都是公有的，自然场地就会划到一块比较容易，后来土地慢慢私有后，这些土地仍然是作为场地，只是被划的一小块一小块的。 割麦子的时候偶尔会碰到鹌鹑蛋，这也是一些额外的小收获，但遗憾的是从没看到过鹌鹑。 场地要想能晾晒麦秸和麦粒，自然就需要特别的平整，而要想土地特别的平整，在石器时代，仅有一个办法，那就是用轱辘一遍一遍的撵，一般至少需要两个人用绳子拉着轱辘满地里转，要想将土地撵的特别平整且不能有缝隙，哪怕一个麦粒掉在地上能够看的见捡的起来，想想都不是一件特别容易的事情，尤其是刚开始土地特别不平的时候。 并非所有的麦秸都是需要在场地里暴晒的，这个要看麦秸的干燥程度，有些麦秸割完后已经非常干燥了，就不需要再暴晒了。 干燥的麦秸在干燥后就需要将麦粒从麦秸上分离出来，毕竟农民们想要的是麦粒，那才是实打实的粮食。要想麦粒从麦秸上分离，这时候仍然是重量级的轱辘上场，这时候场地相对平整了，自然拉起来会省力气很多，我也曾经拉过一次，但每次都是拉一会就不知道去哪里玩了，小孩子干这么枯燥的事情自然没有耐心。 拖完粒的麦秸一般是就地放到的场地的地头上，为了省空间，会将麦秸垛起来，一个剁的高度差不多在两三米高的样子，差不多是农民拿着叉子往上扔麦秸扔不上去的高度，叉子的长度差不多在两米高的样子。 白天的时间是晒麦粒的最佳时机，到了傍晚时分就需要将晒的麦粒堆起来盖住，这个时候也是场上人最多的时候，几乎每家的地里都有人在忙着。我脑中闪现的场景就是在这个时候，村民们在场地上忙来忙去，边干活边聊着天，扯东扯西的。我在场地上跑来跑去，从这个地里跑到另外一家，时不时村民们还会那我来开个玩笑。累了靠着垛儿打个滚，不一会体力就恢复了，想来那时体力真是好。那时虽然很多事情还不懂，但我确实能从中体会村民们丰收的喜悦。 麦子收完后，用轱辘碾压了无数遍的场地也就失去了存在的价值，村民们在场地上该种点其他作物就种上其他作物了，总之场地不会空闲特别久。场地上总会落下一些麦粒，雨后空闲的场地上的缝隙处时长会生出新的绿油油的麦苗。 现在随着机械化的进步，收麦子这种事自然不需要手工割麦子了，晒麦子的地方也开始变多了，柏油路上，自家的房顶上，自家的院子里都是及其不错的晒麦子的地方，打场这种原始的方式也自然就退出了历史的舞台，而且再也不会回来。 但就是那幅金黄色的画面却在我的记忆中留下了一道抹不去的色彩，让我时长回忆起来那温馨的画面。纵科技的发展，村民们的亲情却在变淡，该进城的都进城了，该出去打工的都出去打工了，一年村民们都见不上几回。也许将来村落的概念会消失，也许人与人之间的关系会更淡，但我曾经经历过村落时代的美好，也许这就足够了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sync.Cond的例子]]></title>
      <url>%2Fpost%2Fcond-example%2F</url>
      <content type="text"><![CDATA[sync.Cond类似于pthread中的条件变量，但等待的为goroutine，而不是线程。比较难理解的为Wait函数，在调用该函数时必须L为Lock状态，调用Wait函数后，goroutine会自动解锁，并等待条件的到来，等条件到来后会重新加锁。 代码量并不多，下面是去掉注释后的代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package syncimport ( "sync/atomic" "unsafe")type Cond struct &#123; noCopy noCopy // L is held while observing or changing the condition L Locker notify notifyList checker copyChecker&#125;func NewCond(l Locker) *Cond &#123; return &amp;Cond&#123;L: l&#125;&#125;func (c *Cond) Wait() &#123; c.checker.check() t := runtime_notifyListAdd(&amp;c.notify) c.L.Unlock() runtime_notifyListWait(&amp;c.notify, t) c.L.Lock()&#125;func (c *Cond) Signal() &#123; c.checker.check() runtime_notifyListNotifyOne(&amp;c.notify)&#125;func (c *Cond) Broadcast() &#123; c.checker.check() runtime_notifyListNotifyAll(&amp;c.notify)&#125;// copyChecker holds back pointer to itself to detect object copying.type copyChecker uintptrfunc (c *copyChecker) check() &#123; if uintptr(*c) != uintptr(unsafe.Pointer(c)) &amp;&amp; !atomic.CompareAndSwapUintptr((*uintptr)(c), 0, uintptr(unsafe.Pointer(c))) &amp;&amp; uintptr(*c) != uintptr(unsafe.Pointer(c)) &#123; panic("sync.Cond is copied") &#125;&#125;// noCopy may be embedded into structs which must not be copied// after the first use.//// See https://github.com/golang/go/issues/8005#issuecomment-190753527// for details.type noCopy struct&#123;&#125;// Lock is a no-op used by -copylocks checker from `go vet`.func (*noCopy) Lock() &#123;&#125; 具体的使用例子如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package mainimport ( "fmt" "sync")func main() &#123; mutex := &amp;sync.Mutex&#123;&#125; cond := sync.NewCond(mutex) wg := &amp;sync.WaitGroup&#123;&#125; wait := func(i int, c chan int) &#123; defer wg.Done() fmt.Println("start chan ", i) cond.L.Lock() defer cond.L.Unlock() fmt.Printf("chan %d wait before\n", i) c &lt;- i // Wait是理解起来稍微麻烦的点，Cond.Wait会自动释放锁等待信号的到来，当信号到来后，第一个获取到信号的Wait将继续往下执行并从新上锁 cond.Wait() fmt.Printf("chan %d wait end\n", i) &#125; signal := func(count int, c chan int) &#123; defer wg.Done() for i := 0; i &lt; count; i++ &#123; fmt.Printf("read chan %d ready\n", &lt;-c) &#125; fmt.Println("call signal") cond.Signal() &#125; broadcast := func(count int, c chan int) &#123; defer wg.Done() for i := 0; i &lt; count; i++ &#123; fmt.Printf("read chan %d ready\n", &lt;-c) &#125; fmt.Println("call broadcast") cond.Broadcast() &#125; c := make(chan int) wg.Add(2) go wait(0, c) go signal(1, c) wg.Wait() fmt.Println("signal test finished\n\n") count := 3 for i := 0; i &lt; count; i++ &#123; wg.Add(1) go wait(i, c) &#125; wg.Add(1) go broadcast(count, c) wg.Wait()&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识分享第一期]]></title>
      <url>%2Fpost%2Fknowledge-share-1%2F</url>
      <content type="text"><![CDATA[自从阮一峰的博客中增加了每周分享栏目，自己每周五都是主动的浏览一下阮老师的每周分享，一来阮老师的涉猎非常广泛，可以提高自己的视野；二来，阮老师的文章都特别容易懂，给人一种一直想看下去的冲动。 我个人平常也会看很多的技术类文章，也会遇到各种工具或者特别不错的文章，也有分享的冲动，也想搞一些分享的文章，当然我没有阮老师勤奋和涉猎广泛，但也期望能够对他人有所帮助，哪怕文中的一条分享能够让读者觉得有价值，那么也是值的做的一件事情。 具体的分享版块可能不会特别固定，分享的间隔也不会特别勤快，很难做到阮老师的一周一次的频次。 教程1. The Go Memory ModelGolang的内存模型，建议Golang开发者读一遍。 2. 《极客时间》-左耳听风知名博客酷壳的作者陈皓的技术专栏，花钱购买一下专栏还是非常值得的，尤其是最近写的程序员练级攻略系列，能提供大量有价值的学习资料及方向指导，非常赞。 http:// 3. 深入解析 kubernetes 资源管理，容器云牛人有话说对kubernetes的资源管理讲解的非常到位和深入，文章略长，需要花点时间才能读完，值的一看。 4. 《TCP/IP详解 卷1: 协议》网络方面的经典著作，每个工程师必读，虽然是写给工程师看的，但很多的学术著作中引用到了该书中内容。 5. 《深入解析GO》对于go的内部实现原理讲解的挺到位，对于理解go的原理挺有帮助。涉及少量汇编，我不太懂汇编，涉及汇编的地方直接跳过了。 6. Red Hat Enterprise Linux DocumentRedHat官方的Linux文档，我个人还没怎么读过。 资源1. 腾讯大学-CEO来了货真价实的互联网CEO的视频分享，谈创业、谈感悟，目前已经有蔚来汽车、VIPKID、每日优鲜、快手、Keep、知乎的CEO的分享。 2. nginx-upsync-module新浪微博开源的nginx module，用于动态更改upstream server。 3. 语义化版本规范SemVer软件版本在取名上会比较混乱，有的使用1.0.1，有的使用1.0等，SemVer用于规范软件版本的命名。 4. 《见识》吴军老师的最新图书，内容整理自吴军的专栏《硅谷来信》，每篇文章一个主题，值的一读。 5. 嗨！济南又听到了一首关于济南的歌曲，曾在济南生活多年，必须要分享一下。 6. termtosvgGithub上的开源项目，将命令行工具单独保存为SVG动画。 7. teleport提供了ssh的审计和回放，基于SSH的RBAC管理，同时还有一个带管理功能的ui界面，目的是用于取代系统自带的sshd。 工具1. cloc统计代码行数的工具，下面是kubernetes项目的v1.11.2版本的代码行数统计，go的代码行数已经超过了100万行。 2. SpaceVim我个人不是vim工具党，刚毕业那会曾经一度热衷于将vim打造成为一个开发C++的IDE，但经过复杂的配置后仍然难以达到CLion这种IDE的水平。最近偶然看到SpaceVim，心中为之一振，这就是我想要的vim，虽达不到IDEA的高度，但已经可以跟vscode的易用度差不多了。 SpaceVim的强大之处在于Space键的使用，默认情况下按下空格键会给出快捷键的提示，类似于桌面系统中的菜单功能。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux macvlan network]]></title>
      <url>%2Fpost%2Fmacvlan%2F</url>
      <content type="text"><![CDATA[macvlan的原理是在宿主机物理网卡上虚拟出多个子接口，每个子接口有独立的mac地址，通过不同的MAC地址在数据链路层（Data Link Layer）进行网络数据转发的。达到的效果类似，一块物理网卡上有多个IP地址，多个IP地址有自己的mac地址。 它是比较新的网络虚拟化技术，需要较新的内核支持（Linux kernel v3.9–3.19 and 4.0+）。 macvlan设备跟物理设备之间并不直接互通。 macvlan不以交换机端口来划分vlan，一个交换机端口可接收来自多个mac地址的数据。 一个交换机端口要处理多个vlan的数据，需要开启trunk模式。 四种模式以下四种模式为每个macvlan设备单独配置，而不是一个物理设备就只有一个配置。 VEPA所有发送出去的报文都经过交换机，交换机再发送到对应的目标地址。默认模式。物理网卡接收到macvlan设备的数据后，总是将数据发送出去，即使是发往本设备上其他macvlan设备的数据包。这样在交换机设备上可以看到所有网络的流量。如果是本机的macvlan设备流量仍然是发往本机的macvlan设备，可能会被交换机的生成树协议阻止。需要交换机开启hairpin模式或者reflective relay模式，该模式在目前的交换机上未广泛支持，vepa模式的应用较少。 linux的网桥支持hairpin模式。 bridge最常用，同一个物理设备上的不同macvlan设备间的通讯可以直接转发，不再需要经过外部的交换机。转发非常快速，macvlan设备相对固定，不需要生成树协议。 Private本质上是VEPA模式，但同一个物理设备上的macvlan设备之间无法直接通讯，不常用。 Passthru后来增加的模式，比较少用 vepa和passthru都会将不同macvlan接口之间的数据发送到交换机，然后发回，对性能的影响比较明显。 物理网卡收到包后，根据包的mac地址来判断这个包交给哪个虚拟接口。 手工创建实践以下实验为在virturalbox虚拟机下 1234567891011121314151617181920212223242526# 创建两个network namespace net1和net2ip netns add net1ip netns add net2# 创建macvlan接口# enp0s8相当于物理网卡eth0ip link add link enp0s8 mac1 type macvlan# 可以看到创建了接口mac1@enp0s8[root@localhost vagrant]# ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:6c:3e:95 brd ff:ff:ff:ff:ff:ff3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000 link/ether 08:00:27:cf:b0:b4 brd ff:ff:ff:ff:ff:ff4: docker_gwbridge: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:8e:cf:a9:da brd ff:ff:ff:ff:ff:ff5: br-b3e83aa45886: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:bf:b4:e5:36 brd ff:ff:ff:ff:ff:ff6: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:25:e7:40:32 brd ff:ff:ff:ff:ff:ff19: br-ec6c4e77321d: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT link/ether 02:42:e0:d8:38:6d brd ff:ff:ff:ff:ff:ff24: mac1@enp0s8: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff 创建macvlan接口的格式为：ip link add link &lt;PARENT&gt; &lt;NAME&gt; type macvlan， 是macvlan接口的父接口名称，name是新创建的macvlan接口名称。 123456789101112131415161718192021222324# 将mac1放入到net1 namespace中[root@localhost vagrant]# ip link set mac1 netns net1[root@localhost vagrant]# ip netns exec net1 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0024: mac1@if3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff link-netnsid 0# 在net1中将mac1接口命名为eth0[root@localhost vagrant]# ip netns exec net1 ip link set mac1 name eth0[root@localhost vagrant]# ip netns exec net1 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0024: eth0@if3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff link-netnsid 0# 在net1中分配eth0网卡的ip地址为192.168.8.120[root@localhost vagrant]# ip netns exec net1 ip addr add 192.168.8.120/24 dev eth0[root@localhost vagrant]# ip netns exec net1 ip link set eth0 up[root@localhost vagrant]# ip netns exec net1 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0024: eth0@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT link/ether 3e:b8:e9:1d:3b:c8 brd ff:ff:ff:ff:ff:ff link-netnsid 0 在docker上的实践123456789101112[root@localhost vagrant]# docker network create -d macvlan --subnet=10.0.2.100/24 --gateway=10.0.2.2 -o parent=enp0s3 mcv5c637798d559471bd8d1036cdd947d3949e1973f724568da066d9c60c00fb5e6[root@localhost vagrant]# docker network lsNETWORK ID NAME DRIVER SCOPEb6a128f1730e bridge bridge locala65957cd6c3f docker_gwbridge bridge local540bb390028a host host localb3e83aa45886 isolated_nw bridge localec6c4e77321d local_alias bridge local5c637798d559 mcv macvlan local3d247d0414d0 none null local referenceSome notes on macvlan/macvtap]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pushd和popd命令的用法]]></title>
      <url>%2Fpost%2Fpushd-popd%2F</url>
      <content type="text"><![CDATA[pushd和popd命令的用法在编写shell的时候，经常会在目录之间进行切换，如果使用cd命令经常会切换错误，pushd和popd使用栈的方式来管理目录。 dirs用于显示当前目录栈中的所有记录。 pushd将目录加入到栈顶部，并将当前目录切换到该目录。若不加任何参数，该命令用于将栈顶的两个目录进行对调。 popd删除目录栈中的目录。若不加任何参数，则会首先删除目录栈顶的目录，并将当前目录切换到栈顶下面的目录。 命令格式：pushd [-N | +N] [-n] +N 将第N个目录删除（从左边数起，数字从0开始） -N 将第N个目录删除（从右边数起，数字从0开始） -n 将目录出栈时，不切换目录 example123456789101112131415161718192021222324[root@localhost tmp]# mkdir /tmp/dir&#123;1,2,3,4&#125;[root@localhost tmp]# pushd /tmp/dir1/tmp/dir1 /tmp[root@localhost dir1]# pushd /tmp/dir2/tmp/dir2 /tmp/dir1 /tmp[root@localhost dir2]# pushd /tmp/dir3/tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp[root@localhost dir3]# pushd /tmp/dir4/tmp/dir4 /tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp# dirs的显示内容跟pushd完成后的输出一致[root@localhost dir4]# dirs/tmp/dir4 /tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp[root@localhost dir4]# popd/tmp/dir3 /tmp/dir2 /tmp/dir1 /tmp# 带有数字比较容易出错[root@localhost dir3]# popd +1/tmp/dir3 /tmp/dir1 /tmp# 清除目录栈[root@localhost dir3]# dirs -c[root@localhost dir3]# dirs/tmp/dir3]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[理解OverlayFS]]></title>
      <url>%2Fpost%2Foverlayfs%2F</url>
      <content type="text"><![CDATA[docker image结构docker可以通过命令docker image inspect ${image}来查看image的详细信息，其中包含了所使用的底层文件系统及各层的信息。 docker container的存储结构分为了只读层、init层和可读写层。 只读层跟docker image的层次结构恰好对应，主要包含操作系统的文件、配置、目录等信息，不包含操作系统镜像。 init层在只读层和读写层中间，用来专门存放/etc/hosts /etc/resolv.conf等信息，这些文件往往需要在启动的时候写入一些指定值，但不期望docker commit命令对其进行提交。 可读写层为容器在运行中可以进行写入的层。 overlay 采用了两层结构，lowerdir为镜像层，只读。upperdir为容器层。 每层都会在/var/run/docker/overlay创建一个文件夹，文件夹中为实际层的内容，文件采用硬链接的方式链接到真实层中的文件，每一层都包含该层该拥有的所有文件，而该文件的真实存储可能是采用硬链接的方式链接到上层中的真实文件，因此比较耗费inode。 创建一个容器时，会新增两个目录，一个为读写层，一个为初始层。初始层中保存了容器初始化时的环境信息，如hostname、hosts文件等。读写层用于记录容器的所有改动。 overlay2为了规避overlay消耗inode节点过多的问题，overlay2采用在每层中增加lower文件的方式来记录所有底层的信息，类似于链表的形式。 docker pull ubuntu 12345678910[root@localhost runc]# docker pull ubuntuUsing default tag: latestlatest: Pulling from library/ubuntuc64513b74145: Pull complete01b8b12bad90: Pull completec5d85cf7a05f: Pull completeb6b268720157: Pull completee12192999ff1: Pull completeDigest: sha256:3f119dc0737f57f704ebecac8a6d8477b0f6ca1ca0332c7ee1395ed2c6a82be7Status: Downloaded newer image for ubuntu:latest 会在/var/run/docker/overlay2目录下创建如下文件： 123456789101112131415161718192021222324252627282930313233343536[root@localhost overlay2]# tree -L 2.|-- 664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672 // 第0层| |-- diff| `-- link // MZUEUOFHBNVTRCJYJEG7QY4VWT|-- 783ad02709b67ac47b55198e9659c4592f0972334987ab97f42fd10f1784cbba // 第2层| |-- diff| |-- link // HXATFASQ4E2JBG434DUEN54EZZ| |-- lower // l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT| |-- merged| `-- work|-- 89f7a20dda3d868840e20d9e8f1bfe20c5cca51c27b07825f100da0f474672f6 // 第3层| |-- diff| |-- link // 5PHT7S3MCZTTQOXVPA4CKJRRFD| |-- lower // l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT| |-- merged| `-- work|-- backingFsBlockDev|-- bad073a2d1f79a03af6caa0b3f51a22e6762cebbc0c30e45458fe6c1ff266f68 // 第4层| |-- diff| |-- link // QMKHIPSDT4JTPE4FLT7QGJ33ND| |-- lower // l/5PHT7S3MCZTTQOXVPA4CKJRRFD:l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT| |-- merged| `-- work|-- cb40b5b47c699050305676b35b1cea1ce08b38604dd68243c4be48934125b1a3 // 第1层| |-- diff| |-- link // WUZC5WSTQTPJUJ4KFAYCUT5IPD| |-- lower // l/MZUEUOFHBNVTRCJYJEG7QY4VWT| |-- merged| `-- work`-- l |-- 5PHT7S3MCZTTQOXVPA4CKJRRFD -&gt; ../89f7a20dda3d868840e20d9e8f1bfe20c5cca51c27b07825f100da0f474672f6/diff |-- HXATFASQ4E2JBG434DUEN54EZZ -&gt; ../783ad02709b67ac47b55198e9659c4592f0972334987ab97f42fd10f1784cbba/diff |-- MZUEUOFHBNVTRCJYJEG7QY4VWT -&gt; ../664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672/diff |-- QMKHIPSDT4JTPE4FLT7QGJ33ND -&gt; ../bad073a2d1f79a03af6caa0b3f51a22e6762cebbc0c30e45458fe6c1ff266f68/diff `-- WUZC5WSTQTPJUJ4KFAYCUT5IPD -&gt; ../cb40b5b47c699050305676b35b1cea1ce08b38604dd68243c4be48934125b1a3/diff l目录下为超链接，缩短后的目录，为了避免mount时超出页大小限制。 每一层中的diff文件夹包含实际内容。 每一层中都有一个link文件，内容为l目录中的超链接，超链接实际指向当前层目录中的diff文件夹。 除去最底层的目录外，其余每一层中包含一个lower文件，包含了该层的所有更底层名称和顺序，可以根据该文件构建出整个镜像的层次结构。 work目录用于OverlayFS内部使用。 最底层只有link文件，无lower文件，因此664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672为最底层。 以上五层为lower，只读。 当使用docker run -it ubuntu:latest /bin/bash启动一个容器后，在overlay2目录下会多出两个文件夹。 123456789101112131415161718192021[root@localhost overlay2]# tree -L 1 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08 l0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init|-- diff|-- link // ZJVMGTB2IOJ6QF57TYM5O7EWXW|-- lower // l/QMKHIPSDT4JTPE4FLT7QGJ33ND:l/5PHT7S3MCZTTQOXVPA4CKJRRFD:l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT|-- merged`-- work0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08|-- diff|-- link|-- lower // l/ZJVMGTB2IOJ6QF57TYM5O7EWXW:l/QMKHIPSDT4JTPE4FLT7QGJ33ND:l/5PHT7S3MCZTTQOXVPA4CKJRRFD:l/HXATFASQ4E2JBG434DUEN54EZZ:l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:l/MZUEUOFHBNVTRCJYJEG7QY4VWT|-- merged`-- workl|-- 5PHT7S3MCZTTQOXVPA4CKJRRFD -&gt; ../89f7a20dda3d868840e20d9e8f1bfe20c5cca51c27b07825f100da0f474672f6/diff|-- AOLYGFOHIAHWU5CBAJFULNAXI7 -&gt; ../0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/diff|-- HXATFASQ4E2JBG434DUEN54EZZ -&gt; ../783ad02709b67ac47b55198e9659c4592f0972334987ab97f42fd10f1784cbba/diff|-- MZUEUOFHBNVTRCJYJEG7QY4VWT -&gt; ../664ae13f1c21402385076025d68476eb8d1cc4be6c6a218b24bd55217ac62672/diff|-- QMKHIPSDT4JTPE4FLT7QGJ33ND -&gt; ../bad073a2d1f79a03af6caa0b3f51a22e6762cebbc0c30e45458fe6c1ff266f68/diff|-- WUZC5WSTQTPJUJ4KFAYCUT5IPD -&gt; ../cb40b5b47c699050305676b35b1cea1ce08b38604dd68243c4be48934125b1a3/diff`-- ZJVMGTB2IOJ6QF57TYM5O7EWXW -&gt; ../0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init/diff 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init用于存放容器初始化时的信息，通过下面查看更直观。 123456789101112131415[root@localhost overlay2]# tree 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08-init|-- diff| |-- dev| | `-- console| `-- etc| |-- hostname| |-- hosts| |-- mtab -&gt; /proc/mounts| `-- resolv.conf|-- link|-- lower|-- merged`-- work `-- work 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08的直接底层为init层，更详细的目录结构如下。 123456789101112131415161718192021222324252627[root@localhost overlay2]# tree -L 2 0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c080326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08|-- diff|-- link|-- lower|-- merged| |-- bin| |-- boot| |-- dev| |-- etc| |-- home| |-- lib| |-- lib64| |-- media| |-- mnt| |-- opt| |-- proc| |-- root| |-- run| |-- sbin| |-- srv| |-- sys| |-- tmp| |-- usr| `-- var`-- work `-- work merged文件夹中内容较多，为overlay2的直接挂载点，对容器的修改会反应到该目录中。例如在容器中增加/root/hello.txt文件，在merged目录下会增加root/hello.txt文件。 123[root@localhost overlay2]# mount | grep overlay2/dev/mapper/centos-root on /var/lib/docker/overlay2 type xfs (rw,relatime,attr2,inode64,noquota)overlay on /var/lib/docker/overlay2/0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/ZJVMGTB2IOJ6QF57TYM5O7EWXW:/var/lib/docker/overlay2/l/QMKHIPSDT4JTPE4FLT7QGJ33ND:/var/lib/docker/overlay2/l/5PHT7S3MCZTTQOXVPA4CKJRRFD:/var/lib/docker/overlay2/l/HXATFASQ4E2JBG434DUEN54EZZ:/var/lib/docker/overlay2/l/WUZC5WSTQTPJUJ4KFAYCUT5IPD:/var/lib/docker/overlay2/l/MZUEUOFHBNVTRCJYJEG7QY4VWT,upperdir=/var/lib/docker/overlay2/0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/diff,workdir=/var/lib/docker/overlay2/0326c1da0af912a6ea5efda77b65b04e796993e0f111ed8f262c55b2716f1c08/work) ref Use the OverlayFS storage driver Docker存储驱动—Overlay/Overlay2「译」]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Golang面试题]]></title>
      <url>%2Fpost%2Fgolang-interview%2F</url>
      <content type="text"><![CDATA[本文绝大多数题目来源于网络，部分题目为原创。 slice相关以下代码有什么问题，说明原因12345678910111213141516type student struct &#123; Name string Age int&#125;func pase_student() &#123; m := make(map[string]*student) stus := []student&#123; &#123;Name: "zhou", Age: 24&#125;, &#123;Name: "li", Age: 23&#125;, &#123;Name: "wang", Age: 22&#125;, &#125; for _, stu := range stus &#123; m[stu.Name] = &amp;stu &#125;&#125; 每次遍历的时候stu变量为值拷贝，stu变量的地址未改变，即&amp;stu未改变，遍历结束后stu指向stus中的最后一个元素。 使用reflect.TypeOf(str)打印出的类型为main.student，如果使用stu.Age += 10这样的语法是不会修改stus中的值的。 可修改为如下形式： 123for i, _ := range stus &#123; m[stus[i].Name] = &amp;stus[i]&#125; 有一个slice of object, 遍历slice修改name为指定的值12345678type foo struct &#123; name string value string&#125;func mutate(s []foo, name string) &#123;// TODO&#125; 意在考察range遍历的时候是值拷贝，以及slice的内部数据结构，slice的数据结构如下： 123456struct Slice&#123; // must not move anything byte* array; // actual data uintgo len; // number of elements uintgo cap; // allocated number of elements&#125;; 执行append函数后会返回一个新的Slice对象，新的Slice对象跟旧Slice对象共用相同的数据存储，但是len的值并不相同。 该题目中，可以通过下面的方式来修改值: 123456789// range方式for i, _ := range s &#123; s[i].name = name&#125;// for i形式for i:=0; i&lt;len(s); i++ &#123; s[i].name = name&#125; 从slice中找到一个元素匹配name，并将该元素的指针添加到一个新的slice中，返回新slice1234func find(s []foo, name string) []*foo &#123;// TODO&#125; 仍旧是考察range是值拷贝的用法，此处使用for i 循环即可 1234567891011func find(s []foo, name string) []*foo &#123; res := []*foo&#123;&#125; for i := 0; i &lt; len(s); i++ &#123; if s[i].name == name &#123; res = append(res, &amp;(s[i])) break &#125; &#125; return res&#125; 下面输出什么内容1234567891011121314151617181920package mainimport &quot;fmt&quot;func m(s []int) &#123; s[0] = -1 s = append(s, 4)&#125;func main() &#123; s1 := []int&#123;1, 2, 3&#125; m(s1) s2 := make([]int, 3, 6) m(s2) s2 = append(s2, 7) s3 := [3]int&#123;1, 2, 3&#125; fmt.Println(s1) fmt.Println(s2) fmt.Println(s3)&#125; slice的函数传递为值拷贝方式，在函数m中对下标为0的元素的修改会直接修改原slice中的值，因为slice中的指针指向的地址是相同的。 append之后的slice虽然可能是在原数组上增加了元素，但原slice中的len字段并没有变化。 make([]int, 3, 6)虽然指定了slice的cap，但对于append没有影响，还是会在slice中最后一个元素的下一个位置增加新元素。 数组由于是值拷贝，对新数组的修改不会影响到原数组。 输出内容如下： 123[-1 2 3][-1 0 0 7][1 2 3] 下面输出什么内容该题目为我自己想出来的，非来自于互联网，意在考察对slice和append函数的理解。 123456789func f() &#123; s1 := make([]int, 2, 8) fmt.Println(s1) s2 := append(s1, 4) fmt.Println(s2) s3 := append(s1, 5) fmt.Println(s3) fmt.Println(s2)&#125; 输出结果如下，在执行第二个append后，第一个append在内存中增加的元素4会被5覆盖掉。执行结果可以通过fmt.Println(s1, cap(s1), &amp;s1[0])的形式将第一个元素的内存地址打印出来查看。 123[0 0 4][0 0 5][0 0 5] goroutine以下代码输出内容：12345678910111213141516package mainimport ( &quot;fmt&quot; &quot;runtime&quot;)func main() &#123; runtime.GOMAXPROCS(1) go func() &#123; fmt.Println(1) &#125;() for &#123; &#125; fmt.Println(1)&#125; 不会有任何输出 下面输出的内容12345678910111213141516171819202122type People struct&#123;&#125;func (p *People) ShowA() &#123; fmt.Println("showA") p.ShowB()&#125;func (p *People) ShowB() &#123; fmt.Println("showB")&#125;type Teacher struct &#123; People&#125;func (t *Teacher) ShowB() &#123; fmt.Println("teacher showB")&#125;func main() &#123; t := Teacher&#123;&#125; t.ShowA()&#125; 输出 12showAshowB 有点出乎意料，可以举个反例，如果ShowA()方法会调用到Teacher类型的ShowB()方法，假设People和Teacher并不在同一个包中时，编译一定会出现错误。 Go中没有继承机制，只有组合机制。 下面代码会触发异常吗？请详细说明12345678910111213func main() &#123; runtime.GOMAXPROCS(1) int_chan := make(chan int, 1) string_chan := make(chan string, 1) int_chan &lt;- 1 string_chan &lt;- &quot;hello&quot; select &#123; case value := &lt;-int_chan: fmt.Println(value) case value := &lt;-string_chan: panic(value) &#125;&#125; 会间歇性触发异常，select会随机选择。 以下代码能编译过去吗？为什么？1234567891011121314151617181920212223242526package mainimport ( &quot;fmt&quot;)type People interface &#123; Speak(string) string&#125;type Student struct&#123;&#125;func (stu *Student) Speak(think string) (talk string) &#123; if think == &quot;bitch&quot; &#123; talk = &quot;You are a good boy&quot; &#125; else &#123; talk = &quot;hi&quot; &#125; return&#125;func main() &#123; var peo People = Student&#123;&#125; think := &quot;bitch&quot; fmt.Println(peo.Speak(think))&#125; 不能编译过去，提示Stduent does not implement People (Speak method has pointer receiver)，将Speak定义更改为func (stu Stduent) Speak(think string) (talk string)即可编译通过。 main的调用方式更改为如下也可以编译通过var peo People = new(Stduent)。 func (stu *Stduent) Speak(think string) (talk string)是*Student类型的方法，不是Stduent类型的方法。 下面输出什么1234567891011121314151617181920212223package mainimport ( &quot;fmt&quot; &quot;time&quot; &quot;runtime&quot;)func main() &#123; runtime.GOMAXPROCS(1) arr := [10000]int&#123;&#125; for i:=0; i&lt;len(arr); i++ &#123; arr[i] = i &#125; for _, a := range arr &#123; go func() &#123; fmt.Println(a) &#125;() &#125; for &#123; time.Sleep(time.Second) &#125;&#125; 一直输出9999.涉及到goroutine的切换时机，仅系统调用或者有函数调用的情况下才会切换goroutine，for循环情况下一直没有系统调用或函数切换发生，需要等到for循环结束后才会启动新的goroutine。 以下代码打印出来什么内容，说出为什么。。。12345678910111213141516171819202122232425262728package mainimport ( &quot;fmt&quot;)type People interface &#123; Show()&#125;type Student struct&#123;&#125;func (stu *Student) Show() &#123;&#125;func live() People &#123; var stu *Student return stu&#125;func main() &#123; if live() == nil &#123; fmt.Println(&quot;AAAAAAA&quot;) &#125; else &#123; fmt.Println(&quot;BBBBBBB&quot;) &#125;&#125; 打印BBBBBBB。 byte与rune的关系 byte alias for uint8 rune alias for uint32，用来表示unicode 12345678func main() &#123; // range遍历为rune类型，输出int32 for _, w:=range &quot;123&quot; &#123; fmt.Printf(&quot;%T&quot;, w) &#125; // 取数组为byte类型，输出uint8 fmt.Printf(&quot;%T&quot;, &quot;123&quot;[0])&#125; 写出打印的结果1234567891011121314151617type People struct &#123; name string `json:"name"`&#125;func main() &#123; js := `&#123; "name":"11" &#125;` var p People p.name = "123" err := json.Unmarshal([]byte(js), &amp;p) if err != nil &#123; fmt.Println("err: ", err) return &#125; fmt.Println("people: ", p)&#125; 打印结果为people: {123} 下面函数有什么问题？123func funcMui(x,y int)(sum int,error)&#123; return x+y,nil&#125; 函数返回值命名 在函数有多个返回值时，只要有一个返回值有指定命名，其他的也必须有命名。 如果返回值有有多个返回值必须加上括号； 如果只有一个返回值并且有命名也需要加上括号； 此处函数第一个返回值有sum名称，第二个为命名，所以错误。 以下函数输出什么123456789101112131415161718192021222324252627282930313233343536package mainfunc main() &#123; println(DeferFunc1(1)) println(DeferFunc2(1)) println(DeferFunc3(1)) println(DeferFunc4(1))&#125;func DeferFunc1(i int) (t int) &#123; t = i defer func() &#123; t += 3 &#125;() return t&#125;func DeferFunc2(i int) int &#123; t := i defer func() &#123; t += 3 &#125;() return t&#125;func DeferFunc3(i int) (t int) &#123; defer func() &#123; t += i &#125;() return 2&#125;func DeferFunc4(i int) (t int) &#123; t = 10 return 2&#125; 输出结果为: 4 1 3 2 return语句不是一个原子指令，分为两个阶段，执行return后面的表达式和返回表达式的结果。defer函数在返回表达式之前执行。 执行return后的表达式给返回值赋值 调用defer函数 空的return DeferFunc1在第一步执行表达式后t=1，执行defer后t=4，返回值为4 DeferFunc2在第一步执行表达式后t=1，执行defer后t=4，返回值为第一步表达式的结果1 DeferFunc3在第一步表达式为t=2，执行defer后t=3，返回值为t=3 DeferFunc4在第一步执行表达式后t=2，返回值为t=2 是否可以编译通过？如果通过，输出什么？123456789101112131415161718192021222324252627282930313233343536373839404142434445package mainimport ( &quot;fmt&quot;)func main() &#123; sn1 := struct &#123; age int name string &#125;&#123;age: 11, name: &quot;qq&quot;&#125; sn2 := struct &#123; age int name string &#125;&#123;age: 11, name: &quot;qq&quot;&#125; sn3 := struct &#123; name string age int &#125;&#123;age: 11, name: &quot;qq&quot;&#125; if sn1 == sn2 &#123; fmt.Println(&quot;sn1 == sn2&quot;) &#125; if sn1 == sn3 &#123; fmt.Println(&quot;sn1 == sn3&quot;) &#125; sm1 := struct &#123; age int m map[string]string &#125;&#123;age: 11, m: map[string]string&#123;&quot;a&quot;: &quot;1&quot;&#125;&#125; sm2 := struct &#123; age int m map[string]string &#125;&#123;age: 11, m: map[string]string&#123;&quot;a&quot;: &quot;1&quot;&#125;&#125; if sm1 == sm2 &#123; fmt.Println(&quot;sm1 == sm2&quot;) &#125;&#125; 结构体比较 进行结构体比较时候，只有相同类型的结构体才可以比较，结构体是否相同不但与属性类型个数有关，还与属性顺序相关。 还有一点需要注意的是结构体是相同的，但是结构体属性中有不可以比较的类型，如map,slice。 如果该结构属性都是可以比较的，那么就可以使用“==”进行比较操作。 是否可以编译通过？如果通过，输出什么？123456789101112131415161718package mainimport ( &quot;fmt&quot;)func Foo(x interface&#123;&#125;) &#123; if x == nil &#123; fmt.Println(&quot;empty interface&quot;) return &#125; fmt.Println(&quot;non-empty interface&quot;)&#125;func main() &#123; var x *int = nil Foo(x)&#125; 输出“non-empty interface” 交替打印数字和字母使用两个 goroutine 交替打印序列，一个 goroutine 打印数字， 另外一个 goroutine 打印字母， 最终效果为: 12AB34CD56EF78GH910IJ1112KL1314MN1516OP1718QR1920ST2122UV2324WX2526YZ2728 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package mainimport ( "fmt" "sync")func main() &#123; number, letter := make(chan bool), make(chan bool) wait := new(sync.WaitGroup) go func () &#123; num := 1 for &#123; &lt;-number fmt.Printf("%d%d", num, num+1) num += 2 letter &lt;- true if num &gt; 28 &#123; break &#125; &#125; wait.Done() &#125;() go func () &#123; begin := 'A' for &#123; &lt;- letter if begin &lt; 'Z' &#123; fmt.Printf("%c%c", begin, begin+1) begin+=2 number &lt;- true &#125; else &#123; break &#125; &#125; wait.Done() &#125;() number &lt;- true wait.Add(2) wait.Wait()&#125; struct类型的方法调用假设T类型的方法上接收器既有T类型的，又有T指针类型的，那么就不可以在不能寻址的T值上调用T接收器的方法。 请看代码,试问能正常编译通过吗？ 12345678910111213141516import ( "fmt")type Lili struct&#123; Name string&#125;func (Lili *Lili) fmtPointer()&#123; fmt.Println("poniter")&#125;func (Lili Lili) fmtReference()&#123; fmt.Println("reference")&#125;func main()&#123; li := Lili&#123;&#125; li.fmtPointer()&#125; 能正常编译通过，并输出”poniter” 请接着看以下的代码，试问能编译通过？ 123456789101112131415import ( "fmt")type Lili struct&#123; Name string&#125;func (Lili *Lili) fmtPointer()&#123; fmt.Println("poniter")&#125;func (Lili Lili) fmtReference()&#123; fmt.Println("reference")&#125;func main()&#123; Lili&#123;&#125;.fmtPointer()&#125; 不能编译通过。“cannot call pointer method on Lili literal”“cannot take the address of Lili literal” 其实在第一个代码示例中，main主函数中的“li”是一个变量，li的虽然是类型Lili，但是li是可以寻址的，&amp;li的类型是Lili，因此可以调用Lili的方法。 golang context包的用法 goroutine之间的传值 goroutine之间的控制 在单核cpu的情况下，下面输出什么内容？123456789101112131415161718package mainimport ( &quot;fmt&quot; &quot;sync&quot;)func main() &#123; wg := sync.WaitGroup&#123;&#125; for _, i:=range []int&#123;1, 2, 3, 4, 5&#125; &#123; wg.Add(1) go func() &#123; defer wg.Done() fmt.Println(i) &#125; () &#125; wg.Wait()&#125; 考察golang的runtime机制，goroutine的切换时机只有在有系统调用或者函数调用时才会发生，本例子中的for循环结束之前不会发生goroutine的切换，所以最终输出结果为5. 下面输出什么1234567891011121314151617181920212223242526package mainimport ( &quot;fmt&quot;)type People interface &#123; Speak(string) string&#125;type Stduent struct&#123;&#125;func (stu *Stduent) Speak(think string) (talk string) &#123; if think == &quot;bitch&quot; &#123; talk = &quot;You are a good boy&quot; &#125; else &#123; talk = &quot;hi&quot; &#125; return&#125;func main() &#123; var peo People = Stduent&#123;&#125; think := &quot;bitch&quot; fmt.Println(peo.Speak(think))&#125; 编译不通过，仅*Student实现了People接口，更改为var peo People = &amp;Student{}即可编译通过。 下面输出什么12345678910package mainconst cl = 100var bl = 123func main() &#123; println(&amp;bl, bl) println(&amp;cl, cl)&#125; 编译失败，常量cl通常在预处理阶段会直接展开，无法取其地址。 以下代码是否存在问题，请解释你的判断和理由12345678import &quot;sync&quot;func f(m sync.Mutex) &#123; m.Lock() defer m.Unlock() // Do something...&#125; Mutex对象不能被值拷贝,后续传递需要使用指针的形式 以下代码输出是什么 解释一下12345678910111213141516171819202122func main() &#123; case1() case2()&#125;func case1() &#123; s1 := make([]string, 1, 20) s1[0] = &quot;hello&quot; p1 := &amp;s1[0] s1 = append(s1, &quot;world&quot;) *p1 = &quot;hello2&quot; fmt.Printf(&quot;value of p1 is %s, value of s1[0] is %s \n&quot;, *p1, s1[0])&#125;func case2() &#123; s1 := make([]string) s1[0] = &quot;hello&quot; p1 := &amp;s1[0] s1 = append(s1, &quot;world&quot;) *p1 = &quot;hello2&quot; fmt.Printf(&quot;value of p1 is %s, value of s1[0] is %s \n&quot;, *p1, s1[0])&#125; 本题意在考察string和slice的数据结构，string的数据结构如下： case1的内存结构变化情况如下： case2由于s1默认长度为0，直接使用s1[0]复制会出现panic错误。 ref golang 面试题 Go面试题答案与解析 golang面试笔试题(第二版) interview-go Awesome Go Interview Questions and Answers Golang面试题解析（二） Golang面试题解析（三） golang错题集]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[resolve.conf配置文件解析]]></title>
      <url>%2Fpost%2Fresolve%2F</url>
      <content type="text"><![CDATA[一直以来对/etc/resolv.conf配置文件中的search和domain字段的含义不是很理解，这里重新学习记录一下。 实验一修改/resolv.conf配置文件的内容如下： 1nameserver 8.8.8.8 可以看到该nameserver是生效的，但是访问map域名是不生效的，因为没有map这个域名. 123456[vagrant@localhost ~]$ ping map.baidu.comPING map.n.shifen.com (119.75.222.71) 56(84) bytes of data.64 bytes from 119.75.222.71: icmp_seq=1 ttl=63 time=4.22 ms[vagrant@localhost ~]$ ping mapping: unknown host map 实验二修改文件内容如下： 123nameserver 8.8.8.8search baidu.com google.com 此时可以ping通map域名，解析到了跟map.baidu.com相同的域名.如果map.baidu.com的域名没有解析到，会继续解析map.google.com的域名。 123[vagrant@localhost ~]$ ping mapPING map.n.shifen.com (112.80.248.48) 56(84) bytes of data.64 bytes from 112.80.248.48: icmp_seq=1 ttl=63 time=28.6 ms domain的作用跟search类似，作为search的默认值，因为search可以使用多个域.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[google autheticator应用现状]]></title>
      <url>%2Fpost%2Fgoogle-autheticator%2F</url>
      <content type="text"><![CDATA[通过使用Google的登陆二步验证（即Google Authenticator服务），我们在登陆时需要输入额外由手机客户端生成的一次性密码。大大提高登陆的安全性。 实现Google Authenticator功能需要服务器端和客户端的支持。服务器端负责密钥的生成、验证一次性密码是否正确。客户端记录密钥后生成一次性密码。 google实现了基于时间的TOTP算法（Time-based One-time Password），客户端实现包括了android和ios。 算法为公开算法，google没有提供服务端的实现，各个语言都有单独的实现。自己系统使用可以直接使用网上的代码。 linux下有libpam-google-authenticator模块，可以使用yum或者源码编译安装，github上有源码，编译出来的为so文件，可以加到sshd的配置文件中，用于给sshd提供二次认证机制。 客户端和服务端存在时间差的问题，google authenticator的超时时间为30s，服务端可以使用两个30s的时间来验证，包括当前和上一个30s。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo添加gitment评论系统]]></title>
      <url>%2Fpost%2Fgitment%2F</url>
      <content type="text"><![CDATA[曾经使用多说和网易云评论作为博客的评论系统，不幸都相继倒闭后，博客就一直没有评论系统。虽博客的访问量可以忽略不计，但本着折腾和好奇的原则，还是折腾一下gitment。 更新hexo-theme-next主题最新版本的next主题已经默认支持gitment，需要将next主题升级到最新版本。 我的hexo-theme-next使用单独的git项目进行管理，git地址为：https://github.com/kuring/hexo-theme-next。接下来需要将fork出来的git项目跟next的git项目进行同步。 在本地创建名字为upstream的remote，指向地址为：git remote add upstream https://github.com/theme-next/hexo-theme-next.git 拉取next项目到本地分支，本地的分支，执行git fetch upstream 将upsteam/master分支合并到master分支上 12345678910git checkout master# 由于修改了_config.yml文件，存在冲突，合并失败lvkai@osx:~/blog/kuring/themes/hexo-theme-next% git merge upstream/master 128 ↵Removing source/css/_common/components/third-party/gentie.stylRemoving layout/_third-party/comments/gentie.swigAuto-merging _config.ymlCONFLICT (content): Merge conflict in _config.ymlRemoving README.en.mdAutomatic merge failed; fix conflicts and then commit the result. 解决冲突后提交并将master分支push到github仓库 注册gitment前往：https://github.com/settings/profile Developer settings -&gt; Register a new application 在界面中输入如下内容： 获取到Client ID和Client Secret. 新建github repo创建新的github项目：https://github.com/kuring/gitment-comments 在next主题中设置gitmentnext主题的配置文件为theme/next/_config.yml，修改其中的gitment设置如下， client_id为在github中注册所获取到的client id client_secret为在github中注册所获取到的client secret github_repo为上面新创建的github repo名称 123456789101112131415# Gitment# Introduction: https://imsun.net/posts/gitment-introduction/gitment: enable: true mint: true # RECOMMEND, A mint on Gitment, to support count, language and proxy_gateway count: true # Show comments count in post meta area lazy: false # Comments lazy loading with a button cleanly: false # Hide &apos;Powered by ...&apos; on footer, and more language: # Force language, or auto switch by theme github_user: kuring # MUST HAVE, Your Github ID github_repo: gitment-comments # MUST HAVE, The repo you use to store Gitment comments client_id: xxx # MUST HAVE, Github client id for the Gitment client_secret: xxxx # EITHER this or proxy_gateway, Github access secret token for the Gitment proxy_gateway: # Address of api proxy, See: https://github.com/aimingoo/intersect redirect_protocol: # Protocol of redirect_uri with force_redirect_protocol when mint enabled 执行hexo clean &amp;&amp; hexo g &amp;&amp; hexo s重新生成页面并在本地运行，可以看到gitment组件已经可以显示了，但是提示Error: Comments Not Initialized错误，点击login，然后允许认证，即可消除该错误。 在界面上添加评论后，可以在github repo的issuse中看到，整个搭建完毕。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[iptables基本知识]]></title>
      <url>%2Fpost%2Fiptables%2F</url>
      <content type="text"><![CDATA[概要netfilter与iptables的关系linux在内核中对数据包过滤和修改的模块为netfilter，netfilter模块本身并不对数据包进行过滤，只是允许将过滤数据包或修改数据包的函数hook到内核网络协议栈的适当位置。 iptables是用户态的工具，用于向netfilter中添加规则从而实现报文的过滤和修改等功能，工作在ip层。ebtables工作在数据链路层，用于处理以太网帧。 图中绿色代表iptables的表，可以看到有部分位于了数据链路层，之所以产生这种奇怪的架构，原因是bridge_nf模块，因为bridge工作在数据链路层，不一定会经过网络层，但仍然需要iptables的功能。详细信息可以在ebtables/iptables interaction on a Linux-based bridge中了解。 概念：tables -&gt; chains -&gt; rules iptabels介绍chain每个表都由一组内置的链，还可以添加用户自定义链，只是用户自定义链没有钩子可以触发，需要从其他链通过-j即JUMP进行触发。 INPUT 链：发往本机的报文 OUTPUT 链：由本机发出的报文 FORWARD 链：经由本机转发的报文 PREROUTING 链：报文到达本机，进行路由决策之前 POSTROUTING 链：报文由本机发出，进行路由决策之后 从chain的角度考虑数据包的流向： 到本机某进程的报文：PREROUTING -&gt; INPUT 由本机转发的报文：PREROUTING -&gt; FORWARD -&gt; POSTROUTING 由本机某进程发出的报文：OUTPUT -&gt; POSTROUTING 当一个网络包进入一台机器的时候，首先拿下 MAC 头看看，是不是我的。如果是，则拿下 IP 头来。得到目标 IP 之后呢，就开始进行路由判断。在路由判断之前，这个节点我们称为 PREROUTING。如果发现 IP 是我的，包就应该是我的，就发给上面的传输层，这个节点叫作 INPUT。如果发现 IP 不是我的，就需要转发出去，这个节点称为 FORWARD。如果是我的，上层处理完毕完毕后，一般会返回一个处理结果，这个处理结果会发出去，这个节点称为 OUTPUT，无论是 FORWARD 还是 OUTPUT，都是路由判断之后发生的，最后一个节点是 POSTROUTING。 table有了chain的概念后，为了便于chain中rule的管理，又引入了table的概念，用于存放相同功能的rule，不同功能的rule放到不同的table中。 包括：filter nat mangle raw filter默认表，管理本机数据包的进出，用于实现包的过滤，对应内核模块iptables_filter input：想要进入linux主机的包output：linux主机要发送的包forward：传递包到后端计算机，与nat table关联较多 nat管理后端主机进出，与linux主机没有关系，与linux后的主机有关 prerouting：进行路由判断之前的规则(dnat/redirect)postrouting：路由判断之后执行的规则(snat/masquerade)output：与发出去的包有关 mangle较少使用，用于拆解报文，修改数据包，并重新封装。 rawraw表的主要作用是允许我们给某些特定的数据包打上标记。 rule包含了匹配条件和处理动作。 匹配条件包括：source ip、destination ip、source port、destination port 处理动作包括： accept: 将包交给协议栈 drop：直接丢弃数据包，不给任何回应 reject：拒绝数据包通过，并给一个响应信息，客户端会收到拒绝消息 queue: 交个某个用户态进程处理 dnat：目的地址转换 snat：源地址转换，必须要指定SNAT地址，即–to-source参数，可以是单个ip，也可以是网段。用在POSTROUTING链上。 masquerade: 源地址伪装，跟snat类似，不需要指定SNAT地址，会自动从服务器上获取SNAT的ip地址。如果有多个网卡的情况下，会使用路由选择算法。 mark: 对数据包进行打标签操作 table filter rule的关系这三者之间的关系还是相当的绕。 链中的规则存在的表chain中存放了rule，某些chain中注定不包含某些rule。例如prerouting链中的rule仅存在于nat raw mangle三张表中。 prerouting链中的规则存在的表：raw mangle natinput链中的规则存在的表：mangle filter natforward链中的规则存在的表：mangle filteroutput链中的规则存在的表：raw mangle filter natpostrouting链中的规则存在的表：mangle nat 表中的规则可以被哪些链使用raw表中的规则可以被链使用：prerouting output 表的名字为小写，链的名字为大写 常用操作查询 -t 用于指定要操作的表，支持raw mangle filter nat，省略-t选项，默认使用filter表 -L 列出rule -v 可查看更详细的信息 -n 规则中以ip地址的形式进行显示 –line-number 显示规则的编号 -x 包的计数以精确数字显示 iptables -t filter -L：从表的角度查询规则，用于查看filter表中的所有规则 iptables -L INPUT: 从链的角度查询规则，用于查看INPUT链中的所有规则 iptables -vL INPUT: 从链的角度查询规则，用于查看INPUT链中的所有规则，可查看更详细信息，包含了规则的匹配信息 iptables -nvL：以精确数字显示 修改-F: 清空规则-I: 表示插入规则-A: 表示以追加的方式插入规则 iptables -F INPUT：清空filter表中的INPUT链中的所有规则。 删除-D: 删除一条规则 iptables -D 链名 规则编号，其中规则编号可以通过--line-number查看到。 trace开启trace功能 12345# centos7 系统下有效，centos6下内核模块为ipt_LOG$ modprobe nf_log_ipv4# 用来验证module是否加载成功$ sysctl net.netfilter.nf_log.2 要开启icmp协议的追踪，执行如下的命令 12iptables -t raw -A OUTPUT -p icmp -m comment --comment &quot;TRACE&quot; -j TRACEiptables -t raw -A PREROUTING -p icmp -m comment --comment &quot;TRACE&quot; -j TRACE –dport: 目的端口 –sport: 源端口 -s: 源ip -d: 目的ip 可以通过如下的命令看到插入的iptabels规则： 1iptables -t raw -nvL --line-number 追踪日志最终会在/var/log/message或者/var/log/kern下看到： 1Feb 6 11:22:04 c43k09006.cloud.k09.am17 kernel: TRACE: raw:PREROUTING:policy:3 IN=docker0 OUT= PHYSIN=bond0.9 MAC=02:42:30:fb:43:94:5c:c9:99:de:c4:8b:08:00 SRC=10.45.8.10 DST=10.45.4.99 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=25550 DF PROTO=ICMP TYPE=0 CODE=0 ID=24191 SEQ=2 格式这块的含义如下： “TRACE: tablename:chainname:type:rulenum “ where type can be “rule” for plain rule, “return” for implicit rule at the end of a user defined chain and “policy” for the policy of the built in chains. 环境清理，删除刚刚创建的规则即可，其中1为规则的编号： 12345# 可以通过此来查询之前创建的规则编号iptables -t raw --line-number -nvL# 删除规则iptables -t raw -D PREROUTING 1iptables -t raw -D OUTPUT 1 实战试验1 基本规则管理插入规则123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 清空filter表中的input链规则[vagrant@localhost ~]$ sudo iptables -F INPUT# 查看filter表中的详细规则，此时从其他机器上ping该ip是通的[vagrant@localhost ~]$ sudo iptables -nvL INPUTChain INPUT (policy ACCEPT 7 packets, 388 bytes) pkts bytes target prot opt in out source destination # 增加规则，拒绝192.168.33.1上的请求# -I：表示插入# INPUT为要插入的链# -s：表示源ip地址# -j：表示要执行的动作[vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.1 -j DROP# 再次查询filter表中的规则，此时192.168.33.1上的报文已经不通[vagrant@localhost ~]$ sudo iptables -t filter -nvLChain INPUT (policy ACCEPT 107 packets, 6170 bytes) pkts bytes target prot opt in out source destination 0 0 DROP all -- * * 192.168.33.1 0.0.0.0/0Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 56 packets, 4355 bytes) pkts bytes target prot opt in out source destination# appent一条接收192.168.33.1的请求规则[vagrant@localhost ~]$ sudo iptables -t filter -A INPUT -s 192.168.33.1 -j ACCEPT# 新增加的序号为2，192.168.33.1的包匹配到1后就停止往下走，因此192.168.33.1还是ping不通当前主机[vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-numberChain INPUT (policy ACCEPT 65 packets, 3572 bytes)num pkts bytes target prot opt in out source destination1 9 756 DROP all -- * * 192.168.33.1 0.0.0.0/02 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0# 插入一条ACCEPT rule，此时192.168.33.1可以ping通当前主机，新插入的规则优先[vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.1 -j ACCEPT[vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-numberChain INPUT (policy ACCEPT 7 packets, 388 bytes)num pkts bytes target prot opt in out source destination1 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/02 10 840 DROP all -- * * 192.168.33.1 0.0.0.0/03 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0# 新插入一条accept 192.168.33.2的规则，插入位置为2，可以看到插入到2的位置了[vagrant@localhost ~]$ sudo iptables -t filter -I INPUT 2 -s 192.168.33.2 -j ACCEPT[vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-numberChain INPUT (policy ACCEPT 7 packets, 388 bytes)num pkts bytes target prot opt in out source destination1 1 84 ACCEPT all -- * * 192.168.33.1 0.0.0.0/02 0 0 ACCEPT all -- * * 192.168.33.2 0.0.0.0/03 10 840 DROP all -- * * 192.168.33.1 0.0.0.0/04 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0 删除规则接下在上面实验的基础上测试删除规则 123456789101112131415# 删除刚刚创建的规则2[vagrant@localhost ~]$ sudo iptables -t filter -D INPUT 2[vagrant@localhost ~]$ sudo iptables -nvL INPUT --line-numberChain INPUT (policy ACCEPT 7 packets, 388 bytes)num pkts bytes target prot opt in out source destination1 1 84 ACCEPT all -- * * 192.168.33.1 0.0.0.0/02 10 840 DROP all -- * * 192.168.33.1 0.0.0.0/03 0 0 ACCEPT all -- * * 192.168.33.1 0.0.0.0/0# 删除source为192.168.33.1，动作为ACCEPT的规则，实际此时执行一次命令仅能删除一条[vagrant@localhost ~]$ sudo iptables -t filter -D INPUT -s 192.168.33.1 -j ACCEPT[vagrant@localhost ~]$ sudo iptables -nvL INPUT --lineChain INPUT (policy ACCEPT 13 packets, 736 bytes)num pkts bytes target prot opt in out source destination1 11 936 DROP all -- * * 192.168.33.1 0.0.0.0/0 修改规则在上面实验的基础上修改规则 12345678910# 将规则动作从REJECT更改为REJECT[vagrant@localhost ~]$ sudo iptables -t filter -R INPUT 1 -s 192.168.33.1 -j REJECT[vagrant@localhost ~]$ sudo iptables -nvL INPUT --lineChain INPUT (policy ACCEPT 7 packets, 388 bytes)num pkts bytes target prot opt in out source destination1 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable# 每个链都有一个默认规则，当前INPUT链中的默认为ACCEPT# 以下可以修改INPUT链的默认规则为DROP，远程连接慎用，不要问我为什么[vagrant@localhost ~]$ sudo iptables -t filter -P INPUT DROP 保存规则防火墙的所有修改都是临时的，重启系统后会失效。iptables会读取/etc/sysconfig/iptables中的规则。 12345# iptables-save命令仅会打印当前的规则，需要使用重定向当前规则到文件中[root@localhost system]# iptables-save &gt; /etc/sysconfig/iptables# 可以从规则文件中载入规则[root@localhost system]# iptables-restore &lt; /etc/sysconfig/iptables 实验二 各类匹配条件的使用匹配条件123456789101112131415161718192021# 可一次性插入两条规则[vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.1,192.168.33.2 -j DROP[vagrant@localhost ~]$ sudo iptables -t filter -nvL INPUT --lineChain INPUT (policy ACCEPT 31 packets, 1744 bytes)num pkts bytes target prot opt in out source destination1 0 0 DROP all -- * * 192.168.33.2 0.0.0.0/02 0 0 DROP all -- * * 192.168.33.1 0.0.0.0/0# 可指定ip网段[vagrant@localhost ~]$ sudo iptables -t filter -F INPUT[vagrant@localhost ~]$ sudo iptables -t filter -I INPUT -s 192.168.33.0/24 -j DROP[vagrant@localhost ~]$ sudo iptables -t filter -nvL INPUT --lineChain INPUT (policy ACCEPT 7 packets, 388 bytes)num pkts bytes target prot opt in out source destination1 0 0 DROP all -- * * 192.168.33.0/24 0.0.0.0/0[vagrant@localhost ~]$ sudo iptables -t filter -I INPUT ! -s 192.168.33.0/24 -j DROP[vagrant@localhost ~]$ sudo iptables -t filter -nvL INPUT --lineChain INPUT (policy ACCEPT 19 packets, 1048 bytes)num pkts bytes target prot opt in out source destination1 0 0 DROP all -- * * !10.0.2.0/24 0.0.0.0/0 协议类型使用-p来指定协议类型，支持tcp udp icmp等，不指定时默认匹配所有协议 网卡接口-i来指定从某个网卡进入的流量，仅使用于PREROUTING INPUT FORWARD三条链。 -o来指定从某个网络流出的流量，仅适用于FORWARD OUTPUT POSTROUTING三条链。 实验三 扩展模块端口使用了扩展模块tcp udp，默认可以省略 --dport来匹配报文的目的端口，使用时必须指定协议，即-p选项。--sport来匹配报文的源端口，使用时必须指定协议，即-p选项。 端口可以指定范围，例如22:25表示22-25之间的所有端口，22,25表示22和25端口，还可以配合起来使用，比如22,80:88表示22和80-88之间的端口。 123456# 可以指定目的端口的范围[root@localhost vagrant]# iptables -t filter -I INPUT -s 192.168.33.1 -p tcp --dport 22:25 -j REJECT[root@localhost vagrant]# iptables -t filter -nvL INPUT --lineChain INPUT (policy ACCEPT 70 packets, 4024 bytes)num pkts bytes target prot opt in out source destination1 0 0 REJECT tcp -- * * 192.168.33.1 0.0.0.0/0 tcp dpts:22:25 reject-with icmp-port-unreachable iprange扩展模块iprange扩展模块可以指定一段连续的ip地址范围。 --src-range和--dst-range用来指定源地址和目的范围。 12345[root@localhost vagrant]# iptables -t filter -I INPUT -m iprange --src-range 192.168.33.1-192.168.33.10 -j DROP[root@localhost vagrant]# iptables -t filter -nvL INPUT --lineChain INPUT (policy ACCEPT 17 packets, 968 bytes)num pkts bytes target prot opt in out source destination1 0 0 DROP all -- * * 0.0.0.0/0 0.0.0.0/0 source IP range 192.168.33.1-192.168.33.10 string扩展模块匹配报文中包含的字符串 123456# 匹配报文中包含XXOO的报文[root@localhost vagrant]# iptables -t filter -I INPUT -m string --algo bm --string &quot;XXOO&quot; -j REJECT[root@localhost vagrant]# iptables -t filter -nvL INPUT --lineChain INPUT (policy ACCEPT 15 packets, 852 bytes)num pkts bytes target prot opt in out source destination1 0 0 REJECT all -- * * 0.0.0.0/0 0.0.0.0/0 STRING match &quot;XXOO&quot; ALGO name bm TO 65535 reject-with icmp-port-unreachable 其他扩展time扩展用来根据时间段进行匹配 connlimit用来对ip的并发连接数进行限制 limit模块限制单位时间内进出包的数量 tcp扩展中可以使用--tcp-flags可根据tcp flag进行匹配 state扩展可根据tcp的连接状态进行匹配 实验四 自定义链12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# 创建自定义链 IN_WEB[root@localhost vagrant]# iptables -t filter -N IN_WEB[root@localhost vagrant]# iptables -nvLChain INPUT (policy ACCEPT 31 packets, 1780 bytes) pkts bytes target prot opt in out source destinationChain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 16 packets, 1216 bytes) pkts bytes target prot opt in out source destinationChain IN_WEB (0 references) pkts bytes target prot opt in out source destination [root@localhost vagrant]# iptables -t filter -I IN_WEB -s 192.168.33.1 -j REJECT[root@localhost vagrant]# iptables -t filter -I IN_WEB -s 192.168.33.2 -j REJECT[root@localhost vagrant]# iptables -t filter -nvL IN_WEB --lineChain IN_WEB (0 references)num pkts bytes target prot opt in out source destination1 0 0 REJECT all -- * * 192.168.33.2 0.0.0.0/0 reject-with icmp-port-unreachable2 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable# 将IN_WEB自定义链添加到INPUT链上[root@localhost vagrant]# iptables -t filter -I INPUT -p tcp --dport 80 -j IN_WEB# 可以看到INPUT链中多出了IN_WEB链[root@localhost vagrant]# iptables -nvLChain INPUT (policy ACCEPT 35 packets, 2012 bytes) pkts bytes target prot opt in out source destination 0 0 IN_WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 18 packets, 1408 bytes) pkts bytes target prot opt in out source destinationChain IN_WEB (1 references) pkts bytes target prot opt in out source destination 0 0 REJECT all -- * * 192.168.33.2 0.0.0.0/0 reject-with icmp-port-unreachable 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable# 重新定义自定链名字 [root@localhost vagrant]# iptables -E IN_WEB WEB[root@localhost vagrant]# iptables -nvLChain INPUT (policy ACCEPT 39 packets, 2244 bytes) pkts bytes target prot opt in out source destination 0 0 WEB tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:80Chain FORWARD (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destinationChain OUTPUT (policy ACCEPT 20 packets, 1520 bytes) pkts bytes target prot opt in out source destinationChain WEB (1 references) pkts bytes target prot opt in out source destination 0 0 REJECT all -- * * 192.168.33.2 0.0.0.0/0 reject-with icmp-port-unreachable 0 0 REJECT all -- * * 192.168.33.1 0.0.0.0/0 reject-with icmp-port-unreachable # 由于iptables有自定义链，不能删除[root@localhost vagrant]# iptables -X WEBiptables: Too many links.# 将INPUT链引用的WEB链删除[root@localhost vagrant]# iptables -D INPUT 1# 此时仍不能删除自定义链，因为自定义链删除，需要上面没有任何规则[root@localhost vagrant]# iptables -X WEBiptables: Directory not empty.# 先清空自定义链的规则后可以删除[root@localhost vagrant]# iptables -F WEB[root@localhost vagrant]# iptables -X WEB ref iptables详解系列 Iptables Tutorial 1.2.2]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[docker私有仓库搭建]]></title>
      <url>%2Fpost%2Fdocker-registry%2F</url>
      <content type="text"><![CDATA[为了其他主机可访问docker registry，必须采用https协议。 1234mkdir -p ~/docker_registry/certssigndomain=103-17-184-lg-201-k08openssl req -nodes -subj &quot;/C=CN/ST=BeiJing/L=BeiJing/CN=$signdomain&quot; -newkey rsa:4096 -keyout ~/docker_registry/certs/$signdomain.key -out ~/docker_registry/certs/$signdomain.csropenssl x509 -req -days 3650 -in ~/docker_registry/certs/$signdomain.csr -signkey ~/docker_registry/certs/$signdomain.key -out ~/docker_registry/certs/$signdomain.crt 从docker hub拉取registry镜像，并启动镜像 123456docker run -d -p 5000:5000 --restart=always --name registry \ -v /data/docker_registry:/var/lib/registry \ -v /home/worker/docker_registry/certs:/certs \ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/103-17-184-lg-201-k08.yidian.com.crt \ -e REGISTRY_HTTP_TLS_KEY=/certs/103-17-184-lg-201-k08.yidian.com.key \ registry:2 停止registry镜像并删除的命令为 1docker stop registry &amp;&amp; docker rm -v registry 下载最新的centos7镜像 1docker pull centos:7.3.1611 将centos7镜像增加tag 12345678docker tag centos:7.3.1611 103-17-184-lg-201-k08.yidian.com:5000/centos:7.3# 可以看到列表中会多出一个镜像[root@103-17-184-lg-201-k08 data]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/registry 2 047218491f8c 4 weeks ago 33.17 MB103-17-184-lg-201-k08.yidian.com:5000/centos 7.3 67591570dd29 3 months ago 191.8 MBdocker.io/centos 7.3.1611 67591570dd29 3 months ago 191.8 MB docker push命令仅支持https协议，签名已经启动了自签名的https协议的registry，为了能够让docker能够信任registry，需要在/etc/docker/certs.d/目录下增加相应的crt文件，增加后的目录结构为/etc/docker/certs.d/103-17-184-lg-201-k08.yidian.com:5000/103-17-184-lg-201-k08.yidian.com.crt，添加完成后需要重启docker服务。 将image push到registry 1docker push 103-17-184-lg-201-k08.yidian.com:5000/centos:7.3 api 列出images：https://10.103.17.184:5000/v2/_catalog 列出image的tags：https://10.103.17.184:5000/v2/centos/tags/list 可以直接通过curl命令来访问api：curl --cacert 103-17-184-lg-201-k08.yidian.com.crt -v https://103-17-184-lg-201-k08.yidian.com:5000/v2 ref registry docker创建私有仓库 Docker Registry（官方教程） Registry API]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[yum源搭建]]></title>
      <url>%2Fpost%2Fyum-build%2F</url>
      <content type="text"><![CDATA[某些情况下需要搭建自己的yum源，比如维持特定的软件包版本等，只需要从网上下载合适的rpm包，即可构建yum源。 repodata数据创建/data/yum.repo目录用来存放rpm包。 可以使用yumdownloader命令来下载rpm包到本地，并且不安装。这里以安装mesos为例，在/data/yum.repo目录下执行yumdownloader mesos即可下载mesos的rpm包到本地。 安装createrepo：yum install createrepo，用来根据rpm包产生对应的包信息。 每加入一个rpm包需要更新下repo的信息，执行createrepo --update /data/yum.repo。会自动产生repodata目录。 搭建web服务需要对外提供web服务，通常会使用nginx或者apache来对外提供服务，这里使用python SimpleHTTPServer来对外提供服务，执行cd /data/yum.repo &amp;&amp; python -m SimpleHTTPServer 1080。 客户端的repo文件设置安装yum优先级插件，用来设置yum源的优先级: yum install -y yum-plugin-priorities 每个需要使用该yum源的客户端需要在/etc/yum.repo.d/目录下增加devops.repo文件。 123456[devops]name=dev-opsbaseurl=http://10.103.17.184:1080/enabled=1gpgcheck=0priority=1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[grafana升级]]></title>
      <url>%2Fpost%2Fgrafana-upgrade%2F</url>
      <content type="text"><![CDATA[本次grafana的升级从版本3.1.1，变更为4.4.3，涉及到一个大的版本跨度。同时之前在使用的存储为sqlite，趁着这次升级更改为mysql。 grafana升级直接从官网下载对应的4.4.3版本的二进制包，修改部分配置即可，该部分没任何难度。 sqlite to mysql由于grafana使用的表结构在3.1.1到4.4.3之间有变更，不能直接将3.1.1版本的sqlite中的数据导入到4.4.3的mysql中。我的方法为先使用3.1.1版grafana将数据从sqlite导入到mysql中，然后再升级grafana的版本，grafana可以自动修改表结构。 在的mysql中创建grafana的数据库，并修改数据库的编码为utf-8. 修改grafana 3.1.1配置文件conf/defaults.ini如下： 1234567891011[database]# You can configure the database connection by specifying type, host, name, user and password# as separate properties or as on string using the url property.# Either &quot;mysql&quot;, &quot;postgres&quot; or &quot;sqlite3&quot;, it&apos;s your choicetype = mysqlhost = xx.xx.xx.xx:3306name = grafanauser = dev# If the password contains # or ; you have to wrap it with triple quotes. Ex &quot;&quot;&quot;#password;&quot;&quot;&quot;password = dev 启动grafana后会自动在grafana数据库中创建相应的表结构，接下来就是将sqlite中的数据导入到mysql中。 在data目录下增加如下脚本sqlitedump.sh，并执行sh sqlitedump.sh grafana.db &gt; grafana.sql。 123456789#!/bin/shDB=$1TABLES=$(sqlite3 $DB .tables | grep -v migration_log)for t in $TABLES; do echo &quot;TRUNCATE TABLE $t;&quot;donefor t in $TABLES; do echo -e &quot;.mode insert $t\nselect * from $t;&quot;done | sqlite3 $DB 然后将grafana.sql导入到新创建的mysql。 将grafana 3.1.1版本停掉，将grafana 4.4.3版本的配置指向到mysql数据库，启动grafana 4.4.3后，mysql中的表结构会自动变更。 至此，grafana的升级完成。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过rsync来绕过relay同步文件]]></title>
      <url>%2Fpost%2Frsync-bypass-ralay%2F</url>
      <content type="text"><![CDATA[由于不允许通过ssh直接连接服务器，即服务器的22端口是不开放的，但是其他端口号可以访问。这就造成了往服务器上传输文件会特别麻烦，需要通过relay中转一下。 rsync命令有shell模式和daemon模式，为了解决该问题，可以通过rsync的daemon模式，rysnc的daemon模式会默认使用873端口，不使用ssh协议，以此来绕过ssh的22端口限制。 最终可以实现在本地通过rsync一条命令直接同步文件或文件夹到服务器的指定目录下。 首先在服务器上搭建rsync的服务端，rsync的安装不再介绍。 修改服务器的rsync配置文件/etc/rsyncd.conf如下： 123456[worker]path = /home/workerlist = trueuid = workergid = workerread only = false 这里为了简便，并未设置rsync的用户名和密码。 客户端同步文件的命令如下： 1rsync -avz $SRC worker@$HOST::worker --exclude=target --exclude=.git --exclude=.idea --delete 命令中的第一个worker为HOST的登录用户名，第二个worker为rysncd配置文件中配置的组名。–exclude选项可以用来屏蔽需要同步的文件夹。–delete选项用来同步删除的文件或文件夹。 daemon模式跟ssh模式相比，无法指定服务器的具体某一个路径，使用不够灵活，但也基本可以满足需求。只能通过daemon配置文件中配置的组中的path参数，同步时仅能通过::组名的形式来指定。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[自动通过跳板机登录到其他服务器]]></title>
      <url>%2Fpost%2Frelay-auto-login%2F</url>
      <content type="text"><![CDATA[最近公司需要首先登录跳板机relay，然后通过跳板机才能登录服务器，操作上略显麻烦。为了节省登录服务器的时间，我编写了一个简单的脚本来简化登录操作。 实现效果为在本地terminal下，执行wrelay $host，即可自动登录到相应的主机。 在relay服务器上增加对其他服务器的免登录命令在relay服务器上ssh到其他主机时需要输入密码，使用expect命令来登录到其他主机时通过expect脚本来实现自动输入密码并登录的功能。 在/home/$user目录下新建mybin文件夹，并将mybin文件夹添加到$PATH环境变量中，具体修改方法不展开。 在mybin目录下增加gw脚本，内容如下： 1234567891011121314151617181920212223242526#!/usr/bin/expectif &#123;$argc &lt; 1&#125; &#123; puts &quot;Usage:cmd &lt;host&gt;&quot; exit 1&#125;set host [lindex $argv 0]# 在这里填写要登录的用户set username &quot;worker&quot;# 在这里填写要登录的密码set password &quot;worker&quot;spawn ssh $username@$hostset timeout 2expect &#123; &quot;*password:&quot; &#123; send &quot;$password\n&quot; &#125; &quot;Are you sure you want to continue connecting (yes/no)?&quot; &#123; send &quot;yes\r&quot; exp_continue &#125;&#125;expect &quot;*#&quot;interact 执行gw 10.1.1.8，即可登录到对应的主机上。 本地主机免登录relay服务器，并自动登录到对应的服务器在本地自动登录relay主机同样使用expect的方式，脚本名称为wrelay，内容如下： 1234567891011121314151617181920212223242526272829303132#!/usr/bin/expectif &#123;$argc &lt; 1&#125; &#123; puts &quot;Usage:cmd &lt;remote_host&gt;&quot; exit 1&#125;# 下面指定relay主机set host &quot;relay.name&quot;# 这里输入relay的用户名set username &quot;&quot;# 这里输入relay的密码set password &quot;&quot;set remote_host [lindex $argv 0]spawn ssh $username@$hostset timeout 2expect &#123; &quot;*password:&quot; &#123; send &quot;$password\n&quot; &#125; &quot;Are you sure you want to continue connecting (yes/no)?&quot; &#123; send &quot;yes\r&quot; exp_continue &#125;&#125;expect &quot;*#&quot;sleep 0.1# 在relay上自动登录到其他服务器主机send &quot;gw $remote_host\n&quot;interact]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[blog从farbox迁移到了hexo]]></title>
      <url>%2Fpost%2Fmv-farbox-to-hexo%2F</url>
      <content type="text"><![CDATA[清明节假期突然想起了我好久不更的blog，看到Farbox官网上的《2016，终结了几个产品》，说明Farbox已经停止更新了。虽然我挺喜欢Farbox这个项目，也见证了Farbox的成长及作者做产品的思考，在这里也向作者致敬。 我当时开始准备启用Farbox之前试用过jekyll，翻遍了整个github，也没找到个合我心意的theme。幸好是Farbox的出现，让我眼前一亮，这就是我想好的blog系统了。Farbox的停更使我不得不考虑重新换个blog，虽然2016的文章数量仅为罕见的个位数，但有可能今年有时间会多写一些。 近几年hexo特别的火，在试看了官方文档了解功能及考虑了blog的迁移成本后，心想，这就是我想要的blog系统了。hexo该有的功能全都有，甚至比Farbox要强大很多。Farbox的很多设计思想跟hexo相仿，但hexo显的更加自由，blog需要自己一手搭建完成。 当然hexo要想使用的好，做一些全面的了解及折腾是必不可少的，毕竟最终利用的Github pages是个静态的系统。早已没有了想当年翻遍整个github上jekyll theme的精力了，我这次的基调是能少折腾就少折腾，毕竟blog我也不是经常写，访问量也更是少的可怜，就当全面了解下当前最火的hexo就好了。 theme本着不折腾原则，直接启用了很火的hexo-theme-next，文档比较全，维护比较及时。基本上按照文档走一遍，该配置的就都可以配置上了。 代码同步代码通过git同步是必备技能。 hexo项目代码同步hexo采用的是node.js环境，而Github pages是静态的，因此Github pages上仅能存储的是hexo编译后的静态文件，这些静态文件直接通过hexo d部署到kuring.github.com仓库中就可以了。 而对于项目中的_config.yml、md文件我直接用git同步到Github上另外一个项目hexo_bak中了。网上还有思路是同步到kuring.github.com上的另外一个分支，我感觉太啰嗦，容易出错，还不如直接分开来的简便。 theme项目的代码同步theme项目中也包含了部分自己的配置及修改，我这里选择的同步策略为从github上fork对应的theme项目，然后clone fork下来的项目到本地，然后直接在theme的项目中通过git命令同步到github fork的项目中。 网上也有思路是通过git subtree的方式来解决，我仍然感觉太啰嗦，不采用。 但这样一个blog项目需要多个git仓库，git push起来会比较麻烦，好在theme一般不怎么修改。 评论系统之前用多说的时候也没几个评论的，用起来还不错，至少比被墙了的disqus要好很多，可是多说这么好的项目要关闭了。我直接使用了国内的网易云跟帖来满足评论的需求。 站内搜索站内搜索是必不可少的功能，next主题提供了多种选择，我直接使用了hexo-generator-searchdb通过本地搜索来完成，生成的xml文件目前还比较小，效果还可以。 常用命令 启用本地server端：hexo clean &amp;&amp; hexo g &amp;&amp; hexo s 部署到github：hexo d 发布文章：hexo new 文章url 使用hexo new draft test会在source/_drafts目录下创建对应文件，此时文件不会生成页面，用于存放未写完的文章。hexo publish draft test命令可将_drafts下的文章移动到_posts目录下，并添加创建时间等信息。 收个尾blog总算迁移完成了，期望今年能多写上几篇。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用logstash收集php-fpm slow log]]></title>
      <url>%2Fpost%2Flogstash-php-fpm%2F</url>
      <content type="text"><![CDATA[目前php-fpm的服务部署在了docker中，对php-fpm的log和php error log可以通过syslog协议的形式发送出去，而php-fpm的slow log却不能配置为syslog协议，只能输出到文件中，因为一条slow log的是有多行组成的。 在docker中使用时发现fpm-slowlog不能正常输出，后经发现是docker默认没有ptrace系统调用的权限，而slow log的产生需要该系统调用。通过在docker启动的时候增加”–cap-add SYS_PTRACE”启动项可修正该问题。 为了收集slow log，可以通过logstash、flume等工具进行收集，本文采用logstash对slow log进行收集，并将收集的log写入到kafka中，便于后续的处理。logstash的input采用读取文件的方式，即跟tail -f的原理类似。为了能够将多行日志作为一行，采用了filter中的multiline来对多行日志进行合并操作。logstash的配置如下： 123456789101112131415161718192021222324input &#123; file &#123; path =&gt; [“/var/log/php-fpm/fpm-slow.log&quot;] &#125;&#125;filter &#123; multiline &#123; pattern =&gt; &quot;^$&quot; negate =&gt; true what =&gt; &quot;previous&quot; &#125;&#125;output &#123; stdout&#123;codec =&gt; rubydebug&#125; kafka &#123; codec =&gt; plain &#123; format =&gt; “tag|%&#123;host&#125;%&#123;message&#125;&quot; &#125; topic_id =&gt; &quot;fpm-slowlog&quot; bootstrap_servers =&gt; “kafka1.hostname:8082,kafka2.hostname:8082&quot; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ELK解析nginx日志]]></title>
      <url>%2Fpost%2Felk_nginx%2F</url>
      <content type="text"><![CDATA[ELK解析nginx日志 最近使用ELK搭建了一个nginx的日志解析环境，中间遇到一些挫折，好不容易搭建完毕，有必要记录一下。 nginxnginx配置文件中的日志配置如下： 123456error_log /var/log/nginx/error.log;log_format main &apos;$remote_addr [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;access_log /var/log/nginx/access.log main; logstash由于是测试环境，我这里使用logstash读取nginx日志文件的方式来获取nginx的日志，并且仅读取了nginx的access log，对于error log没有关心。 使用的logstash版本为2.2.0，在log stash程序目录下创建conf文件夹，用于存放解析日志的配置文件，并在其中创建文件test.conf，文件内容如下： 1234567891011121314151617181920212223input &#123; file &#123; path =&gt; [&quot;/var/log/nginx/access.log&quot;] &#125;&#125;filter &#123; grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;IPORHOST:clientip&#125; \[%&#123;HTTPDATE:time&#125;\] \&quot;%&#123;WORD:verb&#125; %&#123;URIPATHPARAM:request&#125; HTTP/%&#123;NUMBER:httpversion&#125;\&quot; %&#123;NUMBER:http_status_code&#125; %&#123;NUMBER:bytes&#125; \&quot;(?&lt;http_referer&gt;\S+)\&quot; \&quot;(?&lt;http_user_agent&gt;\S+)\&quot; \&quot;(?&lt;http_x_forwarded_for&gt;\S+)\&quot;&quot; &#125; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;10.103.17.4:9200&quot;] index =&gt; &quot;logstash-nginx-test-%&#123;+YYYY.MM.dd&#125;&quot; workers =&gt; 1 flush_size =&gt; 1 idle_flush_time =&gt; 1 template_overwrite =&gt; true &#125; stdout&#123;codec =&gt; rubydebug&#125;&#125; 需要说明的是，filter字段中的grok部分，由于nginx的日志是格式化的，logstash解析日志的思路为通过正则表达式来匹配日志，并将字段保存到相应的变量中。logstash中使用grok插件来解析日志，grok中message部分为对应的grok语法，并不完全等价于正则表达式的语法，在其中增加了变量信息。 具体grok语法不作过多介绍，可以通过logstash的官方文档中来了解。但grok语法中的变量类型如IPORHOST并未找到具体的文档，只能通过在logstash的安装目录下通过grep -nr &quot;IPORHOST&quot; .来搜索具体的含义。 配置文件中的stdout部分用于打印grok解析结果的信息，在调试阶段一定要打开。 可以通过这里来验证grok表达式的语法是否正确，编写grok表达式的时候可以在这里编写和测试。 对于elasticsearch部分不做过多介绍，网上容易找到资料。 kibanakibana不做过多介绍，使用可以查看官方文档和自己摸索。 referencelogstash中的grok插件介绍]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[利用aws科学上网]]></title>
      <url>%2Fpost%2Faws_net%2F</url>
      <content type="text"><![CDATA[曾经使用过多种科学上网方式，​最近尝试了使用aws的免费试用一年的功能搭建shadowsocks，访问google的速度非常不错，比很多收费的服务要好用，amazon真是良心企业！ 本文用于记录在aws上搭建服务的步骤及其中的一些注意事项，步骤不会太详细，aws上关于主机的功能需要读者自己在试验的过程中去自己探索。 注册aws账号为了能够搭建搭建aws服务，拥有一个amazon账号是必须的，在aws免费套餐的页面点击『创建免费账号』按钮即可按照步骤创建aws账号。 值得一提的是，注册aws的账号需要一张信用卡。 开启EC2主机实例该步骤的目的是开启aws上的主机实例。​ 进入aws的控制面板，在左上角的服务中选择EC2，aws提供了多种类型的主机，这里选择EC2即可。 在EC2控制面板界面中需要选择右上角的区域，这个用于选择EC2主机所在的机房，不同机房之间主机是不可以共享的。我这里选择了『美国西部（俄勒冈）』，感觉速度还不错，没有试验过亚洲地区的，新加坡的速度是不是会更好些。后续经过验证，首尔的服务器确实速度更快一些。​下面即可创建EC2的实例了，点击界面上的『启动实例』按钮即可按照步骤创建EC2实例了，创建实例的时候一定要选择免费的EC2主机，否则就会悲剧了。我选择了ubuntu14.04的主机，redhat7.2的主机yum源不太全，没有选择使用。 最终会得到ssh登录用的pem文件，用于ssh远程登录主机。并在界面上启动刚刚创建的实例。 按照shadowsocks接下来就是在EC2实例上安装sock5代理工具了。 登录刚刚启动的EC2，需要pem文件。可以通过ssh -i &quot;key.pem&quot; ubuntu@ec2-52-26-2-14.us-west-2.compute.amazonaws.com命令来登录到远程主机，其他工具请自行google。 使用命令pip install shadowsocks来安装shadowssocks，pip命令的安装自行解决。 在ubuntu的home目录下执行mkdir shadowsocks创建保存配置文件的文件夹，并创建配置文件config.json，内容如下： 12345678&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:10001, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;xxx&quot;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;bf-cfb&quot;&#125; 需要说明的是最好配置一下server_port选项，更改shadowsocks的默认端口号。method选项用于控制加密方式，我这里更改为了bf-cfb。 执行nohup ssserver -c config.json &amp;命令即可启动shadowsocks服务。 由于对外增加了10001端口号，aws的默认安全策略为仅对外提供22端口，需要在EC2主机的安全策略中增加外放访问tcp端口10001的权限。 脚本为了安装方便，我简单写了个脚本如下 12345678910111213141516yum -y install epel-release#yum update -yyum install python2-pip -ypip install shadowsocksmkdir ~/shadowsocksecho &apos;&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:10001, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;xxx&quot;, &quot;timeout&quot;:600, &quot;method&quot;:&quot;aes-256-cfb&quot;&#125;&apos; &gt; ~/shadowsocks/config.jsonsystemctl disable firewalld.servicesystemctl stop firewalld.servicenohup ssserver -c ~/shadowsocks/config.json &amp; 在某些云主机的CentOS7系统发现无法使用yum install python2-pip进行安装，原因是有些源被禁用了，可以使用yum repolist disabled来查看被禁用的源，其中会包含epel源。可以使用yum install python2-pip -y --enablerepo=epel的方式来安装。 安装shadowsocks客户端这里是支持的客户端列表，​我这里仅使用的mac客户端ShadowsocksX，支持Auto Proxy Mode和Global Mode两种方式，其中Auto方式会自动下载使用sock5代理的列表，非常方便。 kcptun为了加快访问速度，推荐使用kcp + shadowsocks kcp的服务端配置如下，即启用20001端口，该端口会将流量导入到127.0.0.1:10001端口，即本机的shadowsocks端口 1234cd ~ &amp;&amp; mkdir kcptun &amp;&amp; cd kcptunwget https://github.com/xtaci/kcptun/releases/download/v20190109/kcptun-linux-amd64-20190109.tar.gztar zvxf kcptun-linux-amd64-20190109.tar.gznohup ./server_linux_amd64 -l :20001 -t 127.0.0.1:10001 -key xxx -mode fast2 --log ~/kcptun/20001.log &amp; 配置了kcptun的shadowsocks客户端仅需要配置代理为远程的kcpdun端口即可，不再需要指定shadowsocks的端口，相当于shadowsocks是透明的。 监控为了避免aws产生额外的费用，一定要设置一下费用报警，否则被扣费了就麻烦了。 另外，可定期查看下aws的费用。试用期为一年，一年后一定要记得停掉aws服务。 最后，祝你玩的愉快！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2015年年终总结]]></title>
      <url>%2Fpost%2F2015_summary%2F</url>
      <content type="text"><![CDATA[按照惯例，年终总结依旧是按照农历算，农历乙未年的年终总结。 2015年的夏天对我而言是个转折点，终于实现了我工作以来一直想北漂的梦想，受够了各种束缚，受够了带同事的各种无奈，受够了跟同事没有话题的工作，受够了天天雾霾比北京不知严重多少倍却没有一点声音，受够了满城市找不到一家互联网公司，受够了满城市找不到一个技术会议，受够了于为了工作而工作的同事们共事，受够了天天受到官本位思想的侵蚀。 上半年在济南工作生活，下半年在北京工作生活。 似乎一直以来我的一些人生大事都是在夏天发生的。 工作在济南的工作没什么好总结的，我已经在济南工作多年，似乎也没有太大的变化。 由于互联网公司和传统行业软件公司存在较大的差异，来到一点资讯后大约适应了两个月才完全适应。到现在，让我回想之前的工作状态，感觉好陌生，好遥远。 我的岗位为运维开发，叫做基础平台的研发更合适些。由于之前没有运维方面的工作经验，对运维开发这样的职位没有清晰的认识。对运维的需求也是在工作中逐步去摸索的。 之前在济南工作的时候，由于从事的是传统软件行业，对技术的使用比较保守，公司中很少会采用很新的技术，比如storm、kafka等。很多行业我个人利用业余时间倒是学习了很多互联网中会用到的技术，但是由于缺乏实践机会，时间一长就忘记了。 另外由于处理业务类型不同，使用的技术往往也不一样。比如haproxy这种反向代理软件在非互联网行业中其实用到比较少，因为非互联网行业面向的群体往往是政府和企业类的，不是使用互联网的广大用户群体，因此反向代理软件很少有永无之地。我之前待过的几家有些技术背景的非互联网公司往往会自己开发一些适合公司自己业务的类库或者技术架构，很多技术含量也是蛮高的，只是不为外界所知，组件无法公用，更没有开源的。 当然了，随着互联网行业的发展，传统的软件企业所使用的技术也在更新，也在采用互联网企业使用的技术。 因此，来到了互联网行业对我而言最重要的就是掌握互联网行业中会用到的技术，除了工作过程中会使用一些新技术之外，晚上下班之后也会将时间充分利用上，学习一些工作中会用到的技术。由于要学的技术实在太多了，至于先学什么后学什么，我采取了“用到什么就学什么，广撒网，后深入”的原则，这样既能达到不太影响工作，又可以达到一定广度，等到广度够了再深入了解各个技术。 由于自己的各种问题，也造成过几次系统的线上的故障，靠谱程度还有待提高。 在济南的环境下从来没有见过@福波和@凯荣为了工作这般拼得的同事，虽然他们这种拼得方式我学不来，但至少要从精神上拼一下。 工作的原因，正在使用的编程也变得更多，工作中会用到C++、Java、Python、Golang、前端技术等，不同的语言使用于不同的场景下。 工作上并没有太大的成就，主要是因为需要学习的东西太多，需需要了解，各种技术需要学习，一些开源的大数据处理技术和存在的坑也需要学习。相信随着学习的不断深入，2016年工作能小有成就。 学习由于我学习新知识很多时候还是看技术类的书籍，比较喜欢系统一点的学习，纸质书是我的最爱。今年买过不少的技术书籍，特别是下半年以来，至少有20本技术书的样子。之前都会系统的讲书籍看完，下半年买的技术书籍基本都是看上一半或找个技术重点就扔到一边去了。主要是因为没有了足够的时间来系统的学习整本书的内容，更多的时候是对症下药而已。 学习的方向上逐渐转为务实，为工作所用，解决工作之所需，不像之前学的很多东西都是纸上谈兵，缺少实践的机会。 生活上半年生活在济南，下半年生活在北京。 在济南的生活相对惬意，加班相对少些，虽然是单双周轮休的生活，但周末仍然是有时间出去转转的。 来北京后，很多时候变成了工作在北京，周末回济南的生活，有时候两周回去一次，有时候一周回去一次，一般周一早上坐最早的高铁回北京。这样的生活相对单调，也确实非常辛苦，基本被工作和学习占去了绝大多数时间，剩下的时间非常有限。可以说工作就是我的生活，我的生活就是工作。 由于很多时间都是远离家的，自己为家庭付出的确实非常少，这点深表遗憾。 运动之前上班的时候每天可以骑一个多小时的自行车来运动，而且坡度也比较大。后来，运动对我而言就相当匮乏了，没有活动的次数都非常有限，运动这一点做的相当不好。 今年从夏天开始学习游泳，终于将游泳学会了，而且游泳的次数也是很多的，虽然仅仅熟练的是蛙泳。公司每周三下午有个活动时间，有一段时间每周三下午都会去游泳的，正好此时学会了游泳。 偶然的一次机会，同学邀请打了一次台球，竟对台球感兴趣起来。 北京对我而言还是个陌生的地方，偶尔周末有时间，会去一些地方转转。 健康今年下半年开始，职业症状逐渐明显，每天坐得时间一长，腰疼就来了，而且有愈演愈烈之势。后来尝试了站着办公，本来以为每天只要站着办公腰疼就不要紧了，试验后才发现站久了也是会腰疼的。目前每天基本都是站着办公了，只有站累了的情况下才会坐下来办公。 来到北京后睡的床垫不是太舒服，后来干脆更改为了睡硬板床，早上起来后确实能明显感觉到睡硬板床轻松的多。 颈椎也不是太好，虽然感觉到时候没有那么多，但至少没有那么健康了。 做按摩的时候才发现自己的腰肌劳损挺严重了，也许是按摩师故意说的严重，至少按摩时疼的我直叫，也能感觉到按摩时腰部的肌肉硬块。听到按摩师的一句话挺伤感的，『你为了工作也是够拼的，把腰伤成这样』。 汽车一向比较排斥汽车的我也在今年的三月份拿到了驾照，并且一度产生了年底购买汽车的想法，曾经也是痴迷过一段时间的汽车节目，在做饭和骑着自行车去上班的路上听汽车广播，对常见的车型都有所了解，还去过春季车展，进过4S店，自己对车的了解也是与日俱增。曾经想过目标车型为标致2008，后来更换为马六，后来更换为昂科塞拉。曾经痴迷到在马路上见到不认识的车型就打开汽车之家的app来查看详情的地步。 但后来来北京工作后，之前做饭和骑自行车上班的时光都变成了在公司上班，也就没有了时间来听汽车广播，渐渐的对汽车的兴趣在逐渐下降，直到现在再也不关注任何和汽车相关的信息。 通过此来看，兴趣是可以培养的，但培养起来的兴趣，一旦放下了，兴趣就会逐渐淡掉，直到恢复到最初的状态。 旅游说来惭愧，2015年没有任何的旅行计划。我唯一的一次旅行是公司校园招聘时在哈尔滨稍微转了下。曾多少次家人建议出去旅行，都被我否决了。理由要么是不愿跟团，要么时间不够，要么自己不愿去，要么考虑钱的问题。 游戏最近几年每到周末是必然会晚上几盘dota的，主要是用来放松，另外也有点玩游戏的瘾。我之所以一直没有戒掉玩游戏，是因为我心里清楚到了一定的年龄段对dota自然不就感兴趣了。只是没想到来的这么快，近几个月对游戏玩的越来越少，有时候一周都不会晚上一次了，而且也没有特别想玩的冲动了。 也许是因为玩游戏需要久坐，而久坐容易腰痛，也许是因为我现在已经达到了特定的年龄段，总之，都在说明，我在变老，精力确实没有之前旺盛了。 展望 深入学习各种技术，技术更上多层楼。 至少要旅行一次，期望的目的地是日本或韩国，如果实在不行就国内。 期望能学会自由泳，蝶泳估计没戏。 多加强台球的练习。 熟悉下北京，多转转北京，去过的地方还非常非常的少。 如果有时间的话，多学一些锻炼智力的游戏，比如魔方。 工作中提高些效率，多一些生活。 多读英文技术文档。 多参加一些技术交流活动或其他活动。 …]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[『狗』的故事]]></title>
      <url>%2Fpost%2Fmydog%2F</url>
      <content type="text"><![CDATA[本文讲述的是我家狗的普通一生，我家的狗即没有名贵的种族，也没有喜人的外貌，就是农村家中最常见的看家狗。 狗的一生中连个正儿八经的名字都没有，父母平日里呼唤狗都是我们那方言中通用的『嗷（一声）嗷(三声)嗷（一声）』（我们那方言对家里的牲口都有一种特殊的呼唤方式），而我通常会通过舌头跟上颚发出的声音来呼唤。以至于在写本文时，我不知道该如何称呼了，只能约定俗成为『狗』。 『狗』在家中的作用为看家护院的作用，在村中养狗的目的大抵如此，狗的作用也仅仅是传统意义上的一条狗。村民们还没有清闲到靠养狗来娱乐的地步，何况邻里邻外的都认识，随便找个人都能聊上个把小时，靠拉呱聊天来娱乐比遛狗更丰富直接。 自打我记事起，家里共养过三只狗，第一只养了至少五年的样子，最终已经记不清楚为何而失去了，或是因为出去走丢了，亦或是因为误食了东西而死去。第三只狗是只有种族的狗，仅仅是中间的过客，确实家人的最爱，有个优雅的名字『点点』，仅此一点就能将此『狗』秒杀N条街，事实也是如此。 家中也养过一些其他的动物，猪、猫、鸡等。猪等到长肥了也就卖掉了，也就四五个月的样子。小时候家里养过母猪，估计也得有个五年以上，但母猪除了吃和睡之外，似乎也没啥了。猫养过多只，确实跟有些猫是有感情的，但大都比较短暂，猫经常跑出去就回不来了。有些鸡在家里也养过四五年，但鸡给我留下的印象中除了吃和下蛋之外，就剩下美味的鸡汤了。唯独『狗』在家中的时间最长，在家中的地位也最高，给家里的贡献也是最大的，给我留下的印象也是颇为深刻。 『狗』大概是我在刚上初中那会，父亲从集市上花了15或者30元钱抑或60元买的。刚买来的时候记得还不满月的样子，特别的可爱，白灰色毛居多，背上有个大大的灰黑色大圆点，不正不斜圆心就在脊梁骨的位置，而且圆是非常的标准的圆，头和尾巴是黄色的毛。由于小，不会造成什么破坏，就直接放在屋子总天天跑，我们吃饭时，它就在下面转啊转，等待着给点馒头或者菜来吃，一见到有东西吃，那尾巴就摇啊摇，摇啊摇。有时候在屋子里碍事了，我就用手揪住它背上的肉皮，扔到一边去，不一会又会回来，然后又让我给揪到一边去，这样揪来揪去的好好玩，听大人们说，这样狗是不疼的。 揪着揪着『狗』就这样被我揪大了，大了之后自然要发挥指责了，总不能天天让家里白白养着，凡事必然有其存在价值，它的价值就是看家护院。这样一干可就是一辈子，它的一辈子就在家里不大的院子里面绕着铁链转，转啊转，直到再也没有力气转下去。自从被拴上铁链的时刻开始，直到生命的最后一刻，再也没有进过曾经在饭桌底下绕啊绕找食物的屋子，饭桌下再也容不下它。 『狗』的看家还是非常尽职尽责，当然了，要不怎么是狗呢。只要是家里来的是陌生人，都会咬个不停，甚至是我爷爷来我家，都会咬个不停。要是觉得听起来太吵了，只要对它喊一声「狗」就会消停很多，跟家人的默契配合的相当不错。 『狗』的记性也是相当不错的，记得上高中时，一个月只能回一次家，每次回家狗都会认识，从来不会当成陌生人狂咬。后来，上了大学，成了半年回一次家了，刚进家门狗就开始叫，也就是脚踏进院子没多久，狗就不叫了，已经认出我也曾经是这里的主人。每次回家都会跟『狗』玩上半天，喂点吃的，挠挠肚子，看着它围着我转。 虽然对陌生人总是咬个不停，但是当人走到旁边时却不会上去咬人，对它凶点甚至还会吓得跑到狗窝里去，完全不是疯狗那样会咬人。但也确实咬过一次人，记得那是上高一那会，姨夫在我家喝多了，走到它旁边，它还在不停的咬，姨夫由于喝的多，对其踹了几脚，抑或身上酒味过重，这下『狗』可不干了，照着姨夫的脚脖子就咬了一口。这是已知的仅有的一次咬人经历。 『狗』是母狗，大概一岁多的时候生过三只狗，其中两只已经夭折了，另外一只目前在我爷爷家里养着。家里来陌生人时，狗都会从狗窝猛然间钻出来，仿佛早发现一会陌生人就能领到奖赏一般。两只小狗的死都是它猛然间出来时将正在吃奶的小狗用铁链给带出来摔死的，挺可怜的。不知是不是脑子缺根筋，看家的本领远比看护自己孩子来的高超，就好比在休产假的妈妈，却天天想着工作，自此之后再也没生过小狗。 『狗』的一声是用铁链禁锢的一生，成年后99%的时光都是用铁链拴着的，比互联网公司的服务可靠性都好的多。偶尔铁链会松掉，它仍然不知道自己的地盘之外仍然可以活动，直到偶然间走出了自己的活动范围，才发现原来自己的地盘外也可以肆意走动了，不过美好的时光不会太长，因为等着家人发现了，禁锢的铁链又跑到自己的脖子里了。有时家人发现的不及时，发现它又回到了原地了，也许它发现原来曾经向往的陌生地方也不过如此，远没有自己的地盘来的安全可靠和温馨。 记得有一次冬天大雾，能见度非常低，『狗』跑出了家门，一路向北，由于雾太大，迷失了方向。我跑出去在家附近找了半个小时未果，爷爷骑着自行车出去打听到别人看见过，父亲终于在离家二里地外的地方找到了，也是虚惊一场。 『狗』对吃得从来不挑，只要有吃的就行。记得之前家里还喂猪的时候，它就在旁边等着猪们吃完后再去吃猪剩下的，每次都会把猪槽舔的溜滑，比可以刷过的还干净。后来家里不喂猪了，就在喂鸡的时候一块喂一下。夏天吃完西瓜后，把西瓜皮扔过去，狗也可以啃得仅剩下一层薄薄的皮。偶尔忘记喂了，『狗』就会zhengzheng的叫，家人自然也能领悟『狗』的意图。 大约刚开始工作那会，姑家的狗『点点』由于没时间照顾在我家里养了半年，『点点』是姐姐给取的名字，长得一副小巧可爱样，在城市的笼子中靠吃狗粮长大。在我们那喂狗几乎是不用狗粮的，估计也买不到，因为没市场。『点点』自然是不能跟『狗』吃一样的，每天都会味一些狗粮，后来家人就买了些过期的方便面和馒头搀和着来喂，而『狗』的主食依旧是吃着跟鸡一样的凉水活玉米面。『点点』白天都在笼子里，只有放出来的时候才会找个地方拉屎撒尿，自然比随地大小便的『狗』更讨人喜欢。晚上『点点』是放在院子里的，院子里的任何角落都是『点点』的活动范围，这点也是『狗』可望不可及的。白天『点点』的笼子是放在大门下的，逢人从门口经过都能看到可人的样子，自然也会吸引很多人的目光。 也就是从『点点』刚来我家的时候，发现有时候从它身旁经过，它都没有任何反应了，因为『狗』的耳朵不好用了。人老了耳朵会聋，狗亦如此。有时候陌生人都进到家里了，发现『狗』仍然在狗窝里或外面太阳下睡觉，即使耳朵是贴在地面上的。狗天生骄傲的就是耳朵，试想一名工程师如果不能用双手敲代码，那怎么称之为码农。 自从『狗』聋了后，明显感觉到『狗』的情绪变得低沉了，也许是因为耳朵不好用了，也许是进入晚年了，也许是因为『点点』的缘故，虽贵为正室，却不再受宠幸。直到半年后的『点点』走后，也一直没有缓过来。 近几年，我每次回家都能感受到『狗』是一年不如一年，最后一次见到是在2015年的十一，那时它很多时候都是趴在窝里睡觉的，除了看家，也没啥本事，既然看家技能已失，那就只能睡觉了。 2016.1.22傍晚听到了『狗』去世的消息，脑海中自然也是联想关于『狗』的往事，历历在目，眼角泪花直流。没有任何仪式，『狗』悄悄的离开了奉献了自己一生的岗位，摆脱了束缚了自己一辈子的锁链，黯然离开了自己的主人。 当天天气极度寒冷，应该是一年中最冷的天气了，家中下起了大雪，也许是老天送给『狗』的葬礼，雪花仿佛跟『狗』的灵魂一般纯洁。 算下来从2000年左右到现在该有16岁的样子了，这16年是我人生中最年轻的时光，我从懵懂的少年已变成了社会中的青年，却是『狗』的一生。 下图是2016.1.22上午时的照片，已经生命垂危，生命在倒计时。 一个月前，父亲又弄了条小狗，凑巧的是，小狗的模样跟『狗』长的颇为相似，毛色也一致，甚至连背上也有一个灰黑色的圆点。虽没当面见过，但看小狗的照片不禁联想到『狗』的小时候仿佛是狗的小时候，这也许是『狗』的生命的延续。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[keepalived简易教程]]></title>
      <url>%2Fpost%2Fkeepalived_easy%2F</url>
      <content type="text"><![CDATA[keepalived的作用为保持存活服务，服务启动后会在两台物理机器之间维护一个vip，但是仅有一台物理机器拥有该vip，这样就保证了两台机器之间是主备。 安装在ubuntu下直接执行：sudo apt-get install keepalived. 使用本例子两台机器的物理ip地址分别为10.101.185和10.101.1.186，要增加的虚拟ip地址为10.101.0.101、10.101.0.102、10.101.0.107和10.101.0.108，其中10.101.0.101和10.101.0.102在10.101.185上为主，10.101.0.107和10.101.0.108在10.101.1.186上为主。 keepalived的默认配置文件位于/etc/keepalived/keepalived.conf目录下，由于两台物理机器之间的主辅关系不同，配置文件也不相同。 10.101.185机器上的配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445! Configuration File for keepalivedglobal_defs &#123; # 报警邮箱配置 notification_email &#123; ops@yidian-inc.com &#125; smtp_server 10.101.1.139 smtp_connect_timeout 30 router_id 101-1-185-lg-201-l10.yidian.com // 运行机器的唯一标识，每个机器应该都不一样，可以直接使用hostname代替，具体用在什么地方暂时不是很清楚&#125;vrrp_instance ha-internal-1 &#123; state MASTER interface eth0 virtual_router_id 1 // VRID标记，可以设置为0-255，对应VRRD协议中的Virtual Rtr Id priority 100 // 对应VRRD协议中的priority选项 advert_int 1 // 检测间隔，默认为1s，对应VRRD协议中的adver int authentication &#123; auth_type PASS // 认证方式，支持PASS和AH auth_pass 1-internal-ha // 认证的密码，从抓取的包中看到 &#125; // 声明的虚拟ip地址，这些ip会在VRRP一些的一个包发送 // 另外VRRP协议中还有一个Count IP Addrs用来指明需要声明多少个VIP virtual_ipaddress &#123; 10.101.0.101/22 dev eth0 10.101.0.102/22 dev eth0 &#125;&#125;vrrp_instance ha-internal-2 &#123; state BACKUP interface eth0 virtual_router_id 2 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 2-internal-ha &#125; virtual_ipaddress &#123; 10.101.0.107/22 dev eth0 10.101.0.108/22 dev eth0 &#125;&#125; 10.101.1.186上的配置文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; ops@yidian-inc.com &#125; smtp_server 10.101.1.139 smtp_connect_timeout 30 router_id 101-1-186-lg-201-l10.yidian.com&#125;vrrp_instance ha-internal-1 &#123; state BACKUP interface eth0 virtual_router_id 1 priority 99 advert_int 1 authentication &#123; auth_type PASS auth_pass 1-internal-ha &#125; virtual_ipaddress &#123; 10.101.0.101/22 dev eth0 10.101.0.102/22 dev eth0 &#125;&#125;vrrp_instance ha-internal-2 &#123; state MASTER interface eth0 virtual_router_id 2 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 2-internal-ha &#125; virtual_ipaddress &#123; 10.101.0.107/22 dev eth0 10.101.0.108/22 dev eth0 &#125;&#125; 配置文件搭建完毕后，通过sudo service keepalived start即可启动服务，执行ip addr命令即可看到vip。需要注意的是，通过ifconfig命令是看不到vip的。 有了vip，其他服务就可以利用该vip做一些绑定vip的端口来作为主辅热备模式了。 about vrrp关于VRRP的详细说明可以查看RFC3768，我这里记录几点说明。 协议中的以太网Destination Address的值必须为多播地址224.0.0.18。 当前正在使用的VRRP版本为version 2，认证功能已经取消，但为了向下兼容，仍然可用。在抓取的包中，仍在使用认证信息 download这里提供两个vrrp协议的pcap包]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大顶堆小顶堆与堆排序]]></title>
      <url>%2Fpost%2Falgorithm_heap%2F</url>
      <content type="text"><![CDATA[一直对堆排序算法用的不错，但是又是在排序中挺重要的算法，并且可以求解其他问题，比如top k问题。已经对堆排序学习过好多次了，无奈每次都记不太清楚具体的细节问题，本文对堆的问题进行整理。本文的排序例子来源于严蔚敏的《数据结构》，本文的知识点来自《算法导论》。 最大堆为堆中的最大元素位于根节点，顾名思义，最小堆的根节点为最小值。堆的性质决定了堆中节点一定大于等于其子节点。在堆排序算法中用到的是最大堆，最小堆用于构造优先队列，要是使用最小堆进行排序，得到的排序结果为倒序。 堆的结构为完全二叉树，因此可以用数组存储来代替树的链式存储结构。 建最大堆过程我这里通过图表的形式对建大顶堆的过程进行了展示，不再对文字进行叙述。在堆排序的过程中会不断进行建最大堆过程的调用。 堆排序的核心步骤清楚了堆的初始化，再看一下下面的堆排序步骤就非常清楚了，不需要图片进行描述了。堆排序的步骤： 将待排序的数组初始化为大顶堆，该过程即建堆。 将堆顶元素与最后一个元素进行交换，除去最后一个元素外可以组建为一个新的大顶堆。 新建立的堆不是大顶堆，需要重新建立大顶堆。重复上面的处理流程，直到堆中仅剩下一个元素。 top k问题的堆解法该问题最常规和通用的解决思路为使用快速排序，还可以在N的范围不大的情况下采用哈希（桶）的方式。 另外一种解法就是堆排序的思路来解决。这里用到的为最小堆，用最小堆来存储最大的k个数，其中堆顶元素为最大k个数种最小的数。这个解法初看有些别扭，求最大的k个数，居然会用到小顶堆。 该算法必然是一个个遍历N个数一次就够了，这点很好理解。每读取一个新的数x，如果x比堆顶的元素y小，则抛弃；如果x比y大，则用x替换y，并重新更新堆。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[记2015年十一假期的一件小事]]></title>
      <url>%2Fpost%2F2015_10_01%2F</url>
      <content type="text"><![CDATA[十一回农村老家，有件小事有些感悟。 只要家里有人，家里的大门白天一直都是敞着的，外面的人都可以直接走进来，这在农村是很正常的一件事情，门敞着才说明家里是有人的，农村的人没有城里人这么多的隔阂。 正巧地里的活都干得差不多了，母亲在院子里晒着太阳，我在屋子里收拾东西。从门口走到院里一个人来，只见一身道姑打扮，还带一顶帽子，嘴里振振有辞，说是泰山娘娘庙里的保佑家里平安之类的。母亲见到来者第一反应就是骗子，立马上前说是地里有活要干，马上要出去了。来者压根不理会母亲的话，依旧是保佑平安之类的，像极了大街上迎上前去得乞讨者。来者说道，要捐款之类的，并有个小本子，上面写着捐款者的名字。母亲看了眼捐款者的名单，很多都是邻居家的，说明来者刚从家里过来，迟疑了一会便签上了名字，回屋里去取钱。 这时还在屋子里的我才看到并明白过来是这么一回事，我一看母亲名字都签上了，钱肯定是要给的了，想逃掉避免少不了一番纠结。我便回屋子去取钱，在城市待习惯了，知道乞讨者五毛一块就能打发的很高兴，我便从钱包里去了两张一块的，其中一张还是备用的，我先给一块，要是嫌少再给两块。我先于母亲出了屋子，给到了来者一块钱，岂知来者说道别人都是给三十五十的，这些太少了我不要。靠！这年头乞讨还嫌钱少啊，还是我太out了？我直接说那你走吧，我没钱，并转头回屋子，可最烦的是她也跟着往屋里走。 就在这时母亲从屋子里走出来并拿着10块钱，迎上前去给了她。又是嫌少之类的话，最后也不情愿的收下了，并留下了一根红丝带，说些全家平安之类的话就走了，当然了骗子的目的已经完成了，只需匆匆收场就行了。 此事的经过到此结束，但却有个问题挺令我深思的，这也是为什么写下本文的原因。 父母并不富裕，家里一直也是过得比较平淡的生活，在我看来勤奋和勤俭节约一直是我们老家那块的美德。平常家里买个菜什么的都要为了几毛钱掂量半天，但却在面对乞讨施舍这种事情上扔掉了10块钱的巨款，而且母亲对于骗钱这件事从始至终都是知情的。 事后，我的同样上当受骗的邻居也来到了我家，通过邻居和我母亲的谈话我大体理解了他们在经历此事时的心理活动。由于每年都会有多次来到家里进行骗钱的，骗钱的方式是多种多样的，无非是找不到孩子，回不了家之类的，母亲一开始见到骗子就知道是个骗子，这第一印象的判断是过关的。 母亲对骗子的第二个行为是阻拦，母亲用了要去地里干活的信息来阻拦，但是阻拦不彻底，见阻拦不成功就放弃了。这一点上就显现出了的缺点，不知道用合理的手段来保护自己，并完全从主动状态变为了被动状态。之所以意志这么不坚定，其中有一个很大的因素就是母亲知道强加阻拦的后果就是骗子可能会爆粗口，完全不想听骗子絮絮叨叨个没完没了，撵都撵不走，还不如给点钱省事。另外得罪的骗子的后果可能会更麻烦，毕竟骗子往往都是一个团伙，且知道家庭住址，怕有什么报复行为。所以之所有给钱的原因就是花钱买个安宁，免得带来一身的麻烦，当然从这个出发点上给钱是对的。骗子也正是利用了这一点才在农村屡试不爽。 但给钱的数目跟平时的生活水平是完全不相符的，要知道在农村买上10块钱的菜可以吃上好几顿。我觉得之所以出现这个问题，根源在于对自己的不够重视。在中国这种权力的社会中，母亲一直觉得处于权力的最底层，事实也确实如此。即使在自己的家中也很容易变主动为被动，让骗子得手。 可能有人会说农村的法律意识淡薄，不知道合理的维权，完全可以打110来解决。我曾经打110处理过店铺扰民这种鸡毛小事，110给的处理时间为5天，这还是在城市里。我估计换做农村的派出所，这种小事估计110是请不动的。 我在此次事件中并没有第一时间作出应有的反应，这点我需要反省。首先，我见到母亲签字后没有跟骗子要一下工作证明，这样子至少让骗子没这么容易得逞，给我的反驳增加大大的筹码。其次，没有阻拦母亲给骗子送钱，按照我的意思是跟骗子死缠到底的，毕竟是在我家，给不给钱也是我的自由。但我怕在家里生活时间不长，骗子的规矩我不懂，还是顺从了母亲的行为。 之所以写这篇文章是想梳理下农民身上的共同缺点及我身上的缺点。我不是歧视农民，我是农村出来的，我知道农村人的辛苦，忙起来的时候他们的辛苦程度是我等码农不能企及的。 在这里为广大奋斗在田地里的农民致敬！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SaltStack使用]]></title>
      <url>%2Fpost%2Fsaltstack%2F</url>
      <content type="text"><![CDATA[本文在学习saltstack的过程中编写，内容比较基础，方便使用时查阅命令。 安装为了方便起见，直接采用yum的安装方式，centos源中并没有salt，需要手工添加一下。 CentOS 7安装master 12rpm -Uvh http://ftp.jaist.ac.jp/pub/Linux/Fedora/epel/7/x86_64/e/epel-release-7-5.noarch.rpmyum install salt-master 修改/etc/salt/master配置文件，在其中指定salt文件根目录位置，默认路径为/srv/salt/。 123file_roots: base: - /svr/salt/ salt在安装的时候已经创建了systemctl命令启动程序需要的service文件，位于/usr/lib/systemd/system/salt-master.service，重启systemctl restart salt-master.service生效。 CentOS 6.5安装minion 12rpm -Uvh http://ftp.linux.ncsu.edu/pub/epel/6/i386/epel-release-6-8.noarch.rpmyum install salt-minion 修改/etc/salt/minion配置文件，在其中指定master主机的地址 1master: 192.168.204.128 执行service salt-minion restart对服务进行重启。 连通性测试执行salt-key -L命令可以看到已认证和未认证的minion，执行salt-key -a 192.168.204.149可接收minion。 在master主机中执行salt &#39;*&#39; test.ping可测试连接的minion主机。 1234567891011121314151617➜ stats salt-key -LAccepted Keys:Denied Keys:Unaccepted Keys:192.168.204.149Rejected Keys:➜ stats salt-key -a 192.168.204.149The following keys are going to be accepted:Unaccepted Keys:192.168.204.149Proceed? [n/Y] yKey for minion 192.168.204.149 accepted.➜ stats salt &apos;*&apos; test.ping192.168.204.149: True state可以通过预先定义好的sls文件对被控主机进行管理，这里演示一个简单的文件复制的例子，该例子可以将master主机上的vimrc文件复制到目标主机上。 在master主机的/svr/salt/edit目录下新建vim.sls文件，文件内容如下： 123456/etc/vimrc: file.managed: - source: salt://edit/vimrc - mode: 644 - user: root - group: root 另外在edit目录下需要存在一个空的init.sls，以确保state.sls可以找到该目录下的sls文件。同时该目录下还需要存在要复制的vimrc文件。 执行salt &#39;*&#39; state.sls edit.vim即可以执行该命令。 如果将vim.sls更改为init.sls文件，执行salt &#39;*&#39; state.sls edit命令即可。 常用命令salt ‘192.168.204.149’ cmd.run ‘free -m’ salt ‘192.168.204.149’ sys.list_modules 列出minion支持哪些模块，默认已经支持很多模块 salt ‘192.168.204.149’ cp.get_file salt://test_file /root/test_file 将master主机file_roots目录下的文件复制到minion任意目录下，该命令不可以将master主机任意目录下的文件进行复制 salt ‘192.168.204.149’ cp.get_dir salt://test_dir/ /root/ 实验未成功 salt ‘*’ file.mkdir dir_path=/root/test_dir user=root group=root mode=700 在minion主机上创建目录s 参考文章*saltstack官方文档 *《python自动化运维技术与最佳实践》]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2015年找工作的一段经历]]></title>
      <url>%2Fpost%2F2015_interview%2F</url>
      <content type="text"><![CDATA[前段时间我的工作有了一个比较大的变动，我的工作地点从济南到了北京，离开了待了9年的济南，离开了温馨的家，离开了我的亲人，独自一人开启了北漂模式。 本文不打算叙述面试细节问题，具体的面试细节我自己在印象笔记中整理过，但不打算放出来。本文仅选取有代表性的几家公司来叙述。 之所以有如此大的变动，最大的因素是我的个人技术的发展遇到了瓶颈。我近几年对个人发展的定位是在技术上能够更上一层楼，尽量不走管理路线，还是以踏踏实实学技术为主要任务。可是在济南工作已经慢慢被推上了管理的岗位，我怕会逐渐脱离技术，直到完全走上了管理的岗位，这跟我对技术非常感兴趣的初衷是有些违背的。 工作这几年，技术的进步主要来自于自己业余时间的学习，工作中越来越学不太着东西，不是因为工作中不需要牛逼的技术，而是没有人和精力去研究新技术，济南缺乏这个环境，而且是非常匮乏。我已经深深感觉到济南的IT行业未来会逐渐缩小，实际上目前济南的圈子就是很小的，稍微有点规模的公司也就那么几家而已。 我自从开始工作就以互联网公司为目标，可以济南没有一家真正意义上的互联网公司，用的技术也都不咋地，工作五年后，有了一定的技术积累，家庭也算稳定了，是时候出来闯闯了，不能在济南的安逸环境中，像水煮青蛙般等待着济南IT业的下滑。 这次来北京找工作的目标非常明确，互联网公司，最好是规模能够稍微大些的，BAT更好，不想加入A轮的公司，我需要的是成熟的互联网公司的环境和技术，来洗刷我已经在传统行业奋斗了多年的旧习。 我是裸辞的，因为毕竟面试是需要去北京的，在职请假面太麻烦，且不能够全身心的找工作。我给自己找工作的期限为一个月的时间，我最终上班的时间是在20天多点的时间。 在辞职后的第一周我在家里边休息边看了一遍《STL源码剖析》，之前一直觉得没必要看此书，结果看起来效果还不错，比我想象的要简单的多的多。另外，我制作了自己的简历，包括了pdf版本和markdown版本，并在拉勾网上投递了几份简历。 就这样第一周过去了，而我没有收到任何的面试通知，第二周我必须加紧开始找工作了。首先在100offer上申请拍卖了我的简历，之前一直关注100offer，微信公众账号和知乎上经常看到100offer的文章，感觉是个靠谱的平台，事实证明确实是一个靠谱的平台。另外，恰巧我在微信公共账号“余晟以为”的文章看到了怎样写简历的文章，就跟作者聊了一会，并且作者在twitter上推荐了我的简历，我的博客有史以来日pv达到了600多，这是我没有想到的，在这里非常感谢余晟的无私帮助。 100offer上拍卖后没多久就收到了二个面试通知，周三上午就赶到了北京参加面试。上午参加的一家游戏公司的面试，用C++做后台的业务逻辑处理，其实我并不喜欢游戏类工作，来面试也仅仅是为了了解北京面试的流畅增加一些资历而已。两天后收到了该公司的offer，而我理所当然是拒绝了。 周四上午参加了一个创业团队的面试，面试官年龄跟我差不多，问的问题非常多，非常杂，最后还有一个我最讨厌的逻辑题，一直面试到下午一点。这家公司的面试是通过twitter上看到我的简历联系我的，我来的目的其实也是出于学习和了解行业。面试完后跟团队成员一起吃了个饭，令我感动的是我面试完已经一点钟了，而大家都在等着我一起吃饭，团队的成员都是做技术的码农，还是非常好相处的。如果我已经有了几年的工作经历，或许我会选择这样一家公司。 下午参加了我目前所在公司一点资讯的面试，面试流程还是非常nice的，感觉跟我心目中的互联网公司基本吻合，虽然面试的时候问的问题过于简单些，令我有点怀疑公司的真实技术实力。另外还问了一堆我并不深入的web技术问题，令我非常捉急。 没有面试的时候，我就直接回济南了，毕竟在自己家里比北京不知舒服多少倍。 第三周的时候，我已经开始有些慌了。该用的能有面试机会的方式我都用了，而却收不到面试的邀请了。我用到了100offer、拉勾网、内推、jobdeer、同学内推的方式。难道是我简历写的太水了，可是我已经很难改进自己的简历了，我不想将简历写的夸张了。 我开始反思原因。我发现互联网用到的我擅长的C++技术的公司非常少，互联网追求的是短平快，C++并不具备开发速度快的特点，因此并不受互联网公司欢迎。像BAT类的公司用C++技术还是比较多的，因为做到一定程度会深究程序的性能，而这是C++擅长的。而BAT类的公司，我并没有任何优势，我虽有几年工作经验，但是都是在传统行业，互联网行业的经验却为0。很多大公司的hr在看到我的简历后直接就给pass掉了，压根没有面试的机会。 我找同学内推了百度的简历，另外在拉勾网上也投了一些百度的简历。在我入职之前的几天百度的hr妹子给我电话沟通说一周之内会给我面试通知，最终的结果是一周后百度hr妹子给我打电话让我去面试，看来大公司的流程真是复杂，连面试都得排队，而那天是我入职的第一天。没办法，只能委婉的拒绝了，hr倒是很爽快的挂掉了电话。 目前，我已经入职有三周的时间了，工作已经趋于稳定。我也搬到了公司附近，离公司就几分钟的路程，毕竟就自己一个人，没必要离公司太远，上下班太折腾。晚上一般会在公司待到十点以后，毕竟回去了也没太有什么事情。周末会抽时间回济南跟家人团聚，或者家人来北京，不期望因为工作的原因而对家庭有损伤。工作方面的内容还算满意，能涉及到很多新技术，对个人的成长还不错，只是组内的人员较少，沟通交流的机会不够多，通过招聘慢慢就会解决了。由于用到的很多技术都不够熟悉，自己俨然变成了一个菜鸟，有大量的技术需要学习。同事也还比较给力，大家对工作也很认真负责，团队的凝聚力也符合我的预期。 总结来看，这段找工作的经历虽有很多失误的地方，错误的对行业的需求进行了估计，以为C++程序员很抢手，事实并不是如此，但结果还算满意。现在自己一个人在北京奋斗，期望通过自己的努力能够有个好的收货。要想写的东西很多，很多都一笔带过了，非技术类的文章写起来还是挺费脑力的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[getaddrinfo函数调用问题]]></title>
      <url>%2Fpost%2Fquestion_getaddrinfo%2F</url>
      <content type="text"><![CDATA[最近在开发程序的过程中遇到了一个getaddrinfo函数的问题，令我感到非常奇怪。 程序中调用了librdkafka库，当程序选择用-static方式链接所有库时程序会在librdkafka库中某个函数core dump，但是选择动态链接系统库（包括libpthread、libdl、libz、libm、libc等）时程序却能正常运行。 每次程序都回core dump在getaddrinfo函数中，经过搜索发现有人跟我遇到同样的问题，但是却没有解决方案。 我这里实验了文中提到了例子，在静态链接的时候确实会报错，动态链接却非常正常，编译选项为g++ -o test_getaddrinfo test_getaddrinfo.cpp -lpthread -Wall -static。 123456789101112131415161718192021222324include &lt;stdio.h&gt;#include &lt;netdb.h&gt;#include &lt;pthread.h&gt;#include &lt;unistd.h&gt;void *test(void *)&#123; struct addrinfo *res = NULL; fprintf(stderr, "x="); int ret = getaddrinfo("localhost", NULL, NULL, &amp;res); fprintf(stderr, "%d ", ret); return NULL;&#125;int main()&#123; for (int i = 0; i &lt; 512; i++) &#123; pthread_t thr; pthread_create(&amp;thr, NULL, test, NULL); &#125; sleep(5); return 0;&#125; 发现程序在链接的时候会提示如下警告： 1234/tmp/cc0WILtn.o: In function `test(void*)&apos;:test_getaddrinfo.cpp:(.text+0x49): warning: Using &apos;getaddrinfo&apos; in statically linked applications requires at runtime the shared libraries from the glibc version used for linking/usr/lib/gcc/x86_64-redhat-linux/4.8.3/../../../../lib64/libpthread.a(libpthread.o): In function `sem_open&apos;:(.text+0x685b): warning: the use of `mktemp&apos; is dangerous, better use `mkstemp&apos; 从网上查看有该警告的人还是非常多的，都是在-static方式链接glibc库时遇到的，但是没有发现很好的解决方案。该问题的原因产生估计是glibc在静态链接时调用libnss库存在问题，因此不提倡静态链接方式。 我看到了两种解决方案： 方案一：用newlib或uClibc来代替glibc来静态链接，这种方案我没有去尝试是否可行。 方案二：用--enable-static-nss重新编译glibc。我试了一下问题仍然存在。 我之所以采用静态链接的方式，是因为开发机器和运行机器的glibc版本不一致造成的。我尝试将libc.so相关文件复制运行机器上，并让程序链接我复制过去的文件，ldd查看可执行文件没有错误，但是当运行程序时会报如下错误： 1./xxx: relocation error: /home/kuring/lib/libc.so.6: symbol _dl_starting_up, version GLIBC_PRIVATE not defined in file ld-linux-x86-64.so.2 with link time reference 最终，我放弃了静态链接的方式，采用了动态链接方式来暂时解决了问题。如果你知道解决方案，请告诉我。 参考文章 getaddrinfo causes segfault if multithreaded and linked statically Even statically linked programs need some shared libraries which is not acceptable for me. Create statically-linked binary that uses getaddrinfo? Static Linking Considered Harmful]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[LNMP开发环境搭建]]></title>
      <url>%2Fpost%2Fbuild_lnmp%2F</url>
      <content type="text"><![CDATA[这是一篇拿来主义的文章，所有的安装步骤仅为互联网上查找，网络上的教程各种凌乱，这里根据我的实践情况进行了更改，本文仅记录了我的安装过程，由于不同环境可能导致安装步骤不甚相同。 MAC OS X 10.10phpMac OSX 10.10的系统自带了php、php-fpm，省去了安装的麻烦，可以执行php -v查看php的版本。这里需要简单地修改下php-fpm的配置，否则运行php-fpm会报错。 12sudo cp /private/etc/php-fpm.conf.default /private/etc/php-fpm.confvim /private/etc/php-fpm.conf 修改php-fpm.conf文件中的error_log项，默认该项被注释掉，这里需要去注释并且修改为error_log = /usr/local/var/log/php-fpm.log。如果不修改该值，运行php-fpm的时候会提示log文件输出路径不存在的错误。 如果系统中存在多个php-fpm.conf，不知道需要编辑哪一个，可以执行php-fpm -t命令查看php-fpm要读取的配置文件。 通过php-fpm -D来启动php-fpm，可以通过lsof -Pni4 | grep LISTEN | grep php命令来查看php-fpm是否监听在9000端口。 nginx这里为了简单，直接采用了brew的方式安装。执行 1brew install nginx nginx的配置文件位于/usr/local/etc/nginx/nginx.conf，默认只能解析html文件，需要配置后才能调用php-fpm解析php文件。下面内容为该修改后的文件全部有效内容： 123456789101112131415161718192021222324252627282930worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 8080; server_name localhost; root /Users/kuring/www; // 页面存放路径 location / &#123; index index.html index.htm index.php; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; &#125;&#125;include servers/*; 然后执行nginx命令即可启动，默认监听的端口为8080，在浏览器中输入http://127.0.0.1:8080即可看到nginx的初始界面。nginx要想监听1024以下端口还需要进一步的配置，8080端口既能满足我需求，不再更改。 mysql1brew install mysql 在启动mysql之前可对mysql的配置文件进行更改，我这里需要更改mysql的编码方式，将所有的编码方式都更改为utf8，防止乱码问题的发生。mysql的配置文件为my.cnf，我的位于/usr/local/Cellar/mysql/5.6.25/my.cnf，对文件添加如下内容，有些选项不存在，可手动添加。 12345678[mysqld]character-set-server=utf8[client]default-character-set=utf8[mysqld_safe]default-character-set=utf8 输入mysqld命令即可启动mysql，启动mysql后输入mysql_secure_installation命令对mysql进行配置，可以设置root用户的密码。 通过mysql -uroot -p命令连接到mysql后，输入status命令可查看刚才更改的编码是否生效。 由于不需要长期使用mysql，这里不设置mysql自启动命令。 CentOS6我首先采用的方案为完全用普通用户安装，尝试失败后采用root安装依赖库普通用户编译程序的方案。 普通用户安装依赖库我首选选择在普通用户kuring下进行安装和运行整个web环境。因此不能使用yum安装方式，必须采用源码安装的方式。通过普通用户安装，最麻烦的地方就在于需要安装很多的依赖库，而依赖的库的安装可能又有需要的库，且库之间存在版本问题。 首先普及几个小知识： bash查找命令的先后顺序为： alias别名 shell中的关键字，如if等 shell中的函数 shell内置命令，如echo等 $PATH环境变量，PATH中的匹配顺序为从前向后的。 程序查找lib库的先后顺序为： 编译程序时指定的链接库路径，g++编译器可以通过-Wl,-rpath,路径来指定链接库的路径。 环境变量LD_LIBRARY_PATH指定的搜索路径。 /etc/ld.so.conf指定的路径。 默认的系统动态库搜索路径，如/usr/lib64、/usr/local/lib64等。 很多程序采用pkg-config程序来检查库的版本号，pkg-config命令依赖于动态链接库对应的.pc文件，这些.pc文件一般位于系统的/usr/local/lib/pkgconfig目录下。为了能够将安装完成的库通过pkg-config找到对应的.pc文件，需要将.pc文件所在的路径/home/kuring/local/lib/pkg-config设置到环境变量PKG_CONFIG_PATH中。 安装php依赖的libxml2库时提示找不到libtool、autoconf和automake，首先安装libtool。执行./configure --prefix=/home/kuring/local;make; make install将其安装到当前用户的local目录下。 用同样的步骤安装autoconf，执行./configure --prefix=/home/kuring/local;make; make install。 为了能够将安装的程序起作用，需要将/home/kuring/local目录添加到PATH环境变量中，在.bash_profile文件中添加PATH=$PATH:$HOME/local/bin语句，并执行source ~/.bash_profile。 安装之前需要先安装libxml2库，下载地址采用git clone git://git.gnome.org/libxml2的方式下载。在执行sh autogen产生configure配置文件的过程中，发现提示 12./configure: line 13094: syntax error near unexpected token `LZMA,liblzma,&apos;./configure: line 13094: ` PKG_CHECK_MODULES(LZMA,liblzma,&apos; 经过发现是由于找不到PKG_CHECK_MODULES造成的，正常情况下该函数定义在aclocal.m4文件，而该情况下aclocal.m4文件中并不存在该函数。之所以不存在是由于aclocal命令找不到pkg.m4文件造成的，可以通过aclocal --print命令查看查找的pkg.m4文件的路径。我这里的解决思路为直接从其他机器上复制一个pkg.m4文件过来。 在产生了configure命令后，执行./configure --prefix=/home/kuring/local命令后发现提示找不到Python.h命令的错误。 鉴于遇到了如此之多的错误，本着不浪费生命的原则还是采用yum来安装依赖库吧。 php这里的mysql直接采用了yum命令安装的。 在执行php的./configure命令后提示libxml2找不到错误，直接yum install libxml-devel命令安装libxml-devel即可。然后执行./configure --enable-fpm --prefix=/home/kuring/php5.5 --with-mysqli=/usr/bin/mysql_config;make; make install;。在编译php的时候要加上php-fpm选项来安装php-fpm命令。 安装后配置~/.bash_profile文件的$PATH环境变量的值为：PATH=$HOME/bin:$HOME/php5.5/bin:$HOME/php5.5/sbin:$PATH。 此时即可通过php-fpm -D命令来启动php-fpm命令了。 安装完成后通过phpinfo()函数查看里面有MySQLi的选项，但是实际程序运行的时候居然不支持mysqli的一些力函数，说明mysqli的扩展安装不成功。在/home/kuring/php5.5/include/php/ext/mysqli目录中找到了对应的.h文件，却没有找到mysqli.so的动态链接库文件。大概是由于在编译php时mysql的路径配置有些问题造成的，因为mysql是通过yum安装的，路径比较乱一些。 为了能够产生mysqli.so文件，采用单独编译的方式，在php的源码目录中已经包含了mysqli的源码，进入mysqli源码目录下执行phpize;./configure --prefix=/home/kuring/php5.5/mysqli --with-php-config=/home/kuring/php5.5/bin/php-config --with-mysqli=/usr/bin/mysql_config;make;make install；。将mysqli.so文件安装到了/home/kuring/php5.5/lib/php/extensions/no-debug-non-zts-20121212目录下，不知道为什么目录末尾还要加个这么长的文件夹名，直接将文件复制到上一级目录下。 在/home/kuring/php5.5目录下没有找到php.ini文件，通过php --ini命令查看php的配置文件路径为/home/kuring/php5.5/lib，直接从php的源码文件中复制一个php.ini文件到该目录下。并将php.ini中的增加如下内容： 12extension_dir = &quot;/home/kuring/php5.5/lib/php/extensions&quot;extension=mysqli.so 再运行程序，发现mysqli的系列函数已经支持了，好一段折腾。 php-fpm执行cp $HOME/php5.5/etc/php-fpm.conf.default $HOME/php5.5/etc/php-fpm.conf来增加配置文件。 nginx首先安装pcre库，该库为正则表达式库。下载后通过 下载源码后执行./configure --prefix /home/kuring/nginx;make;make install;即可安装完成。 修改nginx的配置文件为如下内容： 123456789101112131415161718192021222324252627282930worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 8080; server_name localhost; root /home/kuring/www; location / &#123; index index.html index.htm index.php; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125; &#125;&#125;include servers/*; 常见操作nginx nginx -s stop：关闭 nginx -t：检测nginx的配置是否正确 mysql mysqld_safe：启动mysql mysqladmin shutdown -u root -p：关闭mysql create user kuring identified by ‘kuring_pass’：mysql创建用户（我尝试过几次，每次创建的用户密码都为空） drop user kuring：删除一个用户 grant all privileges on . to ‘root‘@’%’ identified by ‘root’ with grant option ：允许mysql的root用户通过远程登录 创建用户的操作使用create user kuring identified by &#39;kuring_pass&#39;命令创建用户kuring。默认创建完成的用户在本机无法登陆，但是远程却可以登陆。 这是因为mysql数据库中的user表中存在一条记录造成的。 12345678910111213141516use mysql;select user,host,password from user;// 在表中存在一条用户名为空的记录+---------+-----------+-------------------------------------------+| User | host | password |+---------+-----------+-------------------------------------------+| root | localhost | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B || root | 127.0.0.1 | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B || root | ::1 | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B || | localhost | || root | % | *81F5E21E35407D884A6CD4A731AEBFB6AF209E1B || report1 | % | *884CAA4D6FA1C3F7E4849C8DAF1B5B37FCB3EC0B |+---------+-----------+-------------------------------------------+// 将mysql中的为空的记录删除掉，这样就可以通过创建的用户连接了 在mysql命令行中执行grant all privileges on kuring_db.* to kuring identified by &#39;kuring_pass&#39;命令可以给刚创建的用户对数据库的权限。 修改mysql用户密码mysql将用户名和密码存放到了mysql数据库的user表中，在mysql命令行中执行use mysql;update user set password=password(&quot;new password&quot;) where user=&quot;username&quot;;flush privileges;即可更新相应用户的密码。 php-fpm php-fpm -D：启动php-fpm，如果需要指定php.ini文件，可以使用-c参数 php-fpm -t：检查php-fpm的配置文件 kill -USR2: 重启php-fpm kill -INT: 停止php-fpm php php –ini：显示php.ini文件路径 参考文章 运用Autoconf和Automake生成Makefile的学习之路]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[公司问题及经验总结]]></title>
      <url>%2Fpost%2Fcompany_question%2F</url>
      <content type="text"><![CDATA[我在软件行业工作已经有五个年头了，在现在这家公司已经有两个年头了。虽然身为公司的研发部经理可以参与公司的一些决定，但是没有绝对的话语权，对于公司的很多决定我深知是错误的，虽然后来也证明是错误的，但是我仍然无能为力。这里总结一下在公司中遇到的问题。 宁可招聘一个技术水平高的也不愿招聘三个技术水平低的。在工作中能够非常有力的证明这一点，三个刚工作的技术人员，尤其对于C++这样门槛稍微高一些，需要工作经验来弥补C++中坑的语言，三个C++技术人员远没有一个高水平的工作效率高，因为三个菜鸟需要将大牛踩过的坑全部踩一遍，踩过多少坑就代表走了多少弯路。 兴趣是最大的老师。我带过不少人，很多都是新人，我给他们制定了学习计划，期望他们能够在业余时间多学习，但实际上哪有几个人能够充分利用业余时间的。我就非常怀疑他们对技术的兴趣问题，如果他们对技术不感兴趣那为什么要加入该行业，为没有兴趣的工作而工作就是自己对自己耍流氓。如果他们对技术感兴趣，那只能说明他们业余时间中有更大的诱惑。 在招聘中不要过于在意金钱，便宜无好货在招聘行业中仍然非常适用。在招聘中千万不要吝惜给员工的那点钱，因为一千块钱而错失一个好的员工是非常不值得的。 盈利模式决定了公司对产品的态度。我所在的软件行业属于传统的软件行业，传统软件行业的盈利模式为销售，由于软件具有可复制性的特点，因此只要一套产品卖的越多就赚的越多。对于传统软件行业的产品使用者很多情况下就是几个人，至少跟互联网产品的用户数量不在一个量级。使用的人数决定了传统软件行业的用户体验可以做的很烂，技术水平可以不用那么高，只要能用就行，慢点无所谓，只要能卖出去就行了。身为一个技术人员，一个对技术有追求的技术人员，这令我非常反感，我做技术我不能对技术无所谓，我讨厌听到无所谓这样的字眼。 一定要明确公司的定位，明白什么时候应该干什么，什么应该干，野心太大也是问题。公司处于成长阶段提出了今年营业额比去年增长10倍的目标，我听到之后就是嗤之以鼻，这压根就是不可能的任务，而事实证明这也根本不可能完成，实际上当年营业额仅比去年增长了一倍。 一家公司一定要有自己的明确产品线，要抵住外界的诱惑。公司的产品线本来是非常明确的，后来由于客户需求和各种方面的原因，开始考虑疯狂扩展产品，这就造成了本来人手就紧蹙的情况下，没有时间去改善现有的系统，不得不去研发新的产品。自己没有的产品甚至跟客户合作或者完全购买别人的产品，导致公司很多人都在考虑跟其他公司合作的事宜。结果可想而知，新产品的销售并不理想，旧有的产品升级维护的也开始变慢。ps：我是非常讨厌在技术上跟其他公司之间考虑合作的问题，因为这从本质上讲并没有产生任何的社会价值，技术上必然涉及到接口的问题，只要是接口必然会有很多细节问题，这些往往会出现技术人员扯皮的问题，一个问题你可以解决他也可以解决，但是谁都不愿意解决，你说烦不烦。 技术人员后来要么转行要么做管理了。在济南技术人员就这两种出路吧，没见过多少大龄的程序员，很多情况下写着写着程序突然发现自己转为公司的中层了，比如我，并逐渐参与公司的事务。很多对程序不感兴趣的，可能就直接换个行业或者转行做销售了。 有些人再怎么培养也成不了高手。在工作我发现，有些人即使有了几年的工作经验，对公司的产品也非常了解，但是在解决问题的时候总是找不到点子上，占了一大堆资源，最后解决起问题来即慢又绕弯路，还留下一堆bug。对于这部分人，我想说也许这个行业不适合你。 领导千万不可三天两头一个想法，这在员工看来就是一个不靠谱的领导。谁都不愿意追随一个拿着自己当猴耍的领导，一会一个想法只能说明领导不够成熟，不适合做领导。跟随杰出的人，为杰出的人工作。 搞公司最好不要搞施工太久的。公司很多做工程的都在为现场的情况忙碌，一个点架设完毕后往往还需要耗费大量的时间来维护，维护对于公司而言牵涉到精力太大，尽量避免需要整天跟客户打交道和整天维护的业务。 专科生是很难撑起一家科技企业。虽然我不完全认同学历就能决定能力，但学历跟能力之间是成正比关系的。我的朋友中有专科生在工作几年后可以做专业的视频教程，并且业余时间写过几部玄幻小说。由于学习经历的不同这就造就了科班出身的程度不同，自然能力之间是有差异的。虽然中国的大学教育跟工作很脱节，但是在工作中还是能够跟大学教育挂起钩来的。学历跟素质之间也是成正比关系的，这里的素质体现在工作中就包括了工作中的责任心，工作态度等方面，这里就不展开了，要展开的话我可以举出非常多活生生的例子。因此，我非常不提倡在公司招聘中招聘专科生。我发现在很多情况下，很多专科人员是连普通话都不会的，操着各种方言或各种被普通话的方言。基本上能不能说普通话也是一个断定人素质的标准，扩展到其他行业同样适用。 也许本文的观点有些偏激，没错我就是一个偏激的IT工程师，就酱。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[C++11中的右值引用]]></title>
      <url>%2Fpost%2Fcpp11_right_reference%2F</url>
      <content type="text"><![CDATA[在C++98中有左值和右值的概念，不过这两个概念对于很多程序员并不关心，因为不知道这两个概念照样可以写出好程序。在C++11中对右值的概念进行了增强，我个人理解这部分内容是C++11引入的特性中最难以理解的了。该特性的引入至少可以解决C++98中的移动语义和完美转发问题，若你还不清楚这两个问题是什么，请向下看。 温馨提示，由于内容比较难懂，请仔细看。C++已经够复杂了，C++11中引入的新特性令C++更加复杂了。在学习本文的时候一定要理解清楚左值、右值、左值引用和右值引用。 移动构造函数首先看一个C++98中的关于函数返回类对象的例子。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class MyString &#123;public: MyString() &#123; _data = nullptr; _len = 0; printf("Constructor is called!\n"); &#125; MyString(const char* p) &#123; _len = strlen (p); _init_data(p); cout &lt;&lt; "Constructor is called! this-&gt;_data: " &lt;&lt; (long)_data &lt;&lt; endl; &#125; MyString(const MyString&amp; str) &#123; _len = str._len; _init_data(str._data); cout &lt;&lt; "Copy Constructor is called! src: " &lt;&lt; (long)str._data &lt;&lt; " dst: " &lt;&lt; (long)_data &lt;&lt; endl; &#125; ~MyString() &#123; if (_data) &#123; cout &lt;&lt; "DeConstructor is called! this-&gt;_data: " &lt;&lt; (long)_data &lt;&lt; endl; free(_data); &#125; else &#123; std::cout &lt;&lt; "DeConstructor is called!" &lt;&lt; std::endl; &#125; &#125; MyString&amp; operator=(const MyString&amp; str) &#123; if (this != &amp;str) &#123; _len = str._len; _init_data(str._data); &#125; cout &lt;&lt; "Copy Assignment is called! src: " &lt;&lt; (long)str._data &lt;&lt; " dst" &lt;&lt; (long)_data &lt;&lt; endl; return *this; &#125; operator const char *() const &#123; return _data; &#125;private: char *_data; size_t _len; void _init_data(const char *s) &#123; _data = new char[_len+1]; memcpy(_data, s, _len); _data[_len] = '\0'; &#125; &#125;; MyString foo()&#123; MyString middle("123"); return middle;&#125;int main() &#123; MyString a = foo(); return 1;&#125; 该例子在编译器没有进行优化的情况下会输出以下内容，我在输出的内容中做了注释处理，如果连这个例子的输出都看不懂，建议再看一下C++的语法了。我这里使用的编译器命令为g++ test.cpp -o main -g -fno-elide-constructors，之所以要加上-fno-elide-constructors选项时因为g++编译器默认情况下会对函数返回类对象的情况作返回值优化处理，这不是我们讨论的重点。 123456Constructor is called! this-&gt;_data: 29483024 // middle对象的构造函数Copy Constructor is called! src: 29483024 dst: 29483056 // 临时对象的构造，通过middle对象调用复制构造函数DeConstructor is called! this-&gt;_data: 29483024 // middle对象的析构Copy Constructor is called! src: 29483056 dst: 29483024 // a对象构造，通过临时对象调用复制构造函数DeConstructor is called! this-&gt;_data: 29483056 // 临时对象析构DeConstructor is called! this-&gt;_data: 29483024 // a对象析构 在上述例子中，临时对象的构造、复制和析构操作所带来的效率影响一直是C++中为人诟病的问题，临时对象的构造和析构操作均对堆上的内存进行操作，而如果_data的内存过大，势必会非常影响效率。从程序员的角度而言，该临时对象是透明的。而这一问题正是C++11中需要解决的问题。 在C++11中解决该问题的思路为，引入了移动构造函数，移动构造函数的定义如下。 123456MyString(MyString &amp;&amp;str) &#123; cout &lt;&lt; "Move Constructor is called! src: " &lt;&lt; (long)str._data &lt;&lt; endl; _len = str._len; _data = str._data; str._data = nullptr;&#125; 在移动构造函数中我们窃取了str对象已经申请的内存，将其拿为己用，并将str申请的内存给赋值为nullptr。移动构造函数和复制构造函数的不同之处在于移动构造函数的参数使用&amp;&amp;，这就是下文要讲解的右值引用符号。参数不再是const，因为在移动构造函数需要修改右值str的内容。 移动构造函数的调用时机为用来构造临时变量和用临时变量来构造对象的时候移动语义会被调用。可以通过下面的输出结果看到，我们所使用的编译参数为g++ test.cpp -o main -g -fno-elide-constructors --std=c++11。 123456Constructor is called! this-&gt;_data: 22872080 // middle对象构造Move Constructor is called! src: 22872080 // 临时对象通过移动构造函数构造，将middle申请的内存窃取DeConstructor is called! // middle对象析构Move Constructor is called! src: 22872080 // 对象a通过移动构造函数构造，将临时对象的内存窃取DeConstructor is called! // 临时对象析构DeConstructor is called! this-&gt;_data: 22872080 // 对象a析构 通过输出结果可以看出，整个过程中仅申请了一块内存，这也正好符合我们的要求了。 C++98中的左值和右值我们先来看下C++98中的左值和右值的概念。左值和右值最直观的理解就是一条语句等号左边的为左值，等号右边的为右值，而事实上该种理解是错误的。左值：可以取地址，有名字的值，是一个指向某内存空间的表达式，可以使用&amp;操作符获取内存地址。右值：不能取地址，即非左值的都是右值，没有名字的值，是一个临时值，表达式结束后右值就没有意义了。我想通过下面的例子，读者可以清楚的理解左值和右值了。 12345678910111213141516// lvalues://int i = 42;i = 43; // i是左值int* p = &amp;i; // i是左值int&amp; foo();foo() = 42; // foo()返回引用类型是左值int* p1 = &amp;foo(); // foo()可以取地址是左值// rvalues://int foobar();int j = 0;j = foobar(); // foobar()是右值int* p2 = &amp;foobar(); // 编译错误，foobar()是右值不能取地址j = 42; // 42是右值 C++11右值引用和移动语义在C++98中有引用的概念，对于const int &amp;m = 1，其中m为引用类型，可以对其取地址，故为左值。在C++11中，引入了右值引用的概念，使用&amp;&amp;来表示。在引入了右值引用后，在函数重载时可以根据是左值引用还是右值引用来区分。 12345678910111213141516void fun(MyString &amp;str)&#123; cout &lt;&lt; "left reference" &lt;&lt; endl;&#125;void fun(MyString &amp;&amp;str)&#123; cout &lt;&lt; "right reference" &lt;&lt; endl;&#125;int main() &#123; MyString a("456"); fun(a); // 左值引用，调用void fun(MyString &amp;str) fun(foo()); // 右值引用，调用void fun(MyString &amp;&amp;str) return 1;&#125; 在绝大多数情况下，这种通过左值引用和右值引用重载函数的方式仅会在类的构造函数和赋值操作符中出现，被例子仅是为了方便采用函数的形式，该种形式的函数用到的比较少。上述代码中所使用的将资源从一个对象到另外一个对象之间的转移就是移动语义。这里提到的资源是指类中的在堆上申请的内存、文件描述符等资源。 前面已经介绍过了移动构造函数的具体形式和使用情况，这里对移动赋值操作符的定义再说明一下，并将main函数的内容也一起更改，将得到如下输出结果。 1234567891011121314151617181920212223242526272829MyString&amp; operator=(MyString&amp;&amp; str) &#123; cout &lt;&lt; "Move Operator= is called! src: " &lt;&lt; (long)str._data &lt;&lt; endl; if (this != &amp;str) &#123; if (_data != nullptr) &#123; free(_data); &#125; _len = str._len; _data = str._data; str._len = 0; str._data = nullptr; &#125; return *this; &#125;int main() &#123; MyString b; b = foo(); return 1;&#125;// 输出结果，整个过程仅申请了一个内存地址Constructor is called! // 对象b构造函数调用Constructor is called! this-&gt;_data: 14835728 // middle对象构造Move Constructor is called! src: 14835728 // 临时对象通过移动构造函数由middle对象构造DeConstructor is called! // middle对象析构Move Operator= is called! src: 14835728 // 对象b通过移动赋值操作符由临时对象赋值DeConstructor is called! // 临时对象析构DeConstructor is called! this-&gt;_data: 14835728 // 对象b析构函数调用 在C++中对一个变量可以通过const来修饰，而const和引用是对变量约束的两种方式，为并行存在，相互独立。因此，就可以划分为了const左值引用、非const左值引用、const右值引用和非const右值引用四种类型。其中左值引用的绑定规则和C++98中是一致的。 非const左值引用只能绑定到非const左值，不能绑定到const右值、非const右值和const左值。这一点可以通过const关键字的语义来判断。 const左值引用可以绑定到任何类型，包括const左值、非const左值、const右值和非const右值，属于万能引用类型。其中绑定const右值的规则比较少见，但是语法上是可行的，比如const int &amp;a = 1，只是我们一般都会直接使用int &amp;a = 1了。 非const右值引用不能绑定到任何左值和const右值，只能绑定非const右值。 const右值引用类型仅是为了语法的完整性而设计的， 比如可以使用const MyString &amp;&amp;right_ref = foo()，但是右值引用类型的引入主要是为了移动语义，而移动语义需要右值引用是可以被修改的，因此const右值引用类型没有实际意义。 我们通过表格的形式对上文中提到的四种引用类型可以绑定的类型进行总结。 引用类型/是否绑定 非const左值 const左值 非const右值 const右值 备注 非const左值引用 | 是 | 否 | 否 | 否 |无 |const左值引用 | 是 | 是 | 是 | 是 | 全能绑定类型，绑定到const右值的情况比较少见 |非const右值引用 | 否 | 否 | 是 | 否 | C++11中引入的特性，用于移动语义和完美转发 |const值引用 | 是 | 否 | 否 | 否 | 没有实际意义，为了语法完整性而存在 | 下面针对上述例子，我们看一下foo函数绑定参数的情况。 如果只实现了void foo(MyString &amp;str)，而没有实现void fun(MyString &amp;&amp;str)，则和之前一样foo函数的实参只能是非const左值。 如果只实现了void foo(const MyString &amp;str)，而没有实现void fun(MyString &amp;&amp;str)，则和之前一样foo函数的参数即可以是左值又可以是右值，因为const左值引用是万能绑定类型。 如果只实现了void foo(MyString &amp;&amp;str)，而没有实现void fun(MyString &amp;str)，则foo函数的参数只能是非const右值。 强制移动语义std::move()前文中我们通过右值引用给类增加移动构造函数和移动赋值操作符已经解决了函数返回类对象效率低下的问题。那么还有什么问题没有解决呢？ 在C++98中的swap函数的实现形式如下，在该函数中我们可以看到整个函数中的变量a、b、c均为左值，无法直接使用前面移动语义。 1234567template &lt;class T&gt; void swap ( T&amp; a, T&amp; b )&#123; T c(a); a=b; b=c;&#125; 但是如果该函数中能够使用移动语义是非常合适的，仅是为了交换两个变量，却要反复申请和释放资源。按照前面的知识变量c不可能为非const右值引用，因为变量a为非const左值，非const右值引用不能绑定到任何左值。 在C++11的标准库中引入了std::move()函数来解决该问题，该函数的作用为将其参数转换为右值。在C++11中的swap函数就可以更改为了： 1234567template &lt;class T&gt; void swap (T&amp; a, T&amp; b)&#123; T c(std::move(a)); a=std::move(b); b=std::move(c);&#125; 在使用了move语义以后,swap函数的效率会大大提升，我们更改main函数后测试如下: 1234567891011121314151617int main() &#123; // move函数 MyString d("123"); MyString e("456"); swap(d, e); return 1;&#125;// 输出结果，通过输出结果可以看出对象交换是成功的Constructor is called! this-&gt;_data: 38469648 // 对象d构造Constructor is called! this-&gt;_data: 38469680 // 对象e构造Move Constructor is called! src: 38469648 // swap函数中的对象c通过移动构造函数构造Move Operator= is called! src: 38469680 // swap函数中的对象a通过移动赋值操作符赋值Move Operator= is called! src: 38469648 // swap函数中的对象b通过移动赋值操作符赋值DeConstructor is called! // swap函数中的对象c析构DeConstructor is called! this-&gt;_data: 38469648 // 对象e析构DeConstructor is called! this-&gt;_data: 38469680 // 对象d析构 右值引用和右值的关系这个问题就有点绕了，需要开动思考一下右值引用和右值是啥含义了。读者会凭空的认为右值引用肯定是右值，其实不然。我们在之前的例子中添加如下代码，并将main函数进行修改如下： 123456789101112131415161718192021void test_rvalue_rref(MyString &amp;&amp;str)&#123; cout &lt;&lt; "tmp object construct start" &lt;&lt; endl; MyString tmp = str; cout &lt;&lt; "tmp object construct finish" &lt;&lt; endl;&#125;int main() &#123; test_rvalue_rref(foo()); return 1;&#125;// 输出结果Constructor is called! this-&gt;_data: 28913680Move Constructor is called! src: 28913680DeConstructor is called!tmp object construct startCopy Constructor is called! src: 28913680 dst: 28913712 // 可以看到这里调用的是复制构造函数而不是移动构造函数tmp object construct finishDeConstructor is called! this-&gt;_data: 28913712DeConstructor is called! this-&gt;_data: 28913680 我想程序运行的结果肯定跟大多数人想到的不一样，“Are you kidding me?不是应该调用移动构造函数吗？为什么调用了复制构造函数？”。关于右值引用和左右值之间的规则是： 如果右值引用有名字则为左值，如果右值引用没有名字则为右值。 通过规则我们可以发现，在我们的例子中右值引用str是有名字的，因此为左值，tmp的构造会调用复制构造函数。之所以会这样，是因为如果tmp构造的时候调用了移动构造函数，则调用完成后str的申请的内存自己已经不可用了，如果在该函数中该语句的后面在调用str变量会出现我们意想不到的问题。鉴于此，我们也就能够理解为什么有名字的右值引用是左值了。如果已经确定在tmp构造语句的后面不需要使用str变量了，可以使用std::move()函数将str变量从左值转换为右值，这样tmp变量的构造就可以使用移动构造函数了。 而如果我们调用的是MyString b = foo()语句，由于foo()函数返回的是临时对象没有名字属于右值，因此b的构造会调用移动构造函数。 该规则非常的重要，要想能够正确使用右值引用，该规则必须要掌握，否则写出来的代码会有一个大坑。 完美转发前面已经介绍了本文的两大主题之一的移动语义，还剩下完美转发机制。完美转发机制通常用于库函数中，至少在我的工作中还是很少使用的。如果实在不想理解该问题，可以不用向下看了。在泛型编程中，经常会遇到的一个问题是怎样将一组参数原封不动的转发给另外一个函数。这里的原封不动是指，如果函数是左值，那么转发给的那个函数也要接收一个左值；如果参数是右值，那么转发给的函数也要接收一个右值；如果参数是const的，转发给的函数也要接收一个const参数；如果参数是非const的，转发给的函数也要接收一个非const值。 该问题看上去非常简单，其实不然。看一个例子： 1234567891011121314151617181920212223242526#include &lt;iostream&gt;using namespace std;void fun(int &amp;) &#123; cout &lt;&lt; "lvalue ref" &lt;&lt; endl; &#125; void fun(int &amp;&amp;) &#123; cout &lt;&lt; "rvalue ref" &lt;&lt; endl; &#125; void fun(const int &amp;) &#123; cout &lt;&lt; "const lvalue ref" &lt;&lt; endl; &#125; void fun(const int &amp;&amp;) &#123; cout &lt;&lt; "const rvalue ref" &lt;&lt; endl; &#125;template&lt;typename T&gt;void PerfectForward(T t) &#123; fun(t); &#125; int main()&#123; PerfectForward(10); // rvalue ref int a; PerfectForward(a); // lvalue ref PerfectForward(std::move(a)); // rvalue ref const int b = 8; PerfectForward(b); // const lvalue ref PerfectForward(std::move(b)); // const rvalue ref return 0;&#125; 在上述例子中，我们想达到的目的是PerfectForward模板函数能够完美转发参数t到fun函数中。上述例子中的PerfectForward函数必然不能够达到此目的，因为PerfectForward函数的参数为左值类型，调用的fun函数也必然为void fun(int &amp;)。且调用PerfectForward之前就产生了一次参数的复制操作，因此这样的转发只能称之为正确转发，而不是完美转发。要想达到完美转发，需要做到像转发函数不存在一样的效率。 因此，我们考虑将PerfectForward函数的参数更改为引用类型，因为引用类型不会有额外的开销。另外，还需要考虑转发函数PerfectForward是否可以接收引用类型。如果转发函数PerfectForward仅能接收左值引用或右值引用的一种，那么也无法实现完美转发。 我们考虑使用const T &amp;t类型的参数，因为我们在前文中提到过，const左值引用类型可以绑定到任何类型。但是这样目标函数就不一定能接收const左值引用类型的参数了。const左值引用属于左值，非const左值引用和非const右值引用是无法绑定到const左值的。 如果将参数t更改为非const右值引用、const右值也是不可以实现完美转发的。 在C++11中为了能够解决完美转发问题，引入了更为复杂的规则：引用折叠规则和特殊模板参数推导规则。 引用折叠推导规则为了能够理解清楚引用折叠规则，还是通过以下例子来学习。 12345678910typedef int&amp; TR;int main()&#123; int a = 1; int &amp;b = a; int &amp; &amp;c = a; // 编译器报错，不可以对引用再显示添加引用 TR &amp;d = a; // 通过typedef定义的类型隐式添加引用是可以的 return 1;&#125; 在C++中，不可以在程序中对引用再显示添加引用类型，对于int &amp; &amp;c的声明变量方式，编译器会提示错误。但是如果在上下文中（包括使用模板实例化、typedef、auto类型推断等）出现了对引用类型再添加引用的情况，编译器是可以编译通过的。具体的引用折叠规则如下，可以看出一旦引用中定义了左值类型，折叠规则总是将其折叠为左值引用。这就是引用折叠规则的全部内容了。另外折叠规则跟变量的const特性是没有关系的。 1234A&amp; &amp; =&gt; A&amp;A&amp; &amp;&amp; =&gt; A&amp;A&amp;&amp; &amp; =&gt; A&amp;A&amp;&amp; &amp;&amp; =&gt; A&amp;&amp; 特殊模板参数推导规则下面我们再来学习特殊模板参数推导规则，考虑下面的模板函数，模板函数接收一个右值引用作为模板参数。 12template&lt;typename T&gt;void foo(T&amp;&amp;); 说白点，特殊模板参数推导规则其实就是引用折叠规则在模板参数为右值引用时模板情况下的应用，是引用折叠规则的一种情况。我们结合上文中的引用折叠规则， 如果foo的实参是上文中的A类型的左值时，T的类型就为A&amp;。根据引用折叠规则，最后foo的参数类型为A&amp;。 如果foo的实参是上文中的A类型的右值时，T的类型就为A&amp;&amp;。根据引用折叠规则，最后foo的参数类型为A&amp;&amp;。 解决完美转发问题我们已经学习了模板参数为右值引用时的特殊模板参数推导规则，那么我们利用刚学习的知识来解决本文中待解决的完美转发的例子。 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;using namespace std;void fun(int &amp;) &#123; cout &lt;&lt; "lvalue ref" &lt;&lt; endl; &#125;void fun(int &amp;&amp;) &#123; cout &lt;&lt; "rvalue ref" &lt;&lt; endl; &#125;void fun(const int &amp;) &#123; cout &lt;&lt; "const lvalue ref" &lt;&lt; endl; &#125;void fun(const int &amp;&amp;) &#123; cout &lt;&lt; "const rvalue ref" &lt;&lt; endl; &#125;//template&lt;typename T&gt;//void PerfectForward(T t) &#123; fun(t); &#125;// 利用引用折叠规则代替了原有的不完美转发机制template&lt;typename T&gt;void PerfectForward(T &amp;&amp;t) &#123; fun(static_cast&lt;T &amp;&amp;&gt;(t)); &#125;int main()&#123; PerfectForward(10); // rvalue ref，折叠后t类型仍然为T &amp;&amp; int a; PerfectForward(a); // lvalue ref，折叠后t类型为T &amp; PerfectForward(std::move(a)); // rvalue ref，折叠后t类型为T &amp;&amp; const int b = 8; PerfectForward(b); // const lvalue ref，折叠后t类型为const T &amp; PerfectForward(std::move(b)); // const rvalue ref，折叠后t类型为const T &amp;&amp; return 0;&#125; 例子中已经对完美转发的各种情况进行了说明，这里需要对PerfectForward模板函数中的static_cast进行说明。static_cast仅是对传递右值时起作用。我们看一下当参数为右值时的情况，这里的右值包括了const右值和非const右值。 1234567// 参数为右值，引用折叠规则引用前template&lt;int &amp;&amp; &amp;&amp;T&gt;void PerfectForward(int &amp;&amp; &amp;&amp;t) &#123; fun(static_cast&lt;int &amp;&amp; &amp;&amp;&gt;(t)); &#125;// 引用折叠规则应用后template&lt;int &amp;&amp;T&gt;void PerfectForward(int &amp;&amp;t) &#123; fun(static_cast&lt;int &amp;&amp;&gt;(t)); &#125; 可能读者仍然没有发现上述例子中的问题，“不用static_cast进行强制类型转换不是也可以吗？”。别忘记前文中仍然提到一个右值引用和右值之间关系的规则，如果右值引用有名字则为左值，如果右值引用没有名字则为右值。。这里的变量t虽然为右值引用，但是是左值。如果我们想继续向fun函数中传递右值，就需要使用static_cast进行强制类型转换了。 其实在C++11中已经为我们封装了std::forward函数来替代我们上文中使用的static_cast类型转换，该例子中使用std::forward函数的版本变为了： 12template&lt;typename T&gt;void PerfectForward(T &amp;&amp;t) &#123; fun(std::forward&lt;T&gt;(t)); &#125; 对于上文中std::move函数的实现也是使用了引用折叠规则，实现方式跟std::forward一致。 引用 《深入理解C++11-C++11新特性解析与应用》 C++11 标准新特性: 右值引用与转移语义 如何评价 C++11 的右值引用（Rvalue reference）特性？ C++11 完美转发 C++ Rvalue References Explained 详解C++右值引用 （对C++ Rvalue References Explained的翻译）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大型网站技术架构读书笔记]]></title>
      <url>%2Fpost%2Flarge_website_architecture%2F</url>
      <content type="text"><![CDATA[最近粗读了一遍《大型网站技术架构-核心原理与案例分析》，并对其中的内容通过思维导图的形式进行了整理。本书的所讲解的内容均为大型网站中涉及到的问题及相关技术，但并未展开深入讨论相关技术的解决办法，非常适合入门。下面我将我的思维导图以图片的形式贴出来，并提供XMind编辑的.xmid格式的文件。 下载大型网站技术架构读书笔记]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[解决airodump-ng显示ssid名称的乱码问题]]></title>
      <url>%2Fpost%2Fairodump-ng_ssid_messy%2F</url>
      <content type="text"><![CDATA[问题描述无线wifi的essid支持英文和中文，中文的编码在802.11协议并没有规定，对于802.11协议而言仅将essid看作是二进制。而中文又存在多种编码方式，最常见的就是GB18030（我这里直接用GB18030代替了GB系列的字符集）和UTF-8了。 iwlist程序通过命令iwlist wlan0 scanning可以在终端上正常显示UTF-8编码的essid，对于其他编码的中文仍然是乱码，这也就非常容易理解了。因为具体的essid能否将中文正常显示在终端屏幕上跟essid的编码和当前终端环境的编码是否能够匹配有关，如果essid的编码和当前终端环境的编码均为UTF-8，则essid可以在屏幕上正常显示。如果当前网络中的可以搜索到的essid即包含了GB18030编码又包含了UTF-8编码，则打印在终端上的essid必然会有乱码的情况出现。 airodump-ng程序问题对于airodump-ng程序而言，即时是essid的编码和终端编码一致也会出现某些中文字符乱码的问题，这一点比较奇怪。比如“免费”中的“免”字是乱码，“费”却能正常显示。通过这一现象有理由怀疑airodump-ng对essid做了某些处理。 经过查看源码发现，在airodump-ng.c文件中存在三处如下类似代码，作用为将essid中的ascii值在(126,160)之间的转换为”.”。看来airodump-ng程序并没有考虑到中文的情况，仅将ascii中无法显示的字符做了转换。将程序中的三处代码注释后就可以正常显示了。具体三处代码可以通过搜索’.’来查找。 123456789for( i = 0; i &lt; n; i++ )&#123; c = p[2 + i]; if( c == 0 || ( c &gt; 126 &amp;&amp; c &lt; 160 ) ) &#123; c = '.'; //could also check ||(c&gt;0 &amp;&amp; c&lt;32) &#125; st_cur-&gt;probes[st_cur-&gt;probe_index][i] = c;&#125; NetworkManager通过实践发现，GNOME和KDE桌面下的查看无线网络连接的ssid是可以正常显示的，即可以正常显示GB18030，又可以正常显示UTF-8编码的essid。则可以推测，在桌面环境下的搜索网络的程序肯定对编码做了某些处理，顺着这个思路，就可以查找GNOME或KDE的代码了。 在GNOME的源码中看到了network-manager-applet，该程序即为桌面上查看无线网络连接的小控件。在applet-device-wifi.c文件中看到了如下代码，其中的nm_utils_ssid_to_utf8函数即为将其他编码转换为UTF-8编码的函数。 12345678910111213141516static char *get_ssid_utf8 (NMAccessPoint *ap)&#123; char *ssid_utf8 = NULL; const GByteArray *ssid; if (ap) &#123; ssid = nm_access_point_get_ssid (ap); if (ssid) ssid_utf8 = nm_utils_ssid_to_utf8 (ssid); &#125; if (!ssid_utf8) ssid_utf8 = g_strdup (_("(none)")); return ssid_utf8;&#125; nm_utils_ssid_to_utf8函数定义在NetworkManager工程中的nm-utils.c文件中。该函数的代码如下，该函数具体功能可以查看代码中的注释，已经非常详细了。其中以g_开头的函数是glib库中的函数。 1234567891011121314151617181920212223242526272829303132333435363738char *nm_utils_ssid_to_utf8 (const GByteArray *ssid)&#123; char *converted = NULL; char *lang, *e1 = NULL, *e2 = NULL, *e3 = NULL; g_return_val_if_fail (ssid != NULL, NULL); if (g_utf8_validate ((const gchar *) ssid-&gt;data, ssid-&gt;len, NULL)) return g_strndup ((const gchar *) ssid-&gt;data, ssid-&gt;len); /* LANG may be a good encoding hint */ g_get_charset ((const char **)(&amp;e1)); if ((lang = getenv ("LANG"))) &#123; char * dot; lang = g_ascii_strdown (lang, -1); if ((dot = strchr (lang, '.'))) *dot = '\0'; get_encodings_for_lang (lang, &amp;e1, &amp;e2, &amp;e3); g_free (lang); &#125; converted = g_convert ((const gchar *) ssid-&gt;data, ssid-&gt;len, "UTF-8", e1, NULL, NULL, NULL); if (!converted &amp;&amp; e2) converted = g_convert ((const gchar *) ssid-&gt;data, ssid-&gt;len, "UTF-8", e2, NULL, NULL, NULL); if (!converted &amp;&amp; e3) converted = g_convert ((const gchar *) ssid-&gt;data, ssid-&gt;len, "UTF-8", e3, NULL, NULL, NULL); if (!converted) &#123; converted = g_convert_with_fallback ((const gchar *) ssid-&gt;data, ssid-&gt;len, "UTF-8", e1, "?", NULL, NULL, NULL); &#125; return converted;&#125; nm_utils_ssid_to_utf8该函数位于libnm-util.so.1动态库中，可通过nm -D /usr/lib64/libnm-util.so.1 | grep nm_utils_ssid_to_utf8命令查看导出表中存在该函数。但是系统中并不存在该函数的头文件libnm-util.h，给该库的调用增加了不少难度。可以通过将相关头文件引入到该工程编译的方式来完成，但是可能会牵涉到的头文件比较多，比较繁琐。 我这里直接采用了将NetworkManager中相关代码抓取出来的思路，并将其封装成类的形式以方便调用。具体代码可以参照demo中的例子。 glibglib是GTK底层调用的核心库，跟glibc是没有关系的，虽然名字中仅差一个字母。为了调用该库需要在编译的时候添加pkg-config --cflags --libs glib-2.0信息，以引入需要的头文件和要链接的库。 相关下载文中用到的软件源码和程序demo 引用 GNOME源码列表]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leetcode题目之Single Number]]></title>
      <url>%2Fpost%2Fsingle_number%2F</url>
      <content type="text"><![CDATA[题目一 Single Number Given an array of integers, every element appears twice except for one. Find that single one.Note:Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? 题目二 Single Number II Given an array of integers, every element appears three times except for one. Find that single one.Note:Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? 题目一分析及解答针对题目一，一看就能看出是考察异或操作的特点，并迅速写出了解答方法。 1234567891011class Solution &#123;public: int singleNumber(int A[], int n) &#123; int result = 0; for (int i = 0; i &lt; n; i++) &#123; result ^= A[i]; &#125; return result; &#125;&#125;; 题目二分析及解答要想实现时间复杂度为O(n)，空间复杂度为O(1)的算法，还是跟题目一一样需要充分利用位操作特性，但是并没有直接可用的位操作特性可以完成，于是想到肯定是各种位操作的组合操作，但是并没有继续向下想到具体的算法。本质上该题目就是模拟一个三进制的操作，当一个位的最大值为2，当为3时直接清0。 参照网上的算法，利用一个int类型的数组来模拟一个三进制数，每个int值的最大值为3，当然这样存在一定空间上的浪费。算法需要将A中的每个值通过移位运算获取到该位的状态，并将值添加到用来模拟三进制的int数组中相应的位置，最后将模拟三进制int数组中的值为3的更改为0。 12345678910111213141516class Solution &#123;public: int singleNumber(int A[], int n) &#123; int count[32] = &#123;0&#125;; int result = 0; for (int i = 0; i &lt; 32; i++) &#123; for (int j = 0; j &lt; n; j++&#123; if ((A[j] &gt;&gt; i) &amp; 1) &#123; count[i]++; &#125; &#125; result |= ((count[i] % 3) &lt;&lt; i); &#125; return result; &#125;&#125;; 另外，还有上述算法的改进算法，更为节省空间，效率更高，但是确实不容易理解和记忆，属于下次仍然无法记忆的算法类型。这里仅提供代码，不再给出解释，自己领悟。 123456789101112int singleNumber(int A[], int n) &#123; int ones = 0, twos = 0, threes = 0; for (int i = 0; i &lt; n; i++) &#123; twos |= ones &amp; A[i]; ones ^= A[i];// 异或3次 和 异或 1次的结果是一样的 //对于ones 和 twos 把出现了3次的位置设置为0 （取反之后1的位置为0） threes = ones &amp; twos; ones &amp;= ~threes; twos &amp;= ~threes; &#125; return ones;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[牛客网内推笔试卷题目2015.3.12]]></title>
      <url>%2Fpost%2Fnowcoder_2015.3.12%2F</url>
      <content type="text"><![CDATA[前段时间参加了牛客网的答题活动，共两套试题，每套题目3个算法题，我只做了每套题的前两道。最近想查看之前做的题目的答案，却发现非常不方便，特此将我做过的4道题目记录一下，算法的思路就不再解释了。 题目一 奇数位上都是奇数或者偶数位上都是偶数 给定一个长度不小于2的数组arr。 写一个函数调整arr，使arr中要么所有的偶数位上都是偶数，要么所有的奇数位上都是奇数上。 要求：如果数组长度为N，时间复杂度请达到O(N)，额外空间复杂度请达到O(1),下标0,2,4,6…算作偶数位,下标1,3,5,7…算作奇数位，例如[1,2,3,4]调整为[2,1,4,3]即可。 12345678910111213141516171819202122232425262728293031class Solution &#123;public: void oddInOddEvenInEven(vector&lt;int&gt;&amp; arr, int len) &#123; int odd = 1; int even = 0; while (odd &lt; len &amp;&amp; even &lt; len) &#123; if (arr[odd] % 2 == 0) &#123; while (arr[even] % 2 == 0) &#123; even += 2; &#125; if (even &lt; len) &#123; int tmp = arr[even]; arr[even] = arr[odd]; arr[odd] = tmp; &#125; else &#123; break; &#125; &#125; else &#123; odd += 2; &#125; &#125; &#125;&#125;; 题目二 求正数数组的最小不可组成和 给定一个全是正数的数组arr，定义一下arr的最小不可组成和的概念： 1，arr的所有非空子集中，把每个子集内的所有元素加起来会出现很多的值，其中最小的记为min，最大的记为max； 2，在区间[min,max]上，如果有一些正数不可以被arr某一个子集相加得到，那么这些正数中最小的那个，就是arr的最小不可组成和； 3，在区间[min,max]上，如果所有的数都可以被arr的某一个子集相加得到，那么max+1是arr的最小不可组成和； 举例： arr = {3,2,5} arr的min为2，max为10，在区间[2,10]上，4是不能被任何一个子集相加得到的值中最小的，所以4是arr的最小不可组成和； arr = {3,2,4} arr的min为2，max为9，在区间[2,9]上，8是不能被任何一个子集相加得到的值中最小的，所以8是arr的最小不可组成和； arr = {3,1,2} arr的min为1，max为6，在区间[2,6]上，任何数都可以被某一个子集相加得到，所以7是arr的最小不可组成和； 请写函数返回arr的最小不可组成和。 1234567891011121314151617181920212223242526272829class Solution &#123;public: int getFirstUnFormedNum(vector&lt;int&gt;&amp; arr, int len) &#123; set&lt;int&gt; res; for (int i=0; i&lt;len; i++) &#123; set&lt;int&gt; tmp = res; for (set&lt;int&gt;::iterator iter = res.begin(); iter != res.end(); iter++) &#123; tmp.insert(*iter + arr[i]); &#125; res = tmp; res.insert(arr[i]); &#125; set&lt;int&gt;::iterator iter = res.begin(); int before = *iter; iter++; for (; iter != res.end(); iter++) &#123; if (*iter - before &gt; 1) &#123; return before + 1; &#125; before = *iter; &#125; return before + 1; &#125;&#125;; 题目三 最大的LeftMax与rightMax之差绝对值 给定一个长度为N的整型数组arr，可以划分成左右两个部分： 左部分arr[0..K]，右部分arr[K+1..arr.length-1]，K可以取值的范围是[0,arr.length-2] 求这么多划分方案中，左部分中的最大值减去右部分最大值的绝对值，最大是多少？ 例如： [2,7,3,1,1] 当左部分为[2,7]，右部分为[3,1,1]时，左部分中的最大值减去右部分最大值的绝对值为4; 当左部分为[2,7,3]，右部分为[1,1]时，左部分中的最大值减去右部分最大值的绝对值为6; 最后返回的结果为6。 注意：如果数组的长度为N，请尽量做到时间复杂度O(N)，额外空间复杂度O(1) 1234567891011121314151617181920212223242526class Solution &#123;public: int getMaxABSLeftAndRight(vector&lt;int&gt; vec, int len) &#123; if (len == 0) &#123; return 0; &#125; // find the max in array int max = vec[0]; for (int i=1; i&lt;(int)vec.size(); i++) &#123; if (vec[i] &gt; max) &#123; max = vec[i]; &#125; &#125; // compare the head and tail in array if (vec[0] &lt; vec[len - 1]) &#123; return max - vec[0]; &#125; return max - vec[len - 1]; &#125;&#125;; 题目四 按照左右半区的方式重新组合单链表 给定一个单链表的头部节点head，链表长度为N。 如果N为偶数，那么前N/2个节点算作左半区，后N/2个节点算作右半区； 如果N为奇数，那么前N/2个节点算作左半区，后N/2+1个节点算作右半区； 左半区从左到右依次记为L1-&gt;L2-&gt;…，右半区从左到右依次记为R1-&gt;R2-&gt;…。请将单链表调整成L1-&gt;R1-&gt;L2-&gt;R2-&gt;…的样子。 例如： 1-&gt;2-&gt;3-&gt;4 调整后：1-&gt;3-&gt;2-&gt;4 1-&gt;2-&gt;3-&gt;4-&gt;5 调整后：1-&gt;3-&gt;2-&gt;4-&gt;5 要求：如果链表长度为N，时间复杂度请达到O(N)，额外空间复杂度请达到O(1) 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123;public: void relocateList(struct ListNode* head) &#123; if (head == NULL || head-&gt;next == NULL) &#123; return ; &#125; // use one loop, find the right head ListNode *right_head = head; ListNode *node = head; while (node != NULL) &#123; if (node-&gt;next == NULL) &#123; break; &#125; if (node-&gt;next-&gt;next == NULL) &#123; right_head = right_head-&gt;next; break; &#125; right_head = right_head-&gt;next; node = node-&gt;next-&gt;next; &#125; ListNode *left_node = head; ListNode *right_node = right_head; while (left_node-&gt;next != right_head) &#123; ListNode *tmp = left_node-&gt;next; left_node-&gt;next = right_node; right_node = right_node-&gt;next; left_node-&gt;next-&gt;next = tmp; left_node = left_node-&gt;next-&gt;next; &#125; left_node-&gt;next = right_node; &#125;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leetcode题目之Majority Element]]></title>
      <url>%2Fpost%2Fmajority_element%2F</url>
      <content type="text"><![CDATA[题目 Given an array of size n, find the majority element. The majority element is the element that appears more than ⌊ n/2 ⌋ times.You may assume that the array is non-empty and the majority element always exist in the array. 分析本题是一道非常简单的题目，但我能想到的思路有限，仅能想到排序法和哈希法两种算法，在Solution中提供了另外几种方法，这是非常值得我学习和思考的。本文仅将网站的思路拿过来，可以直接看该问题的Solution。 解答暴力枚举法最原始的解决办法，逐个元素比较是否为该数组中的最多元素，只要满足条件即可终止。时间复杂度为O(n^2)。 哈希表法将数组中的元素遍历一遍，并将数组中元素的个数保存到哈希中。然后遍历哈希，从哈希中找到最多元素。时间复杂度O(n)，但需要占用一定的空间。 1234567891011121314151617181920212223242526int majorityElement(vector&lt;int&gt; &amp;num) &#123; std::map&lt;int, int&gt; result_map; for (vector&lt;int&gt;::iterator iter = num.begin(); iter != num.end(); iter++) &#123; if (result_map.find(*iter) == result_map.end()) &#123; result_map.insert(map&lt;int, int&gt;::value_type(*iter, 1)); &#125; else &#123; result_map[*iter]++; &#125; &#125; int max_count = 0; int result; for (std::map&lt;int, int&gt;::iterator iter = result_map.begin(); iter != result_map.end(); iter++) &#123; if (iter-&gt;second &gt; max_count) &#123; result = iter-&gt;first; max_count = iter-&gt;second; &#125; &#125; return result; &#125; 排序法直接对元素进行排序，排序后元素的中间元素即为要求的最多元素。时间复杂度为O(nlogn)。 1234int majorityElementSort(vector&lt;int&gt; &amp;num) &#123; sort(num.begin(), num.end()); return num[num.size() / 2];&#125; 随机抽取法随机从数组中抽取元素，然后遍历数组判断该元素是否为最多元素。该算法利用了最多元素被随机抽取的概率最大的特点，但该算法效率的随机性较大，最好时间复杂度为O(n)，最坏情况下一直随机不到最多元素。 二分法将数组均分为两份，分别求出两个数组中的最多元素A和B，则整个数组中的最多元素必然在两个子数组的最多元素A和B中，这一点可以通过举例子的方式来证明，但是仅凭感觉不太容易得出该结论。如果A==B，则结果就是A。如果A!=B，则分别求出A和B在这个数组中的元素个数。时间复杂度接近O(nlogn)。 其他还有一些比较不容易想到的算法，这里就不列举了。至少我看过一次之后，下次这些算法仍然是记不住的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[asleap中的简单文件索引机制]]></title>
      <url>%2Fpost%2Faleap_idx%2F</url>
      <content type="text"><![CDATA[asleap是一个开源的vpn破解工具，最近查看了asleap的源码，该项目地址。本文的重点是对其中的带索引的字典文件的产生过程进行介绍，产生带索引的字典文件并不复杂，但是要想用简洁易懂的语言将该问题描述明白却不容易。 asleap破解vpn的机制是通过字典文件暴力破解的方式，该字典文件有dat数据文件和idx索引文件两个文件组成，两个文件均为二进制格式。asleap工程中自带了genkey程序，可以将文本的字典文件转换为asleap程序需要的带索引的字典文件。 本文以字典文件为以下内容讲解： 123turquoisedatest 读取字典文件并产生md4值md4编码占16个字节，三个字典进行md4编码后的结果分别为： 12318 07 33 43 f6 30 b5 f8 2c 38 c0 34 37 f2 81 6b01 19 a3 80 94 40 60 3c 57 39 5e 73 f3 60 95 980c b6 94 88 05 f7 97 bf 2a 82 80 79 73 b8 95 37 将字典信息写入到临时文件为了能够对最终生成的dat文件中的内容进行排序和便于索引，程序生成了256个临时文件，文件名格式为从genk-bucket-00.tmp到genk-bucket-ff.tmp。程序根据md4编码中的第14位将字典对应的信息分别写入到临时文件中，一个字典写入到临时文件的内容如下，如果一个临时文件中存在多个字典则依次存放： 12345struct hashpass_rec &#123; unsigned char rec_size; // 一个字典占用文件的大小，包括该变量+字典+字典对应的md4值共占用的字节数 char *password; // 字典 unsigned char hash[16]; // 字典对应的md4值&#125; __attribute__ ((packed)); 本例子中turquoise对应结构体会写入到genk-bucket-81.tmp中，da和test对应结构体会依次写入到genk-bucket-95.tmp中。 读取临时文件并写入到dat数据文件中最终dat文件中的数据内容为hashpass_rec的有序集合，排序的原则是按照md4的第14和15两个字节。依次读取256个临时文件中的hashpass_rec可以保证dat文件中的数据内容是按照第14字节排序的，但是不能够保证是按照第15个字节排序的。为了保证最终dat文件中的数据内容是按照第14和15字节有序的，在将一个临时文件中的内容写入到dat文件中前需要对该临时文件中的hashpass_rec结果按照hash变量的第15字节进行排序，直接使用C语言中的qsort进行排序。 该例子中da和test位于同一个临时文件中，需要根据hash变量的第15字节排序的结果为test、da，最终写入到dat文件中的排序结果为turquoise、test、da。 根据dat数据文件产生idx索引文件idx索引文件中存放的是多个hashpassidx_rec结果，最多有256*256项，其结构定义如下： 12345struct hashpassidx_rec &#123; unsigned char hashkey[2]; // 对应md4编码的第14和15字节 off_t offset; // 第一个匹配的hashpass_rec结构在dat文件中的偏移，占用4个字节 unsigned long long int numrec; // dat文件中共有多少个匹配的hashpass_rec结果&#125; __attribute__ ((packed)); // 字节对齐，需要填充4个字节 最终完成的dat文件和idx文件的指向如下图所示： buggenkeys.c文件中在读取字典文件时存在bug，在文件的207行将内容更改为： 123456while (!feof(inputfl)) &#123; memset(password, 0, MAX_NT_PASSWORD + 1); fgets(password, MAX_NT_PASSWORD+1, inputfl); if (strlen(password) == 0) &#123; continue; &#125; 相关下载字典文件等相关文件下载]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[常用排序算法整理及代码实现]]></title>
      <url>%2Fpost%2Falgorithm_sort_code%2F</url>
      <content type="text"><![CDATA[本文对我编写的常用的排序算法进行整理和总结，方便用时进行查阅和参考。 快速排序快速排序是实际应用中的最好选择，采用了分治法的思想。通过一趟排序将待排序记录分割成独立的两部分，其中一部分的关键字均比另外一部分的小，分别对这两部分记录进行排序，已达到整个有序。 是否稳定：不稳定 时间复杂度：O(nlogn) 空间复杂度：O(logn)，需要栈来实现递归用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;vector&gt;#include &lt;iostream&gt;int partition(std::vector&lt;int&gt; &amp;numbers, int low, int high)&#123; int pivotkey = numbers[low]; while (low &lt; high) &#123; while (low &lt; high &amp;&amp; numbers[high] &gt;= pivotkey) &#123; --high; &#125; numbers[low] = numbers[high]; while (low &lt; high &amp;&amp; numbers[low] &lt;= pivotkey) &#123; ++low; &#125; numbers[high] = numbers[low]; numbers[low] = pivotkey; &#125; return low;&#125;void quick_sort(std::vector&lt;int&gt; &amp;numbers, int low, int high)&#123; if (low &lt; high) &#123; int pivotloc = partition(numbers, low, high); quick_sort(numbers, low, pivotloc - 1); quick_sort(numbers, pivotloc + 1, high); &#125;&#125;void quick_sort(std::vector&lt;int&gt; &amp;numbers)&#123; quick_sort(numbers, 0, numbers.size() - 1);&#125;int main()&#123; std::vector&lt;int&gt; numbers = &#123;49, 38, 65, 97, 76, 13, 27, 49&#125;; quick_sort(numbers); for (int i=0; i&lt;numbers.size(); i++) &#123; printf("%d\t", numbers[i]); &#125; printf("\n");&#125; 归并排序将两个或两个以上的有序表组合成一个新的有序表。合并两个有序表的方法为：比较两个有序表中第一个数，谁小先取谁。继续进行比较，只要有一个有序表为空，直接将另一个有序表取出即可。 是否稳定：稳定 时间复杂度：O(nlogn) 空间复杂度：O(n) （当使用顺序存储时，为了能够实现两个有序表之间的合并），或O(1)（当使用链式存储的时候，不再需要临时的空间来存储排序的结果） 顺序存储代码以下为采用顺序存储结构的代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/** * 归并排序使用递归算法的效率比较低，具体应用中会采用非递归算法代替 */void merge(std::vector&lt;int&gt; &amp;numbers, std::vector&lt;int&gt; &amp;extra, int low, int high, int middle)&#123; int i = low, j = middle+1, k = low; for (; i&lt;=middle &amp;&amp; j&lt;=high; k++) &#123; if (numbers[i] &lt;= numbers[j]) &#123; extra[k] = numbers[i]; i++; &#125; else &#123; extra[k] = numbers[j]; j++; &#125; &#125; while (i &lt;= middle) &#123; extra[k++] = numbers[i++]; &#125; while (j &lt;= middle) &#123; extra[k++] = numbers[j++]; &#125; for (int m = low; m &lt;= high; m++) &#123; numbers[m] = extra[m]; &#125;&#125;void merge_sort(std::vector&lt;int&gt; &amp;numbers, std::vector&lt;int&gt; &amp;extra, int low, int high)&#123; if (low == high) &#123; return ; &#125; int middle = (low + high) / 2; merge_sort(numbers, extra, low, middle); merge_sort(numbers, extra, middle + 1, high); merge(numbers, extra, low, high, middle);&#125;void merge_sort(std::vector&lt;int&gt; &amp;numbers)&#123; // 申请额外的存储空间来用于排序处理 std::vector&lt;int&gt; extra = numbers; merge_sort(numbers, extra, 0, numbers.size() - 1);&#125;int main()&#123; std::vector&lt;int&gt; numbers = &#123;49, 38, 65, 97, 76, 13, 27, 49&#125;; merge_sort(numbers); for (int i=0; i&lt;numbers.size(); i++) &#123; printf("%d\t", numbers[i]); &#125; printf("\n");&#125; 链式存储代码以下为采用链式存储结构的代码，本答案为我在LeetCode上的Sort List 题目的答案，源码放在我的Github上。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798struct ListNode &#123; int val; ListNode *next; ListNode(int x) : val(x), next(NULL) &#123;&#125;&#125;;/** * 使用归并排序方法，核心思想为将数组拆分为两半，分别对两半进行排序，排序完成后再进行一次排序，排序算法就可以采用插入排序的方式。 * 对两半排序的算法仍然采用归并排序算法，即问题为递归问题 * 在使用线性存储结果的归并排序算法中，会使用额外的空间来存储临时结果，空间复杂度为O(n)，而在链式存储中，空间复杂度为O(1) * 归并排序的时间复杂度为O(nlogn) * 如果存储结构为双向链表，可以使用快速排序 */ListNode* sortList(ListNode* head)&#123; if (head == nullptr || head-&gt;next == nullptr) &#123; return head; &#125; if (head-&gt;next-&gt;next == nullptr) &#123; // 仅有两个元素，对两个元素进行排序后直接返回 if (head-&gt;val &lt; head-&gt;next-&gt;val) &#123; return head; &#125; else &#123; ListNode *tmp = head-&gt;next; tmp-&gt;next = head; tmp-&gt;next-&gt;next = nullptr; return tmp; &#125; &#125; // 为了找到中间节点，这里采用快慢指针的方式，否则需要使用先遍历一次取长度，然后找到中间位置的两次遍历方式 ListNode *fast = head; ListNode *slow = head; ListNode *slow_prev = nullptr; while (fast != nullptr &amp;&amp; fast-&gt;next != nullptr) &#123; fast = fast-&gt;next-&gt;next; slow_prev = slow; slow = slow-&gt;next; &#125; fast = slow_prev-&gt;next; slow_prev-&gt;next = nullptr; // 分别对两段链表进行排序 slow = sortList(head); fast = sortList(fast); // 对两段链表进行合并 ListNode *node = nullptr, *result = nullptr; while (slow != nullptr &amp;&amp; fast != nullptr) &#123; if (slow-&gt;val &lt; fast-&gt;val) &#123; if (result != nullptr) &#123; node-&gt;next = slow; node = node-&gt;next; &#125; else &#123; node = slow; result = slow; &#125; slow = slow-&gt;next; &#125; else &#123; if (result != nullptr) &#123; node-&gt;next = fast; node = node-&gt;next; &#125; else &#123; node = fast; result = fast; &#125; fast = fast-&gt;next; &#125; &#125; if (slow != nullptr) &#123; node-&gt;next = slow; &#125; if (fast != nullptr) &#123; node-&gt;next = fast; &#125; return result;&#125; 直接插入排序该排序算法的时间复杂度为O(n^2)，算法复杂度过高。分为顺序存储和链式存储两种算法，其中顺序存储每比较一个元素是从该元素往前比较的，而链式存储是从链头开始比较的，这点有所不同，造成不同的是由存储结构决定的。 顺序存储代码123456789101112131415161718192021222324252627282930313233343536373839#include &lt;stdio.h&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;using namespace std;void insertion_sort(std::vector&lt;int&gt; &amp;numbers)&#123; if (numbers.size() &lt;= 1) &#123; return; &#125; for (int i = 1; i &lt; numbers.size(); i++) &#123; for (int j = i; j &gt; 0; j--) &#123; if (numbers[j] &lt; numbers[j - 1]) &#123; swap(numbers[j], numbers[j - 1]); &#125; else &#123; break; &#125; &#125; &#125;&#125;int main()&#123; int a[] = &#123; 49, 38, 65, 97, 76, 13, 27, 49 &#125;; vector&lt;int&gt; numbers(a, a + sizeof(a) / sizeof(int)); insertion_sort(numbers); for (int i = 0; i&lt;numbers.size(); i++) &#123; printf(&quot;%d\t&quot;, numbers[i]); &#125; return 0;&#125; 链式存储代码以下代码为LeetCode上的链式存储的情况时的直接插入排序算法的代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950ListNode* insertionSortList(ListNode* head) &#123; if (head == NULL) &#123; return NULL; &#125; // 初始化 ListNode *node = head-&gt;next; ListNode *new_head = head; new_head-&gt;next = NULL; while (node != NULL) &#123; ListNode *node_next = node-&gt;next; // 先将当前遍历的下一个节点保存 // 将当前节点插入到新链表中 ListNode *new_node_tmp = new_head; if (node-&gt;val &lt; new_node_tmp-&gt;val) &#123; // 当前节点插入新链表的第一个位置 node-&gt;next = new_head; new_head = node; &#125; else &#123; // 将当前节点插入到中间 while (new_node_tmp-&gt;next != NULL) &#123; if (node-&gt;val &lt; new_node_tmp-&gt;next-&gt;val) &#123; node-&gt;next = new_node_tmp-&gt;next; new_node_tmp-&gt;next = node; break; &#125; new_node_tmp = new_node_tmp-&gt;next; &#125; // 将该节点插入到最后位置 if (new_node_tmp-&gt;next == NULL) &#123; new_node_tmp-&gt;next = node; node-&gt;next = NULL; &#125; &#125; // 开始遍历当前节点的下一个节点 node = node_next; &#125; return new_head;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2014年总结]]></title>
      <url>%2Fpost%2F2014_summary%2F</url>
      <content type="text"><![CDATA[我的年总结是依照农历的，因为在我心中春节才算是一年的真正开始，因为只有春节的时候才能找到年的滋味，年的感觉，才能称之为年，阳历的年只能称之为year。 2014年又在不经意间过去了，很多地方跟2013年一样是平淡，工作和学习仍然是生活的主旋律，闲暇时间抽个时间玩玩dota放松一下，周末偶尔爬个小山锻炼下身体，对我的人生中算是比较重要的一年。 学习2014年经历了结婚、毕业答辩和考驾照几件占用时间的事情，留给我业余时间用来学习的就少之又少了。结婚占用两个月时间，毕业论文占用了我两个月时间，考驾照占用了多个周末，工作出差占用了我一个月时间，留给我能够独立学习的晚上也就6个月时间。 开始的时候小看了硕士论文，以为很简单一事情，搞过这么多软件项目还搞不了一篇硕士论文。一直以来我看不起软件工程类论文，就一项目套个模板一介绍就是一篇论文，于是我选择了写一篇理论研究类论文，没有高大上的理论，而是在公司实践中真正用到的，《将Windows平台的C/C++程序向Linux平台移植的技术研究》，选择论文的时候我已经看到了该题目不太适合作为硕士论文，当我还是毅然作为了我的硕士论文题目，不得不说这是我今年的一大败笔。第一篇论文失败后，我重新走起了保守路线，以之前熟悉的系统为主线，辅以各种文档的拼凑，完成了一篇我曾经嗤之以项目类硕士论文，并顺利通过答辩。对于我这样的新手而言，写论文是一件漫长又痛苦的过程，占用了我大量的宝贵时间。 一直以来对嵌入式linux方向比较好奇，今年终于抵不住好奇，买了个2140开发板自己捣鼓了一段时间，由于时间关系虽然到现在也没有入门，多多少少对嵌入式已经有所了解。 感谢我的另一半，给我提供了足够的时间来干我想干的事情。 书籍比起2013年，今年读过的书籍少了一些。 《Linux/Unix系统编程手册》 《编程珠玑》 《LINUX设备驱动程序》（部分章节） 《LINUX内核设计与实现》（大部分章节） 《深度探索linux操作系统》 《剑指Office》 《大规模C++程序设计》 《程序员的自我修养》（第二遍） 《文明之光》 《程序员健康指南》 《黄金时代》 《一只特立独行的猪》 《算法导论》（部分章节） 工作已经比较熟练掌握了公司产品的大部分技术，工作起来算是得心应手，但是却少了许多挑战，是时候该接收大挑战的了。工作的职位从后台组长到研发部副经理到研发部经理，开始了工作的转型，这是我不期望这么早来到的，我自认为技术的成长空间还很大，不想过早的接触管理岗位。 五月份的出差成为了我心中抹不去的痛，莫名其妙接受了任务，匆忙出差，不过坑才刚刚开始。要维护的是一个我没见过界面的产品，不过基本原理我是清楚的。产品有很多bug，这我可以理解，要不然也不会让我出差了，但要命的是我没有产品的代码，我接受的任务仅是去应付客户、发现bug后反馈、更新产品、施工，这明白着是市场+测试+维护的话，跟我半毛钱关系都没有。以上这些都无所谓，要知道我可不是一个顽固不化的程序员，但工作地点竟然是机房，而且机房是我见过最脏最乱的机房，我就站在一排机柜的后面的一堆烂纸箱子上吹着空调的冷风和机器的热风办公，时而蹲着，时而站着，时而坐着，时而将衣领撩起保暖，时而浑身打颤，以至于到现在我留下了膝盖隐隐作痛的毛病。而且系统bug不断，一连在不吃晚饭的情况下加班到大半夜好多次，而我竟然坚持下来了。原本三五天的出差计划，一待就是二十多天。这短短的二十多天成为了我今年最难忘的痛，多少次期望回到家里温暖的被窝，多少次期望在家吃着我做的炖土豆。 今年面试了不少人，通过面试也发现大部分技术人员的水平太差了，我甚至都搞不明白他们是怎么厚着脸皮来面试技术岗位的。济南软件行业实在是不景气，甚至找一个靠谱点的web或者php程序员都成为了公司的一大难题。 生活长达五年的恋情在今年终于得到了升华，当然这升华是我不愿意这么快就看到的，多么希望能迟来几年，给原本一直以为自己还是个孩子的我一个接受的缓冲区，但站在两家人面前，我的想法竟然不能主导我。我对结婚这件事情持怎么简单怎么办的态度，结婚本就一仪式，豪华也罢，没有也罢，都是过眼云烟，为一天忙碌了一阵子仅为了那一天，而之后又有谁记得。结婚之所以在中国的古代非常重视，那是因为在农业社会中人民的娱乐方式非常单一，结婚可以成为人民心中的一个盼头和没有灯光的饭后侃的资本，现在娱乐方式早已多元化，相比之下结婚的光鲜早已显得微不足道。可结婚毕竟不是我一个人的事，甚至不是两个人的事，而是两家人的事。 结婚定在酷夏，定下来的时间比较匆忙，从定下来要结婚到结婚仅一个月时间，对我来说是莫大的好事，因为拖得时间越长占用的准备时间就越多。半年的婚前和半年的婚后生活，其实真的没人什么两样，都是美满的二人世界，希望这种状况能持续几年。 我一向对车比较排斥，始终认为汽车是一个比较失败的发明，用户体验特别差。好的设计应该让用户忽略其内部实现细节，好的发明不应该让用户花费大量的时间来学习怎样使用，甚至需要多个课时的专业培训。汽车不仅是一个毫无用户体验的发明，而且危险到极致，危险到一失误就会要掉人的性命。 但今年我随波逐流了，毕竟驾照是早晚要考的，晚考成本只会更高。于是考驾照提上了议程，8月初已经计划报名，只可惜流程过于复杂，到现在也才到了科目二的程度。先是报名需要办暂住证，暂住证一办就是15个工作日，直接拖到了十一之后。找个离家近的驾校报个名，一等就是一个月才考科目一。科目一考完一等又是一个月才开始分车学科目二。科目二刚开始学又开始继续了，一共练了两个工作日后驾校又开始集训了，又没我啥事了。好在我找了个陪练，练了几把就顺手了。 虽然驾照没有考出来，仅考到了科目二，算是完成了驾照的一半，但却耗去了我的部分经历。找驾校、准备科目一、学习科目二、找陪练，这些花费的都是我的时间。要是驾校培训行业能够再成熟些，再人性化些能给多少学车的人带来方便。 玩游戏多少有些过了，虽然每周也就不想学习的两个晚上用来玩游戏，但我深知自己不是玩游戏的料。很多时候为了能够赢一局，会熬夜到下半夜，这是非常不理智的。另外，以后尽量用其他方式来代替游戏放松，当然我深知其中的苦难。 旅行去过一次云南，都在这里了。 清明节时间去过一次天津，天津比我想象的要好很多，各个地方特色比较明显，有别墅区、意大利风情区、现代的商业区等，这之间能够非常明显的区分，不像济南太混杂。不过天津的人却给我留下的印象不是很好，这也是小小的遗憾。 展望在我心中已经为2015年制定好了一些计划，为家庭，为自己，2015年会是我人生的一个转折点，期望2015年能够顺利。我会努力的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leetcode题目之Maximum Subarray]]></title>
      <url>%2Fpost%2FMaximum_Subarray%2F</url>
      <content type="text"><![CDATA[题目Find the contiguous subarray within an array (containing at least one number) which has the largest sum. For example, given the array [−2,1,−3,4,−1,2,1,−5,4],the contiguous subarray [4,−1,2,1] has the largest sum = 6. 分析该题目为经典题目，存在多种解题思路。 动态规划求动态规划的关键在于找到状态方程。 12345678910111213141516171819202122232425262728/** * 问题的关键是找到状态方式，找到状态方程后问题就迎刃而解 * 状态方程如下： * b[j]表示第j处，以a[j]结尾的子序列的最大和 * b[j]=max(a[j] + b[j-1], a[j]) * b数据的最大值即为问题的解 * 问题转换为求解b数组 * 时间复杂度为O(1)，空间复杂度为(n)，空间复杂度可以降为O(1)，为了使程序易读，不做调整 */int maxSubArray(int A[], int n) &#123; if (n == 0) &#123; return 0; &#125; int *b = new int[n]; b[0] = A[0]; int max_b = b[0]; for (int i=1; i&lt;n; i++) &#123; b[i] = std::max(A[i] + b[i-1], A[i]); if (max_b &lt; b[i]) &#123; max_b = b[i]; &#125; &#125; delete[] b; return max_b;&#125; 分治法《算法导论》的分治策略一章有关于该问题的详细解释。该题利用分治法来解决要比二分查找类最简单的分治算法要复杂。将数组一分为二后，最大数组存在三种情况：在左半或右半部分、跨越中点分别占据左部分一点和右部分一点。对于跨越中点的情况，转化为求从中点开始向左的最大值和从中点开始向右的最大值之和。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116class Solution &#123;public: int compare_array[4]; int maxSubArray(int A[], int n) &#123; return maxSubArray(A, 0, n - 1); &#125; /** * leetcode not support stdarg.h */ int max(int count, ...) &#123; va_list ap; va_start(ap, count); int max = INT_MIN; for (int i=0; i&lt;count; i++) &#123; int temp = va_arg(ap, int); if (max &lt; temp) &#123; max = temp; &#125; &#125; va_end(ap); return max; &#125; int max_compare_array() &#123; int max_num = compare_array[0]; for (int i=1; i&lt;4; i++) &#123; if (max_num &lt; compare_array[i]) &#123; max_num = compare_array[i]; &#125; &#125; return max_num; &#125; int maxSubArray(int A[], int begin, int end) &#123; //printf("begin : %d, end : %d\n", begin, end); if (begin == end) &#123; return A[begin]; &#125; else if ((end - begin) == 1) &#123; //return max(3, A[begin], A[begin] + A[end], A[end]); compare_array[0] = A[begin]; compare_array[1] = A[begin] + A[end]; compare_array[2] = A[end]; compare_array[3] = INT_MIN; return max_compare_array(); &#125; int middle = (begin + end) / 2; // 处理左边子数组 int max_left = maxSubArray(A, begin, middle); // 处理右边子数组 int max_right = maxSubArray(A, middle + 1, end); // 处理跨越中点的情况 int max_cross = maxCrossMiddle(A, begin, end); printf("begin : %d, end : %d, max_left = %d, max_right = %d, max_cross = %d\n", begin, end, max_left, max_right, max_cross); // 返回三者中的最大值 compare_array[0] = max_left; compare_array[1] = max_right; compare_array[2] = max_cross; compare_array[3] = INT_MIN; return max_compare_array(); &#125; /** * 处理跨越中点的情况 */ int maxCrossMiddle(int A[], int begin, int end) &#123; if (begin == end) &#123; return A[begin]; &#125; int middle = (begin + end) / 2; // 求得[begin -- middle-1]的最大值 int max_left = A[middle - 1]; int sum = 0; for (int i=middle - 1; i&gt;=begin &amp;&amp; i &gt;= 0; i--) &#123; sum += A[i]; if (max_left &lt; sum) &#123; max_left = sum; &#125; &#125; // 求得[middle+1 -- end]的最大值 int max_right = A[middle + 1]; sum = 0; for (int i=middle + 1; i&lt;=end; i++) &#123; sum += A[i]; if (max_right&lt; sum) &#123; max_right = sum; &#125; &#125; compare_array[0] = A[middle]; compare_array[1] = A[middle] + max_left; compare_array[2] = A[middle] + max_right; compare_array[3] = A[middle] + max_left + max_right; return max_compare_array(); &#125;&#125;; 扫描算法《编程珠玑》一书8.4节提到该算法，时间复杂度为O(1)，是解决该问题最好的算法。 12345678910111213141516171819202122232425int maxSubArray(int A[], int n) &#123; if (n == 0) &#123; return 0; &#125; int current_sum = 0; int max_sum = INT_MIN; for (int i=0; i&lt;n; i++) &#123; if (current_sum &lt;= 0) &#123; current_sum = A[i]; &#125; else &#123; current_sum += A[i]; &#125; if (current_sum &gt; max_sum) &#123; max_sum = current_sum; &#125; &#125; return max_sum;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux函数高级特性]]></title>
      <url>%2Fpost%2Flinux_funtion_advance_feature%2F</url>
      <content type="text"><![CDATA[最近在看《Linux/Unix系统编程手册》一书，这里对书中提到的函数类型进行总结。 可重入POSIX标准中的解释如下： Reentrant Function:A function whose effect, when called by two or more threads,is guaranteed to be as if the threads each executed thefunction one after another in an undefined order, even ifthe actual execution is interleaved. 可重入函数跟信号相关，一种更容易理解的解释为： 程序执行到某个函数foo()时，收到信号，于是暂停目前正在执行的函数，转到信号处理函数，而这个信号处理函数的执行过程中，又恰恰也会进入到刚刚执行的函数foo()，便发生了所谓的重入。此时如果foo()能够正确的运行，而且处理完成后，之前暂停的foo()也能够正确运行，则说明它是可重入的。 可重入函数需要满足如下几个条件： 不在函数内部使用静态或全局数据 不返回静态或全局数据，所有数据均有函数调用者提供 使用本地数据或通过复制全局数据来保护全局数据 不调用不可重入函数 标准的异步安全信号函数异步信号安全的函数指当从信号处理函数调用时，可保证实现是安全的。如果某一个函数是可重入的，或者信号处理函数无法将其中断时，称该函数是异步信号安全的。 我的理解是可重入函数和标准的异步安全信号函数基本等同，只是描述层面不同。 线程安全若函数可同时供多个线程安全的调用，则该函数为线程安全的函数。比较容易理解。 线程安全与可重入之间的关系可重入函数一定为线程安全的函数。线程安全函数不一定是可重入函数。 不可重入函数，函数调用结果不具有可再现性，可通过互斥锁等机制供多个线程安全的调用，这样该不可重入函数即为线程安全的函数。 malloc函数内部维护了全局数据结构，因此为不可重入的，但是内部通过递归互斥量来确保为线程安全的函数。并且该互斥量必须是可递归的，否则当malloc函数重入的情况下，会造成死锁。在glibc中，malloc有线程安全和非线程安全两个版本，两个区别在于内部是否使用递归锁，当编译程序时使用了_pthreads选项时使用线程安全版本，否则使用非线程安全版本。 自动重启Linux中的某些系统调用在阻塞的过程中，如果接受到信号并转去处理信号处理函数，当从信号处理函数返回时这些阻塞的系统调用默认会返回EINTR。为了避免信号处理函数对阻塞中的系统调用的打断，可以通过设置SA_RESTART标志的sigaction()来建立信号处理函数，从而令内核代表进程自动重启系统调用，而无需处理系统调用返回的EINTR错误。 并非所有的系统调用都支持自动重启，具体可参考《Linux/Unix系统编程手册（上册）》的21.5节。 参考资料《Linux/Unix系统编程手册（上册）》 对可重性和线程安全的小结]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux信号机制学习]]></title>
      <url>%2Fpost%2Flinux_signal%2F</url>
      <content type="text"><![CDATA[[TOC] 信号机制在Linux编程中一直是一个难点，因为信号往往跟进程、线程、定时器、I/O等多个层面都有牵涉，这些情况存在错综复杂的关系，堪比娱乐圈错综复杂的男女关系，要想全面理解信号机制确实不易。 信号种类在Linux中可以通过如下命令来查看所有的信号： 1234567891011121314[kuring@localhost ~]$ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR111) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU 25) SIGXFSZ26) SIGVTALRM 27) SIGPROF 28) SIGWINCH 29) SIGIO 30) SIGPWR31) SIGSYS 34) SIGRTMIN 35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+338) SIGRTMIN+4 39) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+12 47) SIGRTMIN+1348) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-14 51) SIGRTMAX-13 52) SIGRTMAX-1253) SIGRTMAX-11 54) SIGRTMAX-10 55) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-758) SIGRTMAX-6 59) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 共64个信号，分为两种信号：非实时信号和实时信号。其中1-31个信号为非实时信号，32-64为实时信号。 当一信号在阻塞状态下产生多次信号，当解除该信号的阻塞后，非实时信号仅传递一次信号，而实时信号会传递多次。 对于非实时信号：内核会为每个信号维护一个信号掩码，并阻塞信号针对该进程的传递。如果将阻塞的信号发送给某进程，对该信号的传递将延时，直至从进程掩码中移除该信号为止。当从进程掩码中移除该信号时该信号将传递给该进程。如果信号在阻塞期间传递过多次该信号，信号解除阻塞后仅传递一次。 对于实时信号：实时信号采用队列化处理，一个实时信号的多个实例发送给进程，信号将会传递多次。可以制定伴随数据，用于产生信号时的数据传递。不同实时信号的传递顺序是固定的，优先传递信号编号小的。 信号阻塞内核会为每个信号维护一个信号掩码，来阻塞内核将信号传递给该进程。如果将阻塞的信号发送给该进程，信号的传递将延后，从进程信号掩码中移除该信号后内核立刻将信号传递给该进程。如果一个信号在阻塞状态下产生多次，对于非实时信号稍后仅会传递一次，对于实时信号内核会进行排队处理，会传递多次。 信号处理函数要想在进程中设置信号处理函数有两种选择：signal()和sigaction()。其中signal()函数提供的接口比较简单，但是在不同的UNIX系统之间存在差异，跨平台特性不是很好,signal()函数由于是C库函数，实现往往是采用sigaction()系统调用完成。sigaction()具有很好的跨平台性，但是使用较为复杂，但是却可以在信号处理程序中完成阻塞信号的作用。 在sigaction函数中可以指定调用信号处理函数时要阻塞的信号集，不允许这些信号中断信号处理函数的调用，直到信号处理函数调用完毕后信号才会传递。这一点通过signal函数是完不成的，利用signal函数设定的信号处理函数只能在信号处理函数开始时使用sigprocmask设置要阻塞的信号，在信号处理函数尾部利用sigprocmask还原信号，但在调用第一次调用sigprocmask函数之前和第二次调用sigprocmask函数之后的空白期内却无法防止要阻塞信号的传递。 信号处理函数中调用的函数尽量是异步信号安全的，C库中的函数不是异步信号安全的函数。 在信号处理函数中尽量避免访问全局变量，要访问全局变量可以使用volatile sig_atomic_t flag，volatile防止将编译器将变量优化到内存中，sig_atomic_t是一种整形数据类型，用来保证读写操作的原子性。 系统调用的中断当系统调用阻塞时，之前创建了处理函数的信号传递过来。在信号处理函数返回后，默认情况下，系统调用会失败，并将errno置为EINTR。 如果调用指定了SA_RESTART标志的sigaction()函数来创建信号处理器函数，内核会在信号处理函数返回后自动重启系统调用，从而避免了信号处理函数对阻塞的系统调用产生的影响。比较不幸的是，并非所有的系统调用都支持该特性。 信号的同步生成和异步生成这里的同步是对信号产生方式的描述，跟具体哪个信号无关。所有的信号均可同步生成，也可异步生成。 异步生成：引发信号产生的事件与进程的执行无关。例如，用户输入了中断字符、子进程终止等事件，这些信号的产生该进程是无法左右的。 同步生成：当执行特定的机制指令产生硬件异常时或进程使用raise()、kill()等向自身发生信号时，信号是同步传递的。这些信号的产生时间该进程是可以左右的。 信号传递的时机和顺序同步产生的信号会立即传递给该进程。例如，当使用raise()函数向自身发送信号时，信号会在raise()调用前发生。 异步产生一个信号时，且在进程并未阻塞的情况下，信号也不会立即被传递。当且仅当进程正在执行，并且由内核态到用户态的下一次切换时才会传递信号。说人话就是在以下两种情况下会传递信号：进程获得调度时和系统调用完成时。这是因为内核会在进程在内核态和用户态进行的切换的时候才会检测信号。 非实时信号的传递顺序无法保障，实时信号的传递顺序是固定的，当多个不同的实时信号处于等待状态时，优先传递最小编号的信号。 信号和线程信号模型是基于进程模型而设计的，应尽量避免在多线程中使用信号模型。 信号的发送可以针对整个进程，也可以针对特定线程。 当进程收到一个信号后，内核会任选一个线程来接收信号，并调用信号处理函数对信号进行处理。 每个线程可以独立设置信号掩码。 如果信号处理程序中断了对pthread_mutex_lock()和pthread_cond_wait()的调用，该调用会自动重启。 参考文章《Linux/Unix系统编程手册》]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[UNIX网络编程读书笔记]]></title>
      <url>%2Fpost%2Flinux_unp%2F</url>
      <content type="text"><![CDATA[第一遍阅读unpv3后，对书中讲述的内容有了大体的认识，但对书中的具体细节地方却是早已忘记。重新阅读unpv3，这次不希望仍然是阅后即忘，于是通过编写代码的方式对书中的例子和注意事项加深理解。 为了能够将书中的很多细节问题理解清楚并且便于记忆，本文采用了编写书中代码并运行的方式，并将书中容易出错和意想不到的问题记在代码中。 本文的代码实例并未完全按照书中的代码实例，本着单个文件即能编译通过并运行的原则，本文对于很多系统调用并未做防御式编程处理。针对每个版本的程序中缺点和注意事项在代码中已经进行了标注。 鉴于高性能的epoll机制出现比较晚，晚于unp的编写时间，书中并未做介绍。 TCP客户端程序客户端函数执行效率情况：select非阻塞式I/O版本&gt;线程化版本&gt;fork版本&gt;select阻塞式I/O版本&gt;停等版本，停等版本的执行效率非常低，在实际生产环境中不建议使用。 其中poll和select机制基本类似，书中并未给出poll版本。 停-等版本最常规的实现思路，但效率非常低，且当程序阻塞在读取要发送内容时，程序是无法收到服务端的状态变化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273/** * 停-等版本 * 该版本缺陷为当服务端发生某些事件时，客户端可能仍然阻塞于fgets调用中 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;signal.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */void str_cli(FILE *fp, int sockfd)&#123; char sendline[MAXLINE], recvline[MAXLINE]; while (fgets(sendline, MAXLINE, fp) != NULL) &#123; /* 当阻塞在fgets函数时将服务器进程关闭时虽然给客户端发送了FIN信号，客户端并不会知道， * 服务端关闭时第一次调用write服务器会返回RST， * 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 * 该问题需要使用I/O复用技术来解决，或者使用fork处理的方式来解决 * */ write(sockfd, sendline, strlen(sendline)); int n = read(sockfd, recvline, MAXLINE); if (n == 0) &#123; printf("str_cli: server terminated prematurely\n"); exit(1); &#125; // 向标准输出写内容，既可以使用write也可以使用fputs write(STDOUT_FILENO, recvline, n); // 使用fputs时需要注意将recvline数组有效内容的后面一位设置为'\0'// recvline[n] = '\0';// fputs(recvline, stdout); &#125;&#125;int main(int argc, char **argv)&#123; int sockfd; struct sockaddr_in servaddr; if (argc != 2) &#123; printf("usage: tcpcli &lt;IPaddress&gt;\n"); exit(1); &#125; // 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 // 最好的方式是忽略此信号的处理方式，并在程序下面处理该异常情况 signal(SIGPIPE, SIG_IGN); sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr); connect(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)); str_cli(stdin, sockfd); /* do it all */ exit(0);&#125; fork版本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * 阻塞式I/O的fork版本 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;signal.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length *//** * 即使服务端已经退出，子进程的read方法仍然能够感知到并且退出while循环，并给父进程发送SIGTERM,父进程对该信号的默认处理方式为退出 * 优点：代码量比较少，每个进程只处理2个I/O流，从一个复制到另一个 */void str_cli(FILE *fp, int sockfd)&#123; char sendline[MAXLINE], recvline[MAXLINE]; pid_t pid; if ((pid = fork()) == 0) &#123; // child process : server -&gt; stdout int n; while ((n = read(sockfd, recvline, MAXLINE)) &gt; 0) &#123; recvline[n] = '\0'; fputs(recvline, stdout); &#125; kill(getppid(), SIGTERM); exit(0); &#125; // parent process : stdin -&gt; server while (fgets(sendline, MAXLINE, fp) != NULL) &#123; write(sockfd, sendline, strlen(sendline)); &#125; shutdown(sockfd, SHUT_WR); pause(); return ;&#125;int main(int argc, char **argv)&#123; int sockfd; struct sockaddr_in servaddr; if (argc != 2) &#123; printf("usage: tcpcli &lt;IPaddress&gt;\n"); exit(1); &#125; // 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 // 最好的方式是忽略此信号的处理方式，并在程序下面处理该异常情况 signal(SIGPIPE, SIG_IGN); sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr); if (connect(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) != 0) &#123; printf("connect error...\n"); exit(1); &#125; str_cli(stdin, sockfd); /* do it all */ exit(0);&#125; 阻塞式I/O的select版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113/** * 阻塞式I/O的select版本 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;signal.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length *//** * 缺点：使用了阻塞式I/O，如果在向套接字调用write发送给服务器时，套接字缓冲区已满，write调用会阻塞，从而影响了后续的套接字缓冲区的读取 */void str_cli(FILE *fp, int sockfd)&#123; int maxfdp1; fd_set rset; char sendline[MAXLINE], recvline[MAXLINE]; int stdineof = 0; FD_ZERO(&amp;rset); for (; ;) &#123; // select FD_SET(fileno(fp), &amp;rset); FD_SET(sockfd, &amp;rset); maxfdp1 = (fileno(fp) &gt; sockfd ? fileno(fp) : sockfd) + 1; select(maxfdp1, &amp;rset, NULL, NULL, NULL); // socket if (FD_ISSET(sockfd, &amp;rset)) &#123; int n = read(sockfd, recvline, MAXLINE); if (n == 0) &#123; if (stdineof == 1) &#123; return ; &#125; else &#123; printf("str_cli: server terminated prematurely\n"); exit(1); &#125; &#125; else if (n == -1) &#123; exit(1); &#125; recvline[n] = '\0'; fputs(recvline, stdout);// write(STDOUT_FILENO, recvline, n); &#125; // input if (FD_ISSET(fileno(fp), &amp;rset)) &#123; // 此处不能使用fgets函数，该函数带有缓冲区功能，select跟带有缓冲区的c函数混合使用有问题// if (fgets(sendline, MAXLINE, fp) == NULL)// &#123;// return ;// &#125; int n = read(fileno(fp), sendline, MAXLINE); if (n == 0) &#123; stdineof = 1; shutdown(sockfd, SHUT_WR); // 关闭写 FD_CLR(fileno(fp), &amp;rset); continue; &#125; else if (n == -1) &#123; exit(1); &#125; write(sockfd, sendline, n); &#125; &#125;&#125;int main(int argc, char **argv)&#123; int sockfd; struct sockaddr_in servaddr; if (argc != 2) &#123; printf("usage: tcpcli &lt;IPaddress&gt;\n"); exit(1); &#125; sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr); if (connect(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) != 0) &#123; printf("connect error...\n"); exit(1); &#125; str_cli(stdin, sockfd); /* do it all */ exit(0);&#125; 非阻塞式I/O的select版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298/** * 非阻塞式I/O的select版本 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdlib.h&gt;#include &lt;signal.h&gt;#include &lt;fcntl.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */#define max(a,b) ( ((a)&gt;(b)) ? (a):(b) )/** * 优点：速度是最快的，可以防止进程在做任何工作时发生阻塞 * 缺点：同时管理4个不同的I/O流，每个流都是非阻塞的，需要考虑到4个流的部分读和部分写问题。编码量是最多的，需要引入缓冲区管理机制。 */void str_cli(FILE *fp, int sockfd)&#123; // 将socket、标准输入和标准输出描述符设置为非阻塞方式 int val = fcntl(sockfd, F_GETFL, 0); fcntl(sockfd, F_SETFL, val | O_NONBLOCK); val = fcntl(STDIN_FILENO, F_GETFL, 0); fcntl(STDIN_FILENO, F_SETFL, val | O_NONBLOCK); val = fcntl(STDOUT_FILENO, F_GETFL, 0); fcntl(STDOUT_FILENO, F_SETFL, val | O_NONBLOCK); char to[MAXLINE], fr[MAXLINE]; char *toiptr, *tooptr, *friptr, *froptr; toiptr = tooptr = to; friptr = froptr = fr; int stdineof = 0; int maxfdp1 = max(max(STDIN_FILENO, STDOUT_FILENO), sockfd) + 1; fd_set rset, wset; for (; ;) &#123; FD_ZERO(&amp;rset); FD_ZERO(&amp;wset); if (stdineof == 0 &amp;&amp; toiptr &lt; &amp;to[MAXLINE]) &#123; FD_SET(STDIN_FILENO, &amp;rset); &#125; if (friptr &lt; &amp;fr[MAXLINE]) &#123; FD_SET(sockfd, &amp;rset); &#125; if (tooptr != toiptr) &#123; FD_SET(sockfd, &amp;wset); &#125; if (froptr != friptr) &#123; FD_SET(STDOUT_FILENO, &amp;wset); &#125; select(maxfdp1, &amp;rset, &amp;wset, NULL, NULL); // select函数仍然是阻塞的 // 标准输入 if (FD_ISSET(STDIN_FILENO, &amp;rset)) &#123; int n; if ((n = read(STDIN_FILENO, toiptr, &amp;to[MAXLINE] - toiptr)) &lt; 0) &#123; // 对于非阻塞式IO，如果操作不能满足，相应系统调用会返回EWOULDBLOCK错误 if (errno != EWOULDBLOCK) &#123; printf("read error on stdin\n"); exit(1); &#125; &#125; else if (n == 0) &#123; fprintf(stderr, "EOF on stdin\n"); stdineof = 1; if (tooptr == toiptr) &#123; shutdown(sockfd, SHUT_WR); // 缓冲区中没有数据要发送，关闭socket &#125; &#125; else &#123; fprintf(stderr, "read %d bytes from stdin\n", n); toiptr += n; FD_SET(sockfd, &amp;wset); &#125; &#125; // 从套接字读 if (FD_ISSET(sockfd, &amp;rset)) &#123; int n; if ((n = read(sockfd, friptr, &amp;fr[MAXLINE] - friptr)) &lt; 0) &#123; if (errno != EWOULDBLOCK) &#123; printf("read error on socket\n"); exit(1); &#125; &#125; else if (n == 0) &#123; fprintf(stderr, "EOF on socket\n"); if (stdineof) &#123; return ; &#125; else &#123; printf("server terminated prematurely\n"); exit(1); &#125; &#125; else &#123; fprintf(stderr, "read %d bytes from socket\n", n); friptr += n; FD_SET(STDOUT_FILENO, &amp;wset); &#125; &#125; // 标准输出 int n; if (FD_ISSET(STDOUT_FILENO, &amp;wset) &amp;&amp; ((n = friptr - froptr) &gt; 0)) &#123; int nwritten; if ((nwritten = write(STDOUT_FILENO, froptr, n)) &lt; 0) &#123; if (errno != EWOULDBLOCK) &#123; printf("write error to stdout\n"); exit(1); &#125; &#125; else &#123; fprintf(stderr, "wrote %d bytes to stdout\n", nwritten); froptr += nwritten; if (froptr == friptr) &#123; froptr = friptr = fr; &#125; &#125; &#125; // 向socket写 if (FD_ISSET(sockfd, &amp;wset) &amp;&amp; ((n = toiptr - tooptr)) &gt; 0) &#123; int nwritten; if ((nwritten = write(sockfd, tooptr, n)) &lt; 0) &#123; if (errno != EWOULDBLOCK) &#123; printf("write error to socket\n"); exit(1); &#125; &#125; else &#123; fprintf(stderr, "wrote %d bytes to socket\n", nwritten); tooptr += nwritten; if (tooptr == toiptr) &#123; toiptr = tooptr = to; if (stdineof) &#123; shutdown(sockfd, SHUT_WR); &#125; &#125; &#125; &#125; &#125; return ;&#125;/** * connect的非阻塞版本 * 连接建立成功时，描述符变为可写；连接建立错误时，描述符变为即可读又可写 * 优点： * 1、阻塞式的connect调用会消耗CPU时间，非阻塞式connect可以充分利用CPU时间，在等待的过程中可以处理其他工作 * 2、可以同时建立多个连接，浏览器中会用到此技术 * 3、阻塞式connect的函数超时过长，可以通过该函数设置超时时间 * 4、阻塞式的套接字调用connect时，在TCP的三次握手完成之前被某些信号中断时并且connect未设置内核自动重启的标志时，connect将返回EINTR错误 * 当再次调用connect等待未完成的连接时将会返回EADDRINUSE错误 */int connect_nonb(int sockfd, const struct sockaddr *saptr, socklen_t salen, int nsec)&#123; // 将套接字设置为非阻塞状态 int flags = fcntl(sockfd, F_GETFL, 0); fcntl(sockfd, F_SETFL, flags | O_NONBLOCK); int error = 0; int n; if ((n = connect(sockfd, saptr, salen)) &lt; 0) &#123; // 连接未成功建立，正常情况下返回EINPROGRESS错误，表示操作正在处理 if (errno != EINPROGRESS) &#123; // EINPROGRESS表示连接建立已经启动，但是尚未完成 return -1; &#125; &#125; else if (n == 0) &#123; // 当服务器和客户端在一台主机上时会立即建立连接 goto done; &#125; // 当代码执行到如下过程中时，connect正在建立连接，可以在此位置执行业务相关代码 // 当然真正使用时，在此位置加入其他代码并不合适，需要根据具体情况重新调整代码 // 可以参照书中的web客户程序例子 fd_set rset, wset; FD_ZERO(&amp;rset); FD_SET(sockfd, &amp;rset); wset = rset; struct timeval tval; tval.tv_sec = nsec; tval.tv_usec = 0; if ((n = select(sockfd + 1, &amp;rset, &amp;wset, NULL, nsec ? &amp;tval : NULL)) == 0) &#123; // 发生超时 close(sockfd); errno = ETIMEDOUT; return -1; &#125; // 当连接建立成功时sockfd变为可写，当连接建立失败时sockfd变为即可读又可写 if (FD_ISSET(sockfd, &amp;rset) || FD_ISSET(sockfd, &amp;wset)) &#123; int len = sizeof(error); // 非可移植性函数，连接建立成功返回0，连接建立失败将错误值返回给error // 连接建立失败时，有返回-1和返回0的情况 if (getsockopt(sockfd, SOL_SOCKET, SO_ERROR, &amp;error, &amp;len) &lt; 0) &#123; // solaris连接建立失败返回-1 return -1; &#125; &#125; else &#123; printf("select error:sockfd not set"); exit(1); &#125;done: // 恢复套接字的文件状态标志 fcntl(sockfd, F_SETFL, flags); if (error) &#123; close(sockfd); errno = error; return -1; &#125; return 0;&#125;int main(int argc, char **argv)&#123; int sockfd; struct sockaddr_in servaddr; if (argc != 2) &#123; printf("usage: tcpcli &lt;IPaddress&gt;\n"); exit(1); &#125; // 当一个进程向某个收到RST的套接字执行写操作时，内核会向该进程发送一个SIGPIPE信号 // 最好的方式是忽略此信号的处理方式，并在程序下面处理该异常情况 signal(SIGPIPE, SIG_IGN); sockfd = socket(AF_INET, SOCK_STREAM, 0); bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr); //connect(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)); if (connect_nonb(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr), 50) &lt; 0) &#123; printf("socket connect error\n"); exit(1); &#125; str_cli(stdin, sockfd); /* do it all */ exit(0);&#125; TCP服务端程序服务器程序要处理大量并发，在设计时更要注重效率。 fork版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123/** * fork版本 * PPC(Process per Connection)模型 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;strings.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */void sig_chld(int signo)&#123; if (signo != SIGIO) &#123; return; &#125; int stat; /* 此处不可以使用wait函数，当多个SIGCHLD信号同时发出时会因为信号覆盖而出现僵尸进程的情况 pid_t pid = wait(&amp;stat); printf("child %d terminated\n", pid); // 非异步信号安全函数，此处不应该调用 */ /* 使用非阻塞的参数WNOHANG来循环处理信号，避免信号丢失问题 */ pid_t pid; while ((pid = waitpid(-1, &amp;stat, WNOHANG)) &gt; 0) &#123; printf("child %d terminated\n", pid); &#125;&#125;void str_echo(int sockfd)&#123; ssize_t n; char buf[MAXLINE];again: while ( (n = read(sockfd, buf, MAXLINE)) &gt; 0) &#123; write(sockfd, buf, n); &#125; if (n &lt; 0 &amp;&amp; errno == EINTR) goto again; else if (n &lt; 0) printf("str_echo: read error\n");&#125;/** * fork版本 * 缺点： * 1.fork需要将父进程的内存映像复制到子进程，并在子进程中复制所有的描述符，尽管现在的操作系统已经都实现了写时复制技术，但是耗时仍然比较多 * 2.父进程和子进程之间需要IPC机制进行通信，从子进程返回信息到父进程比较麻烦 */int main(int argc, char *argv[])&#123; signal(SIGCHLD, sig_chld); int listenfd, connfd; pid_t childpid; struct sockaddr_in cliaddr, servaddr; // socket listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) &#123; printf("socket error\n"); exit(1); &#125; // bind bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) == -1) &#123; printf("bind error\n"); exit(1); &#125; // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) &#123; printf("listen error\n"); exit(1); &#125; for ( ; ; ) &#123; socklen_t clilen = sizeof(cliaddr); // 处理accept被信号中断时返回EINTR错误 if ((connfd = accept(listenfd, (struct sockaddr*)&amp;cliaddr, &amp;clilen)) &lt; 0) &#123; if (errno == EINTR) &#123; continue; &#125; else &#123; printf("accept error"); exit(1); &#125; &#125; if ((childpid = fork()) == 0) &#123; /* child process */ close(listenfd); /* close listening socket */ str_echo(connfd); /* process the request */ exit(0); &#125; close(connfd); /* parent closes connected socket */ &#125;&#125; select版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144/** * select版本 * 缺点： * 1. 有最大并发数限制，一个进程最多打开FD_SETSIZE个文件描述符，FD_SETSIZE往往是1024或2048字节 * 2. select每次调用都会线性扫描全部的FD集合，这样效率就会呈现线性下降，把FD_SETSIZE改大的后果就是所有FD处理都慢慢来 * 3. 内核/用户空间内存拷贝问题，内核把FD消息通知给用户空间采取了内存拷贝方法 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;sys/select.h&gt;#include &lt;strings.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length *//** * 使用select的需要维护client数组和allset的描述符集 */int main(int argc, char *argv[])&#123; struct sockaddr_in cliaddr, servaddr; // socket int listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) &#123; printf("socket error\n"); exit(1); &#125; printf("finish socket...\n"); // bind bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) == -1) &#123; printf("bind error\n"); exit(1); &#125; printf("finish bind...\n"); // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) &#123; printf("listen error\n"); exit(1); &#125; printf("finish listening...\n"); int maxfd = listenfd; int maxi = -1; int client[FD_SETSIZE]; for (int i=0; i&lt;FD_SETSIZE; i++) &#123; client[i] = -1; &#125; fd_set allset; FD_ZERO(&amp;allset); FD_SET(listenfd, &amp;allset); for ( ; ; ) &#123; fd_set rset = allset; int nready = select(maxfd + 1, &amp;rset, NULL, NULL, NULL); if (FD_ISSET(listenfd, &amp;rset)) &#123; // 设置client数组 socklen_t clilen = sizeof(cliaddr); // 调用select时有个问题，见书中16.6节 // 如果调用accept时客户端已经关闭连接，此时accept会阻塞并直到新的客户端连接到来 // 为了解决该问题可以将套接字设置为非阻塞再调用accept int connfd = accept(listenfd, (struct sockaddr*)&amp;cliaddr, &amp;clilen); printf("accept one client:%d...\n", connfd); int i = 0; for (; i&lt;FD_SETSIZE; i++) &#123; if (client[i] &lt; 0) &#123; client[i] = connfd; break; &#125; &#125; FD_SET(connfd, &amp;allset); if (i == FD_SETSIZE) &#123; printf("too many clients"); exit(-1); &#125; if (connfd &gt; maxfd) &#123; maxfd = connfd; &#125; if (i &gt; maxi) &#123; maxi = i; &#125; if (--nready &lt;= 0) &#123; continue; &#125; &#125; // 检测所有客户端的数据 for (int i=0; i&lt;=maxi; i++) &#123; if (client[i] &lt; 0) &#123; continue; &#125; if (FD_ISSET(client[i], &amp;rset)) &#123; int n; char buf[MAXLINE]; printf("start reading form one client...\n"); if ((n = read(client[i], buf, MAXLINE)) == 0) &#123; close(client[i]); FD_CLR(client[i], &amp;allset); client[i] = -1; &#125; else &#123; write(client[i], buf, n); &#125; if (--nready &lt;= 0) &#123; break; &#125; &#125; &#125; &#125;&#125; poll版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146/** * poll版本 * poll版本的解决了select文件描述符限制问题，但是仍然具备select的缺点中的2和3 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;strings.h&gt;#include &lt;poll.h&gt;#include &lt;stropts.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */#define OPEN_MAX 1024 // 该宏已经从limit.h中移除，用来表示一个进程可以打开的最大描述符数目/** * 使用select的缺点为需要维护client数组 */int main(int argc, char *argv[])&#123; struct sockaddr_in cliaddr, servaddr; // socket int listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) &#123; printf("socket error\n"); exit(1); &#125; printf("finish socket...\n"); // bind bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) == -1) &#123; printf("bind error\n"); exit(1); &#125; printf("finish bind...\n"); // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) &#123; printf("listen error\n"); exit(1); &#125; printf("finish listening...\n"); struct pollfd client[OPEN_MAX]; client[0].fd = listenfd; client[0].events = POLLIN; for (int i=1; i&lt;OPEN_MAX; i++) &#123; client[i].fd = -1; &#125; int maxi = 0; // 当前client正在使用的最大下标 for ( ; ; ) &#123; int nready = poll(client, maxi + 1, -1); if (client[0].revents &amp; POLLIN) &#123; // 设置client数组 socklen_t clilen = sizeof(cliaddr); int connfd = accept(listenfd, (struct sockaddr*)&amp;cliaddr, &amp;clilen); printf("accept one client:%d...\n", connfd); int i = 1; for (; i&lt;OPEN_MAX; i++) &#123; if (client[i].fd &lt; 0) &#123; client[i].fd = connfd; break; &#125; &#125; if (i == OPEN_MAX) &#123; printf("too many clients"); exit(-1); &#125; client[i].events = POLLIN; if (i &gt; maxi) &#123; maxi = i; &#125; if (--nready &lt;= 0) &#123; continue; &#125; &#125; // 检测所有客户端的数据 for (int i=0; i&lt;=maxi; i++) &#123; if (client[i].fd &lt; 0) &#123; continue; &#125; if (client[i].revents &amp; (POLLIN | POLLERR)) &#123; int n; char buf[MAXLINE]; printf("start reading form one client...\n"); if ((n = read(client[i].fd, buf, MAXLINE)) &lt; 0) &#123; if (errno == ECONNRESET) &#123; close(client[i].fd); client[i].fd = -1; &#125; else &#123; printf("read client error\n"); exit(-1); &#125; &#125; else if (n == 0) &#123; printf("client %d close\n", client[i].fd); close(client[i].fd); client[i].fd = -1; &#125; else &#123; write(client[i].fd, buf, n); &#125; if (--nready &lt;= 0) &#123; break; &#125; &#125; &#125; &#125;&#125; 多线程版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * 多线程版本 * TPC(Thread Per Connection)模型 * 线程的开销虽然比进程小，但是仍然有比较大开销，因此并发数不是很高 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;strings.h&gt;#include &lt;pthread.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */void str_echo(int sockfd)&#123; ssize_t n; char buf[MAXLINE];again: while ( (n = read(sockfd, buf, MAXLINE)) &gt; 0) &#123; write(sockfd, buf, n); &#125; if (n &lt; 0 &amp;&amp; errno == EINTR) goto again; else if (n &lt; 0) printf("str_echo: read error\n");&#125;static void *doit(void *arg)&#123; pthread_detach(pthread_self()); str_echo((int)arg); close((int)arg); printf("close socket...\n"); return NULL;&#125;int main(int argc, char *argv[])&#123; int listenfd, connfd; struct sockaddr_in cliaddr, servaddr; // socket listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd == -1) &#123; printf("socket error\n"); exit(1); &#125; // bind bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(listenfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) == -1) &#123; printf("bind error\n"); exit(1); &#125; // listen // 套接字排队的最大连接数为20 if (listen(listenfd, 20) == -1) &#123; printf("listen error\n"); exit(1); &#125; for ( ; ; ) &#123; socklen_t clilen = sizeof(cliaddr); // 处理accept被信号中断时返回EINTR错误 if ((connfd = accept(listenfd, (struct sockaddr*)&amp;cliaddr, &amp;clilen)) &lt; 0) &#123; if (errno == EINTR) &#123; continue; &#125; else &#123; printf("accept error"); exit(1); &#125; &#125; printf("receive new client...\n"); pthread_t tid; pthread_create(&amp;tid, NULL, &amp;doit, (void *)connfd); &#125;&#125; UDP由于udp比较简单，书中并未将udp协议当做重点来讲解。 UDP客户端程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;strings.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */// sendto、recvfrom方式void dg_cli(FILE *fp, int sockfd, const struct sockaddr *pservaddr, socklen_t servlen)&#123; char sendline[MAXLINE], recvline[MAXLINE + 1]; struct sockaddr *preply_addr = (struct sockaddr *)malloc(servlen); while (fgets(sendline, MAXLINE, fp) != NULL) &#123; sendto(sockfd, sendline, strlen(sendline), 0, pservaddr, servlen); int len = servlen; int n = recvfrom(sockfd, recvline, MAXLINE, 0, preply_addr, &amp;len); // 为了防止接收到其他进程的数据，通过条件判断去除 if (len != servlen || memcmp(pservaddr, preply_addr, len) != 0) &#123; printf("reply from others (!ignore)\n"); continue; &#125; recvline[n] = 0; fputs(recvline, stdout); &#125;&#125;// connect、write、read方式void dg_cli2(FILE *fp, int sockfd, const struct sockaddr *pservaddr, socklen_t servlen)&#123; char sendline[MAXLINE], recvline[MAXLINE + 1]; connect(sockfd, (struct sockaddr *)pservaddr, servlen); while (fgets(sendline, MAXLINE, fp) != NULL) &#123; write(sockfd, sendline, strlen(sendline)); int n = read(sockfd, recvline, MAXLINE); recvline[n] = 0; fputs(recvline, stdout); &#125;&#125;int main(int argc, char *argv[])&#123; if (argc != 2) &#123; printf("usage: tcpcli &lt;IPaddress&gt;\n"); exit(1); &#125; struct sockaddr_in servaddr; bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(SERV_PORT); inet_pton(AF_INET, argv[1], &amp;servaddr.sin_addr); int sockfd = socket(AF_INET, SOCK_DGRAM, 0); dg_cli2(stdin, sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)); exit(0);&#125; UDP服务端程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;strings.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */void dg_echo(int sockfd, struct sockaddr *pcliaddr, socklen_t clilen)&#123; char mesg[MAXLINE]; for (;;) &#123; socklen_t len = clilen; int n; bzero(mesg, MAXLINE); if ((n = recvfrom(sockfd, mesg, MAXLINE, 0, pcliaddr, &amp;len)) &lt; 0) &#123; close(sockfd); printf("recvfrom error, error=%m\n"); exit(1); &#125; printf("recv %s\n", mesg); sendto(sockfd, mesg, n, 0, pcliaddr, len); &#125;&#125;int main(int argc, char *argv[])&#123; int sockfd = socket(AF_INET, SOCK_DGRAM, 0); struct sockaddr_in servaddr, cliaddr; bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) &lt; 0) &#123; printf("bind error\n"); exit(1); &#125; dg_echo(sockfd, (struct sockaddr*)&amp;cliaddr, sizeof(cliaddr));&#125; UDP服务端信号驱动式I/O版本信号驱动式I/O：进程执行I/O系统调用告知内核启动某个I/O操作，内核启动I/O操作后立即返回到进程。进程在I/O操作发生期间继续执行。当操作完成或遇到错误时，内核以进程在I/O系统调用中指定的方式通知进程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159/** * 信号驱动式I/O在TCP套接字用途不大，该信号产生的过于频繁，它的出现并未指示发生的事情 */#include &lt;unistd.h&gt;#include &lt;errno.h&gt;#include &lt;sys/types.h&gt;#include &lt;netinet/in.h&gt;#include &lt;sys/socket.h&gt;#include &lt;arpa/inet.h&gt;#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;#include &lt;signal.h&gt;#include &lt;strings.h&gt;#include &lt;fcntl.h&gt;#define SERV_PORT 9877#define MAXLINE 4096 /* max text line length */static int sockfd;#define MAXDG 4096typedef struct&#123; void *dg_data; // 实际数据 size_t dg_len; // 实际数据长度 struct sockaddr *dg_sa; // 包含客户端地址 socklen_t dg_salen; // 客户端地址长度&#125; DG;#define QSIZE 8static DG dg[QSIZE]; // 存放数据的环形缓冲区static long cntread[QSIZE + 1];// 需要处理的下一个数据元素的下标static int iget;// 存放数据元素的下一个位置static int iput;static int nqueue; // 队列中的数据个数static socklen_t clilen;static void sig_hup(int signo)&#123; int i=0; for (; i &lt;= QSIZE; i++) &#123; printf("cntread[%d = %ld\n", i, cntread[i]); &#125;&#125;static void sig_io(int signo)&#123; int nread; // 为了解决非实时信号不排队问题，采用循环读取方式 for (nread = 0; ; ) &#123; // 检查队列是否已满 if (nread &gt;= QSIZE) &#123; printf("receive overflow\n"); exit(1); &#125; DG *ptr = &amp;dg[iput]; ptr-&gt;dg_salen = clilen; ssize_t len = recvfrom(sockfd, ptr-&gt;dg_data, MAXDG, 0, ptr-&gt;dg_sa, &amp;ptr-&gt;dg_salen); if (len &lt; 0) &#123; if (errno == EWOULDBLOCK) &#123; break; &#125; else &#123; printf("recvfrom error\n"); exit(1); &#125; &#125; ptr-&gt;dg_len = len; nread++; nqueue++; if (++iput &gt;= QSIZE) &#123; iput = 0; &#125; &#125; cntread[nread]++;&#125;void dg_echo(int sockfd_arg, struct sockaddr *pcliaddr, socklen_t clilen_arg)&#123; sockfd = sockfd_arg; clilen = clilen_arg; int i = 0; for (; i&lt;QSIZE; i++) &#123; dg[i].dg_data = malloc(MAXDG); dg[i].dg_sa = (struct sockaddr *)malloc(clilen); dg[i].dg_salen = clilen; &#125; iget = iput = nqueue = 0; signal(SIGHUP, sig_hup); // 在启动信号I/O前设置信号处理函数 signal(SIGIO, sig_io); // 设置接收信号通知的进程，让本进程接收SIGIO信号 fcntl(sockfd, F_SETOWN, getpid()); // 为了能够在得到I/O事件后重复执行I/O操作，需要将文件描述符设置为非阻塞方式 // O_ASYNC表示在文件描述符上使用信号驱动I/O int flags = fcntl(sockfd, F_GETFL); fcntl(sockfd, F_SETFL, flags | O_ASYNC | O_NONBLOCK); sigset_t zeromask, newmask, oldmask; sigemptyset(&amp;zeromask); sigemptyset(&amp;newmask); sigemptyset(&amp;oldmask); // 设置新的信号掩码，阻塞SIGIO信号 sigaddset(&amp;newmask, SIGIO); sigprocmask(SIG_BLOCK, &amp;newmask, &amp;oldmask); for (; ;) &#123; while (nqueue == 0) &#123; // 挂起进程直到收到任何信号，该函数返回后SIGIO继续被阻塞 sigsuspend(&amp;zeromask); &#125; // 解除SIGIO的阻塞 sigprocmask(SIG_SETMASK, &amp;oldmask, NULL); sendto(sockfd, dg[iget].dg_data, dg[iget].dg_len, 0, dg[iget].dg_sa, dg[iget].dg_salen); if (++iget &gt;= QSIZE) &#123; iget = 0; &#125; // 为了能够修改nqueue的值，阻塞SIGIO信号 sigprocmask(SIG_BLOCK, &amp;newmask, &amp;oldmask); nqueue--; &#125;&#125;int main(int argc, char *argv[])&#123; int sockfd = socket(AF_INET, SOCK_DGRAM, 0); struct sockaddr_in servaddr, cliaddr; bzero(&amp;servaddr, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); if (bind(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) &lt; 0) &#123; printf("bind error\n"); exit(1); &#125; dg_echo(sockfd, (struct sockaddr*)&amp;cliaddr, sizeof(cliaddr)); return 1;&#125; 相关下载本文中的实例，代码采用eclipse CDT编写，可以直接导入eclipse中运行。 下载实例]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过python来抓取和解析网页内容]]></title>
      <url>%2Fpost%2Fpython_parse_web%2F</url>
      <content type="text"><![CDATA[最近这段时间回顾了下python，距离上次使用python已经超过两年的时间了。 相对于c++语言，python要灵活许多，对于工作中的一些小问题的解决可以通过python来实现比较高效和方便，比如网页的抓取和解析。甚至对于非IT的工作，也可以通过脚本的方式来解决，只要是工作中遇到反复处理的体力活劳动就可以考虑利用编程方式来解决。 本文以我的博客的文档列表页面为例，利用python对页面中的文章名进行提取。 文章列表页中的文章列表部分的url如下： 1234567&lt;ul class="listing"&gt; &lt;li class="listing-item"&gt;&lt;span class="date"&gt;2014-12-03&lt;/span&gt;&lt;a href="/post/linux_funtion_advance_feature" title="Linux函数高级特性" &gt;Linux函数高级特性&lt;/a&gt; &lt;/li&gt; &lt;li class="listing-item"&gt;&lt;span class="date"&gt;2014-12-02&lt;/span&gt;&lt;a href="/post/cgdb" title="cgdb的使用" &gt;cgdb的使用&lt;/a&gt; &lt;/li&gt;...&lt;/ul&gt; requests模块的安装requests模块用于加载要请求的web页面。 在python的命令行中输入import requests，报错说明requests模块没有安装。 我这里打算采用easy_install的在线安装方式安装，发现系统中并不存在easy_install命令，输入sudo apt-get install python-setuptools来安装easy_install工具。 执行sudo easy_install requests安装requests模块。 Beautiful Soup安装为了能够对页面中的内容进行解析，本文使用Beautiful Soup。当然，本文的例子需求较简单，完全可以使用分析字符串的方式。 执行sudo easy_install beautifulsoup4即可安装。 编码问题python的编码问题确实是一个很头大的问题，尤其是对于不熟悉python的菜鸟。 python自身的编码问题就已经够头大的了，碰巧requests模块也有一个编码问题的bug，具体的bug见参考文章。 代码1234567891011121314151617181920212223242526272829303132#!/usr/bin/env python # -*- coding: utf-8 -*-' a http parse test programe '__author__ = 'kuring lv'import requestsimport bs4archives_url = "http://kuring.me/archive"def start_parse(url) : print "开始获取(%s)内容" % url response = requests.get(url) print "获取网页内容完毕" soup = bs4.BeautifulSoup(response.content.decode("utf-8")) #soup = bs4.BeautifulSoup(response.text); # 为了防止漏掉调用close方法，这里使用了with语句 # 写入到文件中的编码为utf-8 with open('archives.txt', 'w') as f : for archive in soup.select("li.listing-item a") : f.write(archive.get_text().encode('utf-8') + "\n") print archive.get_text().encode('utf-8')# 当命令行运行该模块时，__name__等于'__main__'# 其他模块导入该模块时，__name__等于'parse_html'if __name__ == '__main__' : start_parse(archives_url) 参考文章 廖雪峰的python教程 Beautiful Soup 4.2.0 文档 使用 Python 轻松抓取网页 Python+Requests抓取中文乱码改进方案]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我的Makefile文件]]></title>
      <url>%2Fpost%2Fmy_makefile%2F</url>
      <content type="text"><![CDATA[最近学习了《GNU Make项目管理》，改进了我之前一直在用的Makefile文件，解决我之前的Makefile中一直存在的修改依赖头文件后不能自动编译cpp文件的问题。本文列举了我常用的两个Makefile文件，其中第一个为我常用的Makefile，第二个为从网上找到的其他Makefile文件。 第一个Makefile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748all:INCLUDE = -I./FLAGS = -g -Wall $(INCLUDE)FLAGS += -fPICLIBDIR = -lz -lm -lcryptoLINK = $(LIBDIR) -lpthreadGCC = g++# for C++ languageCODE.cpp = main.cpp \ trim.cppCPP.o = $(CODE.cpp:.cpp=.o)OBJS.d = $(CODE.cpp:.cpp=.d)OBJS.o = $(CPP.o)# 解决头文件依赖-include $(subst .cpp,.d,$(CODE.cpp))%.d: %.cpp $(GCC) -M $(FLAGS) $&lt; &gt; $@.$$$$; \ sed 's,\($*\)\.o[ :]*,\1.o $@ : ,g' &lt; $@.$$$$ &gt; $@; \ rm -f $@.$$$$# rule for C++ language%.o : %.cpp $(GCC) $(FLAGS) -o $@ -c $&lt; @echo $*.o build successfully!......TARGET = main $(TARGET) : $(OBJS.o) $(GCC) $(OBJS.o) -o $(TARGET) $(LINK) @echo $(TARGET) BUILD OK!.........all : $(TARGET).PHONY:clean: rm -rf $(TARGET) rm -rf $(OBJS.o) rm -rf $(OBJS.d) rm -rf *.d 该文件特点为需要手工将需要编译的源文件手动添加到Makefile中，可能比较麻烦，但是编译时比较灵活。可以随意修改需要编译源文件的顺序和是否需要编译源文件。 第二个Makefile1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495############################################################# KEFILE FOR C/C++ PROJECT# Author: swm8023 &lt;swm8023@gmail.com&gt;# Date: 2014/01/30############################################################.PHONY: all cleanall: # annotation when release versionDEBUG := yTARGET_PROG := main# project directory DEBUG_DIR := ./DebugRELEASE_DIR := ./ReleaseBIN_DIR := $(if $(DEBUG), $(DEBUG_DIR), $(RELEASE_DIR))# shell commandCC := gccCXX := g++RM := rm -rfMKDIR := mkdir -pSED := sedMV := mv# init sources &amp; objects &amp; dependssources_all := $(shell find . -name "*.c" -o -name "*.cpp" -o -name "*.h")sources_c := $(filter %.c, $(sources_all))sources_cpp := $(filter %.cpp, $(sources_all))sources_h := $(filter %.h, $(sources_all))objs := $(addprefix $(BIN_DIR)/,$(strip $(sources_cpp:.cpp=.o) $(sources_c:.c=.o)))deps := $(addprefix $(BIN_DIR)/,$(strip $(sources_cpp:.cpp=.d) $(sources_c:.c=.d)))# create directory$(foreach dirname,$(sort $(dir $(sources_c) $(sources_cpp))),\ $(shell $(MKDIR) $(BIN_DIR)/$(dirname)))# complie &amp; link variableCFLAGS := $(if $(DEBUG),-g -O, -O2)CFLAGS += $(addprefix -I ,$(sort $(dir $(sources_h))))CXXFLAGS = $(CFLAGS)LDFLAGS := LOADLIBES += #-L/usr/include/mysqlLDLIBS += #-lpthread -lmysqlclient# add vpathvpath %.h $(sort $(dir $(sources_h)))vpath %.c $(sort $(dir $(sources_c)))vpath %.cpp $(sort $(dir $(sources_cpp)))# generate depend files# actually generate after object generated, beacasue it only used when next make)ifneq "$(MAKECMDGOALS)" "clean"sinclude $(deps)endif# make-depend(depend-file,source-file,object-file,cc)define make-depend $(RM) $1; \ $4 $(CFLAGS) -MM $2 | \ $(SED) 's,\($(notdir $3)\): ,$3: ,' &gt; $1.tmp; \ $(SED) -e 's/#.*//' \ -e 's/^[^:]*: *//' \ -e 's/ *\\$$//' \ -e '/^$$/ d' \ -e 's/$$/ :/' &lt; $1.tmp &gt;&gt; $1.tmp; \ $(MV) $1.tmp $1;endef# rules to generate objects file$(BIN_DIR)/%.o: %.c @$(call make-depend,$(patsubst %.o,%.d,$@),$&lt;,$@,$(CC)) $(CC) $(CFLAGS) -o $@ -c $&lt;$(BIN_DIR)/%.o: %.cpp @$(call make-depend,$(patsubst %.o,%.d,$@),$&lt;,$@,$(CXX)) $(CXX) $(CXXFLAGS) -o $@ -c $&lt;# add-target(target,objs,cc)define add-target REAL_TARGET += $(BIN_DIR)/$1 $(BIN_DIR)/$1: $2 $3 $(LDFLAGS) $$^ $(LOADLIBES) $(LDLIBS) -o $$@endef# call add-target$(foreach targ,$(TARGET_PROG),$(eval $(call add-target,$(targ),$(objs),$(CXX))))all: $(REAL_TARGET) $(TARGET_LIBS)clean: $(RM) $(BIN_DIR) 该Makefile为从一个通用的C/C++ Makefile中直接获得的，为了避免原博客以后不能访问的情况，这里备份一下。 该Makefile可以动检测Makefile所在目录及其子目录中的.c和.cpp文件，并进行编译，不需要手动修改Makefile来填写需要编译的源文件，比较自动化。 相关参考第二个Makefile文件的作者博客中的两篇文章：GNU Make学习总结（一）和GNU Make学习总结（二） 相关下载一个包含上述两个Makefile的例子]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[编程珠玑读书笔记第1章开篇]]></title>
      <url>%2Fpost%2Fprogramming_pearls_1%2F</url>
      <content type="text"><![CDATA[问题描述一个最多包含n个正整数的文件，每个数小于n，其中n为10000000。要求升序排列整数列表，最多使用1MB的内存，运行时间尽可能短。 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt;#define BITSPERWORD 32#define SHIFT 5#define MASK 0x1F#define N 10000000int a[1 + N/BITSPERWORD] = &#123;0&#125;;/** * i&gt;&gt;SHIFT相当于i/32，用于确定i在第几个int数组中 * 其中i&amp;MASK含义为i%32，用于确定在int中的第几位 */void set(int i)&#123; a[i &gt;&gt; SHIFT] |= (1 &lt;&lt; (i &amp; MASK));&#125;void clr(int i)&#123; a[i &gt;&gt; SHIFT] &amp;= (0 &lt;&lt; (i &amp; MASK));&#125;int test(int i)&#123; return a[i &gt;&gt; SHIFT] &amp; (1 &lt;&lt; (i &amp; MASK));&#125;int main(void)&#123; int i; while (scanf("%d", &amp;i) != EOF) &#123; set(i); &#125; for (i=0; i&lt;N; i++) &#123; if (test(i)) &#123; printf("%d\n", i); &#125; &#125; return 0;&#125;` 习题5通过shell命令echo &quot;scale=2; 10000000 / 1024 / 8 / 1024.0&quot; | bc计算该程序运行时至少需要的存储空间为1.19MB，如果仅提供了1MB的存储空间，则需要更改上述程序的处理方式。 可采用多趟算法，多趟读入输入数据，每次完成一步。针对该题，可采用2步来完成，int数组的大小变更为5000000/8，比之前小了一半。第一步处理0-4999999之间的数据，第二步处理5000000-999999之间的数据。 习题6如果是每个整数至少出现10次，而不是原先的一次。可以使用4bit来统计出现的次数，申请的数组大小变为了10000000/2。只要是每个整数有出现的最多次数上限该种处理方式就合适，当然整数出现的上限不能太大，否则该算法就没有了任何优势。 习题9对一个大的数组的初始化操作需要耗费一些时间，为了消除数组的初始化，可以通过两个额外的数组来解决，这是典型的用空间换时间的方法。 +---+---+---+---+---+---+---+----+ data | | | 3 | | 2 | | 8 | | +---+---+---+---+---+---+---+----+ +---+---+---+---+---+---+---+----+ from | | | 0 | | 2 | | 1 | | +---+---+---+---+---+---+---+----+ +---+---+---+---+---+---+---+----+ to | 1 | 5 | 3 | | | | | | +---+---+---+---+---+---+---+----+ ^ + top 上图中data为要初始化的数组，from和to为辅助数组。如果data[i]已经初始化，则from[i]&lt;top，to[from[i]]=i。from是一个简单的标识，to和top确保了from中不会写入内存中的随机内容。 习题11该题的答案太他妈逗了，为了能够解决两地之间的数据传输瓶颈，作者给出的答案居然是用信鸽传输图片的底片后再将底片放大的方式来代替原先的用汽车运输的方式，这就是中国古代的飞鸽传书啊。 习题12该题在《三傻大闹宝莱坞》中见过，这跟编程毛线关系也没有啊。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一个实例讲解HTTP的断点续传]]></title>
      <url>%2Fpost%2Fhttp_range%2F</url>
      <content type="text"><![CDATA[本文以从服务器下载一个文件为例，讲解HTTP的断点续传功能。 客户端IP地址为：192.168.1.2服务器IP地址为：192.168.1.3 客户端向服务器发送请求客户端向服务器发送的请求为： 1234567GET /deepc.a HTTP/1.1Host: 192.168.100.189Connection: keep-aliveAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36Accept-Encoding: gzip,deflate,sdchAccept-Language: zh-CN,zh;q=0.8,en;q=0.6 从中可以看出请求的文件名为deepc.a文件。 客户端向服务器发送具体请求123456789GET /deepc.a HTTP/1.1Host: 192.168.100.189Connection: keep-aliveUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36Accept: */*Referer: http://192.168.100.189/deepc.aAccept-Encoding: gzip,deflate,sdchAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Range: bytes=0-32767 Range字段表示请求文件的范围为0-32767。 服务器响应第一次服务器的HTTP响应报文如下： 1234567891011121314151617181920GET /deepc.a HTTP/1.1Host: 192.168.1.3Connection: keep-aliveUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36Accept: */*Referer: http://192.168.1.3/deepc.aAccept-Encoding: gzip,deflate,sdchAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Range: bytes=0-32767HTTP/1.1 206 Partial ContentDate: Sun, 04 May 2014 05:14:54 GMTServer: Apache/2.4.4 (Win32) PHP/5.4.16Last-Modified: Sat, 03 May 2014 00:43:22 GMTETag: &quot;7efce6-4f8742f6ed9b2&quot;Accept-Ranges: bytesContent-Length: 32768Content-Range: bytes 0-32767/8322278Keep-Alive: timeout=5, max=100Connection: Keep-Alive HTTP的状态为206，表示服务器已经处理了部分HTTP相应。其中Content-Range字段表示服务器已经响应了0-32767个字节的文件内容。8322278表示文件的总长度为8322278字节。 客户端继续向服务器发送请求客户端根据上次HTTP报文中服务器已经返回给的客户端的数据情况继续向服务器发送请求报文，向服务器发送的请求报文内容如下： 123456789101112131415161718192021GET /deepc.a HTTP/1.1Host: 192.168.100.189Connection: keep-aliveUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.14 Safari/537.36Accept: */*Referer: http://192.168.100.189/deepc.aAccept-Encoding: gzip,deflate,sdchAccept-Language: zh-CN,zh;q=0.8,en;q=0.6Range: bytes=32768-8322277If-Range: &quot;7efce6-4f8742f6ed9b2&quot;HTTP/1.1 206 Partial ContentDate: Sun, 04 May 2014 05:14:54 GMTServer: Apache/2.4.4 (Win32) PHP/5.4.16Last-Modified: Sat, 03 May 2014 00:43:22 GMTETag: &quot;7efce6-4f8742f6ed9b2&quot;Accept-Ranges: bytesContent-Length: 8289510Content-Range: bytes 32768-8322277/8322278Keep-Alive: timeout=5, max=98Connection: Keep-Alive Content-Range的内容表示客户端向服务器请求文件中32768-8322277之间的字节数据。 第二次服务器的HTTP响应报文如下：12345678910HTTP/1.1 206 Partial ContentDate: Sun, 04 May 2014 05:14:54 GMTServer: Apache/2.4.4 (Win32) PHP/5.4.16Last-Modified: Sat, 03 May 2014 00:43:22 GMTETag: &quot;7efce6-4f8742f6ed9b2&quot;Accept-Ranges: bytesContent-Length: 8289510Content-Range: bytes 32768-8322277/8322278Keep-Alive: timeout=5, max=98Connection: Keep-Alive 表示服务器已经相应完成了32768-8322277之间的数据。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一个Linux下的监听脚本程序]]></title>
      <url>%2Fpost%2Flinux_listen_shell%2F</url>
      <content type="text"><![CDATA[为了避免linux下的控制台程序A死掉，可以通过一个另外一个程序B来监听A程序，当A程序异常退出时将B程序带起来。当然程序设计的最好方式为程序不崩溃，但是程序中存在bug很难避免，该方法还是有一定的实践意义。对于B程序可以通过shell脚本或者单独一个应用程序来解决。本文将通过shell脚本来解决此问题。 shell脚本的内容123456789101112131415161718192021222324252627282930#!/bin/bashcheck_process()&#123; # check parameter if [ $1 = "" ]; then return -1 fi # get the running process process_names=$(ps -ef | grep $1 | grep -v grep | awk '&#123;print $8&#125;') for process_name in $process_names do if [ $process_name = $1 ] ; then return 1 fi done # not run and run the process echo "$(date) : process $1 not run, just run it" $1 return 0&#125;while [ 1 ];do check_process "/usr/bin/app/process" # programe path sleep 5done 将shell脚本在脱离控制台下可以运行一旦断开了控制台，shell脚本就会由于接收到SIGHUP信号而退出。这里有两种思路来解决该问题，一种是通过系统的crontab来定期调用脚本程序，另外一种是通过神奇的screen程序来解决该问题，我这里通过screen程序来解决该问题，具体screen程序的应用见我的另外一篇文章《》。 应用程序为daemon方式运行为了能够保证该脚本监控多个应用程序，需要将应用程序设置为daemon方式运行，可以调用函数daemon实现。也可以调用单独实现的daemon函数，具体代码如下： 1234567891011121314151617181920212223242526void init_daemon(void) &#123; int pid; int i; if(pid=fork()) exit(0);//是父进程，结束父进程 else if(pid&lt; 0) exit(1);//fork失败，退出 //是第一子进程，后台继续执行 setsid();//第一子进程成为新的会话组长和进程组长 //并与控制终端分离 if(pid=fork()) exit(0);//是第一子进程，结束第一子进程 else if(pid&lt; 0) exit(1);//fork失败，退出 //是第二子进程，继续 //第二子进程不再是会话组长 for(i=0;i&lt; NOFILE;++i)//关闭打开的文件描述符 close(i); chdir(&quot;/tmp&quot;);//改变工作目录到/tmp umask(0);//重设文件创建掩模 return; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在github上同步fork的项目]]></title>
      <url>%2Fpost%2Fgithub_fork_sync%2F</url>
      <content type="text"><![CDATA[在github上可以fork别人的项目成为自己的项目，但是当fork的项目更新后自己fork的项目应该怎么怎么更新呢？我从网上看到了两种方式，一种是采用github的web界面中的操作来实现，具体是通过“Pull Request”功能来实现；另外一种是通过在本地合并代码分支的方式来解决。本文将采用第二种方式，以我最近fork的项目为例来说明。 将fork后自己的项目clone到本地执行git clone https://github.com/kuring/leetcode.git即可将自己fork的代码更新到本地。 fork完成后的远程分支和所有分支情况如下： 1234567kuring@T420:/data/git/leetcode$ git remote -vorigin https://github.com/kuring/leetcode.git (fetch)origin https://github.com/kuring/leetcode.git (push)kuring@T420:/data/git/leetcode$ git branch -a* master remotes/origin/HEAD -&gt; origin/master remotes/origin/master 将fork之前的项目clone到本地将fork之前的项目添加到本地的远程分支haoel中，执行git remote add haoel https://github.com/haoel/leetcode。 再查看一下远程分支和所有分支情况： 123456789kuring@T420:/data/git/leetcode$ git remote -vhaoel https://github.com/haoel/leetcode (fetch)haoel https://github.com/haoel/leetcode (push)origin https://github.com/kuring/leetcode.git (fetch)origin https://github.com/kuring/leetcode.git (push)kuring@T420:/data/git/leetcode$ git branch -a* master remotes/origin/HEAD -&gt; origin/master remotes/origin/master 将远程代码halel分支fetch到本地执行git fetch haoel，此时的所有分支情况如下，可以看出多了一个remotes/haoel/master分支。 12345kuring@T420:/data/git/leetcode$ git branch -a* master remotes/haoel/master remotes/origin/HEAD -&gt; origin/master remotes/origin/master 将halel分支merge到本地的分支执行git merge remotes/haoel/master，此时发现有冲突，提示内容如下： 1234uring@T420:/data/git/leetcode$ git merge haoel/master自动合并 src/reverseInteger/reverseInteger.cpp冲突（内容）：合并冲突于 src/reverseInteger/reverseInteger.cpp自动合并失败，修正冲突然后提交修正的结果。 之所以出现上述错误，这是由于我在fork之后在本地修正了源代码中的一处bug，而在fork之后到现在的时间间隔内原作者haoel也正好修正了该bug。打开文件后发现存在如下的内容，其实就是代码风格的问题，我这里将错误进行修正。 1234536 &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 37 while( x != 0 )&#123;38 =======39 while( x != 0)&#123;40 &gt;&gt;&gt;&gt;&gt;&gt;&gt; haoel/master 如果没有冲突的情况下通过merge命令即会将haoel/master分支合并master分支并执行commit操作。可以通过git status命令看到当前冲突的文件和已经修改的文件。执行git status命令可以看到如下内容，说明未冲突的文件已经在暂存区，冲突的文件需要修改后执行add操作： 12345678910111213141516171819kuring@T420:/data/git/leetcode$ git status位于分支 master您的分支与上游分支 &apos;origin/master&apos; 一致。您有尚未合并的路径。 （解决冲突并运行 &quot;git commit&quot;）要提交的变更： 修改: src/3Sum/3Sum.cpp 修改: src/4Sum/4Sum.cpp 修改: src/LRUCache/LRUCache.cpp ...... // 此处省略了很多重复的 ......未合并的路径： （使用 &quot;git add &lt;file&gt;...&quot; 标记解决方案） 双方修改： src/reverseInteger/reverseInteger.cpp 解决完冲突后执行add操作后再通过git status命令查看的内容如下。通过git status命令却看不到已经解决的冲突文件，对于这一点我还是很理解，参考文章中的Git 分支 - 分支的新建与合并是可以看到已经解决的冲突文件的，因为执行git add后将解决完成冲突的文件放到了暂存区中。 1234567891011121314kuring@T420:/data/git/leetcode$ git status位于分支 master您的分支与上游分支 &apos;origin/master&apos; 一致。所有冲突已解决但您仍处于合并中。 （使用 &quot;git commit&quot; 结束合并）要提交的变更： 修改: src/3Sum/3Sum.cpp 修改: src/4Sum/4Sum.cpp 修改: src/LRUCache/LRUCache.cpp ...... // 此处省略了很多重复的 ...... 这里冲突后merge操作并没有执行commit操作，需要解决冲突后再手工执行commit操作，此时整个的同步操作就已经完成了。 结尾如果隔一段时间后又需要同步项目了仅需要执行git fetch haoel命令以下的操作即可。 参考 Git 分支 - 分支的新建与合并 如何在github上fork一个项目来贡献代码以及同步原作者的修改 Github上更新自己fork的代码 由于git命令较多，为了便于查阅增加一处git data transprot commands]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Unique Binary Search Trees II]]></title>
      <url>%2Fpost%2Fleetcode_unique_binary_search_trees_ii%2F</url>
      <content type="text"><![CDATA[问题Given n, generate all structurally unique BST’s (binary search trees) that store values 1…n. For example,Given n = 3, your program should return all 5 unique BST’s shown below. 1 3 3 2 1 \ / / / \ \ 3 2 1 1 3 2 / / \ \ 2 1 2 3 分析要想能够生成多个树并存储到vector中，最容易想到的就是递归算法。要想能够递归，题目中提供的函数仅有一个参数，结合题目不能够完成递归的条件，考虑到unique binary search trees中的解法，需要递归具有两个参数的函数。 考虑到了递归的问题，还需要利用循环不断将树添加到vector中，这编写起来也是比较有难度，需要掌握循环的次数和什么时候将树添加到vector中。 代码123456789101112131415161718192021222324252627282930313233343536vector&lt;TreeNode *&gt; generateTrees(int n) &#123; vector&lt;TreeNode *&gt; sub_tree = generateTrees(1, n); return sub_tree;&#125;vector&lt;TreeNode *&gt; generateTrees(int low, int high) &#123; vector&lt;TreeNode *&gt; result; if (low &gt; high) &#123; result.push_back(NULL); return result; &#125; else if (low == high) &#123; TreeNode *node = new TreeNode(low); result.push_back(node); return result; &#125; for (int i=low; i&lt;=high; i++) &#123; vector&lt;TreeNode *&gt; left = generateTrees(low, i - 1); vector&lt;TreeNode *&gt; right = generateTrees(i + 1, high); for (int j=0; j&lt;left.size(); j++) &#123; for (int k=0; k&lt;right.size(); k++) &#123; TreeNode *root = new TreeNode(i); root-&gt;left = left[j]; root-&gt;right = right[k]; result.push_back(root); &#125; &#125; &#125; return result;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[图的存储和遍历]]></title>
      <url>%2Fpost%2Ftraverse_graph%2F</url>
      <content type="text"><![CDATA[图的存储在leetcode中图的存储形式如下，这种形式的图只能适合用来存储是连通图的情况，且根据leetcode提供的{0,1,2#1,2#2,2}格式的字符串通过程序来自动构造图比较麻烦，预知字符串的含义请移步到leetcode的解释。 1234567/** * Definition for undirected graph. * struct UndirectedGraphNode &#123; * int label; * vector&lt;UndirectedGraphNode *&gt; neighbors; * UndirectedGraphNode(int x) : label(x) &#123;&#125;; * &#125;; 本文为了能够用字符串表示所有图，并且便于程序的构造，使用了邻接表的形式来对图进行存储，即可以用来存储有向图，有可以存储无向图。图一个节点的结构如下： 123456789/** * 图节点的邻接表表示形式 */struct GraphNode &#123; std::string label; std::vector&lt;GraphNode *&gt; neighbors; bool visited; // 深度优先搜索和广度优先搜索的遍历都需要visited数组，为了简化程序，直接在节点的存储结构中设置visited变量 GraphNode(std::string x) : label(x), visited(false) &#123;&#125;;&#125;; 图的创建方面为了简化算法实现，对程序的效率没做太多关注，算法复杂度稍高。本算法的难点在于对字符串的拆解，并根据字符串找到对应的节点指针。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117/** * 图节点的邻接表表示形式 */struct GraphNode &#123; std::string label; std::vector&lt;GraphNode *&gt; neighbors; bool visited; GraphNode(std::string x) : label(x), visited(false) &#123;&#125;;&#125;;/** * 通过字符串的值，找到该字符串对应的图节点 */ GraphNode *get_one_node(const std::vector&lt;GraphNode *&gt; &amp;node_vector, std::string str)&#123; for (std::vector&lt;GraphNode *&gt;::const_iterator iter = node_vector.begin(); iter != node_vector.end(); iter++) &#123; if ((*iter)-&gt;label == str) &#123; return *iter; &#125; &#125; return NULL;&#125;/** * 时间复杂度高,对图的构建一般效率要求较低 * 对于查找某个节点的邻接点的指针操作可以使用map来提高查询效率 * 或者可以通过不需要初始化所有节点的方式来构造图，而是采用需要哪个节点构造哪个节点的方式 */std::vector&lt;GraphNode *&gt; create_graph(std::string str)&#123; std::vector&lt;GraphNode *&gt; node_vector; // init all nodes for (size_t pos = 0; pos &lt; str.length() - 1;) &#123; size_t end = str.find(',', pos); if (end != std::string::npos) &#123; GraphNode *node = new GraphNode(str.substr(pos, end - pos)); node_vector.push_back(node); &#125; else &#123; break; &#125; pos = str.find('#', pos); if (pos == std::string::npos) &#123; break; &#125; else &#123; pos++; &#125; &#125; // add neighbors in every node for (size_t pos = 0; pos &lt; str.length() - 1; ) &#123; GraphNode *current_node = NULL; size_t current_end = str.find(',', pos); if (current_end != std::string::npos) &#123; current_node = get_one_node(node_vector, str.substr(pos, current_end - pos)); pos = current_end + 1; &#125; else &#123; break; &#125; size_t node_end = str.find('#', pos); // 当前节点的字符串的结束位置 if (node_end == std::string::npos) &#123; node_end = str.length(); &#125; else &#123; node_end--; &#125; for ( ; ; ) &#123; current_end = str.find(',', pos); if (current_end &gt; node_end || current_end == std::string::npos) &#123; // 一个节点的最后一个邻接点 current_end = node_end; &#125; else &#123; current_end--; &#125; GraphNode *node = get_one_node(node_vector, str.substr(pos, current_end - pos + 1)); if (node != NULL) &#123; current_node-&gt;neighbors.push_back(node); std::cout &lt;&lt; current_node-&gt;label &lt;&lt; " add " &lt;&lt; node-&gt;label &lt;&lt; std::endl; &#125; if (current_end == node_end) &#123; // 一个节点的最后一个邻接点 break; &#125; else &#123; pos = current_end + 2; // 该节点之后还有其他邻接点 &#125; &#125; pos = node_end + 2; &#125; return node_vector;&#125; 深度优先搜索深度优先搜索遵循贪心算法的原理，如果孩子节点不为空，则一直遍历下去。 递归算法12345678910111213141516171819202122232425262728293031void DFS_traverse_recursion(GraphNode *node)&#123; if (!node-&gt;visited) &#123; std::cout &lt;&lt; node-&gt;label &lt;&lt; '\t'; node-&gt;visited = true; &#125; for (std::vector&lt;GraphNode *&gt;::iterator iter = node-&gt;neighbors.begin(); iter != node-&gt;neighbors.end(); iter++) &#123; if (!(*iter)-&gt;visited) &#123; DFS_traverse_recursion(*iter); &#125; &#125;&#125;/** * 图的深度优先搜索的递归形式 */void DFS_traverse_recursion(std::vector&lt;GraphNode *&gt; &amp;graph)&#123; for (std::vector&lt;GraphNode *&gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) &#123; (*iter)-&gt;visited = false; &#125; for (std::vector&lt;GraphNode *&gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) &#123; if (!(*iter)-&gt;visited) DFS_traverse_recursion(*iter); &#125;&#125; 非递归算法使用栈来实现非递归。 12345678910111213141516171819202122232425262728293031323334353637/** * 图的深度优先搜索的非递归形式 */void DFS_traverse_not_recursion(std::vector&lt;GraphNode *&gt; &amp;graph)&#123; for (std::vector&lt;GraphNode *&gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) &#123; (*iter)-&gt;visited = false; &#125; for (std::vector&lt;GraphNode *&gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) &#123; std::stack&lt;GraphNode *&gt; node_stack; if ((*iter)-&gt;visited) &#123; continue; &#125; node_stack.push(*iter); while (!node_stack.empty()) &#123; GraphNode *node = node_stack.top(); node_stack.pop(); if (node-&gt;visited) continue; std::cout &lt;&lt; node-&gt;label &lt;&lt; '\t'; node-&gt;visited = true; /* 使用反向迭代器遍历后将节点加入到栈中 */ for (std::vector&lt;GraphNode *&gt;::reverse_iterator iter2 = node-&gt;neighbors.rbegin(); iter2 != node-&gt;neighbors.rend(); iter2++) &#123; if (!(*iter2)-&gt;visited) &#123; node_stack.push(*iter2); &#125; &#125; &#125; &#125;&#125; 广度优先搜索该算法不存在递归算法，仅有非递归版本。需要利用队列来保存需要遍历的节点，占用的存储空间稍多。 123456789101112131415161718192021222324252627282930313233343536/** * 图的广度优先搜索的非递归形式 */void BFS_traverse_not_recursion(std::vector&lt;GraphNode *&gt; &amp;graph)&#123; for (std::vector&lt;GraphNode *&gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) &#123; (*iter)-&gt;visited = false; &#125; for (std::vector&lt;GraphNode *&gt;::iterator iter = graph.begin(); iter != graph.end(); iter++) &#123; std::queue&lt;GraphNode *&gt; node_queue; if ((*iter)-&gt;visited) &#123; continue; &#125; node_queue.push(*iter); while (!node_queue.empty()) &#123; GraphNode *node = node_queue.front(); node_queue.pop(); if (node-&gt;visited) continue; std::cout &lt;&lt; node-&gt;label &lt;&lt; '\t'; node-&gt;visited = true; for (std::vector&lt;GraphNode *&gt;::iterator iter2 = node-&gt;neighbors.begin(); iter2 != node-&gt;neighbors.end(); iter2++) &#123; if (!(*iter2)-&gt;visited) &#123; node_queue.push(*iter2); &#125; &#125; &#125; &#125;&#125; 相关下载本文相关源码]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[二叉树的遍历]]></title>
      <url>%2Fpost%2Ftraverse_tree%2F</url>
      <content type="text"><![CDATA[[TOC] 近期准备复习数据结构和算法的知识，参照了网络上各路大神的学习攻略，大部分算法学习的思路为参照一些经典书籍（如算法导论）并结合一些代码的实践来完成，并未找到一条适合我的算法学习之路。经过思考后决定采用代码编写曾经的教科书中代码实例的方式来学习，曾经接触的算法教科书包括《数据结构（C语言版）》和《计算机算法基础》。一来这些算法已经基本熟悉，只是时间久远有些已经忘记；二来，通过思考后编写代码增强自己的记忆。 同时我编写的这些实例可以作为leetcode上的很多题目的基础，为下一个阶段刷leetcode上的题目打好基础。 树的存储形式包括了顺序存储（采用数组形式）和链式存储，其中链式存储更为灵活，可以表示任意形式的树，本文中的代码将采用树的链式存储方式。 树的构建树的构建有多种方式，本文使用字符串采用了自顶向下、自左到右的顺序构建树，跟leetcode的形式一致。其中’#’表示该节点为空，如果该节点为空节点，其左右子孩子节点也要用’#’表示，而不能不用任何字符表示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 二叉树的链式存储结构 */struct TreeNode&#123; char data; struct TreeNode *left; struct TreeNode *right; TreeNode(char data) : data(data), left(NULL), right(NULL) &#123;&#125;&#125;;/** * 构造二叉树，要构造的字符串采用了自顶向下、自左到右的顺序，跟leetcode的形式一致 */TreeNode *create_binary_tree(const char *str)&#123; if (!str || strlen(str) == 0) &#123; return NULL; &#125; // 对每个节点分配存储空间 int node_size = strlen(str); TreeNode **tree = new TreeNode*[node_size]; for (int i=0; i&lt;node_size; i++) &#123; if (str[i] == '#') &#123; tree[i] = NULL; &#125; else &#123; tree[i] = new TreeNode(str[i]); &#125; &#125; for (int i=0, j=0; i&lt;node_size &amp;&amp; j&lt;node_size; i++) &#123; if (tree[i] != NULL) &#123; if ((j + 1) &lt; node_size) &#123; tree[i]-&gt;left = tree[++j]; tree[i]-&gt;right = tree[++j]; &#125; &#125; else &#123; j += 2; &#125; &#125; return *tree;&#125; 二叉树的先序遍历递归实现12345678910111213141516/** * 先序遍历二叉树的递归形式 */void preorder_traverse_recursion(TreeNode *root)&#123; if (!root) &#123; return; &#125; printf(&quot;%c\t&quot;, root-&gt;data); preorder_traverse_recursion(root-&gt;left); preorder_traverse_recursion(root-&gt;right);&#125; 非递归实现1234567891011121314151617181920212223242526/** * 先序遍历的非递归形式 */void preorder_traverse_not_recursion(TreeNode *root)&#123; if (!root) &#123; return ; &#125; std::stack&lt;TreeNode *&gt; tree_stack; tree_stack.push(root); while (tree_stack.size() &gt; 0) &#123; TreeNode *node = tree_stack.top(); tree_stack.pop(); printf("%c\t", node-&gt;data); if (node-&gt;right != NULL) &#123; tree_stack.push(node-&gt;right); &#125; if (node-&gt;left != NULL) &#123; tree_stack.push(node-&gt;left); &#125; &#125;&#125; 二叉树的中序遍历递归实现12345678910111213141516/** * 中序遍历二叉树的递归形式 */void inorder_traverse_recursion(TreeNode *root)&#123; if (!root) &#123; return; &#125; inorder_traverse_recursion(root-&gt;left); printf("%c\t", root-&gt;data); inorder_traverse_recursion(root-&gt;right);&#125; 非递归实现123456789101112131415161718192021222324252627/** * 中序遍历二叉树的非递归形式 */void inorder_traverse_not_recursion(TreeNode *root)&#123; if (!root) &#123; return ; &#125; std::stack&lt;TreeNode *&gt; tree_stack; while (root != NULL || tree_stack.size() &gt; 0) &#123; // 遍历到左子树的叶子节点 while (root) &#123; tree_stack.push(root); root = root-&gt;left; &#125; // 遍历栈顶节点 root = tree_stack.top(); tree_stack.pop(); printf("%c\t", root-&gt;data); root = root-&gt;right; &#125;&#125; 二叉树的后序遍历递归实现12345678910111213141516/** * 后序遍历二叉树的递归形式 */void postorder_traverse_recursion(TreeNode *root)&#123; if (!root) &#123; return; &#125; postorder_traverse_recursion(root-&gt;left); postorder_traverse_recursion(root-&gt;right); printf("%c\t", root-&gt;data);&#125; 非递归实现仅用一个栈不能够实现后序遍历非递归算法，需要保存一个上次访问过节点的变量。 123456789101112131415161718192021222324252627282930313233343536/** * 后序遍历二叉树的非递归形式 */void postorder_traverse_not_recursion(TreeNode *root)&#123; if (!root) &#123; return ; &#125; std::stack&lt;TreeNode *&gt; tree_stack; TreeNode *visited = NULL; while (root != NULL || tree_stack.size() &gt; 0) &#123; // 遍历到左子树的叶子节点 while (root) &#123; tree_stack.push(root); root = root-&gt;left; &#125; root = tree_stack.top(); if (root-&gt;right == NULL || root-&gt;right == visited) &#123; // 如果没有右孩子，或者右孩子刚刚访问过，则访问当前节点 printf("%c\t", root-&gt;data); tree_stack.pop(); visited = root; root = NULL; &#125; else &#123; root = root-&gt;right; &#125; &#125;&#125; 总结二叉树遍历的递归形式程序结构类似，编写相对简单。但是递归方法在C语言中存在执行效率差（需要维护函数栈），容易出现栈溢出的异常的问题。任何递归问题问题都可以转化为非递归问题，转化的思路包括了直接转化法和间接转化法。直接转化法可以通过循环来解决，间接转化法需要借助栈加循环来解决。 二叉树遍历的非递归形式相对复杂，二叉树的先序遍历的非递归形式容易理解，二叉树的中序遍历稍微困难，后序遍历的非递归形式最复杂。 相关下载程序源代码]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下通过命令行进行进制转换]]></title>
      <url>%2Fpost%2Flinux_convert_ary%2F</url>
      <content type="text"><![CDATA[在Windows下可以通过计算器进行进制之间的转换，非常方便。本文总结Linux下可用的进行进制之间的转换方法。 利用shellshell脚本默认数值是由10进制数处理，除非这个数字某种特殊的标记法或前缀开头，才可以表示其它进制类型数值。如：以0开头就是8进制、以0x开头就是16进制数。使用“BASE#NUMBER”这种形式可以表示其它进制。BASE值的范围为2-64。 其他进制转10进制 123456789101112kuring@T420:~$ echo $((16#4000000))67108864kuring@T420:~$ echo $((2#111))7kuring@T420:~$ echo $((0x10))16kuring@T420:~$ echo $((010))8// 对进制转换为10进制进行运算，稍显啰嗦kuring@T420:~$ echo $(($((16#4000000))/1014/1024));64 利用let命令let用来执行算数运算和数值表达式测试。可以利用该命令完成简单的计算，并将计算结果赋给其他变量。 1234// 对进制转换为10进制后进行运算，比纯shell方式简洁kuring@T420:~$ let a=$((16#4000000))/1014/1024;kuring@T420:~$ echo $a64 利用bc命令该命令是一个强大的计算器软件，可以利用其中的ibase和obase进行输入进制的转换，ibase表示输入数字的进制，obase表示输出数字的进制。 123456// 如果没有制定，则默认为10进制// 对于16进制，要使用F，而不能使用fkuring@T420:~$ echo "ibase=16;FF" | bc255kuring@T420:~$ echo "obase=2;ibase=16;FF" | bc11111111 参考文章linux shell 不同进制数据转换（二进制，八进制，十六进制，base64) Shell进制转换小结]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[leetcode题目之Min Stack]]></title>
      <url>%2Fpost%2Fmin_stack%2F</url>
      <content type="text"><![CDATA[题目 Memory Limit Exceeded Design a stack that supports push, pop, top, and retrieving the minimum element in constant time. push(x) – Push element x onto stack.pop() – Removes the element on top of the stack.top() – Get the top element.getMin() – Retrieve the minimum element in the stack. 错误代码看到此题目，以为是用实现一个简单的栈结构，于是就直接写下了如下代码，采用了双向链表的方式来实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class MinStack &#123;public: struct Node &#123; int data; Node *next; Node *pre; Node() : next(NULL),pre(NULL) &#123;&#125;; &#125;; MinStack() &#123; header = new Node(); tail = header; &#125; void push(int x) &#123; Node *node = new Node(); node-&gt;data = x; node-&gt;pre = tail; tail-&gt;next = node; tail = node; &#125; void pop() &#123; Node *pre = tail-&gt;pre; delete tail; tail = pre; tail-&gt;next = NULL; &#125; int top() &#123; if (tail == header) &#123; return -1; &#125; return tail-&gt;data; &#125; int getMin() &#123; Node *begin = header-&gt;next; int min = INT_MIN; while(begin) &#123; if (min &gt; begin-&gt;data) &#123; min = begin-&gt;data; &#125; begin = begin-&gt;next; &#125; return min; &#125; private: Node *header; Node *tail;&#125;; 提交后提示Memory Limit Exceeded错误，开始考虑是不是双向链表占用的空间过多。 正确代码仔细看题目后发现retrieving the minimum element in constant time，即时间复杂度为O(1)，上述实现代码时间复杂度为O(n)，明显不符合要求。看来理解题目有误，不是为了实现栈类，而是为了利用数据结构解决获取最小值问题。 经过考虑后可以通过在类内部维护存储最小值的栈来解决，存储最小值的栈除了需要存储最小值外，还需要维护最小值个数。 实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class MinStack &#123; struct MinUnit &#123; int count; int value; MinUnit(int count, int value) : count(count), value(value) &#123;&#125;; &#125;;public: void push(int x) &#123; data_stack.push(x); if (min_stack.empty() || x &lt; min_stack.top()-&gt;value) &#123; MinUnit *p_unit = new MinUnit(1, x); min_stack.push(p_unit); &#125; else if (x == min_stack.top()-&gt;value) &#123; min_stack.top()-&gt;count++; &#125; &#125; void pop() &#123; if (data_stack.top() == min_stack.top()-&gt;value) &#123; if (min_stack.top()-&gt;count == 1) &#123; MinUnit *punit = min_stack.top(); min_stack.pop(); delete punit; &#125; else &#123; min_stack.top()-&gt;count--; &#125; &#125; data_stack.pop(); &#125; int top() &#123; return data_stack.top(); &#125; int getMin() &#123; return min_stack.top()-&gt;value; &#125; private: std::stack&lt;int&gt; data_stack; // 最小栈，为了便于修改栈单元中的count值，采用存储指针方式 std::stack&lt;MinUnit*&gt; min_stack;&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux IO模型]]></title>
      <url>%2Fpost%2Flinux_io%2F</url>
      <content type="text"><![CDATA[由于I/O模型现在掌握还不够透彻，本文仅列出了一些I/O模型中的一些常用概念，对于select、poll、epoll和信号驱动I/O等模型理解还不够透彻，待理解透彻后补上。 阻塞I/O与非阻塞I/O阻塞I/O：当进行I/O操作后，程序处于等待状态，直到需要的资源可用为止。 非阻塞I/O：当进行I/O操作后，函数直接返回，不需要等待。 两种文件描述符准备就绪的通知模式水平触发通知：如果文件描述符上可以非阻塞地执行I/O系统调用，此时认为它已经就绪。允许在任意时刻重复检测I/O状态，没有必要每次当文件描述符就绪后尽可能多的执行I/O。其中select()和poll()属于水平触发通知模式。 边缘触发通知：如果文件描述符自上次状态检查以来有了新的I/O活动，此时需要触发通知。只有当I/O事件发生后才会得到通知，在另外一个I/O事件到来之前不会得到通知。因此，在接收到一个I/O事件后，程序在某个时刻应该在相应的文件描述符上尽可能多地执行I/O。每个被检查的文件描述符被置为非阻塞模式，在得到I/O事件通知后重复执行I/O操作，直到系统调用返回错误为止。其中信号驱动I/O属于此种类型。 将的通俗一点，假设通过异步I/O读取socket数据，当接收到100个字节的数据后，这时候会触发通知给应用程序。如果应用程序仅读取了50字节的数据，重新调用异步I/O函数时，在水平触发时仍然会得到通知，而采用边缘触发时不会得到通知。 epoll即可以采用水平触发通知方式，也可以采用边缘触发通知方式。 这两个概念对于理解Linux的I/O模型非常重要，但是却是不容易理解。可以通过举例说明：一个管道内收到了数据，注册该管道描述符的epoll返回，但是用户只读取了一部分数据，然后再次调用了epoll。这时，如果是水平触发方式，epoll将立刻返回，因为当前有数据可读，满足IO就绪的要求；但是如果是边沿触发方式，epoll不会返回，因为调用之后还没有新的IO事件发生，直到有新的数据到来，epoll才会返回，用户可以一并读到老的数据和新的数据。 同步I/O与异步I/O同步I/O：发出I/O操作后，后面操作不能进行，要么等待I/O操作完成，要么放弃I/O操作。后面的操作和当前的操作只能有一个进行。 异步I/O：发出I/O操作后，马上返回继续执行后面的操作，而I/O操作同时执行。 参考文章 《Linux/UNIX系统编程手册》第63章 浅析Linux IO技术体系]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[云南之旅]]></title>
      <url>%2Fpost%2Fyunan_travel%2F</url>
      <content type="text"><![CDATA[前言在这个炎热的夏天，在一个特殊的月份，我开启了一段8天7晚的云南之旅。出于资金方面和省心方面的考虑，选择跟团旅游。本次的旅行包含了昆明、大理、丽江和香格里拉四个地点，如果不是跟团，在短短的6天的旅行时间之内不可能走完四个地方的多个景点，因为这四个地方之间的距离坐车基本都在四个小时以上。大多数人去云南还是会选择跟团的，因为跟团省太多事了。如果是独自出行，每天光查路线路、找酒店、查攻略就会浪费掉很大一部分时间，肯定会更自由一些，但是绝对不会走过这么多地方。 跟团也有很多不好的地方，本次出行旅行社制定的路线为“出发地-昆明-大理-丽江-香格里拉-丽江-大理-昆明-目的地”，可以看到路线是重复了一遍，相当于很大一部分时间都浪费在了路上，至少相当于浪费了4+4+4共12个小时的时间，这也就造成了每天早晨需要很早就起床，基本在6点半就坐在大巴上出发了。之所以每天早晨会很早就起床，另一个原因是因为现在是云南旅游的旺季，一旦稍微晚些，很多景点就会排队等好长时间，可能晚起10分钟，就要在景区多排一个小时的队。 跟团的价格之所以便宜，听导游说，这是云南省旅游局的决定。云南绝大多数都是高原山区地貌，交通不够发达。旅游业在云南的经济中占有很大的比重，政府想通过旅游来拉动经济，跟团价格便宜，自然需要在旅游过程中的消费来赚钱。本次旅游的过程中，也有几次购物的经历，其中行程中的大理十八里铺和七彩云南是赤裸裸的购物环节，导游也明确的告知了。在去景点的过程中，很明显发现每个景点的必经通道中均有购物的部分，这是云南的景点和我去过的其他景点比较大的差别。最奇葩的是长江第一弯的观景楼，大约7层楼左右的样子，每层楼的楼梯都不在一个位置，从一层的楼梯走到另一层的楼梯必经卖土特产的柜台，楼层上全是卖土特产的。其实长江第一弯也仅有一个观景楼而已，因此是免费的。 跟团很大一部分时间都在坐车上，游览景点的时间确实非常紧，而导游给的购物地方的时间确实非常充裕。比如游览石林仅给了一个半小时左右的时间，石林是国家的5A级景区，门票价格是170，是本次行程中最贵的景点了吧。石林虽然不大，但是要想在一个半小时内全部游览完是不可能的，因此只能走马观花，没办法，跟团走就只能认了。 之所以要写下这段行程，是因为这段旅行对我而言有着不一般的意义，我想将这段记忆铭记在心。不幸的是，回家后发现单反的SD卡出问题了，照片丢失了很大一部分。我虽不是很认同旅游就是为了拍照，现在人们已经过度的重视拍照而忽略了旅行本身的意义，但是这些照片在这次旅行中确实不可或缺的。 Day 1下午三点多就到了春城昆明，抬头一看昆明的天果然不一般，云彩非常多，而且离地面非常近，轮廓感很强。阳光非常强烈，气温虽然不高，但是紫外线较强，阳光照在皮肤上生疼。不像在济南只能看到的雾霾，济南即使有云也是在雾霾之上，从地面看去毫无层次感。找到旅行社后机场接送人员送往酒店，稍事休息后去到昆明市中心的南屏街。从整体看，昆明的市容跟济南相仿，属于不是很繁华的类别，但是却属于省会的档次。下图照片我第一晚酒店的照片，大家感受一下云。 晚上和朋友在一家彝族的菜馆吃饭，对于北方的我来说，南方的饭菜吃起来不太习惯，米饭是生硬的，汤是油腻的，菜是辣的且没滋味。 吃完饭后准备坐公交回酒店，已经晚上九点半左右了，想抬头看看能不能看到星星，居然看到满天的云，真是奇特。在山东即使晚上有满天的云，也绝对达不到昆明的云的明亮程度。造成这种现象有两个原因，一是昆明属于东七区，比北京时间要晚上一个小时；二是，云南的云离得地面更近且多。 Day 2这是悲催的一天，按照行程，要走“昆明—大理—丽江（昆明大理行车约4.5小时，大理-丽江行车约2小时车程）”的路程。早晨一起床，要去餐厅吃早饭，发现餐厅乌秧乌秧的全是人啊，找个地方吃饭都难，在艰难中吃完了早餐。期间一女同胞不小心打碎了一个碗，餐厅老板要了女顾客10块钱，心想真黑啊，几毛钱一个碗至于要这么多吗？ 吃完饭后，餐厅外面和酒店大堂全是游客，经过电话沟通后找到了导游。导游点名后坐上了悲催的大巴。幸运的是，导游还不错，东北小伙，在云南上的大学。云南的旅游大巴都是车牌号都是统一编号的，车牌号以“L“开头，表示旅游的含义。旅游大巴启动后，雨就开始稀里哗啦的下起来了，运气还不错。 旅游大巴在高速上要在好多地方做例行检查，大约9点左右行驶到楚雄地段的某个高速检查点时，我们大巴的右侧玻璃跟后面的大巴的左侧后视镜摩擦而过，因为我就坐在后排，眼睁睁的目睹了这一过程。结果我们大巴右侧玻璃和后面大巴的左侧后视镜全部挂掉了，幸好没有驴友受伤情况，有图有真相。 在服务区经过1个小时的等待后大巴向前走了，目的地不是大理，而是前方的某个小镇。因为大巴没了挡风玻璃，速度也上不去了，在后面坐着还是相当凉快的。在小镇吃了个野生菌火锅后，开始了漫长的等待，等待着更换的大巴从大理赶来。从出现事故到新的大巴的到来，花掉了近5个小时的时间，这五个小时就算浪费了。 大巴前往大理，到达大理时接近5点多了，在天龙八部影视城附近吃了个晚餐，开始了赶往丽江的行程。本来计划中的大理古城和束河古镇全部泡汤，只能将行程安排到回来时在游览。 到达丽江时已经晚上十点的样子，刚刚下过一场雨，直接入住酒店。 Day 3相对来说最轻松的一天，今天的行程中仅有玉龙雪山景区和玉水寨。 在云南给我们带团的导游属于省级导游，到了丽江这种地方性景点导游需要更换为地方性导游，这也是一种地方拉动就业的措施。昨天的导游直接休息一天，今天的导游是当地的纳西族导游带领。 玉龙雪山是丽江旅游的必去之地，每天丽江的游客大约在2万人的样子，也就意味着每天也会有2万人会前往玉龙雪山。为了避免在景区排队，早晨6点半就出发前往玉龙雪山，一个小时左右时间到达了玉龙雪山的入口。下车后坐着环保大巴盘山而上，到达云杉坪脚下，乘索道而上到达云杉坪。云龙雪山终年云雾缭绕，尤其是在云南的雨季更是非常难以看到玉龙雪山的面貌，听导游讲他也仅仅看过10次左右的样子。我不是个幸运的人，自然也无法看到玉龙雪山的面貌，此时天还下着小雨，从云杉坪上望去仅有白茫茫的云而已。其实从索道下和索道上看到的玉龙雪山的景色几乎是完全一致的，增加索道仅是为了增加景点的可玩性。 从索道下来后，开始乘坐电瓶车往下走，经过三站，每一站都会停下来观赏风景。三站观赏都是下山的白水河，只是分为了三段，三段景色不尽相同。河水属于碱性，河水颜色是蓝色且清澈，看起来相当漂亮，所以叫做蓝月谷。 从雪山下来后，如果时间来的及可以看到张艺谋导演的《印象丽江》，500名纳西族的群众演员的表演。由于下来晚了，仅看了个5分钟的结尾，不过可以从网上找的到视频。自从有了老谋子的《印象丽江》，带团的必须跟着演出时间走，来晚了就看不上了。 从云龙雪山出来后，前往玉水寨。玉水寨是丽江古城河水主要源头之一，是丽江东巴文化的传承圣地。景点面积不大，但属于东巴教圣地。既然属于圣地，照片就不上了。 在回丽江市的途中，导游领我们去了所谓的螺旋藻配送中心，实实在在的购物环节。螺旋藻的确属于丽江特产，野生的螺旋藻需要生长在碱性环境中，而离丽江不远的程海湖是世界第三大碱性湖泊，绝对是野生螺旋藻的产地，就是不知道市面上的所谓程海湖螺旋藻有多少是真的，因为一个程海湖再大也供应不了市场上这么多的螺旋藻。螺旋藻可以被导游描述出一堆好处，什么降血压、调肠胃、增强抵抗力等。每罐螺旋藻290元，属于批发价。机灵的我在没有了解一件商品之前是不会冲动消费的，淘宝一下发现有更便宜的，好像这个品牌也不是什么最好的，自然不买，何况想买还非得从这里买不成。 下午一点左右到达丽江古城北门后就是我们的自由时间。首先来到了黑龙潭公园，属于市民最常去的健身场所，跟济南的五龙潭公园类似，没什么特别之处，属于市民最常去的健身场所。 从黑龙潭出来后直奔丽江古城，整个游览下来丽江古城给我的感觉就是一座已经完全商业化的现代古城，几乎跟我想象中的一致。导游跟我们说过，丽江古城内其中90%是外地人，因此要在这里找回古城的感觉是比较难了，但在这里文艺一把还是可以的。 丽江古城面积虽大，但是游览下来古城内也就文艺客栈、普通餐馆、艳遇酒吧、彩条围巾、特色小饰品、银首饰、手敲鼓、写真。客栈几乎被鲜花包裹，随便找一家客栈都看上去很美很艺术，看上去像是一件艺术作品。餐馆跟古城外面的没什么两样，只是位置不同罢了。酒吧在古城中占有很大一部分席地，丽江是以艳遇闻名的，艳遇最多的地方还是得在酒吧。甚至大街上都有卖艳遇服装的，可见艳遇在游客心中的地位。彩条围巾基本拿回家都是送人和当桌布的，导游说围巾基本都产自浙江的，但在逛古城的时候买个披肩披在身上看起来还是不错的。导游说银首饰是买不得的，不知真假，不买为好。从店外面看去手敲鼓店里在边劈着腿坐着敲鼓边唱歌的女士怎么看怎么觉得别扭，但那音乐很特别。古城的街道挺多，但是走着走着却厌倦了，因为每条街道上的商品也就那些。 边看地图边问路，再配合上不太好用的百度地图向酒店返回。丽江的特色菜为三文鱼、黑山羊和腊排骨。一路上想找一家可以吃腊排骨的店，愣是没有找到一家合适的。最后累并饿的不行了，在四方街找了家德克士填饱肚子算了。 Day 4一早起来，发觉胃部不适，不知是不是昨天在丽江古城吃了什么不合适的小吃还是晚上睡觉忘记开窗户了还是有点轻微的高原反应还是这几天的劳累还是水土不服。总之身体不舒服是旅游中的大忌，我恰巧就碰到了。稍微喝了点米粥后就上路了。约莫走了一个小时，吐了一次。还好，吐完后整个身体都轻松多了。 在虎跳峡附近接上我们的香格里拉导游，地方导游自然是藏族人，叫什么措，我们的团改成叫“措团”。 今天第一个景点为虎跳峡，虎跳峡为金沙江上的一个峡谷，下车点在峡谷上方，景点为沿着护栏峡谷底部行进，到达底部后再沿上行木栈道到达出发点。下车后听到最多的就是知了叫，知了数量非常之多，以至于木栈道上都有，伸手就可以抓到。沿木栈道而下，江水并没有想象中的急，不过在最下面的观景台上声音听起来确实很大。观景台对面有一个连接两山的石桥远远看去非常漂亮。由于照片全部损坏，不能贴图了。 从虎跳峡走后下午一点抵达“心中的日月–香格里拉”，香格里拉一名源于《消失的地平线》小说中虚构的地名。香格里拉的海拔在3200米，很多人在这种海拔高度会出现高原反应。 下午一到酒店就趴在床上累的起不来了，沉睡了两个小时后，由于晚上要去藏族家里进行家访，不敢睡的太沉。醒后又在床上躺了一个多小时，一闭眼就能睡着，感觉从来没有这么累过。咬咬牙起来后，找个超时买点东西，感觉走起来也非常的累，心想，这辈子再也不会来这地方了，纯粹的花钱找罪受。 吃过晚饭后，就去了藏族家。这个是额外收费的，而且费用不低。带牦牛肉和烤鸡肉的价格为一人260元，不带的为一人180元。胃部本来就不适的自然选择了180元的价格。等到我们去了之后问起其他团的，他们带牦牛和烤鸡的价格才100元，可见我们导游能黑我们多少money。这次家访纯粹是一次商业性质的家访，大巴在藏族家门口停下来，下车后就会有人给佩戴围巾，并且有专人拿着单反拍照。 进屋后直接到二楼，屋里已经有好多个团围着屋中央而坐。坐下后，品尝酥油茶，青稞酒，还有酸奶干，甚至还有青稞拿来当瓜子吃的。 等牦牛肉和烤鸡都上齐后，节目就开始了。主持人很明显经过些专业的训练，嗓门特别大，也特别擅长带动气氛。藏族人好客的方式就是跺地板加鼓掌，声音越大越好。地板为木制的，一群人跺起脚来后声音特别大。节目大约持续了三个小时，无非就是跳舞、唱歌、跺脚、教些藏族的语言等。其实节目挺无聊，但是热闹。挺佩服主持人能够将这么庸俗的一个节目坚持了三个小时之巨。本来身体非常难受的我，在欢呼中也变得热情高涨，使劲跺脚，使劲故障，全身心投入到节目中，最后居然难受的症状全部消失了。 期间，藏民开始按照在门口时拍摄的照片找人，所有人的照片早已经洗好，并且用硬塑料膜保护着。照片20元一位，可见这是多么商业的一次活动。 到了酒店已经11点了，导游嘱咐过不能早睡，不能洗澡，否则很容易出现高原头痛等高原反应的症状。 Day 5照旧是早起，今天的行程为普达措国家森林公园。为了防止出现高原反应带来的事故，同时也可以达到赚钱的目的，导游推荐我们在香格里拉买氧气灌和红景天口服液。仙剑奇侠传三中的景天大概就取自此药名吧。普达措的最高处海拔可以达到4200米，想想这个海拔高度，为了避免高原反应，再加上昨天我本来就很不舒服，为了保险了我买了两罐，并且购买了一盒红景天口服液。后来才发现这些花费都是多余的，普达措之所以称之为森林公园，森林肯定非常之多，空气中氧气含量不会很低。走在普达措中，感觉空气比氧气瓶中的更纯净。 整个森林公园分为了三部分：属都湖、弥里塘和碧塔海。整个森林公园的面积非常之大，三个景点之间大约需要坐10-20分钟的电瓶车，电瓶车上都会有导游对景点进行讲解。 普达措应该是我这次云南旅游中最值得一去的地方，整个游览下来就感觉这个地方好似人间仙境。这里的湖面非常静，远处牦牛和马儿低着头在不停的吃草，一副悠闲自得的样子。遍地的小野花，在牛粪堆中自由的生长。从山上留下来的水就在平地上冲出一条弯弯曲曲的河，令我一直在想河道为什么是S型，这说明山上留下来的水总是缓缓的。水杉树上挂满了蜘蛛网版的白色的毛，更添加了这里的仙境色彩，据说是寄生在水杉上的一种植物。山间偶尔会见到特别小的小松鼠在树下找点吃的，然后一会就不见了，仿佛刚才见到的不是真的。这中景色中有身临其境方能体会。 属都湖为一个天然湖泊，远处就能看到牦牛和马儿。弥里塘是整个森林的海拔最高处，地方非常平静，视野相当开阔，牦牛和马儿在远处的草地上吃着小花小草。碧塔海是整个景区的核心景点，我们选择徒步4公里的路程，沿着碧塔海从湖的一边走到另外一边。在漫步中慢慢体会碧塔海的迷人之处。 从普达措出来后，藏族导游跟我们说拜拜。在香格里拉吃过午饭后，开始马不停蹄的往丽江赶。期间路过“长江第一湾”景点，在观景楼上高高看去，除了恐高之外没有别的感觉。前文已经介绍过“长江第一湾”景点的一些坑况，不再对坑进行过多描述。 下午三点多到达丽江的酒店，在酒店放下东西后直接前往大理古城，这里酒店离的大理古城的南门算是比较近。上次来没有吃上当地特色小吃腊排骨，在南门外找了家店品尝一番，吃起来跟我想象中的一致，除了有点排骨味之外还略带腊味。 吃完腊排骨后已经是晚上8点左右的时间，上次逛丽江古城主要是逛了北门附近多些，这次逛上次没有逛过的南门附近。南门附近与北门不同的是，古城南边有很多的客栈，但是这里的胡同和客栈却略显凄凉，人少了需要，客栈也没有那么文艺。转了几个胡同后，在转了一个又一个弯后的胡同中找到了大名鼎鼎的木府。时间已晚，看着地图加导航接回酒店睡下了。 Day 6今天的行程为从丽江-大理-昆明。上次去大理仅是路过，这次是去体验，虽然时间也仅仅有一天。 下关风，上关花，苍山雪，洱海月。风花雪月便是大理景色的最真实写照，大理城就被下关、上关、苍山和洱海包裹着。本次大理执行风花雪月均为体会到，甚至连洱海边都未到过，仅仅是去了洱源小西湖，也勉强算是到过洱海吧。 快到洱源小西湖的路上遇到了集市，集市上有用竹篓子卖小猪，看起来挺滑稽挺有意思。 进入洱源小西湖后导游会先带领到购物区，都是些当地的特产，价格也不贵。刚坐上木船会看到鱼鹰表演，木船绕着西湖转上一圈，欣赏西湖的美景。 下船后，会去餐馆白族的歌舞表演，并品尝白族三道茶。三道茶“头苦、二甜、三回味”，喝起来味道怪怪的。从歌舞表演厅出来后就直接出西湖了。 从西湖出来后，直接赶到白银购物中心十八里铺。大理是白银产地，在大理旅游购物自然少不了白银。一下车，在十八里铺的停车场已经停了几十辆大巴，显然都是拉来购物的。导游象征性的给我们介绍了下大理白族的房屋布置和婚房布置，顺便看了下几个人在手工制作白银，最后就带我们到白银购物大厅里了，然后导游就撤了。留下来选择购买白银的时间非常的长，有一个半小时之久。 艰难的等待后，在十八里铺吃完午饭后，直接奔向大理古城。由于南门无法停车，大巴在离大理古城南门2里地的停车场停下，导游给我们游玩的时间是1个半小时，偌大一个大理古城仅有一个半小时的游玩时间，而一个十八里铺的购物大厅却也预留了差不多时间。 大理古城相比丽江古城更现代化，古城内有相当多的现代商店，甚至都有超市的存在，除了建筑风格比较古之外，其他都是新的。古城的北面，直接就成了居民区了。不过相比丽江古城，我更喜欢大理古城，因为这里购物更方便，而且街道也建设的更加规整，不会出现迷路的情况。里面的物品价格也更加便宜，整个古城更像是城市中的步行街。 从大理古城走后，直接赶回昆明，到达昆明酒店已经是晚上10点了。 Day 7早餐后赶往中国名牌企业七彩云南，顾名思义就是购物了。导游一而再再而三的强调，导游前五天是天使，第六天就是魔鬼。“什么都可以落在宾馆，唯独你们的武器–钱包不能落在宾馆”。没有比这个更直白的了。 七彩云南园区内共有七个展馆，首先带领我们去的是翡翠馆。翡翠主要产自缅甸和云南接壤地带，自然会成为云南的特产代表。导游带领我们体验一把模拟的翡翠形成过程的动画，紧接着就会带领我们到达翡翠销售大厅。购买翡翠的时间预留的非常充足，期间为了打发时间跑去了孔雀园转了一圈。整个行程中导游是只字未提孔雀园，但是孔雀园却是七彩云南园区中最适合游玩的地方。孔雀园内有几千只孔雀，植物的种类也非常多，还有一面非常大的花墙，看上去也是相当漂亮。 集合后又餐馆了普洱茶馆，直接被关到屋里面品茶，品完熟茶品生茶，品完茶后自然是买茶。 在七彩云南吃过午餐后，大巴赶往天造奇观的石林。去石林的路上，从未看到过石林内形状的石头，可是到了石林内部却到处都是竖状的石头，一直对竖状石头的成因感到颇为好奇。一个5A级的景区仅仅预留了一个半小时的时间游览，而且还包含了排队等电瓶车的时间。 石林分大石林和小石林，大石林的石头看上去更高大一些，小石林的石头看上去稍微矮小一些。听景区的导游介绍，到了大石林找到“石林”二子后拍个照后就相当于花掉了100块钱的门票，到了小石林后找到“阿诗玛的化身”后就相当于又花掉了70元的门票。 到达大石林后找到了“石林”二字拍完照后，没有听导游的话直接原路返回坐电瓶车到小石林，而是出于好奇在石林内部转起迷宫来，石林面积应该不小，至少我在内部至少走了40分钟后也没有走多少，而且重要的是我彻底迷路了，找不到入口也找不到出口。一看时间已经过去了一大半，还没到小石林，就抓紧时间往外赶，转了半天迷宫终于出来了。 排队挤上电瓶车到达小石林，小石林石头之间的间隙要大很多，石头之间还有草地，走在里面不会存在迷路的可能。拐个弯找到阿诗玛的化身拍个照抓紧时间往出口赶。阿诗玛的化身石我怎么看也没看出像一个人来，石头背后的故事就不深究了。 从石林归来后，又回到了七彩云南。这次购物的重点已经不再是翡翠，而是黄龙玉。黄龙玉产自云南，发现较晚，因此价格较便宜。我直接走向黄龙玉旁边的土特产超市，从里面买点鲜花饼、当地咖啡、普洱茶等，价格还算亲民，拿回来送朋友是个很不错的选择。 在七彩云南吃过地道的过桥米线后直接入住温泉酒店，跟陪伴我们六天的导游和司机师傅说再见后，在温泉酒店做了个鱼疗后，收拾所有的物品打包准备回温馨的家。 Day 88点20的飞机，五点半开始往机场赶。12点左右已经回到了温馨的家。 杂方向感我无论到哪里都喜欢找找方向感，看看那是南，我是属于那种第一次到一个地方后只要认错了方向就再也调不过来的那种人。这一周的时间我却几乎没有掉过方向来，以至于到后来干脆就不考虑方向了，管他东西南北，只要能回得了酒店，只要能找到大巴就OK了。 民族本次行程中的几个城市分属于不同的少数民族，风俗习惯、文化传统和建筑风格各有特点。昆明和楚雄是彝族人居多，大理是白族自治州，丽江是纳西族的聚居地，香格里拉主要是藏族人。最有意思的是，这几个少数名族对男女的称呼各有特色。彝族人对男性称呼为“阿黑哥”，对女性称呼为“阿诗玛”；白族分别称呼为“金花”和“阿鹏”；纳西族分别称呼为“胖金妹”和“胖金哥”；藏族分别称呼为“扎西”和“卓玛”。 尾本来是计划写成散文的，文笔有限，写着写着就写成记叙文了。要是有足够的文采，绝对可以写出比《从百草园到三味书屋》中需要背诵的片段更优美的文字，可惜不是作家的料，仅能码字而已。 旅游不易，码字更难，且行且珍惜。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[coolshell博客解谜题游戏]]></title>
      <url>%2Fpost%2Fcoolshell_puzzle%2F</url>
      <content type="text"><![CDATA[有些闲暇时间了解了下酷壳的谜题活动，共10道题，每道题都不是非常简单，我这里参考着攻略做了下。 字符替换题我编写的C++语言程序 1234567891011121314151617181920212223242526272829#include &lt;stdio.h&gt;#include &lt;string.h&gt;char change(char input)&#123; char *after = "abcdefghijklmnopqrstuvwxyz"; char *before = "pvwdgazxubqfsnrhocitlkeymj"; for (int i=0; i&lt;strlen(before); i++) &#123; if (before[i] == input) &#123; return after[i]; &#125; &#125; return input;&#125;int main()&#123; char *input = "Wxgcg txgcg ui p ixgff, txgcg ui p epm. I gyhgwt mrl lig txg ixgff wrsspnd tr irfkg txui hcrvfgs, nre, hfgpig tcm liunz txg crt13 ra \"ixgff\" tr gntgc ngyt fgkgf."; char *output = new char[strlen(input) + 1]; for (int i=0; i&lt;strlen(input); i++) &#123; output[i] = change(input[i]); &#125; output[strlen(input)] = '\0'; printf("%s\n", output); return 1;&#125; 好久没有用shell了，又写了个shell版本的解题方法，该问题可能有更简单的shell解决办法，我这里肯定写复杂了。 123456789101112131415161718192021222324252627282930#!/bin/bashresult=''function change()&#123; result=$1 before='pvwdgazxubqfsnrhocitlkeymj' after='abcdefghijklmnopqrstuvwxyz' for ((i=0; i &lt;= $&#123;#before&#125;; i++)) do if [[ $&#123;before:$&#123;i&#125;:1&#125; = $&#123;1&#125; ]] then result=$&#123;after:$&#123;i&#125;:1&#125; break fi done&#125;input='Wxgcg txgcg ui p ixgff, txgcg ui p epm. I gyhgwt mrl lig txg ixgff wrsspnd tr irfkg txui hcrvfgs, nre, hfgpig tcm liunz txg crt13 ra "ixgff" tr gntgc ngyt fgkgf.'output=''j=0while [ "$j" -le $&#123;#input&#125; ]do change "$&#123;input:$&#123;j&#125;:1&#125;" output="$&#123;output&#125;$&#123;result&#125;" j=$((j+1))doneecho $&#123;output&#125; 关于rol13的转码可以采用rot13这个网址来在线转码。 穷举变量题该题需要不断的请求url来获取最终的网址，我这里写一个shell脚本来穷举。 12345678#!/bin/bashres=2014while [ $&#123;#res&#125; &gt; 0 ]dores=`curl -s "http://fun.coolshell.cn/n/$&#123;res&#125;"`echo $resdone 得到答案tree 参考游戏页面CoolShell puzzle game 攻略我也不产生代码 – Coolshell 谜题一游]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[修改eclipse主题]]></title>
      <url>%2Fpost%2Feclipse_theme%2F</url>
      <content type="text"><![CDATA[Eclipse默认提供的主题过于刺眼，自己比较习惯黑色背景色。本文分享下修改Eclipse主题的步骤。我所使用的Eclipse版本为Kepler。相关的说明在插件的网站都有相关说明，本文不再赘述。安装完插件后的效果图如下： 安装Eclipse Color Theme插件该插件仅会更新编辑栏中的背景颜色和字体颜色等信息，不会修改Eclipse的整体主题颜色。该插件网址为：https://github.com/eclipse-color-theme/eclipse-color-theme。 安装Eclipse UI Themes主题该插件会更新除编辑栏之外的其他Eclipse背景色等信息，其Github地址为：http://eclipsecolorthemes.org/。 相关下载为了方便使用，我这里备份了插件放在百度网盘共享，相关插件下载地址：http://pan.baidu.com/s/1eQnDI86]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下的网卡速度提升方案]]></title>
      <url>%2Fpost%2Flinux_netcard_promote%2F</url>
      <content type="text"><![CDATA[[TOC] 本文研究了Linux下的一些网卡调优，用于提升网卡的性能。 下文中的所有关于网络的参数可以在/etc/sysctl.conf文件中修改，如果没有相应的参数，可以添加。 查看相应参数在当前运行机器的值可以通过/proc/sys/net/目录下的文件内容查看，也可以对该目录下相应文件的值进行修改，但是由于/proc目录下的文件全部位于内存中，修改的值不会保存到下次开机时。因此要修改参数的值可以通过修改/etc/sysctl.conf文件来完成。 还可以通过sysctl -a命令来查看所有的系统配置参数。 IP协议相关参数配置 net.ipv4.ip_default_ttl：设置从本机发出的ip包的生存时间，参数值为整数，范围为0～128，缺省值为64。如果系统经常得到“Time to live exceeded”的icmp回应，可以适当增大该参数的值，但是也不能过大，因为如果你的路由的环路的话，就会增加系统报错的时间。 TCP协议相关参数配置TCP链接是有很多开销的，一个是会占用文件描述符，另一个是会开缓存，一般来说一个系统可以支持的TCP链接数是有限的。 TCP常规参数 /proc/sys/net/ipv4/tcp_window_scaling：设置tcp/ip会话的滑动窗口大小是否可变。参数值为布尔值，为1时表示可变，为0时表示不可变。Tcp/ip 通常使用的窗口最大可达到65535字节，对于高速网络，该值可能太小，这时候如果启用了该功能，可以使tcp/ip滑动窗口大小增大数个数量级，从而提高数据传输的能力。 net.core.rmem_default：默认的接收窗口大小。 net.core.rmem_max：接收窗口的最大大小。 net.core.wmem_default：默认的发送窗口大小。 net.core.wmem_max：发送窗口的最大大小。 /proc/sys/net/core/wmem_max。最大的TCP发送数据缓冲区大小。 /proc/sys/net/ipv4/tcp_timestamps。时间戳在(请参考RFC 1323)TCP的包头增加10个字节，以一种比重发超时更精确的方法（请参阅 RFC 1323）来启用对 RTT 的计算；为了实现更好的性能应该启用这个选项。 配置KeepAlive参数这个参数的意思是定义一个时间，如果链接上没有数据传输，系统会在这个时间发一个包，如果没有收到回应，那么TCP就认为链接断了，然后就会把链接关闭，这样可以回收系统资源开销。（注：HTTP层上也有KeepAlive参数）对于像HTTP这样的短链接，设置一个1-2分钟的keepalive非常重要。 net.ipv4.tcp_keepalive_time：当keepalive打开的情况下，TCP发送keepalive消息的频率。缺省值为7200,即两小时，建议将其更改为1800。 net.ipv4.tcp_keepalive_probes：TCP发送keepalive探测以确定该连接已经断开的次数。(默认值是9，设置为5比较合适)。 net.ipv4.tcp_keepalive_intvl：探测消息发送的频率，乘以tcp_keepalive_probes就得到对于从开始探测以来没有响应的连接杀除的时间。(默认值为75秒，推荐设为15秒) 配置建立连接参数 /proc/sys/net/ipv4/tcp_syn_retries：设置开始建立一个tcp会话时，重试发送syn连接请求包的次数。参数值为小于255的整数，缺省值为10。假如你的连接速度很快，可以考虑降低该值来提高系统响应时间，即便对连接速度很慢的用户，缺省值的设定也足够大了。 net.ipv4.tcp_retries1：建立一个连接的最大重试次数，默认为3,不建议修改。 配置关闭连接参数主动关闭的一方进入TIME_WAIT状态，TIME_WAIT状态将持续2个MSL(Max Segment Lifetime)，默认为4分钟，TIME_WAIT状态下的资源不能回收。有大量的TIME_WAIT链接的情况一般是在HTTP服务器上。 net.ipv4.tcp_retries2：普通数据的重传次数，在丢弃激活(已建立通讯状况)的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒)。 net.ipv4.tcp_tw_reuse：该文件表示是否允许重新应用处于TIME-WAIT状态的socket用于新的TCP连接。可以将其设置为1 net.ipv4.tcp_tw_recycle：打开快速TIME-WAIT sockets回收。默认关闭，建议打开。 net.ipv4.tcp_fin_timeout：在一个tcp会话过程中，在会话结束时，A首先向B发送一个fin包，在获得B的ack确认包后，A就进入FIN WAIT2状态等待B的fin包然后给B发ack确认包。这个参数就是用来设置A进入FIN WAIT2状态等待对方fin包的超时时间。如果时间到了仍未收到对方的fin包就主动释放该会话。参数值为整数，单位为秒，缺省为180秒，建议设置成30秒。 网卡的参数设置调整网卡的txqueuelentxqueuelen的涵义为网卡的发送队列长度，可以通过ifconfig命令找到网卡的txqueuelen参数配置，默认为1000,建议将其更改为5000。 网卡的中断设置网卡的读写是通过硬件的中断机制来实现的，默认网卡是中断在cpu的内核0上。CPU0非常重要，CPU0具有调整功能，如果CPU0利用率过高，其他cpu核心的利用率也会下降。因此可以考虑将linux的网卡中断绑定到其他的cpu内核上。 可以通过/proc/interrupt文件内容来查看网卡在cpu核的中断情况。12345678910111213141516171819202122232425262728 CPU0 CPU1 CPU2 CPU3 0: 16 0 0 0 IO-APIC-edge timer 1: 17144 2552 2312 2011 IO-APIC-edge i8042 5: 0 0 0 0 IO-APIC-edge parport0 8: 1 0 0 0 IO-APIC-edge rtc0 9: 0 0 0 0 IO-APIC-fasteoi acpi 19: 75011 144479 28826 16212 IO-APIC-fasteoi ata_piix, ata_piix 23: 214627 1 0 2 IO-APIC-fasteoi ehci_hcd:usb1, ehci_hcd:usb2 41: 0 0 0 0 PCI-MSI-edge xhci_hcd 42: 245675 11 17 6 PCI-MSI-edge eth0 43: 18 6 0 0 PCI-MSI-edge mei 44: 423481 80408 67559 54037 PCI-MSI-edge i915 45: 268 488 112 16 PCI-MSI-edge snd_hda_intelNMI: 21 18 23 18 Non-maskable interruptsLOC: 7076902 6761203 8711912 9042018 Local timer interruptsSPU: 0 0 0 0 Spurious interruptsPMI: 21 18 23 18 Performance monitoring interruptsIWI: 0 0 0 0 IRQ work interruptsRTR: 2 0 0 0 APIC ICR read retriesRES: 365564 155654 52242 40754 Rescheduling interruptsCAL: 12775 15182 23713 23003 Function call interruptsTLB: 31028 26721 21584 25548 TLB shootdownsTRM: 0 0 0 0 Thermal event interruptsTHR: 0 0 0 0 Threshold APIC interruptsMCE: 0 0 0 0 Machine check exceptionsMCP: 32 32 32 32 Machine check pollsERR: 0MIS: 0 可以看到eth0的中断是在CPU0上，可以通过/proc/irq/42/smp_affinity文件查看eth0默认的中断分配情况，文件的内容为1,对应二进制为0001,对应的为CPU0。 要想修改中断分配方式，需要先停掉IRQ自动调节的服务进程。12/etc/init.d/irqbalance stopecho &quot;2&quot; &gt; /proc/irq/42/smp_affinity 这里的2表示将中断分配到CPU1上。 双网卡负载均衡两个网卡共用一个IP地址，中断用两个核，效率可以提升一倍。 服务器的其他设置修改服务器启动级别可以通过runlevel命令查看机器的启动级别，带图形界面的启动级别为5。可以通过init 3来切换到启动级别3;可以修改/etc/inittab文件中id:5:initdefault:来修改默认级别。 修改系统的文件描述符数如果网络链接较多，可以修改每个进程打开的最大文件描述符数目，默认为1024.可以通过ulimit -a查看，可以通过ulimit -n 32768来修改。 参考网页 性能调优攻略 Linux 内核网络参数配置资料 Linux 多核下绑定硬件中断到不同 CPU（IRQ Affinity） 计算 SMP IRQ Affinity 提高 Linux 上 socket 性能]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[浪潮之巅读书笔记]]></title>
      <url>%2Fpost%2Fon_top_of_tides%2F</url>
      <content type="text"><![CDATA[我的大部分读书时间都花在了技术类书籍上，对于扯淡类和鸡汤类的书籍不是我的最爱。本书受到了很多互联网从业人员的推荐，对于技术人员推荐的书籍感觉还是比较靠谱的，因此试着一读，读完之后果然收获颇多。吴军博士的很多观点都相当理性，很多对未来的预测也都在今天得以验证了。文章行为流畅，观点鲜明，能够窥见作者的写作功底。文中的一些观点非常值得记录和学习，本文主要是记录文中观点的摘要，并体现部分自己的理解。 计算机行业发展规律## 摩尔定律 摩尔定律：每18个月，计算机等IT产品的性能会翻一番，或者说相同性能的计算机等IT产品，每18个月价钱会降一半。 摩尔定律对行业发展的影响主要体现在以下几个方面： IT公司必须在比较短的时间内完成下一代产品的开发。天下武功唯快不破，在互联网行业特别合适。 由于有了强有力的硬件支持，以前都不敢想的应用会不断涌现。手机行业正在深受摩尔定律的影响。 各个公司现在的研发必须针对多年后的市场。不知道google是否在研究PC版的操作系统来取代Windows了，不知道苹果公司内部是否在研究iphone10了？ 安迪-比尔定律 安迪-比尔定律：即比尔要拿走安迪所给的。安迪是原英特尔公司 CEO 安迪·格鲁夫（Andy Grove），比尔就是微软的创始人比尔·盖茨。在过去的二十年里，英特尔处理器的速度每十八个月翻一番，计算机内存和硬盘的容量以更快的速度在增长。但是，微软的操作系统等应用软件越来越慢，也越做越大。所以，现在的计算机虽然比十年前快了一百倍，运行软件感觉上还是和以前差不多。而且，过去整个视窗操作系统不过十几兆大小，现在要几千兆，应用软件也是如此。虽然新的软件功能比以前的版本强了一些，但是，增加的功能绝对不是和它的大小成比例的。因此，一台十年前的计算机能装多少应用程序，现在的也不过装这么多，虽然硬盘的容量增加了一千倍。更糟糕的是，用户发现，如果不更新计算机，现在很多新的软件就用不了，连上网也是个问题。而十年前买得起的车却照样可以跑。 安迪-比尔定律把原本属于耐用消费品的电脑、手机等商品变成了消费性商品，刺激着整个IT领域的发展。 反摩尔定律 反摩尔定律：Google的前CEO埃里克·施密特提出：如果你反过来看摩尔定律，一个IT公司如果今天和18个月前卖掉同样多的、同样的产品，它的营业额就要降一半。IT界把它称为反摩尔定律。反摩尔定理对于所有的IT 公司来讲，都是非常可悲的，因为一个IT公司花了同样的劳动，却只得到以前一半的收入。反摩尔定理逼着所有的硬件设备公司必须赶上摩尔定理规定的更新速度。 信息产业的规律性70-20-10定律 当某个领域发展成熟后（而不是群雄争霸时期），一般在全球容不下三个以上的主要竞争者。这个行业一定有一个老大，斯库利把它比喻成一个猴王，它是这个行业的主导者。毫无疑问，它虽然想顺顺当当地统领好整个行业，就像猴王想让猴子们永远臣服一样，但是，它一定会遇到一两个主要的挑战者，也就是老二（也许还有一个老三）。剩下来的是一大群小商家，就像一大群猴子。老大是这个领域的主导者，不仅占据着超过一半，通常是百分之六七十的市场，并且制定了这个领域的游戏规则。老二有自己稳定的百分之二三十的市场份额，有时也会挑战老大并给老大一些颜色看看，但是总的来讲是受老大欺负的时间多。剩下的一群小猴子数量虽然多，但是却只能占到百分之十甚至更少的市场，它们基本上唯老大马首是瞻。老大总是密切注视着老二，并时不时地打压它，防止它做大。老大和老二通常都不会太在意剩下的小企业，这样就让这一群小的企业能有挣一些小钱的地方。 诺维格定律 谷歌研究院院长彼得.诺威格博士说，当一个公司的市场占有率超过50% 以后，就不要再指望在市场占有率上翻番了。 基因决定定律 一个在某个领域特别成功的大公司一定已经被优化得非常适应这个市场，它的文化、做事方式、商业模式、市场定位等等已经非常适应，甚至过分适应自己传统的市场。这使得该公司获得成功的内在因素会渐渐地、深深地植入该公司，可以讲是这个公司的基因。 基因对于一个公司非常的重要，会影响到公司的整个发展历程。济南的软件公司都是些传统行业的软件公司，在过去十年的时间中发展还算可以。因为行业软件是销售为王，只要销售搞的好，只要不是特别烂的技术都能混过去。现在随着互联网的兴起，很多企业开始转型搞互联网应用，却没有一家搞的起来，到现在济南都没有一家真正意义上的互联网公司，如果有请你告诉我。这些从本质上来说还是基因决定定律的影响。 最佳商业模式Google的广告系统 在这台印钞机里，运营的成本就是数据中心的费用和带宽的费用，而间接的成本则是打造和改进这个印钞机的研发费用。在这台印钞机中，自动化程度必须到达一个阈值，它才能自动运转起来。而当它的自动化越高，成本就越低。 Ebay 和亚马逊的在线市场提供在线交易和支付平台，买卖双方自由交易，赚取手续费用和少量提成。中国的淘宝也基本属于这种模式。 戴尔的虚拟工厂 一个传统的制造业需要通过产品设计、原料采购、仓储运输、加工制造、订单处理、批发经营和零售七个环节才能收回投资、获得利润。戴尔将上述七个环节减少到两个，仅留订单处理和零售。 腾讯的虚拟物品和服务销售虚拟商品商业模式必须解决好两个问题：虚拟商品的使用价值和虚拟社交网站中的用户不但是虚拟商品的消费者，还是它们的创造者。解决好上述两个问题，只要累计起足够多的用户基数，虚拟商品就成了社交网站的印钞机。 下一个Google微软是一家软件公司，百度是一家区域性的互联网公司，而Google更多地是一家科技公司。 下一个Google不可能是搜索公司，而且不太可能是现在意义上依靠广告挣钱的互联网公司，因为这个互联网广告产业不够养活一个像Google这么大的公司。 替代能源和电池在近期很难诞生一个Google这样的公司。 生物和医疗技术有全社会的需求，但是创新周期特别长，加上法律上的风险，不可能在短期内核诞生下一个Google。 在中国和一些亚洲国家，电子商务的潜力可以早就出Google这样的大型新型公司。 如果通过云计算诞生一两个千亿美元的公司，首先会是Google自己，其次是控制开发平台的Facebook，而不会是新面孔。 相关下载非常精美的读书笔记PPT]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下父进程定期杀死超时子进程的例子]]></title>
      <url>%2Fpost%2Flinux_kill_child_process%2F</url>
      <content type="text"><![CDATA[在Linux下会父进程通过fork()出的子进程可能会由于某种原因死锁或睡眠而无法终止，这时候需要父进程杀死子进程。本程序是父进程检测到子进程运行一段时间后杀死子进程的例子。 父进程的检测代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/wait.h&gt;int main(int argc, char *argv[])&#123; int pid = fork(); if (pid &gt; 0) &#123; // parent process int count = 0; while (1) &#123; pid_t result = waitpid(pid, NULL, WNOHANG); if (result == 0) &#123; printf("child process is running\n"); count++; if (count &gt;= 60 * 60) // one hour &#123; kill(pid, 9); // kill child process &#125; &#125; else if (result == pid) &#123; printf("child process has exit\n"); break; &#125; else &#123; printf("result=%d\n", result); &#125; sleep(1); &#125; &#125; else if (pid == 0) &#123; // child process execv("/home/kuring/source/child", NULL); _exit(1); &#125; else &#123; printf("fork() error\n"); return -1; &#125; return 0; &#125; 子进程调用execv()函数执行的child代码如下： 123456789101112#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main(int argc, char *argv[])&#123; while (1) &#123; sleep(1); &#125; return 1;&#125; 例子比较简单，不作过多解释。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我目前的理想工作状态]]></title>
      <url>%2Fpost%2Fmy_perfect_work%2F</url>
      <content type="text"><![CDATA[在外地出差，趁着午休时间躺在机房走廊的地板上想小憩一会却怎么也进入不了状态。突然写着可以写写我心中目前的理想工作状态。随着年龄、阅历和知识水平的增长，每个人心中理想的工作状态也会在变。我现在的理想工作状态跟我刚毕业时肯定是有区别的，刚毕业那会初入茅庐，对公司的很多事情还是懵懂，自然期望找个高手带一带。 我期望加入一家技术为王的公司。 技术才是科技公司真正的核心竞争力，而非销售和公关能力。从硅谷中做大做强的公司哪个不是以技术作为领先竞争对手的筹码，十几年前的google的崛起、雅虎的衰落，近几年Github的火爆无不是技术因素在其决定作用。在中国软件行业还未发展成熟，觉大多数企业仍然在以销售为核心盈利方式的畸形发展方式中，但技术为王绝对是个趋势，这个不是靠关系靠政策可以改变的事实，这个是大势所趋、历史必然。 我不奢望技术为王的公司中牛人辈出但是期望能多几个牛人就多几个牛人，至少不期望在公司中见到一看就没有做技术潜质的同事（一个人适不适合做技术有时候是可以看出来的）。 我期望加入一家互联网行业的公司。 软件行业按照产品类型大致可以分三类：外包、卖产品和做平台。 外包公司完全以订单为导向，往往涉及到复杂的业务逻辑，技术含量低，自然不在我的考虑范围内。 我之前从事的公司都是在卖产品的公司，往往卖产品的公司需要有一定的技术含量，但是技术不能决定公司发展，这一类公司是以销售为导向的公司，一套产品往往可以卖上多年，甚至十几年也不奇怪。 做平台的公司往往不需要过多的销售，大多数是以用户量为王的，而要想拉拢用户到平台上往往需要靠技术或销售取胜，而技术因素相对更加关键。以Dota对战平台为例，前几年比较火爆的是浩方和VS，现在成了11对战平台的天下了。11对战平台后来者居上就是因为11对战平台率先推出了天梯模式和路人模式，从而落下了竞争对手一大截。互联网行业属于第三类，而济南几乎找不到这类公司。就算有一些这样的公司，往往也是以销售为导向的公司。韩都衣舍也有自己的网站，但是流量应该没有淘宝大，估计网站技术上的投入也一般般。银座集团在电商火爆了后也搞了个网上商城，单从界面看可跟苏宁拉在同一个档次，但效果如何影响怎么样可想而知，估计技术都是外包的，仅是领导拍拍脑袋的产物。我实在想不到在济南有什么互联网公司了，如果你知道请告诉我。 我期望加入一家创业型的公司。 我现在还不具备单独创业的实力，但我一直梦想能够加入一家有梦想有激情的创业公司。加入创业公司就意味着不会清闲，我只期望每天能够过的充实并快乐着。《黑客与画家》中提到快速致富的手段就是加入一家创业公司，在美国如此，在中国亦如此。 总结一下，我想加入一家技术为王的互联网行业的创业公司。如果你知道在济南有哪家公司可以满足我的要求，请告诉我，谢谢！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux更改网卡名称]]></title>
      <url>%2Fpost%2Flinux_change_netcardname%2F</url>
      <content type="text"><![CDATA[本实验为在虚拟机环境中实验，操作系统为Red Hat Enterprise6.0 32位，当前网卡列表如下： 1234567891011121314151617181920[root@localhost ~]# ifconfig eth1 Link encap:Ethernet HWaddr 00:0C:29:8C:58:06 inet addr:192.168.124.140 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe8c:5806/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:123 errors:0 dropped:0 overruns:0 frame:0 TX packets:57 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:18770 (18.3 KiB) TX bytes:11684 (11.4 KiB) Interrupt:19 Base address:0x2024eth2 Link encap:Ethernet HWaddr 00:50:56:3F:B3:90 inet addr:192.168.124.141 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::250:56ff:fe3f:b390/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:111 errors:0 dropped:0 overruns:0 frame:0 TX packets:43 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:16101 (15.7 KiB) TX bytes:10343 (10.1 KiB) Interrupt:19 Base address:0x20a4 目的为将网卡eth1更改为eth0，将eth2更改为eth3。 修改grub.conf文件在文件中内核启动时增加biosdevname=0选项。修改后的文件内容如下： 12345678default=0timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle Red Hat Enterprise Linux (2.6.32-71.el6.i686) root (hd0,0) kernel /vmlinuz-2.6.32-71.el6.i686 ro root=/dev/mapper/VolGroup-lv_root rd_LVM_LV=VolGroup/lv_root rd_LVM_LV=VolGroup/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=zh_CN.UTF-8 KEYBOARDTYPE=pc KEYTABLE=us nomodeset crashkernel=auto rhgb quiet biosdevname=0 initrd /initramfs-2.6.32-71.el6.i686.img 更改网卡配置文件内容和文件名称在/etc/sysconfig/network-scripts目录中将原有的网卡配置文件ifcfg_Auto_eth1和ifcfg_Auto_eth2更改为ifcfg_eth0和ifcfg_eth3，同时修改文件的内容，将文件的内容中的网卡设备名称进行替换。替换后的文件ifcfg_eth0内容如下： 123456789101112TYPE=EthernetBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=yesIPV6INIT=noNAME=&quot;Auto eth0&quot;UUID=995d037e-3b65-4490-a1fa-f26f6abf066dONBOOT=yesHWADDR=00:0C:29:8C:58:06PEERDNS=yesPEERROUTES=yesDEVICE=eth0 删除70-persistent-net.rules文件该文件存在于/etc/udev/rules.d目录下。该文件如果不存在，开始时会自动创建，里面包含了网卡名称的配置信息。 在修改完上述内容后重新启动机器配置就修改过来了,修改完成之后的网卡配置如下： 1234567891011121314151617181920[root@localhost rules.d]# ifconfig eth0 Link encap:Ethernet HWaddr 00:0C:29:8C:58:06 inet addr:192.168.124.140 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::20c:29ff:fe8c:5806/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:87 errors:0 dropped:0 overruns:0 frame:0 TX packets:75 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:17174 (16.7 KiB) TX bytes:14520 (14.1 KiB) Interrupt:19 Base address:0x2024eth3 Link encap:Ethernet HWaddr 00:50:56:3F:B3:90 inet addr:192.168.124.141 Bcast:192.168.124.255 Mask:255.255.255.0 inet6 addr: fe80::250:56ff:fe3f:b390/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:72 errors:0 dropped:0 overruns:0 frame:0 TX packets:76 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:14164 (13.8 KiB) TX bytes:14955 (14.6 KiB) Interrupt:19 Base address:0x20a4]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Redhat安装完成之后的设置]]></title>
      <url>%2Fpost%2Fredhat_setup_base%2F</url>
      <content type="text"><![CDATA[[TOC] ＃ 安装kdevelop 确保可以上网，这里采用yum的安装方式进行安装。首先执行命令：yum install kdevelop，会出现如下提示：123456Loaded plugins: rhnplugin, securityThis system is not registered with RHN.RHN support will be disabled.file:///mnt/file/rh5/Cluster/repodata/repomd.xml: [Errno 5] OSError: [Errno 2] 没有那个文件或目录: &apos;/mnt/file/rh5/Cluster/repodata/repomd.xml&apos;Trying other mirror.Error: Cannot retrieve repository metadata (repomd.xml) for repository: Cluster. Please verify its path and try again 出现上述错误是由于redhat没有注册，所有不能使用它自身的源进行更新，可以更换为CentOS系统的源进行更新，操作步骤为：1、进入/etc/yum.repos.d/目录。在命令行输入：wget http://docs.linuxtone.org/soft/lemp/CentOS-Base.repo。2、ls 一下，会看到一个文件名为CentOS-Base.repo的文件。3、将原来的文件rhel-debuginfo.repo改名为rhel-debuginfo.repo.bak。4、将CentOS-Base.repo改名为rhel-debuginfo.repo 再次运行命令：yum install kdevelop，就可以安装kdevelop了。 安装过程中遇到了需要的pcre包无法从centos的源中下载的问题，解决方法为根据yum命令无法下载的包，在google中搜索，下载包然后再redhat上用rpm的升级命令来安装。具体下载网址为：电子科技大学星辰工作室开源镜像服务。 rpm相关命令：安装一个包：rpm -ivh升级一个包：rpm -Uvh移走一个包：rpm -e 安装konsole安装上kdevelop后在执行程序的时候会提示/bin/sh:konsole:command not found。执行yum install kdebase命令来安装konsole。 配置ssh服务修改ssh服务的配置文件/etc/ssh/sshd_config文件，将文件中的#PasswordAuthentication yes注释打开。修改ssh服务的配置文件/etc/ssh/sshd_config文件，将文件中的PermitRootLogin no更改为yes。这样即可以用ssh工具连接到该机器。 xmanager连接配置该部分参考文档的网址为：http://blog.csdn.net/gltyi99/article/details/6141972 修改/usr/share/gdm/defaults.conf文件的权限，默认权限为444，chmod 700 /usr/share/gdm/defaults.conf。 在/usr/share/gdm/defaults.conf文件的末尾添加如下内容： 123456Enable=trueDisplaysPerHost=10Port=177AllowRoot=trueAllowRemoteroot=trueAllowRemoteAutoLogin=false 修改/etc/gdm/custom.conf文件 12[xdmcp]Enable=1 修改/etc/inittab文件，不修改原来的设置，在文件的最后增加一行： 1x:5:respawn:/usr/sbin/gdm 修改/usr/share/gdm/defaults.conf文件，将其中的 12345678910111213141516171819202122[security]# Allow root to login. It makes sense to turn this off for kiosk use, when# you want to minimize the possibility of break in.AllowRoot=true# Allow login as root via XDMCP. This value will be overridden and set to# false if the /etc/default/login file exists and contains# &quot;CONSOLE=/dev/login&quot;, and set to true if the /etc/default/login file exists# and contains any other value or no value for CONSOLE.AllowRemoteRoot=false# This will allow remote timed login.AllowRemoteAutoLogin=false# 0 is the most restrictive, 1 allows group write permissions, 2 allows all# write permissions.RelaxPermissions=0# Check if directories are owned by logon user. Set to false, if you have, for# example, home directories owned by some other user.CheckDirOwner=true# Number of seconds to wait after a failed login#RetryDelay=1# Maximum size of a file we wish to read. This makes it hard for a user to DoS# us by using a large file.#UserMaxFile=65536 AllowRemoteRoot=false更改为AllowRemoteRoot=true。 修改/etc/securetty文件，在文件底部添加如下内容： 12345pts/0pts/1pts/2pts/3pts/4 修改/etc/pam.d/login文件，将其中的一行注释 1#auth [user_unknown=ignore success=ok ignore=ignore default=bad] pam_securetty.so 修改/etc/pam.d/remote，将其中的一行注释 1#auth required pam_securetty.so 修改/etc/xinetd.d/krb5-telnet文件，将文件内容由 12345678910service telnet&#123; flags = REUSE socket_type = stream wait = no user = root server = /usr/kerberos/sbin/telnetd log_on_failure += USERID disable = yes&#125; 更改为：12345678910service telnet&#123; flags = REUSE socket_type = stream wait = no user = root server = /usr/kerberos/sbin/telnetd log_on_failure += USERID disable = no&#125; 同样将/etc/xinetd.d/ekrb5-telnet文件中的disable=yes更改为disable=no。 安装中文字体为了阅读代码方便，安装字体。 将字体文件YaHei.Consolas.1.12.ttf放到Redhat的目录/usr/share/fonts/chinese/TrueType目录下 执行mkfontscale命令，重新生成fonts.scale文件 执行mkfontdir命令，重新生成了fonts.dir文件。 执行chkfontpath –add /usr/share/fonts/chinese/TrueType 更改操作系统编码从utf8到gb18030可以通过locale命令来查看操作系统编码。输出如下：1234567891011121314151617LANG=zh_CN.UTF-8LC_CTYPE=&quot;zh_CN.UTF-8&quot;LC_NUMERIC=&quot;zh_CN.UTF-8&quot;LC_TIME=&quot;zh_CN.UTF-8&quot;LC_COLLATE=&quot;zh_CN.UTF-8&quot;LC_MONETARY=&quot;zh_CN.UTF-8&quot;LC_MESSAGES=&quot;zh_CN.UTF-8&quot;LC_PAPER=&quot;zh_CN.UTF-8&quot;LC_NAME=&quot;zh_CN.UTF-8&quot;LC_ADDRESS=&quot;zh_CN.UTF-8&quot;LC_TELEPHONE=&quot;zh_CN.UTF-8&quot;LC_MEASUREMENT=&quot;zh_CN.UTF-8&quot;LC_IDENTIFICATION=&quot;zh_CN.UTF-8&quot;LC_ALL=``` 打开/etc/sysconfig/i18n文件，文件默认内容为： LANG=”zh_CN.UTF-8”1将文件内容修改为： LANG=”zh_CN.GBK”` 相关下载最适合程序员的字体：微软雅黑+Consolas]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大规模C++程序设计第1部分读书笔记]]></title>
      <url>%2Fpost%2Flarge_scale_c%2B%2B_software_desigin_part_1%2F</url>
      <content type="text"><![CDATA[声明和定义声明将一个名字引入到了程序中，；定义提供了一个实体（如类型、实例、函数）在程序中的唯一描述，为变量分配存储空间； 在一个给定的作用于中重复一个给定的声明是合法的，而重复定义是非法的。声明仅对当前编译单元有效，仅会在.o文件中加入了一个未定义符号。特定的类型不必与实际的定义类型匹配。 内部链接和外部链接内部链接和外部链接是根据链接过程中一个符号是在编译单元内部还是外部进行划分的，编译单元是按照一个.c或.cpp文件的作用域进行划分的。 内部链接：一个标识符对于编译单元（一个目标文件）来说是局部的，并在链接时与其他编译单元中的标识符不冲突。 内部链接包括： static类型的变量、函数 const类型的变量 枚举类型的定义 typedef定义的类型 class、struct、union的定义 inline函数 外部链接：在多文件程序中，链接时这个标识符可以和其他编译单元交互。这些外部符号在程序中必须是唯一的，用来被其他编译单元中未定义的符号访问。 将带有外部链接的定义放在头文件中是错误的。 外部链接包括： 类的非内联成员函数 非内联且非static函数 类的static数据成员 const类型的变量为内部链接的实例test1.h内容如下： 1void print1(); test1.cpp内容如下： 12345678#include &lt;stdio.h&gt;const int max_length = 256;void print1()&#123; printf("max_length=%d\n", max_length);&#125; test2.h内容如下： 1void print2(); test2.cpp内容如下： 12345678#include &lt;stdio.h&gt;const int max_length = 128;void print2()&#123; printf("max_length=%d\n", max_length);&#125; main.cpp内容如下： 1234567891011#include &lt;stdio.h&gt;#include "test1.h"#include "test2.h"int main()&#123; print1(); print2(); return 1;&#125; 从实例可以看出在全局作用域内声明的const类型的变量为内部链接的，在多个文件的作用域中分别定义相同的const类型的变量并不会产生冲突。 基本规则直接访问类的数据成员违反了面向对象中的封装原则，应该将类的数据成员私有化，并提供接口供部访问，类似于java bean规范。 避免使用全局变量，应该将全局变量封装到类中，并提供接口供外部访问。 为避免命名冲突，将全局函数封装到类中。 避免在头文件的作用域中使用enum、typedef和const数据，应该将这些数据封装到头文件的类作用域中。 在头文件中只应该包含如下内容：类、结构体、联合体和运算符函数的声明，类、结构体、联合体和内联函数的定义。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux设备驱动程序实例之hello world]]></title>
      <url>%2Fpost%2Flinux_driver_hello_world%2F</url>
      <content type="text"><![CDATA[本文为Linux设备驱动程序的入门实践文章，编写一个hello world程序，并在Linux上执行。 编写驱动程序驱动程序hello.c文件内容如下： 123456789101112131415161718192021222324252627#ifndef __KERNEL__# define __KERNEL__#endif#ifndef MODULE# define MODULE#endif#include &lt;linux/kernel.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/init.h&gt;MODULE_LICENSE(&quot;GPL&quot;);int hello_init()&#123; printk(KERN_WARNING &quot;Hello kernel!\n&quot;); return 0;&#125;void hello_exit()&#123; printk(&quot;Bye, kernel!\n&quot;);&#125;module_init(hello_init);module_exit(hello_exit) 编写MakefileMakefile文件的写法可以采用传统的make方式，也可以采用kbuild的方式。 采用传统的make方式的写法如下： 1234567891011121314151617ifeq ($(KERNELRELEASE),)KERNELDIR ?= /lib/modules/$(shell uname -r)/buildPWD := $(shell pwd)modules: $(MAKE) -C $(KERNELDIR) M=$(PWD) modulesmodules_install: $(MAKE) -C $(KERNELDIR) M=$(PWD) modules_installclean: rm -rf *.o *~ core .depend .*.cmd *.ko *.mod.c .tmp_versions.PHONY: modules modules_install cleanelse obj-m := hello.oendif 采用kbuild方式的Makefile内容如下： 1234567obj-m := hello.oall : $(MAKE) -C /lib/modules/$(shell uname -r)/build M=$(PWD) modulesclean: $(MAKE) -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean 编译将hello.c和Makefile文件放在任意目录中，执行make命令编译。 安装执行insmod hello.ko命令安装驱动程序，通过lsmod命令即可看到驱动程序已经安装。 通过查看/var/log/messages文件即可看到hello驱动程序打印的内容。 卸载执行rmmod hello.ko命令即可卸载驱动程序模块。 参考文章《深入理解Linux设备驱动程序》《Linux那些事之我是USB》Ubuntu12.10 内核源码外编译 linux模块–编译驱动模块的基本方法]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下内核编译安装]]></title>
      <url>%2Fpost%2Flinux_kernel_setup%2F</url>
      <content type="text"><![CDATA[本文仅为了练习Linux内核源码的编译安装，安装环境为VMWare下的CentOS，现有CentOS版本为2.6.32-358.el6.x86_64。/boot/grub/grub.conf文件内容如下： 123456789# 注释部分去掉default=0timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle CentOS (2.6.32-358.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=/dev/mapper/vg_livedvd-lv_root rd_NO_LUKS rd_LVM_LV=vg_livedvd/lv_root rd_NO_MD crashkernel=auto rd_LVM_LV=vg_livedvd/lv_swap KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM LANG=zh_CN.UTF-8 rhgb quiet initrd /initramfs-2.6.32-358.el6.x86_64.img 获取内核源码首先从Linux的官方网站下载最新版内核Linux3.13。 执行tar Jxvf linux-3.13.tar.xz -C/usr/src/kernels命令将内核源码解压到内核源代码存放目录/usr/src/kernels/，该源码目录并不固定，但推荐将内核源码存放到该目录下。 为了将上次编译时的目标文件及相关设置文件删除，执行make mrproper。 挑选功能可以采用了多种方式，这里采用make menuconfig的方式来挑选内核功能，该方式不需要X Window（make xconfig方式）的支持，而且要比纯命令行方式（make config）要直观。执行make menuconfig遇到如下错误： 1234567891011[root@localhost linux-3.13]# make menuconfig HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o *** Unable to find the ncurses libraries or the *** required header files. *** &apos;make menuconfig&apos; requires the ncurses libraries. *** *** Install ncurses (ncurses-devel) and try again. ***make[1]: *** [scripts/kconfig/dochecklxdialog] 错误 1make: *** [menuconfig] 错误 2 这是因为需要ncurses库的支持，下面采用从源码安装的方式安装ncurses。 从ncurses的官方网站下载最新版的ncurses-5.9.tar.gz。然后分别执行./configure、make、make install`命令安装。 更改内核版本号标识为了能够在编译完成后的内核版本中通过uname -r看到定义的内核版本号，修改Makefile文件。其中EXTRAVERSION字段值为空，将其赋值为kuring。 编译内核执行make命令，该过程需要话费很长时间，我在512MB的VM下跑，花费了大约1个半小时时间。 编译内核模块执行make modules命令。 安装内核模块执行make modules_install命令，会将内核模块安装到/lib/modules目录下。 安装内核执行make install命令，产生如下输出： 123456sh /usr/src/kernels/linux-3.13/arch/x86/boot/install.sh 3.13.0kuring arch/x86/boot/bzImage \ System.map &quot;/boot&quot;ERROR: modinfo: could not find module vmhgfsERROR: modinfo: could not find module vsockERROR: modinfo: could not find module vmware_balloonERROR: modinfo: could not find module vmci 这个错误跟vmware的vmware tools有关，暂时不去管。 这样再去看/boot/grub/grub.conf文件，会看到文件已经变化，已经将新内核添加到开机启动项中。 12345678910111213# 注释部分去掉default=1timeout=5splashimage=(hd0,0)/grub/splash.xpm.gzhiddenmenutitle CentOS (3.13.0kuring) root (hd0,0) kernel /vmlinuz-3.13.0kuring ro root=/dev/mapper/vg_livedvd-lv_root rd_NO_LUKS rd_LVM_LV=vg_livedvd/lv_root rd_NO_MD crashkernel=auto rd_LVM_LV=vg_livedvd/lv_swap KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM LANG=zh_CN.UTF-8 rhgb quiet initrd /initramfs-3.13.0kuring.imgtitle CentOS (2.6.32-358.el6.x86_64) root (hd0,0) kernel /vmlinuz-2.6.32-358.el6.x86_64 ro root=/dev/mapper/vg_livedvd-lv_root rd_NO_LUKS rd_LVM_LV=vg_livedvd/lv_root rd_NO_MD crashkernel=auto rd_LVM_LV=vg_livedvd/lv_swap KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM LANG=zh_CN.UTF-8 rhgb quiet initrd /initramfs-2.6.32-358.el6.x86_64.img 同时在/boot目录下已经多出了vmlinuz-3.13.0kuring、System.map-3.13.0kuring、initramfs-3.13.0kuring.img文件。 重启系统后，在启动菜单中多出了新内核选项。进入新内核后，执行uname -r显示3.13.0kuring，说明新内核已经安装完成。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CentOS6.5下安装svn客户端软件]]></title>
      <url>%2Fpost%2Fcentos6.5_svn%2F</url>
      <content type="text"><![CDATA[在用CentOS默认的svn客户端工具访问Windows下搭建的subversion时会提示如下错误： 12[kuring@localhost 桌面]$ svn checkout https://192.168.100.100/svn/testsvn: 方法 OPTIONS 失败于 “https://192.168.100.100/svn/test: SSL handshake failed: SSL 错误：Key usage violation in certificate has been detected. (https://192.168.100.100) 通过执行如下命令可以看到svn是支持https协议的： 1234567891011121314151617[kuring@localhost ~]$ svn --versionsvn，版本 1.6.11 (r934486) 编译于 Apr 11 2013，16:13:51版权所有 (C) 2000-2009 CollabNet。Subversion 是开放源代码软件，请参阅 http://subversion.tigris.org/ 站点。此产品包含由 CollabNet(http://www.Collab.Net/) 开发的软件。可使用以下的版本库访问模块:* ra_neon : 通过 WebDAV 协议使用 neon 访问版本库的模块。 - 处理“http”方案 - 处理“https”方案* ra_svn : 使用 svn 网络协议访问版本库的模块。 - 使用 Cyrus SASL 认证 - 处理“svn”方案* ra_local : 访问本地磁盘的版本库模块。 - 处理“file”方案 这是由于svn客户端在https协议中使用了GnuTLS库造成的，将其更改为使用openssl库即可。通过执行如下命令可以查看svn使用的库： 123[kuring@localhost bin]$ ldd svn | grep ssl[kuring@localhost bin]$ ldd svn | grep tls libgnutls.so.26 =&gt; /usr/lib64/libgnutls.so.26 (0x00007f33004ad000) 下面选择重新编译的方式来安装svn。 删除subversion执行：yum remove subversion 检查openssl安装情况这里已经安装： 123[kuring@localhost tmp]$ rpm -qa | grep opensslopenssl-1.0.1e-15.el6.x86_64openssl-devel-1.0.1e-15.el6.x86_64 安装neon这里选择的安装版本为0.29.6，subversion对neon的版本有要求。如果不是subversion的版本，在执行subversion下的configure文件时并不会报错12345[kuring@localhost software]$ tar zvxf neon-0.29.6.tar.gz[kuring@localhost software]$ cd neon-0.29.6./configure --with-ssl=opensslmakemake install 安装apr12345tar zvxf apr-1.5.0.tar.gzcd apr-1.5.0./configuremakemake install 安装apr-util12345tar zvxf apr-util-1.5.3.tar.gz cd apr-util-1.5.3./configure --with-apr=/usr/local/aprmakemake install 下载sqllite123unzip sqlite-amalgamation-3080401.zipmv sqlite-amalgamation-3080401 sqlite-amalgamationmv sqlite-amalgamation subversion-1.8.8/ // 将其复制到subversion源码目录下 安装subversion1234tar zvxf subversion-1.7.16.tar.gz./configure --with-ssl --with-neon --with-apr=/usr/local/apr --with-apr-util=/usr/local/aprmakemake install 然后再执行svn --version命令可以看到已经包含了https协议。 参考资料SSL handshake failed: SSL error: Key usage violation in certificate has been detected CentOS 资料下载需要的安装包下载]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Eclipse中安装viplugin插件]]></title>
      <url>%2Fpost%2Feclipse_viplugin%2F</url>
      <content type="text"><![CDATA[从官方网站下载最新的viplugin插件：viPlugin_2.12.0。 将viPlugin_2.12.0.zip文件解压到eclipse安装目录下的dropins目录下。 该插件为收费插件，在eclipse安装的根目录下新建viPlugin2.lic文件，文件内容为:_q1MHdGlxh7nCyn_FpHaVazxTdn1tajjeIABlcgJBc20_。 重启eclipse后即可生效。 官方参考文档地址：http://www.viplugin.com/files/User_Manual_viPlugin.pdf 相关下载：viPlugin_2.12.0.zip]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux难点之权限类型]]></title>
      <url>%2Fpost%2Flinux_permission%2F</url>
      <content type="text"><![CDATA[[TOC] 在Linux操作系统中，每个文件都有一组9个权限位来控制谁能够读写和执行该文件的内容。这组权限对于文件的意义非常容易理解。但是对于目录而言就不是那么容易理解了。要想搞明白权限机制需要了解文件系统中的inode节点和block等概念，并大致了解文件系统的内部实现原理。 文件的权限这部分比较容易理解。比较容易搞错的为： w位：包含对文件的添加、修改该文件内容的权限，但不包含删除文件或移动该文件的权限，因为文件的文件名信息存储在父目录的block中，而不是在该文件的inode节点中。父目录的block中包含该文件的inode节点和文件名信息，通过父目录中的inode节点找到该文件。 目录的权限r位：表示具有读取该目录列表的权限。 w位：对该文件夹下创建新的文件或目录进行增加、删除、修改操作。 x位：这个稍微难以理解。表示用户能否进入该目录成为工作目录，即是否可以cd到该目录。通常和r位组合一块使用。 例子一假设root用户对testing目录和testing目录下的testing文件拥有的如下权限： 123[root@localhost tmp]# ls -ald testing/ testing/testingdrwxr--r--. 2 root root 4096 3月 1 11:44 testing/-rw-------. 1 root root 0 3月 1 11:44 testing/testing kuring用户拥有对该文件夹的读权限，但是没有x权限。当kuring用户访问时会提示如下内容： 123456789// 拥有r的权限可以查询文件名，但是没有x权限，不能读取除文件名外的其他信息，产生了问号。[kuring@localhost tmp]$ ll testing/ls: 无法访问testing/testing: 权限不够总用量 0-????????? ? ? ? ? ? testing // 没有x权限，不能进入该目录[kuring@localhost tmp]$ cd testing/bash: cd: testing/: 权限不够 例子二在上述例子中，给kuring用户增加对目录testing的rwx权限，却只拥有testing目录下的testing文件的r权限。权限情况如下所示： 123[kuring@localhost tmp]$ ls -ald testing testing/testingdrwxr--rwx. 2 root root 4096 3月 1 13:29 testing-rw-r--r--. 1 root root 0 3月 1 13:29 testing/testing kuring用户执行如下操作： 12345678910// 当对文件进行更改时由于没有对文件的w权限，操作失败[kuring@localhost tmp]$ echo &quot;world&quot; &gt;&gt; testing/testingbash: testing/testing: 权限不够// 当对文件进行删除操作时却可以删除该文件，这是因为文件删除操作的权限是由该文件所在目录的w位决定的// 文件删除操作会修改父目录中block节点中的文件名内容，而父目录的权限为rx，不可写。[kuring@localhost tmp]$ rm testing/testingrm：是否删除有写保护的普通文件 &quot;testing/testing&quot;？y[kuring@localhost tmp]$ ls testing/[kuring@localhost tmp]$ 只有了解了原理，就可以理解在多级目录并且目录的权限不一致的情况下相应的权限问题了。 进程用户ID要讲解set uid和set gid，就涉及到进程的用户ID概念。用户ID又可以分为两部分： 实际用户ID和实际组ID：标识了究竟是哪个用户执行了该程序，跟命令行中的登录用户一致，可以通过id命令查看。 有效用户ID和有效组ID：系统通过有效用户ID、有效组ID和附加组ID来决定进行对系统资源的访问权限。在Linux系统中一个用户可以属于多个组，在/etc/passwd文件中一个用户仅能标识出隶属于一个组ID，该组ID叫做默认的组ID。在/etc/group为文件中可以标识出一个用户隶属于多个组，这多个组中除去默认的组ID叫做附加组ID。 set uid 该权限位仅对二进制文件有效 执行者对该程序需要具有x的权限 该权限仅在执行该程序过程中有效 执行者将具有该程序拥有者的权限 以/usr/bin/passwd命令为例，该程序的权限为：rwsr-xr-x。普通用户可以调用该程序修改自己的密码，而密码文件/etc/shadow未设置任何权限，即只有root用户可以操作该文件。 1234[root@localhost tmp]# ll /usr/bin/passwd-rwsr-xr-x. 1 root root 30768 2月 22 2012 /usr/bin/passwd[root@localhost tmp]# ll /etc/shadow----------. 1 root root 1248 11月 30 10:50 /etc/shadow passwd文件的s标志表明setuid位被设置。 passwd的拥有者为root；当普通用户执行passwd命令时，普通用户具有该命令的执行权限；passwd执行该命令时会暂时获得root用户权限；/etc/shadow就可以被修改。 也许看到这里就会有疑问：那岂不是普通用户可以通过/etc/passwd命令修改其他用户的密码了。之所以不可以这么操作是因为passwd命令内部通过逻辑实现。 set gid设置在文件上的情况： 该权限位仅对二进制文件有效 执行者对该程序需要具有x的权限 执行者在运行程序的过程中将会获得该执行群组的权限 设置在目录上的情况： 用户对目录具有r和x的权限 用户在目录下的有效群组将会变成该目录的群组 若用户在此目录下具有w的权限，使用者建立的群组与该目录的群组相同 粘附位（sticky bit）该权限位仅对目录有效，如果在目录上设置了粘附位，只有该目录的属主、该文件的属主或root用户可以删除或重命名该目录文件。 /tmp文件夹的权限如下，其中的t位表示粘附位： 12[kuring@localhost /]$ ll -ad tmp/drwxrwxrwt. 25 root root 4096 3月 1 14:51 tmp/ 如果在/tmp下kuring用户创建了自己的文件kuring_file，并设置权限为777，test用户并不能删除该文件 123456789101112[kuring@localhost tmp]$ touch kuring_file[kuring@localhost tmp]$ ll kuring_file-rw-rw-r--. 1 kuring kuring 0 3月 1 15:05 kuring_file[kuring@localhost tmp]$ chmod 777 kuring_file[kuring@localhost tmp]$ ll kuring_file-rwxrwxrwx. 1 kuring kuring 0 3月 1 15:05 kuring_file[kuring@localhost tmp]$ su - test密码：[test@localhost ~]$ cd /tmp/[test@localhost tmp]$ rm kuring_filerm: 无法删除&quot;kuring_file&quot;: 不允许的操作 表示方法如果设置了setuid位，属主的执行权限中的x用s来代替；如果设置了setgid位，组执行权限中的x用s来代替；如果设置了sticky位，权限最后的那个字符被设置为t；如果设置了setuid、setgid或sticky位中一个，又没有设置相应的执行位，这些位显示为S或T。 权限设定方式4为setuid位，2为setgid位，1为sticky位。具体参考《鸟哥的Linux私房菜》。 参考文档《鸟哥的Linux私房菜》《UNIX/Linux系统管理技术手册》]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下的screen命令]]></title>
      <url>%2Fpost%2Flinux_screen%2F</url>
      <content type="text"><![CDATA[在使用SSH或者telent远程登录到Linux 服务器，经常运行一些需要很长时间才能完成的任务，比如系统备份、ftp 传输等等。通常情况下我们都是为每一个这样的任务开一个远程终端窗口，因为它们执行的时间太长了。必须等待它们执行完毕，在此期间不能关掉窗口或者断开连接，否则这个任务就会被杀掉，一切半途而废了。 安装CentOS下默认没有安装该命令，从screen的官方网站下载，下载地址：http://ftp.gnu.org/gnu/screen/。 解压后执行./configure。 执行make命令。在执行make命令时会遇到错误pty.c:38:26: 错误：sys/stropts.h：没有那个文件或目录，在/usr/install/sys/目录下创建一个stropts.h的空文件即可。 执行make install，该命令并不会将screen命令复制到系统的PATH变量包含的路径下，即不能执行screen命令。 执行install -m 644 etc/etcscreenrc /etc/screenrc。 执行cp screen /bin/。 执行cp doc/man/man1/screen.1 /usr/share/man/man1/，即可以可使用man screen查看帮助。 实现原理当关闭窗口或断开连接时，内核会将SIGHUP信号发送到会话期首进程，进程对SIGHUP的处理动作为终止。如果会话期首进程终止，则该信号发送到该会话期前台进程组。一个进程退出导致一个孤儿进程组中产生时，如果任意一个孤儿进程组进程处于STOP状态，发送SIGHUP和SIGCONT信号到该进程组中所有进程。因此当网络断开或终端窗口关闭后，控制进程收到SIGHUP信号退出，会导致该会话期内其他进程退出。 为解决上述问题，Linux程序在设计时可设计成守护进程的方式启动。 另外也可以通过nohup 命令 &amp;的方式启动来解决问题。 新建screen 直接输入screen命令即可使用。 输入screen -S kuring，这里给screen取了一个名字，方便辨认。 screen 命令直接在screen中执行命令，命令结束后screen退出。 分离与恢复在screen窗口中执行ctrl + a d命令，screen会给出[detached]提示，并恢复到执行screen之前的bash。查找之前的screen执行screen -ls，会列出当前打开的screen。1234There are screens on: 8576.pts-3.localhost (Attached) 8449.kuring (Detached)2 Sockets in /tmp/uscreens/S-kuring. 这里系统中打开了两个screen，一个为Attached，另一个为Detached。 重新连接执行screen -r kuring或screen -r 8449或screen -r，当系统中仅有一个处于Detached状态的screen时就可以直接执行screen -r命令。 关闭窗口在screen的shell中执行exit命令即可关闭screen。 也可以执行ctrl + a k，会杀死当前screen中的所有运行进程。 错误在screen中执行vi命令时会提示E437: terminal capability “cm” required错误，执行echo $TERM查看发现打印值为screen，而未执行screen时的bash打印值为xterm，在screen中执行export TERM=xterm即可解决该问题。 参考文档更多高级使用方法请参考以下文档： linux 技巧：使用 screen 管理你的远程会话linux screen 命令详解]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[用C语言实现的trim函数]]></title>
      <url>%2Fpost%2Fc_trim%2F</url>
      <content type="text"><![CDATA[trim函数在其他语言中比较常见，这里用C语言实现一个，不使用C语言的库函数。该例子中不需要额外的申请空间，算法的时间复杂度为O(1)。 123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;char *trim(char * str)&#123; char *buf1, *buf2; int i; if (str == NULL) &#123; return NULL; &#125; // 处理字符串前面的空格 for (buf1=str; *buf1 &amp;&amp; *buf1==&apos; &apos;; buf1++); // 将去掉前面空格的字符串向前复制 for (buf2=str, i=0; *buf1;) &#123; *buf2++ = *buf1++; i++; &#125; *buf2 = &apos;\0&apos;; // 处理字符串后面的空格 while (*--buf2 == &apos; &apos;) &#123; *buf2 = &apos;\0&apos;; &#125; return str; &#125;int main(int argc, char *argv[])&#123; printf(&quot;trim(\&quot;%s\&quot;) &quot;, argv[1]); printf(&quot;returned \&quot;%s\&quot;\n&quot;, trim(argv[1])); return 0;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CentOS中将光盘作为安装源]]></title>
      <url>%2Fpost%2Fcentos_yum_iso%2F</url>
      <content type="text"><![CDATA[本文是VMWare下的CentOS操作系统将yum源更改为光盘的实例，光盘的iso文件存放在宿主机器上，通过VMWare的共享文件夹功能与CentOS系统共享文件。CentOS中共享文件夹的存放路径为/mnt/hgfs中。CentOS的光盘为两张，分别为CentOS-6.5-x86_64-bin-DVD1.iso、CentOS-6.5-x86_64-bin-DVD2.iso。注意LiveCD版的CentOS系统盘是不可以作为yum源的。 挂载光盘 mkdir -p /media/cdrom;mkdir -p /media/CentOS，创建挂载两个挂载目录，分别挂载DVD1和DVD2。 执行mount /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD1.iso /media/cdrom/ -o loop;mount /mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD2.iso /media/CentOS/ -o loop;，将iso挂载到创建的目录下。 执行df -h命令即可看到挂载的文件系统，输出如下：12/mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD1.iso 4.2G 4.2G 0 100% /media/cdrom/mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD2.iso 1.2G 1.2G 0 100% /media/CentOS 设置本地yum源在/etc/yum.repos.d目录下CentOS-Base.repo记录着yum通过网络更新的源，CentOS-Media.repo记录着通过本地文件更新的源。其中CentOS-Media.repo文件的内容如下：123456789101112131415161718192021# CentOS-Media.repo## This repo can be used with mounted DVD media, verify the mount point for# CentOS-6. You can use this repo and yum to install items directly off the# DVD ISO that we release.## To use this repo, put in your DVD and use it with the other repos too:# yum --enablerepo=c6-media [command]## or for ONLY the media repo, do this:## yum --disablerepo=\* --enablerepo=c6-media [command][c6-media]name=CentOS-$releasever - Mediabaseurl=file:///media/CentOS/ file:///media/cdrom/ file:///media/cdrecorder/gpgcheck=1enabled=0gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6 其中已经包含了/media/cdrom路径和/media/CentOS路径，至此配置已经完毕。 安装软件需要通过命令yum --disablerepo=\* --enablerepo=c6-media [command]，执行yum [command]命令时还是联网执行。 设置开启启动自动挂载iso文件在/etc/fstab文件中的末尾增加如下内容：12/mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD1.iso /media/cdrom/ iso9660 loop 0 0/mnt/hgfs/iso/CentOS-6.5-x86_64-bin-DVD2.iso /media/CentOS/ iso9660 loop 0 0]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[X Window学习]]></title>
      <url>%2Fpost%2Fx_window%2F</url>
      <content type="text"><![CDATA[X Window的实现机制还是比较难以理解的，尤其是跟软件开发中的客户端-服务器模式不太一样。涉及到概念也比较多，甚至很对教程对概念的理解不一。最近深入学习了下X Window的原理，在此做一下整理。先上一个摘自维基百科的图： 常用快捷键ctrl+alt+fn：切换到相应的虚拟控制台，n为1-12。默认情况下，linux操作系统会在1-6上运行6个虚拟控制台。ctrl+alt+退格键：关闭X window系统。在vmware环境下，ctrl+alt快捷键跟vmware冲突，需要先按住ctrl+alt，然后按一下空格键并松开，再按下相应的fn键才能使用。 X Server负责硬件管理、屏幕绘制、字体，并接收输入设备（如键盘、鼠标等）的动作，并且告知X Client。Linux下的X Server软件为Xorg，通过X（Xorg的链接文件）命令即可执行。输入X命令后，会在第7个控制台启动X Server，将会出现一个什么都没有的漆黑界面，这是由于没有任何X client程序输入的原因。可以在Linux下启动多个X Server软件，从0开始编号。如果再执行X:2命令会启动第二个X Server，此时X Server会在第8个控制台运行。如果第一次执行的是X:2命令则X Server会在第7个控制台运行。在Windows操作系统下的Xming、Xmanager等可以远程连接Linux界面的软件其实就是X Server。 X Client即X应用程序，运行在X Window下的窗口程序都属于X client。比如firefox就是一个X Client。接收来自X Server的处理动作，将动作处理成为绘图数据，并将绘图数据传回给X Server。X Client与X Server之间通过X Window System Core Protocol协议进行通讯。xclock是一个简易的X Client的时钟程序，在:1上启动X Server后，执行xclock –display :1&amp;命令将xclock输出到X Server后的画面如下：该程序可以在X Server上执行，但是画面非常简陋，甚至没有窗口的菜单栏和最大化等按钮。 Window Manager一种特殊的X Client，提供了窗口的样式。常用的Window Manager包括GNOME默认的metacity、twm等。将metacity输出到:1上的X Server的命令为metacity –display=:1 &amp;，效果如下：可以看到窗口多出了最小化、最大化、关闭按钮，并且窗口可以移动和缩放等操作。 Display Manager提供用户登录画面、帮助X Server建立Session。gnome采用的Display Manager为gdm，KDE采用kdm，还有tdm、xdm等。 Desktop ManagerX Server、X Client、Window Manager的一个集合。常用的Desktop Manager包括：KDE、GNOME等。 startx启动流程在命令行下执行startx命令后，系统直接进入了桌面环境，并未出现登录界面。进程树如下： startx会调用xinit命令，xinit命令的主要是启动一个X Server软件。 接着xinit命令会调用gnome-session启动gnome的环境所需要的软件。 init 5启动流程在命令行下执行init 5，首先出现的画面为登录信息。进程树如下： 执行/etc/rc.d/rc5.d中的daemon。 执行/etc/X11/prefdm文件，会选择启动gdm、kdm、xdm、tdm。 这里以gdm为例，gdm是一个shell脚本，会启动gdm-binary命令。 实战Windows主机连接Linux的教程参见我的另外一篇文章《Redhat安装完成之后的设置》中的相关部分。两台Linux机器之间通过XWindow实现连接的用法比较少见，通常情况下可以通过vnc代替。Linux主机连接Windows的工具为rdesktop。 参考文章《鸟哥的Linux私房菜》《Linux操作系统之奥秘》视频：RH033-ULE112-16-linux下X图形显示体系视频：Xwindow详解]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux常用网络诊断工具整理]]></title>
      <url>%2Fpost%2Flinux_net_tools%2F</url>
      <content type="text"><![CDATA[本文对Linux常用的网络命令进行整理和总结。 连通性测试ping命令最常用的网络诊断命令。12345678[kuring@localhost ~]$ ping www.baidu.comPING www.a.shifen.com (61.135.169.125) 56(84) bytes of data.64 bytes from 61.135.169.125: icmp_seq=1 ttl=128 time=25.5 ms64 bytes from 61.135.169.125: icmp_seq=2 ttl=128 time=20.3 ms64 bytes from 61.135.169.125: icmp_seq=3 ttl=128 time=25.0 ms64 bytes from 61.135.169.125: icmp_seq=4 ttl=128 time=21.7 ms64 bytes from 61.135.169.125: icmp_seq=5 ttl=128 time=23.4 ms64 bytes from 61.135.169.125: icmp_seq=6 ttl=128 time=21.9 ms 通过上述输出可以看出，ping命令可得到DNS对应的IP信息、ping的数据包大小、网络延迟信息。 另外，可以通过-s参数指定ping的数据包大小。例如：12345678910111213141516kuring@ubuntu:~$ ping www.baidu.com -s 1024PING www.a.shifen.com (61.135.169.125) 1024(1052) bytes of data.1032 bytes from 61.135.169.125: icmp_req=1 ttl=55 time=22.6 ms1032 bytes from 61.135.169.125: icmp_req=2 ttl=55 time=22.9 ms1032 bytes from 61.135.169.125: icmp_req=3 ttl=55 time=53.0 ms1032 bytes from 61.135.169.125: icmp_req=4 ttl=55 time=28.0 ms1032 bytes from 61.135.169.125: icmp_req=5 ttl=55 time=54.7 ms1032 bytes from 61.135.169.125: icmp_req=6 ttl=55 time=93.1 ms1032 bytes from 61.135.169.125: icmp_req=7 ttl=55 time=26.9 ms1032 bytes from 61.135.169.125: icmp_req=8 ttl=55 time=25.2 ms1032 bytes from 61.135.169.125: icmp_req=9 ttl=55 time=25.4 ms^C1032 bytes from 61.135.169.125: icmp_req=10 ttl=55 time=21.2 ms--- www.a.shifen.com ping statistics ---10 packets transmitted, 10 received, 0% packet loss, time 45436msrtt min/avg/max/mdev = 21.225/37.350/93.149/21.966 ms 会发现ping命令的响应时间变长了，这正是由于ping发送数据包变大了。 traceroute可以显示路由信息。 mtrtraceroute命令的升级版，可以动态刷新路由信息，可以显示路由上每个节点的丢包率和时间等信息，信息比较全面和直观。 arp相关arping可以通过该命令查看IP地址对应的mac地址。arping IP地址会立即发送一个arp广播，可以根据收到的arp回应的多少看局域网内是否中arp病毒、IP地址冲突等情况。 arp跟arp协议相关，可以设置arp表、读取arp表等。 端口相关telnet可以利用该命令来测试某个端口是否打开。例如执行telnet localhost 881其中881为本机的未打开端口，会产生如下输出： 12Trying 127.0.0.1...telnet: connect to address 127.0.0.1: Connection refused 执行telnet localhost 22，其中22端口为本机的ssh服务端口且已经打开，会产生如下输出： 1234Trying 127.0.0.1...Connected to localhost.Escape character is &apos;^]&apos;.SSH-2.0-OpenSSH_5.3 则表示本机的22端口已经打开。 netstat查看本机网络端口命令，常用netstat -aunp。 #DNS相关 host1234kuring@ubuntu:~$ host www.google.com.hkwww.google.com.hk is an alias for www-wide.l.google.com.www-wide.l.google.com has address 74.125.128.199www-wide.l.google.com has IPv6 address 2404:6800:4005:c00::c7 nslookup12345678kuring@ubuntu:~$ nslookup www.google.com.hkServer: 127.0.0.1Address: 127.0.0.1#53Non-authoritative answer:www.google.com.hk canonical name = www-wide.l.google.com.Name: www-wide.l.google.comAddress: 74.125.128.199 dig可以代替nslookup的命令，显示的域名信息更为详细。 其他abLinux下的压力测试工具，可以模拟多个客户端发送多个请求。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546kuring@ubuntu:~$ ab -c 100 -n 100 http://kuring.me/This is ApacheBench, Version 2.3 &lt;$Revision: 655654 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/Licensed to The Apache Software Foundation, http://www.apache.org/Benchmarking kuring.me (be patient).....doneServer Software: Tengine/2.0.0Server Hostname: kuring.meServer Port: 80Document Path: /Document Length: 707 bytesConcurrency Level: 100Time taken for tests: 1.547 secondsComplete requests: 100Failed requests: 10 (Connect: 0, Receive: 0, Length: 10, Exceptions: 0)Write errors: 0Non-2xx responses: 90Total transferred: 145330 bytesHTML transferred: 126570 bytesRequests per second: 64.64 [#/sec] (mean)Time per request: 1546.990 [ms] (mean)Time per request: 15.470 [ms] (mean, across all concurrent requests)Transfer rate: 91.74 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 173 223 43.8 200 289Processing: 167 340 301.5 236 1368Waiting: 166 339 300.5 235 1364Total: 345 563 295.4 438 1546Percentage of the requests served within a certain time (ms) 50% 438 66% 555 75% 562 80% 597 90% 1108 95% 1474 98% 1484 99% 1546 100% 1546 (longest request) 参考 解决Linux服务器访问比较慢的问题-网络测试命令讲解 《鸟哥的Linux私房菜-服务器架设篇》]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cgdb的使用]]></title>
      <url>%2Fpost%2Fcgdb%2F</url>
      <content type="text"><![CDATA[长期以来在使用gdb调试代码的时候都会因为调试代码的时候查看代码不方便而烦恼，gdb的list命令不够好用。而且网上的教程中也确实不容易发现可以替代gdb的好的终端下的调试工具，对于图形界面的集成开发环境（比如Eclipse CDT）和图形界面的调试工具（比如DDD）不在本文讨论的范围内，毕竟很多时候连接linux的时候还是终端方式的居多。 tui模式直到后来偶然间发现了gdb的-tui参数，该参数通过文本用户界面模式进行调试代码，使用起来确实方便了许多，再也不用边调试边list代码了，该模式已经满足了我的边调试边查看代码的需求。另外，gdbtui命令也可完成相同的功能。一个tui调试模式的界面如下： 虽然，tui模式大大的提供了调试的友好性，但是仍然有一些缺点。比如显示的代码无法语法高亮，虽然会很影响用户体验，但是我毕竟是一名后台开发的程序员，这点可以忽略不计。源码布局和gdb命令布局之间切换不够方便，这点也不要紧，毕竟可以切换，只是需要输入两个单词就可以切换了。最最有问题的就是，该命令使用的时curses库，当用ssh通过windows下的SecureCRT或者putty连接进行调试时，源码布局部分往往不能够自动刷新，需要手工输入CTRL+L来刷新，具体的原理我没有去深究。 cgdb主要是本着解决gdb tui中的源码布局不能自动刷新的问题，本文的重点cgdb命令终于闪亮登场了。该命令不仅解决了源码布局自动刷新问题，同时也支持了语法高亮，同时源码查看支持vi的部分命令。功能基本跟vimgdb相近，但是安装更容易，在ubuntu下只需执行sudo apt-get install cgdb即可。 下面是一些常用命令： ESC：切换焦点到源码模式，在该界面中可以使用vi的常用命令 i：切换焦点到gdb模式 o：打开文件对话框，选择要显示的代码文件，按ESC取消 空格：在当前行设置一个断点 参考文章Beej’s Quick Guide to GDBcgdb]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux From Scratch学习笔记]]></title>
      <url>%2Fpost%2Flfs%2F</url>
      <content type="text"><![CDATA[最近买了本新书《深度探索Linux操作系统》，在按照书中步骤学习的过程中，无奈在安装步骤中出错，于是只能到网上找书评。在浏览书评的过程中偶然看到LFS三个名词，google之，发现是手动安装Linux的官方学习手册，学习之。 解压命令一直对解压命令的参数记不清楚，记录一下：tar -Jvxf .tar.xztar -zvxf .tar.gztar -xvf *.tar.bz2 将一般用户可以使用sudo命令执行命令的方法执行visudo命令来修改文件内容，将本用户添加到文本文件中。修改的文件为/etc/sudoers，该文件默认为只读的，但是可以通过visudo命令来修改。 login shell和non-login shelllogin shell：shell会重新读取/etc/profile和~/.bash_profile来应用新的环境变量。通过su - 用户名的方式登录为login shell方式。non-login shell：此时shell不会读取/etc/profile和~/.bash_profile，而是读取~/.bashrc来应用新的环境变量。通过su 用户名登录的方式为non-login shell方式。 文中lfs用户下的~/.bash_profile的内容如下：1exec env -i HOME=$HOME TERM=$TERM PS1=&apos;\u:\w\$ &apos; /bin/bash 而lfs用户下的~/.bashrc文件的内容如下：1234567set +humask 022LFS=/mnt/lfsLC_ALL=POSIXLFS_TGT=$(uname -m)-lfs-linux-gnuPATH=/tools/bin:/bin:/usr/binexport LFS LC_ALL LFS_TGT PATH 而最令人奇怪的是即使通过su - lfs命令登录也会执行到.bashrc文件的内容，不信可以通过在.bash_profile和.bashrc文件的开始地方打印内容来验证。之所以出现如此奇怪的问题，原因在于~/.bash_profile中的命令。其中exec命令和linux系统中的exec系列函数的含义是一致的，即在当前bash中直接执行exec后面的命令，而不用fork一个新的进程来执行。env命令会通过当前用户的HOME和TERM环境变量及自定义的PS1环境变量来执行新的/bin/bash，而新执行的bash为non-login shell方式，因此会执行lfs用户下的.bahsrc文件。 总结一下，就是~/.bash_profile文件中的env命令通过non-login shell方式执行了新的bash，exec命令的作用是不在当前bash中执行新的bash，而不是通过产生一个新进程的方式来执行bash。 POSIX &amp;&amp; FHS &amp;&amp; LSBPOSIX.1-2008，通过该网站来查询系统函数等非常方便。Filesystem Hierarchy Standard (FHS)Filesystem Hierarchy Standard (FHS)，可以通过此标准来学习Linux的目录含义。Linux Standard Base (LSB) Specifications set +hThe set +h command turns off bash’s hash function. Hashing is ordinarily a useful feature—bash uses a hash table to remember the full path of executable files to avoid searching the PATH time and again to find the same executable. 虚拟终端PTYsPTY 设备与终端设备（terminal device）相类似——它接受来自键盘的输入，并将文字传递给运行在其上的程序以备输出。PTY 被依次编号，且每个 PTY 的编号就是它在 /dev/pts 目录中对应设备文件的文件名。 devpts file system远程登陆(telnet,ssh等)后创建的控制台设备文件所在的目录。 specs文件gcc 是一个驱动式的程序. 它调用其它程序来依次进行编译, 汇编和链接. GCC 分析命令行参数, 然后决定该调用哪一个子程序, 哪些参数应该传递给子程序. 所有这些行为都是由 SPEC 字符串(spec strings)来控制的. 通常情况下, 每一个 GCC 可以调用的子程序都对应着一个 SPEC 字符串, 不过有少数的子程序需要多个 SPEC 字符串来控制他们的行为. 查看当前shell 查看默认shell可以用命令：echo $SHELL。 查看当前shell：ps | grep $$ | awk &#39;{print $4}&#39;。 通过输入一个不存在的命令来查看，如输入tom，可显示bash: tom: command not found，说明当前的shell为bash。 expect一种提供自动交互的脚本语言。 tee命令重定向输出到多个文本文件命令。 pkg-configconfigure脚本在检查相应依赖环境时(例：所依赖软件的版本、相应库版本等)，通常会通过pkg-config的工具来检测相应依赖环境。 详细内容见：简述configure、pkg-config、pkg_config_path三者的关系]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2013年总结]]></title>
      <url>%2Fpost%2F2013_summary%2F</url>
      <content type="text"><![CDATA[2013年在不经意间过去了，总结起来收获还是不小的。我的大部分业余时间用在了学习上，看书看书再看书。每个周五或周六晚上会玩上一晚上dota以放松，周末会抽一天时间出去散散心。 学习学习上进步还算比较大，学习方式以看书为主，尤其是对Linux有了较深入了解。在年初的时候制定了深入学习Linux的计划，包括Linux内核，大约从五月下旬开始学习。 信息系统项目管理师当时抱着该考试可以挂靠的心态来学习PM，从春节后开始学习，五月末考试，大约花了三个月时间。学习思路为从淘宝上买的YY讲课的培训视频，价格还算便宜，隔一天晚上上一课，一次讲课时间在一个半小时左右。听了几次课之后就发现老师废话挺多，但是如果自己看书复习又抓不住重点，因此还是硬着头皮听老师讲课，就当作休息了。参考图书共三本，加起来足够2000页的样子，还做了历年的部分真题。由于自己手懒，写作在考试之前没有写过一篇，只是在考试之前背了两篇范文，结果考试的时候写作没有压中，离分数线差了两分。 下半年考试没有报名，经过了一次考试后看清了PM的真面目，纯属搞理论，距离实际太远。如果为了挂靠去花费宝贵的业余时间来学习实在有点不得不偿失。现在回想起来多少有些后悔，但是不尝试怎么能知道PM到底是个啥东西呢。 LinuxLinux的系统知识今年进步挺大的，无论是在编码、对系统的理解还是内核方面都有非常大的进步。 《鸟哥的Linux私房菜–基础学习篇》两年多前看过一次，利用工作中的空闲时间重新回顾了一边，收获还是挺大的。 《鸟哥的Linux私房菜–服务器架设篇》两年前买的书，当时没有通读，感觉含金量跟基础学习篇太大。利用工作之余通读一遍，收获也还算可以。 《Unix环境高级编程》Linux编程圣经，花了接近两个月时间看完，看完后收获很大。 《Unix网络编程》Linux网络编程圣经，花了一个月时间看完，看完后对写TCP/IP通信的程序很有帮助，作为《Unix环境高级编程》的很好补充。 《Linux内核完全剖析》该书花了我接近半年的时间来学习，现在看完了五分之四左右，还有文件系统章节没有学习。学习下来收获很大，后续会结合《Linux内核图解》一书来补充学习和归纳。 《程序员的自我修养》通过学习对Linux的elf和windows的pe结构都有了较深入的了解，不过事隔半年之后已经全部忘记了，需要再温习一遍。 微信公共帐号从年初开始，每天会花一定时间来阅读微信公共帐号的文章，初期订阅的文章基本都会阅读，后期订阅量大了有些就仅浏览标题了。我订阅的公共帐号里比较好的有：鬼脚七、道哥的黑板报、小道消息、Mac Talk等。鬼脚七写人生类的文章我比较喜欢看，也比较赞同作者的观点。 其他###《30天自制操作系统》从2012年年底开始读的图书，花了接近三个月的时间通读，读到中间部分还反过头来重读一遍。学习到三分之二的时候感觉对我用处不大，因为不会对我理解Linux或Windows操作系统有太大帮助，放弃之。 《Hadoop权威指南》项目需要，对Hadoop没有足够的兴趣，理解了是怎么回事对我就足够了。同时还研究了Solr、HBase、MonogoDB等大数据软件。 《Git权威指南》向对Git有进一步的理解，目前仅看完了一半，分支管理部分还没有学习。 ###《浪潮之巅》吴军博士的经典，我主要拿来在看书看累的时候消遣用，和《黑客与画家》的效果相当，很抱歉看了大半年了到现在仍然在看。 《深度探索C++对象模型》之前曾一度要学习，无奈一直没有看进去，lippman写的和侯捷翻译的实在是诡异。这次边写博客记录边看，居然效果很不错。最后时间有限，留了个小尾巴没有看完。 工作由于之前的公司经营不善，加上今年全国经济形式不好，公司持续几个月发不下工资来，考虑到个人经济状况换了一份工作。在济南这种小地方，找个理想的工作实在是不容易，甚至今年很多公司都不再招聘。我找工作以技术为导向，家附近一堆做电信BOSS的外包公司，一概不考虑。新工作还算可以，依旧加班较少，只是周六是间歇性上班，这个可以忍，还算有较充足的业余时间来学习。 工作中用到的技术以C、C++为主，兼涉及到Android、PHP等。从元旦之后上班第一天，自己单打独斗从零开始做了一个短信管家类项目，到五月份后终止，结果跟我预想的完全一致，以失败而告终。回想当初真该将我当时对项目的看法跟领导沟通一下，而不是盲目的接下公司安排的任务。当时领导对互联网、APP的理解完全是幼稚园水平，一家传统行业的软件企业想来玩转互联网行业困难可想而知，我作为唯一的项目成员在项目的开始已经看到了没有任何意外的结局。如果当初将这五个多月的时间用来做些Linux的东西，对我个人的成长将是相当巨大的。 生活2013经济上不是很宽裕，没有旅游，仅有的一次市外旅游是南部山区的红叶谷。在济南这种地方，工资压得低，技术牛人又少，实在悲剧。生活虽然对我少了些许精彩，但过的还算比较充实。 博客为了能够更好的总结学习经验，年初准备有写博客的计划。之前零零散散的也写过一些，没有坚持住，现在看来我的博客写作计划坚持的还算不错，从6月份到年底共写了40篇博客，大都是自己的学习经验总结。前期博客写了一些非技术类博文，后期觉得没有这个必要不再写非技术类博客。 为了搭建博客，又不想花钱买个不稳定的空间。我研究了Github上的jekyll、基于网盘（DropBox、Google Driver）的Farbox等搭建博客技术，参照了大量的jekyll主题，几乎将jekyll wiki上列出的主题都看了一个遍，也没有发现一个令我非常满意的主题，后挑选一个主题在Github上实验，不知是我markdown语法写的问题还是Github对markdown的解析问题，解析效果不够理想，但是我写的markdown在Farbox上却没有问题。最后采用了Farbox作为博客平台，并对Farbox的默认主题进行了少量修改。并从Godaddy上购买了kuring.me域名。 2014年计划2014，除了工作之外，我也业余时间仍然会大部分奉献给阅读书籍，以Linux内核方向为主。趁着年轻，多读些书，永远是正确的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下更改屏幕分辨率]]></title>
      <url>%2Fpost%2Fxrandr%2F</url>
      <content type="text"><![CDATA[在Virbutal Box下安装CentOS虚拟机，在安装完Virtual Box增强功能后在分辨率列表中，仍然没有适合屏幕的分辨率1600*900。本文将通过xrandr命令来修改当前屏幕的分辨率。 一. 执行xrand -q列出当前系统中已有的分辨率。显示内容如下：12345678Screen 0: minimum 64 x 64, current 1024 x 768, maximum 32000 x 32000VBOX0 connected 1024x768+0+0 0mm x 0mm 1024x768 60.0*+ 60.0* 1600x1200 60.0 1440x1050 60.0 1280x960 60.0 800x600 60.0 640x480 60.0 需要特别注意的是VBOX0，代表显示器的名字，下面会用到。 二. 执行cvt 1600 900命令列出分辨率1600*900需要的参数，后面会用到。显示内容如下:12# 1600x900 59.95 Hz (CVT 1.44M9) hsync: 55.99 kHz; pclk: 118.25 MHzModeline &quot;1600x900_60.00&quot; 118.25 1600 1696 1856 2112 900 903 908 934 -hsync +vsync 该命令列出的内容下文会用到。 三. 执行xrandr --newmode &quot;1600x900_60.00&quot; 118.25 1600 1696 1856 2112 900 903 908 934 -hsync +vsync命令，该命令中的参数是参考步骤2中的输出信息。 四. 执行xrandr --addmode VBOX0 1600x900_60.00来向系统分辨率组中添加分辨率1600*900。其中VBOX0为步骤1获取的显示器名字，1600x900_60.00为步骤3添加的分辨率模式。 五. 执行xrandr --output VBOX0 1600x900_60.00来应用刚才添加的分辨率。这样屏幕就可以更改为正确的分辨率了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下搭建网桥及脚本编写]]></title>
      <url>%2Fpost%2Flinux_bridge%2F</url>
      <content type="text"><![CDATA[网桥工作在数据链路层，将两个LAN连起来，根据MAC地址来转发帧。Linux下要配置网桥的方法有两种，一种是通过修改配置文件，另外一种是通过brctl工具。修改配置文件的方式是通过修改/etc/sysconfig/network-scripts/ifcgg-eth*文件来完成的，这种方式没有仔细研究。本文将编写两个脚本来完成网桥的创建和删除，脚本的功能为将机器上的网卡eth1和eth2桥接，而网桥本身未设置ip。 网桥创建脚本本脚本利用brctl命令将网卡eth1和eth2桥接，可以通过brctl show命令查看结果。12345678910111213141516171819202122232425#!/bin/bash# 脚本作用为将两个网卡桥接# 检测brctl命令是否存在brctl &gt; /dev/nullif [ $? != 1 ]; then echo Command brctl not exist, please setup it. The setup execute command is \&quot;yum install bridge-utils\&quot; exit 0fi# 检测网桥br0是否存在，如果存在首先删除declare -i result=$(brctl show | grep eth0 | wc -l)if [ $result &gt; 0 ]; then echo detect the bridge br0 have already exist, first delete it ifconfig br0 down brctl delbr br0fiifconfig eth1 0.0.0.0ifconfig eth2 0.0.0.0brctl addbr br0brctl addif br0 eth1brctl addif br0 eth2ifconfig br0 upecho create bridge br0 success, you can use command : \&quot;brctl show\&quot; to check 网桥删除脚本本脚本将桥接网卡br0删除12345678910111213#!/bin/bash# 检测是否存在网桥br0declare -i result=$(brctl show | grep br0 | wc -l)if [ $result == 0 ]; then echo &quot;bridge br0 not exists, exit immediately&quot; exit 0fi# 删除网桥br0ifconfig br0 downbrctl delbr br0echo &quot;delete bridge br0 success&quot; 注意事项brctl命令创建的桥接网卡在机器重启后会删除，最好将创建桥接网卡的命令放入到linux的开机启动脚本中，这样每次开机的时候都可以自动创建桥接网卡了。 参考网址bridge命令介绍]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在ubuntu中更改mac地址的方法]]></title>
      <url>%2Fpost%2Fubuntu_change_mac%2F</url>
      <content type="text"><![CDATA[本文提供简易shell脚本来更改mac地址，在其他linux发行版中去掉sudo即可。脚本内容如下：1234#!/bin/shsudo ifconfig eth0 downsudo ifconfig eth0 hw ether 08:00:27:DF:B3:7Bsudo ifconfig eth0 up]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux下vnc的配置]]></title>
      <url>%2Fpost%2Flinux_vnc%2F</url>
      <content type="text"><![CDATA[[TOC] VNC（Virtual Network Computing）是一套由AT&amp;T实验室所开发的可操控远程的计算机的软件，其采用了GPL授权条款，任何人都可免费取得该软件。VNC软件主要由两个部分组成：VNC server及VNC viewer。用户需先将VNC server安装在被控端的计算机上后，才能在主控端执行VNC viewer控制被控端。VNC与操作系统无关，因此可跨平台使用，例如可用Windows连接到某Linux的电脑，反之亦同。 该软件在RedHat或CentOS中默认是安装的，但是没有启用，一可以通过which vncserver命令来查看该命令是否安装。本文讲解在Linux下的搭建server，在Windows下搭建client的步骤。 设置vncserver的密码vncserver需要设置一个密码，该密码并不等同于系统帐号的密码，而是vnc客户端登录的时候输入的密码。执行vncpasswd命令来创建密码。 修改vncserver配置文件修改文件/etc/sysconfig/vncservers，在该文件末尾添加如下内容：12VNCSERVERS=&quot;1:root&quot;VNCSERVERARGS[1]=&quot;-geometry 1024x768 -alwaysshared -depth 24&quot; 启动vncserver执行service vncserver start命令来开启vncserver服务。 客户端连接到服务端这里采用windows下的vnc viewer工具来连接到vncserver。在输入IP的地方输入：IP地址:1来连接到vncserver端，其中:1要跟/etc/sysconfig/vncservers文件中的对应标号一致。这样就可以连接上vncserver，但是连接后界面非常简单，跟命令行界面类似。还需要对vncserver进一步配置。 配置vncservervncserver的配置文件在~/.vnc/xstartup文件中，该文件默认创建的内容如下：123456789101112#!/bin/sh# Uncomment the following two lines for normal desktop:# unset SESSION_MANAGER# exec /etc/X11/xinit/xinitrc[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresourcesxsetroot -solid greyvncconfig -iconic &amp;xterm -geometry 80x24+10+10 -ls -title &quot;$VNCDESKTOP Desktop&quot; &amp;twm &amp; 将其中的注释打开，即文件内容如下：1234567891011#!/bin/shunset SESSION_MANAGERexec /etc/X11/xinit/xinitrc[ -x /etc/vnc/xstartup ] &amp;&amp; exec /etc/vnc/xstartup[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresourcesxsetroot -solid greyvncconfig -iconic &amp;xterm -geometry 80x24+10+10 -ls -title &quot;$VNCDESKTOP Desktop&quot; &amp;twm &amp; 然后执行service vncserver restart重新启动vncserver服务。客户端再重新连接vncserver既可以看到正常的界面了。 参考文章CentOS Linux下VNC Server远程桌面配置详解]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[再谈Windows和Linux之间的中文编码问题]]></title>
      <url>%2Fpost%2Fwindows_linux_code_2%2F</url>
      <content type="text"><![CDATA[上次文章《Windows和Linux之间的中文编码问题》中提到的在Windows下的源代码程序放到Linux下出现中文编码问题，解决方法为通过iconv工具转换源代码文件的编码为UTF8格式。最近多学习了些字符编码的知识，发现了解决此问题的另外一种办法。 基础知识我们在编译程序的时候会涉及到几个编码问题，包括C++源文件的编码、C++程序的内码和运行环境编码，其中C++程序的内码较难理解。 C++程序的内码是指在可执行文件中字符串常量是以什么编码形式存放的，其中字符串常量为窄字符形式。在Windows系统中C++的内码通常为GB18030，在Linux下的gcc/g++使用的内码默认为utf8，可以通过fexec-charset参数来修改。 运行环境编码即为操作系统的编码，通常情况下，简体中文Windows操作系统编码为GB18030，而Linux下默认为UTF8。 gcc命令的参数gcc有两个参数可以用来解决编码问题。-finput-charset：用来指定源文件的编码。-fexec-charset：用来指定生成的可执行文件的编码。 如果这两个参数均未指定，则GCC不会对编码进行转换。以上这两个参数就可以用来在不修改源文件编码的基础上来达到正确的效果，达到和上篇文章中解决问题同样的效果。 关于Unicode编码一直对Unicode编码比较糊涂，Unicode只是编码方法规范，而不是具体的存储方法。常用的Unicode又分为UCS-2和UCS-两种编码，其中UCS-2采用固定的2个字节存储，UCS-4采用固定的4个字节存储。通常情况下提到的Unicode编码即为UCS-2编码，比如Windows记事本中的保存为Unicode编码，其实就是保存为了UCS-2编码，由于每个字符均为2个字节，所以下次读取的时候仍然可以通过存储格式还原出来。 参考文章字符编码笔记：ASCII，Unicode和UTF-8字符编码详解关于c++的一些编码问题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux上一个界面程序的开机自启动设置]]></title>
      <url>%2Fpost%2Flinux_ui_autorun%2F</url>
      <content type="text"><![CDATA[最近设置一个Linux下的截屏程序的开机自启动，Linux的XWindow系统为gnome。 最先想到的方式是修改/etc/inittab文件，因为Linux在开机自启动的时候会执行该文件，该文件的读取时机是在界面启动之前。我写了一个在main函数中睡眠10分钟的小程序，然后将程序添加到/etc/inittab文件中，结果开不了机了，因为需要睡眠10分钟后才能往下执行程序。幸好我用的虚拟机做的测试，并且在做测试之前备份了虚拟机。 仔细想一下在/etc/rc.d/rc.local文件中启动截屏程序肯定是不合适的，因为如果用户默认是以运行级别非5启动，则程序仍然会被调用，但是没有XWindow，谈何截屏。 接下来考虑将程序放到/etc/rc5.d目录下，这样就可以保证程序在XWindow环境下运行了。仔细一想也不合适，rc5.d仅在系统启动时运行，而Linux系统是多用户系统，允许多个用户同时登陆，多个用户登陆时截屏程序会怎样处理呢？这样显然不合适。 然后想到程序既然为截屏程序，多个用户登陆的时候应该有几个用户就跑几个程序，这样才能保证每个用户的屏幕都能截取到。因此应该放到用户登录后的启动程序列表中。类似于windows系统中的开机启动项。我用的桌面为gnome，找到了gnome-session-properties命令来启动添加程序启动的界面，然后将我的程序添加到界面中即可。 如果桌面系统为KDE，则应该也可以找到相关的设置界面。 题外：利用/etc/inittab和rc5.d目录下添加脚本的方式来启动程序的用户为root，很多程序未了避免root权限带来的安全问题，程序内部采用了su - 用户名的方式切换到一半用户执行代码。程序还可以通过chroot的方式更改根目录的路径达到保护系统的目的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Windows和Linux之间的中文编码问题]]></title>
      <url>%2Fpost%2Fwindows_linux_code%2F</url>
      <content type="text"><![CDATA[在开发Linux程序的时候通常会在Windows下编码，然后拿到Linux下编译调试。而两个操作系统之间的默认编码往往有差别。 文件编码问题在Windows下查看文件编码可以使用记事本打开文件，然后点击“另存为”在右下角即可看到当前文件的编码方式。如果显示为ANSI编码，在简体中文系统下，ANSI 编码代表 GB2312 编码。不同 ANSI 编码之间互不兼容，ANSI是American National Standards Institute的缩写， 记事本默认是以ANSI编码保存文本文档的。 在Linux可以通过vi命令查看文件编码，用vi打开文件，然后输入:set encoding即可显示文件编码。 在VS2008中创建文件的默认编码是根据当前系统的编码格式确定的。VS2008编译器可以同时支持GB2312和UTF-8两种编码。 为了解决在Linux下的乱码问题，Linux下的编码格式为utf8编码，这里采用在Windows下将gb2312编码更改为utf8的方式来解决。iconv是一个可以转换文件编码的工具，编写一个批处理脚本来实现批量转换文件编码的功能。批处理文件的内容如下：1234567@ECHO OFFFOR /R %%F IN (*.h,*.cpp) DO (echo %%~nxFiconv.exe -f GB2312 -t UTF-8 %%F &gt; %%F.utf8move %%F.utf8 %%F &gt;nul)PAUSE 本脚本来自网络，不是我自己写的。注意：在使用文件编码之前一定要备份文件，防止意外发生，否则后果自负。 文件名编码问题Windows的中文系统下文件名的编码默认为gbk，在Linux默认编码为UTF-8。如果将Windows下的中文文件名的文件复制到Linux下肯定会出现乱码的问题。可以利用convmv工具来解决编码的问题。 具体执行操作为：在Linux系统下的要转换编码的目录下执行命令：convmv -f GBk -t UTF-8 --notest -r *，这样就会将该文件夹下的所有文件递归的转换编码为UTF-8。 convmv的帮助文档点这里。 相关下载脚本和iconv程序下载链接]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度探索C++对象模型读书笔记_第五章：构造、析构、复制语意学]]></title>
      <url>%2Fpost%2Finside_the_c%2B%2B_object_model_chapter_5%2F</url>
      <content type="text"><![CDATA[无继承情况下的对象构造在《Unix环境高级编程》的7.6节中提到C程序的内存空间可以分为正文段、初始化数据段、非初始化数据段、栈、堆。其中初始化数据段包含程序中需明确赋初值的变量，如C语言中的全局变量int maxcount = 99;。非初始化数据段又称为bss（block started by symbol）段，在程序开始之前，内核将此段初始化为0或空指针，如出现在函数外面的long sum[1000];，该变量没有明确赋初值，因此放到了bss段中。而在C++语言中，将所有的全局对象当做初始化过的数据来对待，因此不会将全局变量放到bss段中。 POD数据类型书中提到Plain ol’ data，查了下应该叫Plain Old Data,简称POD，指C风格的struct结构体定义的数据结构，其中struct结构体中只能定义常规数据类型，不可以含有自定义数据类型。 123456789101112131415typedef struct Point&#123; float x, y, z;&#125; Point;Point global;Point foobar()&#123; Point local; Point *heap = new Point(); *heap = local; delete heap; return local;&#125; 首先看全局变量global，按照常规的理解，在程序启动的时候编译器会调用Point的合成的默认构造函数来初始化global变量，在程序退出时会调用Point的合成的析构函数来销毁global变量。实际上，C++编译器会将Point看成是一个POD对象，既不会调用合成的构造函数也不会调用合成的析构函数，但C++编译器会将global当成初始化过的数据来对待，不放入BSS段。 foobar函数中的local局部变量不会自动初始化，意味着local.x中的值是不可控的，但是local变量分配了栈空间。 *heap = local;执行时仅简单执行按字节复制操作，不会产生赋值操作符，因为Point是一个POD类型。 return local;同样仅通过字节复制操作产生一个临时对象。 抽象数据类型这次将上面的Point类型从struct变换为class12345678class Point&#123;public: Point(float x=0.0, float y=0.0, float z=0.0) : _x(x), _y(y), _z(z)&#123;&#125;private: float _x, _y, _z;&#125;; 在上节中的foobar函数中，各个对象的默认复制构造函数、赋值操作符和析构函数仍然不会调用，因为调用是没有意义的，因此编译器干脆就不产生。 为继承做准备再次更改Point类，引入虚函数。12345678class Point&#123;public: Point(float x=0.0, float y=0.0) : _x(x), _y(y) &#123;&#125; virtual float z();private: float _x, _y;&#125;; 引入虚函数后，类对象就需要一个vtbl来存放虚函数的地址，类对象中需要添加vptr指针。而vptr的初始化是在对象构造的时候，因此对象初始化的时候需要调用构造函数，同时默认构造函数和赋值构造函数会自动在构造函数的最前面插入初始化vptr的代码。 继承体系下的对象构造C++时会自动扩充类的每一个构造函数。扩充步骤如下： 如果类含有虚基类，则所有虚基类的构造函数被调用，调用顺序为从左到右，从最深到最浅。 如果类含有基类，则基类构造函数会被调用，以基类的声明顺序为顺序。 如果类对象中含有vptr，必须在初始化类的成员变量之前为vptr指定初值，使其指向vtbl。 将成员初始化列表中数据成员的初始化操作放入构造函数内部，并且按照成员在类中的声明顺序。 如果类成员变量不在构造函数的初始化列表中，但是成员变量含有默认构造函数，则默认构造函数必须被调用。 虚拟继承本小节将学习一下引入了虚继承机制之后构造函数的生成是什么样子的。1234567891011121314151617181920212223242526272829303132333435class Point&#123;public: Point(float x=0.0, float y=0.0) : _x(x), _y(y) &#123;&#125; virtual float z();private: float _x, _y;&#125;;class Point3d : public virtual Point&#123;public: Point3d(float x=0.0, float y=0.0, float z=0.0) : Point(x, y), _z(z) &#123;&#125; ~Point3d(); virtual float z() &#123;return _z;&#125;protected: float _z;&#125;;class Vertex : virtual public Point &#123; // 不是重点忽略&#125;;class Vertex3d : public Point3d, public Vertex&#123; // 不是重点忽略&#125;;class PVertex : public Vertex3d&#123; // 不是重点忽略&#125;; 类之间的继承关系如下图所示，已经属于最复杂的继承模型了。如果要构造Vertex3d的实例，在内存中必须仅能有一个Point类型的对象，而如果在Point3d和Vertex基类中都构造一个Point实例显然是不合适的。答案是编译器会在Vertex3d的构造函数中生成Point的对象，在Point3d和Vertex的构造函数中均不会生成Point的对象。Vertex3d和Point3d的构造函数伪码如下面所示，Vertex构造函数的伪码和Point3d类似，这里就不再列出。12345678910111213141516171819202122232425Point3d* Point3d::Point3d(Point3d *this, bool __most_derived, float x, float y, float z)&#123; // 如果子类初始化基类则本构造函数不需要初始化基类 if (__most_derived != false) &#123; this-&gt;Point::Point(x, y); &#125; this-&gt;__vptr_Point3d = __vtbl_Point3d; // 初始化指向本类的vptr this-&gt;__vptr_Point3d_Point = __vtbl_Point3d_Point; // 初始化指向基类的vptr this-&gt;_z = z; return *this;&#125;Vertex3d* Vertex3d::Vertex3d(Vertex3d *this, bool __most_derived, float x, float y, float z)&#123; if (__most_derived != false) &#123; this-&gt;Point::Point(x, y); &#125; this-&gt;Point3d::Point3d(false, x, y, z); this-&gt;Vertex::Vertex(false, x, y); // 初始化vptr // 用户代码 return this;&#125; 编译器在类的构造函数中增加了一个bool变量来判断本类是否需要初始化基类，虚基类的初始化始终在继承最底层的类构造函数中初始化。对于PVertex类来说，Point类的构造函数在该类的构造函数中调用。 vptr初始化语意学vptr的在构造函数中的初始化时机为：在基类构造函数调用操作之后，在成员初始化列表和构造函数中显式代码之前。构造函数的执行先后顺序为： 所有虚基类、基类的构造函数会被调用。 对象的vptr初始化，指向相关的vtbl。 在构造函数内展开成员的初始化列表。 执行显式代码。对象复制语意学]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我的个人数据备份方案]]></title>
      <url>%2Fpost%2Fmy_data_bak%2F</url>
      <content type="text"><![CDATA[没有一成不坏的硬件，尤其是数据放到物理硬盘中，说不定哪天硬盘闹脾气就崩掉了，硬盘不值钱，可是里面的数据值钱。下面分享下我的数据备份方案，我的原则是数据无论何时都至少留有一个备份。 博客我的博客是放到Dropbox中的，在云端和本地均有备份，确保了博客数据的绝对安全，即使云端坏掉还有本地，本地丢了还有云端。 个人照片由于照片都较大，放到本地硬盘很容易占满空间，而且还不经常用。除了在自己电脑上留有照片之外，选择将照片压缩并加密后按照年份放到百度云上。 代码工作几年了，已经积攒了一些代码，有些代码时不时的会查看到。对于可以公开的自己写的代码我以后打算放到我的Github上，一方面是由于Github上可以在线浏览代码，另一方面可以向别人分享我的代码。对于私有的代码，暂时放到了金山快盘上，没有找到可以方便浏览代码的云端。 文档由于文档之类的资料也是经常用到，我选择了金山快盘。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux和Windows平台下的网络通信问题]]></title>
      <url>%2Fpost%2Flinux_windows_network%2F</url>
      <content type="text"><![CDATA[大小端问题跟CPU的架构直接相关，我们常见的80x86系列CPU采用小端字节序模式。Windows平台就采用的80x86系列CPU，因此为小端字节序。而主机之间进行网络通信时往往采用大端字节序，因此小端字节序机器在发送数据前需要进行字节序转换，在接收到数据处理处理数据之前要将网络字节序转换成本地字节序。 在Linux平台下提供了四个函数用来字节序转换：12345#include &lt;arpa/inet.h&gt;uint32_t htonl(uint32_t hostlong);uint16_t htons(uint16_t hostshort);uint32_t ntohl(uint32_t netlong);uint16_t ntohs(uint16_t netshort); Windows平台下也提供了相关的自己序转换函数：1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;WinSock2.h&gt;unsigned __int64 __inline htond( double value);unsigned __int32 __inline htonf( float value);u_long WSAAPI htonl( _In_ u_long hostlong);unsigned __int64 __inline htonll( unsigned __int64 value);u_short WSAAPI htons( _In_ u_short hostshort);double __inline ntohd( unsigned __int64 value);float __inline ntohf( unsigned __int32 value);u_long WSAAPI ntohl( _In_ u_long netlong);u_long __inline ntohll( unsigned __int64 value);u_short WSAAPI ntohs( _In_ u_short netshort); 这里有个技巧需要说明以下，比如要发送如下的结构体：12345struct foo&#123; int a; long b;&#125;; 为了避免每个成员都调用字节序转换函数，可以在结构体的内部定义两个方法用于转换字节序，添加字节序后的foo如下：123456789101112131415struct foo&#123; int a; long b; void ntoh() &#123; a = ntohl(a); b = ntohl(b); &#125; void hton() &#123; a = htonl(a); b = htonl(b); &#125;&#125;; 需要特别注意的是，在发送结构体类型的数据时要注意字节对齐的问题，这里不再展开讨论，不同的平台有不同的解决办法。大体分为Winodws平台、AIX平台和GNU类平台。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[两个通过http获取指定网页内容并解析的简单程序]]></title>
      <url>%2Fpost%2Fparse_http%2F</url>
      <content type="text"><![CDATA[近段时间写了两个通过http协议来获取指定网页的内容并将内容解析出来的程序。程序一可以解析出目前本博客的内容页面的内容、时间、访问次数参数，采用Qt类库实现；程序二可以解析出新浪博客页面的内容、时间等参数，采用Linux下的tcp相关API实现。均采用C++语言实现。 程序一该程序采用Qt类库实现，其中Http协议的发送和接收采用Qt类库封装的类，网页内容的解析采用Qt封装的解析XML的相关类。该程序仅能解析标准的Html语言，对于网页中的所有”&lt;&gt;”标签必须有结尾才行。例如本页面源码中的1&lt;meta content=&quot;black&quot; name=&quot;apple-mobile-web-app-status-bar-style&quot; /&gt; 必须是闭合的。如果是下面这样则无法正确解析网页内容，这是由于采用的Qt类库决定的。1&lt;meta content=&quot;black&quot; name=&quot;apple-mobile-web-app-status-bar-style&quot;&gt; 程序二该程序的Http协议部分采用Linux的tcp协议api实现，解析网页直接采用搜索字符串的方式实现，较上一种方式要底层，仅能运行在Linux系统下运行。 相关下载程序一和二的下载链接]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[vim插件安装]]></title>
      <url>%2Fpost%2Fvim_plugin%2F</url>
      <content type="text"><![CDATA[本文的安装环境为ubuntu13.04。为了以后便于查阅，本文将相关插件的使用放到了文章的开始部分。这里不作插件的相关介绍，相关介绍看文章底部的参考文章。 插件使用本插件快捷键会跟随下文安装内容一块同步。 ctags在源码目录执行ctags -R可生成ctags文件。该文件在源码修改后并不会改变，需要重新生成ctags文件。ctrl+]：转到函数定义处。ctrl+T：回到执行ctrl+]的地方。 taglist:TlistOpen：打开taglist窗口:TlistClose：关闭taglist窗口。:TlistToggle：在打开和关闭间切换。 NERD tree:NERDTree：打开窗口。 winmanagerwm：打开和关闭taglist和NERD tree窗口。 a.vim:A：在新Buffer中切换到c/h文件:AS：横向分割窗口并打开c/h文件:AV：纵向分割窗口并打开c/h文件:AT：新建一个标签页并打开c/h文件F12：代替:A命令 MiniBufExplorer&lt;Tab&gt;：向前循环切换到每个buffer名上&lt;S-Tab&gt;：向后循环切换到每个buffer名上&lt;Enter&gt;：在打开光标所在的bufferd：删除光标所在的buffer 插件安装安装ctags执行： sudo apt-get install ctags。 安装taglist 下载页面：http://www.vim.org/scripts/script.php?script_id=273。下载后得到taglist_46.zip文件。 执行unzip taglist_46.zip解压文件。 将解压出的文件复制到~/.vim目录下。sudo cp ~/tmp/ ~/.vim/。 在~/.vimrc文件中添加如下：123let Tlist_Show_One_File = 1 &quot;不同时显示多个文件的tag，只显示当前文件的let Tlist_Exit_OnlyWindow = 1 &quot;如果taglist窗口是最后一个窗口，则退出vimlet Tlist_Use_Right_Window = 1 &quot;在右侧窗口中显示 参考网址：http://www.cnblogs.com/mo-beifeng/archive/2011/11/22/2259356.html 安装文件浏览器NERD tree 下载页面：http://www.vim.org/scripts/script.php?script_id=1658。 将下载后的nerdtree.zip文件解压到~/.vim目录下。 安装winmanager 下载页面：http://www.vim.org/scripts/script.php?script_id=95 将下载后的winmanager.zip文件解压到~/.vim目录下 修改.vimrc文件，添加：12let g:winManagerWindowLayout=&apos;FileExplorer|TagList&apos;nmap wm :WMToggle&lt;cr&gt; 这样利用winmanager工具将taglist和NERD tree工具整合到了一个块，输入wm可以打开和关闭窗口。 安装cscope 下载页面：http://cscope.sourceforge.net，下载后得到文件cscope-15.8a.tar.gz。 ./configure make。可能会出现错误，执行如下命令：123apt-get install libncurses-devsudo apt-get install flexsudo apt-get install byacc 然后执行make clean后重新make。 sudo make install 安装在h/c文件之间切换插件a.vim 下载页面：http://www.vim.org/scripts/script.php?script_id=31。 将下载的a.vim文件复制到~/.vim/plugin文件夹下。 在~/.vimrc文件中添加nnoremap &lt;silent&gt; &lt;F12&gt; :A&lt;CR&gt; 下面内容为快捷键列表：:A switches to the header file corresponding to the current file being edited (or vise versa):AS splits and switches:AV vertical splits and switches:AT new tab and switches:AN cycles through matches:IH switches to file under cursor:IHS splits and switches:IHV vertical splits and switches:IHT new tab and switches:IHN cycles through matchesih switches to file under cursoris switches to the alternate file of file under cursor (e.g. on &lt;foo.h&gt; switches to foo.cpp)ihn cycles through matches 安装快速浏览和操作Buffer 下载页面：http://www.vim.org/scripts/script.php?script_id=159 将下载的 minibufexpl.vim文件丢到 ~/.vim/plugin 文件夹中即可 在~/.vimrc文件中增加如下行： 123let g:miniBufExplMapCTabSwitchBufs = 1let g:miniBufExplMapWindowNavVim = 1let g:miniBufExplMapWindowNavArrows = 1 快捷键： 向前循环切换到每个buffer名上 向后循环切换到每个buffer名上 在打开光标所在的bufferd 删除光标所在的buffer 参考文章 经典vim插件功能说明、安装方法和使用方法介绍 手把手教你把Vim改装成一个IDE编程环境]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[市长热线12345]]></title>
      <url>%2Fpost%2Fmayor_hotline%2F</url>
      <content type="text"><![CDATA[前段时间在家看书学习，难得的学习的好时机。 楼下有一个卖饭的小摊及其猖狂，不仅占用了人行道来炒菜，而且还在马路上摆了一溜桌子供客人吃饭，不仅占用了人行道，连车行道都给占用了。这些也就罢了，对我影响都不算太大，更可气的是每天中午和晚上吃饭的时候会开着大音响放着恼人的音乐，我不想惹麻烦，我忍。 今天中午我刚开始看书，看到难处需要精心思考，恼人的音乐又开始了，我实在忍不住了，TMD，维权。打市长热线12345投诉，市长热线让我打110投诉。继续打110投诉，然后跟110说了下具体情况后，说给相应的派出所去处理。派出所的小片警立刻就给我回电话了，说外放音乐正常经营范围，只要不在晚上或清晨放音乐就不算违规，他们管不着，建议我去下面跟卖饭的商量，好一个商量。然后我又说，他们非法占道经营，小片警又说这个归城管管，让我给城管打电话，好一个给城管打电话。好一个推卸责任，这些把我给惹毛了。 挂断电话后，寻思这个理不太对，然后继续给市长热线12345打电话，告诉情况后，市长热线的妹子告诉我说这个事情我给你处理，好一个我给你处理，这才是为人民服务的态度，鼓掌。 这是第二次机会接触小片警，每次都是让我失望，绝望，恨之入骨。第一次接触小片警我甚至kill him的心都有了。不一心想着为人民服务，却是一心想着推卸责任，处处刁难市民并从中谋取私利，对市民爱理不理，这就是小片警在我心中的形象，很难改变。越是权利小的小兵，架子越大，这也就决定了永远是个小兵的身份。 如果没有市长热线那这件扰民的事情也就不了了之了，因为投诉110都不管用了，作为市民已经没有可以维权的机构了。还好有市长热线的存在给市民多了一个维权的途径。 上周五打的电话，这个周一给我回复电话问我饭馆在哪一次，周二又打电话问我饭馆在哪，然后周三终于给处理了，下班途中派出所给我电话回复说：“已经处理好了，让小饭馆的音响声音调小了，以后如果再有这种情况可以继续打电话”。等我回家一看，果然音响不见了，世界一下子清净了，zf终于替我办事了。 也许是因为我的事情不是很紧急的原因，整个处理流程过于慢了，等了足足五天的时间才处理好。 当大家的权益受到损害时，请大家多给市长热线打电话维护自己的权益。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[脆弱的生命]]></title>
      <url>%2Fpost%2Fweak_life%2F</url>
      <content type="text"><![CDATA[得知老家三老爷家的三叔去世了，原本还在看代码的我收到消息之后立刻无法平静了，只好出去走走散散心，回家之后依然感觉莫名的胸闷，玩游戏分散分散精力，游戏过后依然胸闷。意料之中的失眠，中途醒了好几次。总感觉消息不是真实的，总感觉昨晚在梦中，真希望一觉醒来之后什么都没有发生。 三叔42岁，正值壮年，在家附近的号称有一万员工的炼钢厂打工，在整个市也算是很大的企业了。工作中意外丧命。总觉得这样的事情不会发生在我身边，客观事实是发生了。 听家人说，三叔小时候调皮爬到树上掏鸟窝从树上掉下来把一个肩膀都磕到身体里了，大家都觉得肯定好不了，在镇上医院住院打吊瓶打够了自己偷偷跑回学校，后来居然奇迹般的好了，而且还没留下任何痕迹。大家都说三叔命大，谁知三叔小时候躲过了一劫却没有躲过这一劫，这难道就是天命？三叔一生勤俭节约，人忠厚老实，到头来却落得如此下场，谁说上帝是公平的，谁说好人有好报，这都是胡扯。 临近三叔出事的前天，我做了一个很不好的梦，梦的内容我已经记不起来了。回家后听家里人说很多人都做了不好的梦，甚至连平常不怎么做梦的都会被梦惊醒。这绝对不是巧合，很明显已经超出了当前科学的范畴。 记得最后一次跟三叔接触还是在过年的时候，三叔到我家来转转，聊了几句，现在还记忆犹新。再上一次见面就是在去年夏天的一个下午，约着三婶去火车站接三叔家的弟弟和我爸，正巧在三叔家的门口碰到三叔，估计是要去上夜班。 每年过年我们一大家20多人就会团聚在一起，男人一桌，女人一桌，还有我们小孩一桌，其乐融融。最近两年过年三叔是唯一缺席的，由于工作的原因，三叔正巧在过年的时候上夜班。总觉得少了三叔过年的时候是个遗憾，现在看来以后过年要永远遗憾下去了。 企业在追求经济效益的同时，往往会忽略员工的安全。员工伤亡事件屡见不鲜，却很难得到企业的重视。相比人类的伤亡，企业的经济效益显得那么苍白无力。听说钢厂每年总会出些事故，但是事故的赔偿是从所有员工的工资中扣除的，而不是工厂承担，这也是工厂对安全问题不够重视的原因，反正出了事掉血的是员工。 现在村中的人大部分出去在外面打工，农忙时回家忙几天。在此提醒相亲们一定要注意人身安全，没有了安全保障赚再多的钱都白搭。 谨以此文献给为工作而献身的三叔。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[地球家园]]></title>
      <url>%2Fpost%2Fthe_earth%2F</url>
      <content type="text"><![CDATA[这段时间，中国大部分的地区都是高温不降，有些地区甚至出现了鸡蛋自然孵化的现象。媒体报道高温热死人的现象也是时有发生，虽说媒体的话不可信，但至少从一个侧面反映了天气的确是较往年同期高出一些，开始渐渐超出人类身体可以承受的温度，开始悄悄的打破往年同期的高温纪录。 最近几年气候变化无常，跟人类的活动绝对脱不了干系。冬天雾霾可以持续一周不散，春天一个月干旱，夏季雨天可以持续一个月不变，秋季如蝉的生命般短暂。在北方已经生活了二十多年的我，这些现象在小时候是极少碰到的，现在却是极其频繁。记得小时候雨天过后时常会在天边挂上一道弯弯的彩虹，记得最后一次见彩虹是在小学一二年纪的时候，自此之后彩虹仅存在了我的脑海里。对于现在的大部分中国人而言，彩虹仅存在于永恒的记忆中和孩子们的画中。 人类近几百年来正在肆无忌惮的向地球母亲索要不该属于人类自己的东西，人类已经占有了迄今为止地球上对人类有价值的且可以占有的所有资源，人类仍然在忘形的开发并破坏着地球上生物赖以生存的家园。 拿中国的三峡大坝举例，从能源的角度考虑的确是有利的。但是从地球生态的角度考虑肯定是有害的。人类的存在时间相对地球是短暂的，地球每一处地形存在就有它存在的理由，已经经过了无数年的实践验证说明地形存在的正确性。可恶的中国ZF，可恨的脑残砖家居然能够利用理论来论证修建三峡的必要及正确性，TMD没学过实践是检验真理的唯一标准。在没有对地球有充足的了解之前不要利用有限的理论来推断并指导实践，因为往往实践之后就再也回不了头，就比如三峡大坝。谁敢说近几年的西南大旱、特大地震跟三峡拖得了干系，可以灾难发生了又有哪个砖家可以站出来声称我可以对这个灾难负责呢？ 我从小就一直在担忧一个问题如果再过几十年后几百年后地球上的煤炭、石油等不可再生资源被人类用过了人类该何去何从，我时长为此而忧虑不已。也许这有点杞人忧天，肯定有人会站出来说到那时候随着人类科技的发展早就发现了新能源了，这是谁给的自信？谁这么大胆敢预言人类几十年后一定可以发现新能源？何况不可再生资源中蕴藏着的价值肯定不仅是燃烧带来的能量这点价值，假如几十年后人类已经将石油资源消耗殆尽了，却发现石油中蕴藏着巨大的能量，估计那时候我们只有哭的份了，楚人一炬，可怜焦土。 人类不过是地球上几百万种生物中的一种，如果硬要从广义公平的角度来考虑，人类在生物界占有的太多了，人类已经把该占有的不该占有的全部占为己有，贪婪的本性暴露无遗。很难想象几十年过后我们人类的家园已经成为了什么样子，四处可见的是拔地而起的高楼，柏油路横一条竖一条，无论在地球的哪个角落都能找到人类留下的痕迹。地球该随着人类的发展何去何从，我不敢想象，我能做到的仅仅是节约点力所能及的资源，仅此而已。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度探索C++对象模型读书笔记_第四章：函数语意学]]></title>
      <url>%2Fpost%2Finside_the_c%2B%2B_object_model_chapter_4%2F</url>
      <content type="text"><![CDATA[成员函数的各种调用方式非静态成员函数的调用方式在C++中必须要保证类的非静态成员函数必须和非成员函数的执行效率一致，在编译的过程中，编译期已经将类的非静态成员函数编译为了非成员函数。 123456789101112131415161718192021class Test&#123;public: int sum() const &#123; return a + b; &#125; int add() &#123; return a; &#125; int add(int size) &#123; return a + size; &#125;public: int a; int b;&#125;; 在上述Test类中，编译器会在编译阶段对类中的成员函数做一些转换。下面列出了编译器可能会做出的变换，不同的编译器实现不太一致。 1234567891011121314int sum_TestFv(const Test * const this)&#123; return this-&gt;a + this-&gt;b;&#125;int add_TestFv(Test * const this)&#123; return this-&gt;a;&#125;int add_Testi(int size, Test * const this)&#123; return this-&gt;a + size;&#125; 通过上面的变换可以总结出如下规律： 将成员函数重新改写为一个外部函数，并且对函数的名字进行处理，使在程序中唯一。一种可能的处理办法就是将函数名更改为：函数名_类名_函数参数。这样即解决了类之间函数名相同的问题，又解决了类之间函数重载的问题。 在函数的参数末尾添加额外的this指针参数。对于const函数添加的this指针为双const类型，对于非const函数则添加的this指针为指向的内容可变的const指针。 在函数内对成员函数的存取采用this指针来实现。 虚成员函数虚函数如果是通过指针类型访问，需要在运行时动态决定指针指向的类型，因此需要访问虚函数表才能够获取正确的虚函数地址。访问虚函数的方式为(*ptr-&gt;vptr[i])(ptr)，其中i代表要调用的虚函数在虚函数表中的索引，最后一个ptr代表要调用虚函数的编译器添加的this指针参数。 关于虚成员函数的更详细问题会在下一个节中进行讨论。 静态成员函数静态成员函数中没有this指针，可以理解成带类作用域的全局函数，执行效率跟全局函数一致。 虚成员函数这部分内容是本书的核心内容，可以参考陈皓的博客相关文章，已经对C++中的虚成员函数和虚成员变量进行了说明。 C++ 虚函数表解析 C++ 对象的内存布局(上) C++ 对象的内存布局(下) 单一继承下的虚函数123456789101112131415161718192021222324252627282930313233343536class Point&#123;public: virtual ~Point()&#123;&#125; virtual Point&amp; mult(float) = 0; float x() const &#123;return _x;&#125; virtual float y() const &#123;return 0;&#125; virtual float z() const &#123;return 0;&#125;protected: Point(float x=0.0) &#123;_x = x;&#125; float _x;&#125;;class Point2d : public Point&#123;public: Point2d(float x=0.0, float y=0.0) : Point(x), _y(y) &#123;&#125; ~Point2d()&#123;&#125; Point2d&amp; mult(float)&#123;return *this;&#125; float y() const &#123;return _y;&#125;protected: float _y;&#125;;class Point3d : public Point2d&#123;public: Point3d(float x=0.0, float y=0.0, float z=0.0) : Point2d(x, y), _z(z) &#123;&#125; ~Point3d()&#123;&#125; Point3d &amp; mult(float) &#123;return *this;&#125; float z() const &#123;return _z;&#125;protected: float _z;&#125;; 三个类对应的虚函数表会转化成下图 通过图中可以看出每个函数在虚函数表中的位置无论在基类还是在子类中位置总是固定的。图中的Point的实例应该是不存在的，因为类中含有纯虚函数mult。 要想调用ptr-&gt;z()就变得非常容易，可以在编译器就可以确定虚函数的调用。虽然ptr所指向的对象在编译器并不能确定，但是编译器可以将其转化成为(*ptr-&gt;vptr[4])(ptr)。因为z()函数总是在虚函数表中的第四个位置，唯一需要在执行期确定的就是ptr所指的对象的实际类型。 多重继承下的虚函数避免重复造轮子，参考上面博文。 虚拟继承下的虚函数避免重复造轮子，参考上面博文。 函数的效率非成员函数、静态成员函数、非静态成员函数都被转换成为了完全相同的形式。inline函数的执行效率最高。虚函数的效率最低。 指向成员函数的指针这里学习到一个新的语法，之前没有接触过。即指向类成员函数的指针及使用方法。1234567891011121314151617181920212223#include &lt;iostream&gt;using namespace std;class Point&#123;public: virtual ~Point() &#123;&#125; float x() &#123;return _x;&#125;public: Point(float x=0.0) &#123; _x = x; &#125; float _x;&#125;;int main()&#123; Point point(1.0); float (Point::*p)(); // 定义指向成员函数的指针 p = &amp;Point::x; // 为指向成员函数的指针赋值 cout &lt;&lt; (point.*p)(); // 调用指向类成员函数的指针 return 1;&#125; 如果成员函数的指针并不用于虚函数、多重继承、虚基类等情况，则成员函数的指针效率跟非成员函数指针的效率一致。 指向虚成员函数的指针书中对于函数取地址的语法在gcc和vs2008下我试验不成功，语法错误。 多重继承下指向成员函数的指针依赖于编译器的实现，用到的情况比较少，没仔细看。 指向成员函数指针的效率在引入了虚函数、多重继承、虚基类等情况后，指向成员函数的指针效率有所下降。 内联函数内联只是一个请求，编译器并不一定会将函数内联的展开。 形式参数内联时每一个形参都会被对应的实参取代。 1234567891011121314inline int min(int i, int j)&#123; return i &lt; j ? i : j;&#125;inline int bar()&#123; int minval; int val1 = 1024; int val2 = 2048; minval = min(val1, val2); minval = min(1024, 2048); minval = min(foo(), bar() + 1);&#125; minval=min(val1, val2)会被内联展开成minval = val1 &lt; val2 ? val1 : val2。minval = min(1024, 2048)会被扩展为minval = 1024。minval = min(foo(), bar() + 1)需要引入临时对象，被扩展为123int t1;int t2;minval = (t1 = foo()) , (t2 = bar() + 1), t1 &lt; t2 ? t1 : t2; 局部变量12345678910111213inline int min(int i, int j)&#123; int minval = i &lt; j ? i : j; return minval;&#125;inline int bar()&#123; int minval; int val1 = 1024; int val2 = 2048; minval = min(val1, val2);&#125; 在内联函数中引入局部变量，内联函数在内联的时候局部变量会拥有一个唯一的名称。代码中的minval = min(val1, val2)会被内联为： 12int _min_lv_minval;minval = (_min_lv_minval = val1 &lt; val2 ? val1 : val2), _min_lv_minval; 内联函数可以代替C语言中的#define宏定义，但是当内联函数调用次数过多，会产生大量的扩展代码，使程序的大小变大。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度探索C++对象模型读书笔记_第三章：数据语意学]]></title>
      <url>%2Fpost%2Finside_the_c%2B%2B_object_model_chapter_3%2F</url>
      <content type="text"><![CDATA[1234class X &#123;&#125;;class Y : public virtual X &#123;&#125;;class Z : public virtual X &#123;&#125;;class A : public Y, public Z &#123;&#125;; 对于上述代码我在vs2008中的实验结果，X的大小为1，Y和Z的大小为4，A的大小为8。X的大小为1是因为编译器给空类了1字节的空间。Y和Z的大小为4是因为内部包含一个vbptr（指向虚基类的指针）占用了4个字节。A的大小包含了两个vbptr，分别指向虚基类的指针X。利用cl main.cpp /d1reportSingleClassLayoutA命令可以查看对象的内存布局，利用vs2008调试界面查看对象的内存布局往往是不全的，不推荐采用此种方式。下面为A的类布局。 在64位的linux的g++下测试X、Y、Z、A的大小分别为1、8、8、16，这是因为指针的大小为8个字节。 一个类占用的空间比类本身非静态数据成员空间大的原因有如下两点： 编译器自动加上额外的数据成员，用来支持某些语言特性，例如virtual特性。 内存边界调整的需要 3.1 数据成员的绑定味同嚼蜡的章节。 3.2 数据成员的布局数据成员在内存中的布局顺序跟数据成员在类中的声明顺序是一致的，而且现在的编译器都不关心数据成员在类中是public、protected还是private的。 为了内存对齐，编译器在变量之间插入了空白字节，不同的编译器内存对齐的原则并不一致。 为了实现虚函数机制，编译器插入了vptr成员变量。 以上这些内容，本章节并没有展开详解。 3.3 数据成员变量的存取数据成员包括静态数据成员和非静态数据成员。 静态数据成员变量放在静态存储区，不会造成任何空间或执行时间上的浪费。 对于非静态数据成员，无论成员变量是struct数据成员、类数据成员、单一继承、多重继承情况下执行效率完全一样。执行效率较静态数据成员变量稍低。 12345678class Test&#123;public: int a; int b; int c;&#125;;Test test; 在上述例子中要想读取test.c的位置，编译器需要执行类似这样的操作：&amp;test + &amp;Test::c，可以看出对类中变量的存取成本多了一个算数运算。 对于虚拟继承的情况由于需要在运行期才能决定存取操作，需要一些额外的成本，在下文讨论。 3.4 继承与数据成员如果类中不包含继承机制，则数据成员的布局和struct中数据成员的布局是一致的。 本节将从单一继承但不包含虚函数、单一继承包含虚函数、多重继承、虚拟继承四个方面讨论数据成员变量。陈浩有几篇博文对此进行了详细的解释，比书上内容要易懂和全面，这几篇文章必看。 C++ 虚函数表解析 C++ 对象的内存布局(上) C++ 对象的内存布局(下) 单一继承且不包含虚函数书中举例解释了为什么类继承时类成员之间的填补空白会比单个类时要多，下图的内存布局图中Concrete3继承自Concrete2，Concrete2继承自Concrete1。Concrete3类占用的空间大小为：bit1占用的1个字节+3个字节的空白，bit2占用的1字节+3字节的空白，bit3占用的1字节+3字节空白。如果Concrete3不继承自任何对象，而是包含bit1、bit2、bit3三个变量，占用的空间大小为1+1+1+1=4。 之所以编译器在继承机制中会作如此处理，是为了在继承机制中对象之间的默认按比特复制操作更方便。例如： 12Concrete1 *p1 = new Concrete1, *p2 = new Concrete2;*p2 = *p1; // 此时编译器只需要按比特复制就可以了 单一继承包含虚函数假设有如下类定义 12345678910111213141516171819202122232425262728293031323334class Base&#123;public: Base() &#123; printf("Base\n"); &#125; virtual ~Base() &#123; printf("~Base\n"); &#125; virtual void foo() &#123;&#125; int base_x;&#125;;class Derived : public Base&#123;public: Derived() &#123; printf("Derived\n"); &#125; ~Derived() &#123; printf("~Derived\n"); &#125; void foo() &#123;&#125; int derived_y;&#125;; 则Derived类的对象模型如下，通过图可以非常清晰的理解单一继承包含虚函数的情况： 多重继承1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950class Base1&#123;public: Base1() &#123; printf("Base1\n"); &#125; virtual ~Base1() &#123; printf("~Base1\n"); &#125; virtual void base1_virtual_func() &#123;&#125; int base1_x;&#125;;class Base2&#123;public: Base2() &#123; printf("Base2\n"); &#125; virtual ~Base2() &#123; printf("~Base2\n"); &#125; void base2_not_virtual_func() &#123;&#125; int base2_x;&#125;;class Derived : public Base1, public Base2&#123;public: Derived() &#123; printf("Derived\n"); &#125; ~Derived() &#123; printf("~Derived\n"); &#125; void derived_func() &#123;&#125; void base1_virtual_func() &#123;&#125; int derived_y;&#125;; 则Vertex3d类的对象模型如下，同样通过图可以非常清晰的理解多重继承的情况： 重复继承书中并没有涉及到重复继承，重复继承是指某个基类被间接重复继承了多次，属于重复继承和钻石级多重虚拟继承的过渡情况，有必要说明一下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Base&#123;public: virtual void base_virtual_func() &#123;&#125; void base_func() &#123;&#125; int base_x;&#125;;class Base1 : public Base&#123;public: Base1() &#123; printf("Base1\n"); &#125; virtual ~Base1() &#123; printf("~Base1\n"); &#125; virtual void base_virtual_func() &#123;&#125; virtual void base1_virtual_func() &#123;&#125; int base1_x;&#125;;class Base2 : public Base&#123;public: Base2() &#123; printf("Base2\n"); &#125; virtual ~Base2() &#123; printf("~Base2\n"); &#125; virtual void base_virtual_func() &#123;&#125; void base2_not_virtual_func() &#123;&#125; int base2_x;&#125;;class Derived : public Base1, public Base2&#123;public: Derived() &#123; printf("Derived\n"); &#125; ~Derived() &#123; printf("~Derived\n"); &#125; void derived_func() &#123;&#125; void base1_virtual_func() &#123;&#125; int derived_y;&#125;; 通过下图的对象模型可以看出，重复继承的类Base在Derived的实例中存在两份，要想直接更改Base类中的base_x变量的值，不能通过derived.base_x = 1直接赋值的方式，需要调用derived.Base1::base_x = 1的方式来更改，更改后的效果仅更改了Base1对象对应的Base类实例中的base_x的值。 钻石型多重虚拟继承该种方式的继承已经是所有继承中最为复杂的了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class Base&#123;public: virtual void base_virtual_func() &#123;&#125; // 虚基类最好是不再提供虚函数 void base_func() &#123;&#125; int base_x;&#125;;class Base1 : virtual public Base&#123;public: Base1() &#123; printf("Base1\n"); &#125; virtual ~Base1() &#123; printf("~Base1\n"); &#125; virtual void base_virtual_func() &#123;&#125; virtual void base1_virtual_func() &#123;&#125; int base1_x;&#125;;class Base2 : virtual public Base&#123;public: Base2() &#123; printf("Base2\n"); &#125; virtual ~Base2() &#123; printf("~Base2\n"); &#125; //virtual void base_virtual_func() &#123;&#125; // 由于是虚拟继承，不再能重复重载父类的虚函数了 void base2_not_virtual_func() &#123;&#125; int base2_x;&#125;;class Derived : public Base1, public Base2&#123;public: Derived() &#123; printf("Derived\n"); &#125; ~Derived() &#123; printf("~Derived\n"); &#125; void derived_func() &#123;&#125; void base1_virtual_func() &#123;&#125; int derived_y;&#125;; 在下图标出的区域中，我认为Base应该是不存在的，这里只是vs2013为了展示的考虑而添加上的。虚拟继承基类Base位于Derived类对象的除该成员外的最后位置。 对象成员的效率作者经过试验测试，继承下的类成员读写效率跟读写普通变量效率相差不大，虚拟继承对程序的读写效率有影响。这跟理论上相差不大。 指向数据成员的指针小技巧：可以通过&amp;类名::变量名的语法来获取类成员变量在类对象中的位置，即相对于类对象起始地址的偏移量。 书中后面的内容个人感觉没有必要看了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度探索C++对象模型读书笔记_第二章：构造函数语义学]]></title>
      <url>%2Fpost%2Finside_the_c%2B%2B_object_model_chapter_2%2F</url>
      <content type="text"><![CDATA[2.1 默认构造函数的生成只有当编译器需要默认构造函数的时候才会合成默认构造函数，并不是类只要没有定义默认构造函数编译器就会合成默认构造函数，而是只有以下四种情况编译器会生成默认构造函数。编译器合成的默认构造函数仅会处理类的基类对象和类中的数据成员对象，对于类中的普通类型的非静态数据成员并不会作任何处理。比如类中一个指针类型的数据成员，编译器合成的默认构造函数不会对该指针作任何处理，该指针就是一个野指针。 带有默认构造函数的类成员对象一个类没有定义任何构造函数，该类中包含了一个带有默认构造函数（包括了合成的默认构造函数和定义的默认构造函数）的类成员对象，那么编译器需要为此类合成一个默认构造函数，合成默认构造函数的时机为该构造函数被调用时。合成的默认构造函数默认为内联函数，如果不适合使用内联函数，就合成explicit static的构造函数。 默认构造函数、复制构造函数和赋值操作符的生成都是inline。inline函数有静态链接，不会被当前文件之外的文件看到。如果函数过于复杂不适合生成inline函数，会生成一个explicit non_inline static实体。 12345678910111213141516171819202122232425262728293031323334class Dopey&#123;public: Dopey();&#125;;class Sneezy&#123;public: Sneezy(int); Sneezy();&#125;;class Bashful&#123;public: Bashful();&#125;;class Snow_White&#123;public: Dopey dopey; Sneezy sneezy; Bashful bashful;private: int mumble;&#125;;void foo()&#123; Snow_White snow_white;&#125; 在上述例子中，foo()中需要调用Bashful的构造函数，编译器会为Bar类生成内联的默认构造函数。Bashful类会生成类似于下面的默认构造函数。 123456inline Bar::Bar()&#123; dopey.Dopey::Dopey(); sneezy.Sneezy::Sneezy(); bashful.Bashful::Bashful();&#125; 默认构造函数的生成原则为：如果类A中包含了一个或一个以上的类成员对象，那么类A的默认构造函数必须调用每一个类成员的默认构造函数。但是不会初始化普通类型的变量，因此在上例中必须手动初始化mumble变量。在编译器合成的默认构造函数中类成员变量的默认构造函数的调用次序为成员变量在类中的声明顺序，该顺序和类成员的构造函数初始化列表顺序是一致的。 如果Snow_White类定义了如下的默认构造函数，则编译器会自动在定义的构造函数中增加调用类成员变量的代码，调用类成员变量相应构造函数的顺序仍然和类成员变量在类中的声明顺序一致。 从中可以看出类成员变量的构造函数的调用要早于类构造函数的调用，这一点是在很多面试题中经常会见到的。 1234567891011121314// 定义的默认构造函数，包含了类成员变量sneezy的初始化列表Snow_White::Snow_White() : sneezy(1024)&#123; mumble = 2048;&#125;// 编译器扩张后的默认构造函数Snow_White::Snow_White() : sneezy(1024)&#123; dopey.Dopey::Dopey(); // 调用默认构造函数 sneezy.Sneezy::Sneezy(1024); // 自动调用合适的构造函数 bashful.Bashful::Bashful(); mumble = 2048;&#125; 带有默认构造函数的基类在继承机制中，一个没有构造函数的子类继承自带有默认构造函数的基类，则子类的构造函数会被合成，并且会调用基类的默认构造函数。若子类没有定义默认构造函数，却定义了多个带参数的构造函数，编译器会扩张所有自定义的构造函数，将调用基类默认构造函数的代码添加到子类的构造函数的最前面。 从这里可以看出继承机制中，首先构造基类，后构造子类，这点也是面试题中经常遇到的。 带有虚函数的类为了实现虚函数或虚继承机制，编译器需要为每一个类对象设定vptr（指向虚函数表的指针）的初始值，使其指向一个vtbl（虚函数表）的地址。如果类包含构造函数则编译器会生成一些代码来完成此工作；如果类没有任何构造函数，则编译器会在合成的默认构造函数中添加代码来完成此工作。 带有虚基类的类需要维护内部指针来实现虚继承。 2.2 复制构造函数的生成复制构造函数被调用有三种情况： 明确的一个对象的内容作为另外一个对象的初始值。如X xx = x或X xx(x)。 对象作为参数传递给函数时。 类对象作为函数返回值时。 合成复制构造函数的情况如果一个类没有提供显式的复制构造函数，同默认构造函数一样，只有编译器认为需要合成复制构造函数时，编译器才会合成一个。那么问题来了，什么时候编译器才合成复制构造函数呢？书中给出的答案为当一个类不展现出bitwise copy semantics1的时候。具体来说有以下四种情况，跟类的默认构造函数的合成基本一致。 当类内包含一个类成员变量且类成员变量声明了复制构造函数。 当类继承的基类有复制构造函数（复制构造函数可以是显示声明或合成的） 一个类中包含了一个或多个虚函数 类继承自一个或多个虚基类 其中前面两种情况必须将成员变量或基类的复制构造函数的调用插入到合成的复制构造函数中，因此不是按照按比特复制的。第三和第四点分别用下面两小节来说明。 重新设定虚函数表的指针当编译器需要在类对象中设定一个指向虚函数表的指针时，该类就不能再使用按位复制的复制构造函数了。 123456789101112131415161718192021222324252627class ZooAnimal&#123;public: ZooAnimal(); virtual ~ZooAnimal(); virtual void animate(); virtual void draw();&#125;;class Bear : public ZooAnimal&#123;public: Bear(); void animate(); void draw(); virtual void dance();&#125;;void foo()&#123; // yogi的vptr指向Bear的虚函数表 Bear yogi; // franny的vptr指向ZooAnimal的虚函数表 ZooAnimal franny = yogi; draw(yogi); // 调用Bear::draw() draw(franny); // 调用ZooAnimal::draw()&#125; 合成出来的ZooAnimal的复制构造函数会明确设定对象的vptr指向ZooAnimal的虚函数表，而不是从右值中复制过来的值。 处理virtual base class subobjects虚基类的存在需要特别处理，一个类对象如果以另外一个类对象作为初始值，而后者有一个virtual base class subobjects，也会使按比特复制的复制构造函数失效。 每一个编译器都必须让派生的类对象的virtual base class subobjects位置在执行期准备完毕。按比特复制的复制构造函数可能会破坏virtual base class subobjects的位置，因此编译器必须在自己合成出来的复制构造函数中修改。 1234567891011121314151617181920212223class ZooAnimal&#123;public: ZooAnimal(); virtual ~ZooAnimal(); virtual void animate(); virtual void draw();&#125;;class Raccoon : public virtual ZooAnimal&#123;public: Raccoon(); Raccoon(int val);&#125;;class RedPanda : public Raccoon&#123;public: RedPanda(); RedPanda(int val);&#125;; 文章的内容没有完全理解，虚继承机制使用较少，可以暂时不用理解。 2.3 程序转化语意学本节涉及到了编译器优化的相关细节，由于较容易理解，可以直接看书上内容，对工作帮助不大。包括类对象的初始化优化，函数参数的初始化优化，函数返回值的初始化优化，使用者层面的优化和编译器层面的优化。 如果不是上节指定的四种情况，不需要显示的声明复制构造函数，因为显示的声明的复制构造函数往往效率不如编译器合成的复制构造函数效率高。编译器合成的复制构造函数利用memcpy()或memset()函数来合成，效率最高。 2.4 类成员的初始化列表说到类成员的初始化列表必然想起一个经常出现的面试题：成员初始化列表的顺序是按照成员变量在类中声明的顺序。如果成员初始化列表的顺序和成员变量在类中声明的顺序不一致时某些编译器会提示警告。编译器将成员初始化列表的代码插入到构造函数的最开始位置，优先级跟调用类类型的成员变量的默认构造函数是一致的，都是跟类类型成员变量在类中的声明次序相关。 类成员初始化必须使用成员初始化列表的四种方式： 初始化一个引用类型的成员变量 初始化一个const的成员变量 调用基类的构造函数，且基类的构造函数采用成员初始化列表的方式 调用类成员的构造函数，且类成员的构造函数采用成员初始化列表的方式 12345678910111213class Word&#123;public: Word() &#123; _name = 0; _cnt = 0; &#125;private: String _name; int _cnt;&#125;; 此例子在构造函数中对成员变量进行测试，编译器对构造函数的扩张方式可能会生成如下的伪码： 12345678Word::Word()&#123; _name.String::String(); String temp = String(0); _name.String::operator=(temp); temp.String::~String(); _cnt = 0;&#125; 构造函数中生成了一个临时性的String对象，这浪费了一部分开销。如果将构造函数该成如下的定义方式： 1234Word() : _name(0) &#123; _cnt = 0; &#125; 即将其中的类成员变量更改为成员初始化列表的方式来初始化，编译器会自动将构造函数扩张为如下方式，这样减少了临时对象，提供了程序效率。 12345Word::Word()&#123; _name.String::String(0); _cnt = 0;&#125; 引申下面例子是对本章内容的一个简单概况，也是面试题中经常碰到的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class A&#123;public: A() &#123; printf("A\n"); &#125; ~A() &#123; printf("~A\n"); &#125; &#125;;class B&#123;public: B(int n) &#123; printf("B_%d\n", n); &#125; ~B() &#123; printf("~B\n"); &#125; &#125;;class Base&#123;public: Base() &#123; printf("Base\n"); &#125; virtual ~Base() &#123; printf("~Base\n"); &#125;&#125;;class Derived : public Base&#123;public: Derived() : _m(1), _b(_m) &#123; printf("Derived\n"); &#125; ~Derived() &#123; printf("~Derived\n"); &#125; int _m; // 下面两个类类型的成员遍历的构造函数的调用次序跟在类中的声明次序是相关的 B _b; // 类类型的类成员变量，初始化列表中包含该变量 A _a; // 类类型的类成员变量&#125;;int main()&#123; // 调用基类的构造函数-&gt;调用子类类类型成员变量的构造函数-&gt;调用子类的构造函数 Derived derived; return 0; // 根据栈的特点，类析构的次序跟构造是相反的&#125; 上述代码执行的结果为： 12345678BaseB_1ADerived~Derived~A~B~Base 总结本章讲述了合成的默认构造函数、合成的复制构造函数和构造函数的成员初始化列表。其中如果类没有定义默认构造函数，只有在文中提到的四种情况下编译器才会合成默认构造函数。合成的复制构造函数在需要的时候编译器就会生成，默认是按对象比特复制的方式实现，有四种情况下编译器是不按照比特复制的方式。 [1] bitwise copy semantics书中翻译为“位逐次拷贝”，就是按照内存中的字节进行复制类，感觉翻译不如不翻译好理解。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[深度探索C++对象模型读书笔记_第一章：关于对象]]></title>
      <url>%2Fpost%2Finside_the_c%2B%2B_object_model_chapter_1%2F</url>
      <content type="text"><![CDATA[C++对象模型是深入了解C++必须掌握知识，而《深度探索C++对象模型》一书基本是理解C++对象模型的必须之作。可惜本书看起来更像是作者Stanley B.Lippman的随笔，语言诙谐，跟作者的另外一本经典之作《C++ Primer》有着天壤之别，侯捷的翻译也是晦涩难懂，跟侯捷翻译的其他作品也有一定差距（这两位大师还真是凑到一块了），所以这本书看起来还是很吃力的。这里挑选文中重点记录笔记，忽略扯淡部分，以备忘。 C++的额外开销C++相比C语言多出了封装、继承和多态等特性，新特性的增加必然会以牺牲一部分性能为代价。额外开销主要是由virtual引起的，包括： 虚函数机制：需要在运行期间动态绑定。 虚基类：多次出现在继承中的基类有一个单一且被共享的实体。 C++对象模型C++类成员变量包括：静态成员变量和非静态成员变量。成员函数包括：静态成员函数、非静态成员函数和虚函数。 单一类的对象模型123456789101112class Point &#123; public: Point(float xval); virtual ~Point(); float x() const; /* 非静态成员函数 */ static int PointCount(); /* 静态成员函数 */ protected: virtual ostream&amp; print(ostream &amp;os) const; /* 虚函数 */ float _x; /* 非静态成员变量 */ static int _point_count; /* 静态成员函数 */ &#125; 该类的c++对象模型如下图： 通过图中可以看出： 非静态数据成员直接放到了类的对象里面。 静态数据成员放到所有的类对象的外面，即静态存储区。 静态和非静态的成员函数放在类对象之外，即代码区。 如果类中存在虚函数，则会产生一个虚函数表（vtbl），表中的表项存储了指向虚函数的指针。在类对象的开始位置添加一个指向虚函数表的指针（vptr）。vptr的赋值由类的构造函数和赋值运算符自动完成。 虚函数表的第一项指向用来作为动态类型识别用的type_info对象。 C++支持的编程范式(programming paradigms) 程序模型：通俗的理解成C语言的面向过程编程方式。 抽象数据类型模型：通过类封装成为了一种新的数据类型，该数据类型有别于基本数据类型。 面向对象模型：利用封装、继承和多态的特性。 C++支持多态的方式 隐含的转换操作，例如通过父类的指针指向子类的对象。shape *ps = new circle(); 通过虚函数机制。 通过dynamic_cast强制类型转换。如if (circle *pc = dynamic_cast&lt;circle*&gt;(ps))。 类对象的内存构成 非静态数据成员。 由于内存对齐而添加到非静态数据成员中的空白。 为了支持虚机制（包括：虚函数和虚继承）而额外占用的内存。 利用工具查看对象模型查看C++类的对象模型有两种比较简便的方式，一种是使用Virtual Studio在调试模式下查看对象的组成，但是往往显示的对象不全;另外一种是通过Virtual Studio中cl命令来静态查看。 本文选择使用cl工具的方式，cl命令位于C:\Program Files (x86)\Microsoft Visual Studio 9.0\VC\bin目录下，为了方便使用可以将该变量添加到Path环境变量中。在命令行中执行cl命令，提示“计算机中丢失mspdb80.dll”，该文件位于C:\Program Files (x86)\Microsoft Visual Studio 9.0\Common7\IDE目录下，将msobj80.dll,mspdb80.dll,mspdbcore.dll,mspdbsrv.exe四个文件复制到C:\Program Files (x86)\Microsoft Visual Studio 9.0\VC\bin目录下即可。 通过cl xxx.cpp /d1reportSingleClassLayoutXX命令即可查看文件中类的对象模型，其中该命令最后的XX需要更换为要查看的类名，中间没有空格。 执行上述命令时提示：无法打开文件“LIBCMT.lib”。该文件位于C:\Program Files (x86)\Microsoft Visual Studio 9.0\VC\lib目录下，将该目录添加到环境变量lib中。重新打开命令行执行cl提示：无法打开文件“kernel32.lib”，将C:\Program Files\Microsoft SDKs\Windows\v6.0A\Lib添加lib环境变量中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Java读取C语言写的二进制文件]]></title>
      <url>%2Fpost%2Fjava_call_c_bit_file%2F</url>
      <content type="text"><![CDATA[本程序将讲解java调用C语言写的二进制文件，并将二进制文件中的内容利用Java读出。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;union data &#123; int inter; char ch; &#125;;struct Test&#123; int length; char arr[20]; void toBigEndian() &#123; union data c; c.inter = 1; if(c.ch == 1) &#123; // 小端 unsigned char temp; unsigned char *tempData = (unsigned char *)&amp;length; for (int i=0; i &lt; sizeof(int) / 2; i++) &#123; temp = tempData[i]; tempData[i] = tempData[sizeof(int) - i - 1]; tempData[sizeof(int) - i - 1] = temp; &#125; &#125; &#125; &#125;;int main()&#123; Test test; memset(&amp;test, 0, sizeof(Test)); test.length = 0x12345678; strcpy(test.arr, &quot;hello world&quot;); test.toBigEndian(); FILE *file = fopen(&quot;test.txt&quot;, &quot;w+&quot;); fwrite(&amp;test, sizeof(Test), 1, file); fclose(file); return 1;&#125; 本例子中的C程序将一个包含int变量和char数组的结构体写入文件中。 其中需要考虑到机器的大小端问题，java程序采用的大端字节序，因此这里将C的结构体在写入文件时转换成大端字节序。在将结构体写入到文件时，将其中的int类型变量转换成大端字节序，如果机器本身即为大端字节序则不需要转换字节序。 java端读取的文件代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;public class Main &#123; public static int byte2int(byte[] res) &#123; int targets = (res[3] &amp; 0xff) | (res[2] &lt;&lt; 8) | (res[1] &lt;&lt; 16) | (res[0] &lt;&lt; 24); return targets; &#125; /** * @param args */ public static void main(String[] args) &#123; File file = new File("test.txt"); if (!file.exists()) &#123; System.out.println("文件不存在"); return; &#125; byte[] data = new byte[50]; try &#123; FileInputStream fis = new FileInputStream(file); int size = fis.read(data); System.out.println("读取到" + size + "个字节的数据"); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; // 转换完成的int值 int value = byte2int(data); System.out.printf("%x\n", value); StringBuffer sb = new StringBuffer(); for (int i=4; i&lt;24; i++) &#123; System.out.print((char)data[i]); &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[药物过度依赖]]></title>
      <url>%2Fpost%2Frely_on_drug_over%2F</url>
      <content type="text"><![CDATA[药是什么？药是人类文明的发展过程中不断克服自身疾病的必然产物。谁让外星人造人的时候没有把人类制造的那么完美，要是人类除了生老死之外没有病这个状态，或许药物就不会产生。 现实生活中，过度依赖药物的人比比皆是，尤其是在中国。在中国，药物已经用到了满天飞的程度了。得个小感冒会去药店买药，医生除了推荐你感冒药之外，肯定会推荐你消炎药。以至于大众普遍认为，有炎症必须吃消炎药。感冒烧到38度，医院医生会推荐你打针。感冒烧到38.5度，医生会推荐你挂上三天吊瓶，美其名曰挂吊瓶好的快，如果患者的心理能起到恢复快的作用的话，那么挂吊瓶应该是管用的。 我没去过西方国家，但我猜测在西方国家，医药分离的占多数。得病了想吃个消炎药解解馋可没那么容易，消炎药岂能是想吃就吃，没有医生的处方药店怎敢卖给患者，又不是口香糖。想打个针过过瘾更是难了，喜欢花钱让针扎着屁股玩的活中国人比较喜欢。要想挂个吊瓶数滴答玩，除非烧到了40度一星期高烧不退，否则还是自己在家挂瓶自来水数着玩吧。以上有杜撰的成分，但是话激理不歪，你懂得。 要知道中国较西方的文明程度还有差距。西方发明的西药在西方都谨慎使用了，反倒在中国大行其道。谁家没一抽屉瓶瓶罐罐，这可都是毒药啊。除了可以拉动GDP外，有理由怀疑中国担心未来人口老龄化加剧给经济发展造成压力而采取的措施。 是时候阐述本文的观点了，药物不是万能的，能少用尽量少用。 中药讲究是药三分毒，换成西药是药五分毒应该毫不夸张吧。当时治好了我们身体的病症，感觉一身轻松了，但是后患无穷。人类本身就具备一定的修复能力，而且修复能力很强。在人类自我修复能力许可的范围内非要硬用药物加速治疗，那造成的必然结果就是身体下次不干了，有药物可以对抗疾病，那用身体的修复能力干嘛。久而久之，身体就再也扛不住疾病了。 吃西药多了往往治的病好了，却带来了其他疾病。比如感冒了吃消炎药，吃着吃着把胃给吃坏了。治感冒的医生才不会管你的其他身体部位情况，反正胃吃坏了找不到他。这也是医院分那么多科室的一个弊端，只从局部看问题，不能从整体上看问题，所以看到的问题总是片面的。 解决药物依赖的问题从三方面着手解决。一方面要从心理上战胜自己，不要迷信药物，更不能依赖药物。另一方面，多运动，生命在于运动，真理中的真理。第三，眼光放长远，多看其他国家，多了解长寿的人是怎么生存的。 真心希望过度依赖药物的人可以走出这个误区。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[C/C++程序员面试宝典读书笔记]]></title>
      <url>%2Fpost%2FC_C%2B%2B_review%2F</url>
      <content type="text"><![CDATA[本文摘录《C/C++程序员面试宝典》一书中我认为需要注意的地方。 数组指针与指针数组的区别数组指针即指向数组的指针，定义数组指针的代码如int (*ap)[2]，定义了一个指向包含两个元素的数组的数组指针。如果数组的每一个元素都是一个指针，则该数组为指针数组。实例代码如下：1char *chararr[] = &#123;&quot;C&quot;, &quot;C++&quot;, &quot;Java&quot;&#125;; int (*ap)[2]和int *ap[2]的区别就是前一个是数组指针，后一个是指针数组。因为”[]”的优先级高于”*”，决定了这两个表达式的不同。 public、protected、private修饰成员变量或成员函数 public：可以被该类中的函数、子类的函数、友元函数和该类的对象访问。 protected：可以被该类中的函数、子类的函数和友元函数访问，不能被该类的对象访问。 private：只能由该类中的函数或友元函数访问。 默认为private权限。用在继承中 public：基类成员保持自己的访问级别，基类的public成员为派生类的public成员，基类的protected成员为派生类的protected成员。 protected：基类的public和protected成员在派生类中未protected成员。 private：基类的所有成员在派生类中为private成员。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[奴隶的心态]]></title>
      <url>%2Fpost%2Fslave_mind%2F</url>
      <content type="text"><![CDATA[今天下午新办的工资银行卡办下来了，原先工资卡用的招商银行的金卡，现在新办了个农村信用社的普通卡，有种从城市的大马路走到了胡同里的城中村的感觉。于是大家高高兴兴的蹦蹦跳跳的屁颠屁颠的去领来的新的工资卡。为什么大家会高兴呢？这也是本文的因子，因为大家都已经有两个月没发工资了，新卡到说明工资在不久的将来也会到，大家当然高兴啦。 可是深入想一下，大家真的应该高兴吗？工资本来就是公司单方面拖欠大家的，给大家造成的损失员工宽宏大量没有跟公司计较也就罢了。工资关系到民生，直接影响了员工每个人的生活。要是搁到法国估计一天不发工资，员工早就集体罢工了吧。换卡势必会造成大家的麻烦，原来的银行卡可能就要销户，新的银行是否足够方便。我们却是心宽体胖，公司的错既往不咎，只要看到发工资的希望我照样努力为公司奉献。 这就好比是在中国假期几天高速路不收费，有车一族就喜出望外，“在中国真幸福，可以赚到国家便宜了”。殊不知，世界已建成14万公里收费公路，10万公里在我国。不知道了解这个之后，有车一族还笑得出来吗？我估计连哭的心态都有了。 那大家应该是什么心态去领工资卡呢？平常心就好。至少不至于表现出屁颠屁颠的心态吧。 中国两千多年的封建统治对人民的思想影响是深远的，人们心中早就建立起了人分三六九等的等级观念。而且这种等级观念影响是深远的，悄无声息的深入到我们所谓的“社会主义国家”内部的各个角落。电视里整天的宫廷剧，格格与皇阿玛齐飞，太监共丫鬟一色，这些可是典型的民权不公，民生不平。我们可是就靠这些文化垃圾养大的，心里怎么会没有了奴隶的思想呢？百姓心中的怕官、傍官的思想及考官的行为直接复制到了现在不曾改变，奴隶的心态也就不会改变。 好在现代文明来了，互联网时代来了。西方的文明渐渐深入，我们有的救。打倒xx，打倒xx，期待来一场轰轰烈烈的革新吧。 如果说奴隶的心态为“给你点阳光你就灿烂，给你点洪水你就泛滥”，期待我们的心态变成“没有阳光也要比比灿烂，没有洪水也要试图泛滥”。 题图：《被释放的姜戈》电影海报]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[一个傻×对股市的吐槽]]></title>
      <url>%2Fpost%2Ffoolish_talk_stock%2F</url>
      <content type="text"><![CDATA[这是来自一个保守主义者的geeker的傻X的吐槽。 最近股市暴跌，我却在思考为何有股票这个东西。我从未入市且对股市一点兴趣都没有甚至是反感。我所认识的股市游戏规则就是价位低的时候大股东入市，价位高的时候大股东抛售，大股东一吐一吞钱到手了，股民被掏空了。很多股民却用侥幸心理来入市，有赔有赚，出市的时候裤子都赔进去了。 搞不懂那么多经济学专家、博士、硕士、学士，却搞不定一个经济学。若是真搞不懂，索性就不要去研究，反正学了也白学，浪费这个资源误人歧途干啥。一个砖家说熊，另一个砖家说牛，相互对掐有意思吗？ 为什么搞股市这个东西，这是嫌经济学不够复杂吗？股市难道能推动人类社会的进步？股市是企业融资的一种手段，而能上市的公司往往是相对不太缺钱的，而最缺钱的是小型创业公司。这也就早就了很多公司把上市套现作为了一个目标，这不明摆着投机取巧，一夜暴富。传销是集合了底层的力量资金而构成了金字塔，企业通过上市手段获取到了股民的资本来运作而发展自身，只不过金字塔只有两层罢了。 假如没有股市，很多上班族就会坐在办公桌上安心工作，而不用时时刻刻关心着像过山车一样的死难看的折线，也不用设个老板键提心吊胆的担心自己的boss悄悄走到自己的面前。难道安安心心全身心投入工作不是更好？ 假如没有股市，或许就少了一个行业，一部分人就可以全身心投入到其他行业，带动其他行业的发展，推动历史进步，岂不快哉！ 假如没有股市，就不会有人赚发了之后，别人也总想着不劳而获，间接助长了人们投机取巧的气焰。 假如没有股市，就不会因此而发家，当然这是少数。郭美美的母亲也不会用4万块钱赚到100万，也不会有郭美美的今天，也不会有红十字会的今天，当然这只是个笑话。没有股市郭照样可以炫富，因为人家本来就不是靠的股市。没有股市红十字会照样会没落，因为他们的本质就是那样。 假如没有股市，就不会有人因为股市而跳楼的新闻，就不会有人将养老钱都搭进去了，就不会… 请不要职责我，因为我是一个傻X，永远不要和傻子讲道理，否则你也会变成一个傻X！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[做自己]]></title>
      <url>%2Fpost%2Fdo_your_self%2F</url>
      <content type="text"><![CDATA[最近有同事想离职了，上午在跟领导一番谈话之后又有些犹豫了。大概领导会说些公司状况及个人发展之类的话语来说服同事不要离职，用大腿想想都知道的事。同事肯定会说自己的几个抱怨，领导肯定会一一还击，到最后领导领导一摊手“看没什么顾虑的吧，年轻人就是冲动，你再想想吧，先回去好好干活吧”。 我在这里想说的事不要管别人怎么想，做自己，坚持自己的想法。 一千个人眼里有一千个郭美美。有人说郭美美的自拍照真漂亮，有人说郭美美卸妆之后直接毁三观，有人说是郭美美毁掉了红十字会，有人说没有郭美美红十字会一样玩完，有人说郭美美炫富就是为了出名，有人说郭美美是偶像，有人说郭美美是呕像，有人说郭美美为咱们的游戏代言应该很棒，有人说… 同一件事情在不同人眼里就会有不同的看法。当然我的观点，在别人看来也许是那么的反感或不屑一顾。 做技术的天天跟机器打交道，往往语言表达能力较弱，不善辩，更别提忽悠别人了。经过领导只言片语的轰炸，自己已经是茫然不知所措，自己最初的坚持在渐渐退去，别人的想法正在填充你的大脑，恭喜你被洗脑了。但是这种洗脑往往是暂时性的，往往当是生效，事后理性分析一下觉得“不对啊，我当时怎么了”。所以搞传销的一培训就是在一个鸟拉屎的地方培训上几个月来彻底洗脑。 我还好，至少我会坚持自己的观点。记得我上次离职时，领导找我谈话，领导叽里呱啦说了很多，我明知道不对头，但却愣是无力反驳，那时候心里的就想说“求求你，别说了”。 离职对同事自身而言是好事。同事工作两年，在公司工作一年多了，由于公司业绩差，工资非但没有涨过，最近甚至都拖欠了一个月的工资了，而且如果待在公司未来一年内都不涨工资的可能性极大。换个工作薪水可以提高，更可以接触到新鲜的技术，对自己无论在物质还是在技术上都是有利无害。我可以想到的唯一坏处就是舍不得公司，舍不得同事，舍不得那个靠窗的小办公桌。可是这些算个屁，公司的发展跟普通员工毛关系都没有，公司好坏员工拿到的都是稀薄的工资，何况公司已经发不下工资来了，是公司待你不厚道。感情在涉及到金钱后往往变得一文不值。 就公司而言是坏事。公司恨不得不发你工资光给公司干活。领导之所以留你是因为你对公司的业务熟悉，公司如果再重新招人没个一个月的时间很难上手。假如换成了在公司里天天闲的蛋疼的人提离职，公司可能就巴不得立马卷铺盖走人。领导留人只能说明领导对你的肯定，其他的说明不了什么。公司的前途未卜，领导凭什么忽悠员工留下来？ 这是一个相互炒鱿鱼的时代。员工跟公司之间是对等的关系，公司对员工不满意可以解除合同，员工对公司不满意照样可以将公司炒掉换个新鲜的工作。 做自己，坚持自己的观点，适当聆听别人的建议，才会赢！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SolrCloud官方文档翻译]]></title>
      <url>%2Fpost%2Fsolrcloud_translate%2F</url>
      <content type="text"><![CDATA[本文是翻译的solrcloud的官方英文文档，本文仅将文中重点翻译，原文地址点这里。英文水平不咋地，翻译篇文章也算练练手。 SolrCloudSolrCloud是Solr的分布式集群。可以通过集群来搭建一个高可用性，容错性的Solr服务。当想搭建一个大规模，容错性，分布式索引，查询性能好的Solr服务时可以采用SolrCloud。 关于SolrCores和Collections的一点小知识在单机运行时，单独的索引叫做SolrCore。如果想要创建多个索引，可以创建多个SolrCore。利用SolrCloud，一个索引可以存放在不同的Solr服务上。意味着一个单独的索引可以由不同的机器上的SolrCore组成。不同机器上的SolrCore组成了逻辑上的索引，这些SolrCore叫做collection。组成collection的SolrCore包括了数据索引和备份。 例子A： 简单两个shard集群这个例子简单创建了包含两个solr服务的集群，一个collection的数据分布到两个不同的shard上。因为在这个例子中我们需要两个服务器，这里仅简单的复制example的数据作为第二个服务器，复制example目录之前需要确保里面没有索引数据。12rm -r example/solr/collection1/data/*cp -r example example2 下面的命令会启动一个solr服务并启动一个新的solr集群。12cd examplejava -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -DzkRun -DnumShards=2 -jar start.jar -DzkRun 参数会在solr服务中启动一个内置的zookeeper服务。 -Dbootstrap_confdir=./solr/collection1/conf 因为在zookeeper中没有solr配置信息，这一参数会将本地的./solr/conf目录下的配置信息上传到zookeeper中作为myconf配置参数。myconf是在下面的collection.configName参数中指定的。 -Dcollection.configName=myconf 为新的collection设置配置名称。如果不加这个参数配置默认名称为configuration1。 -DnumShards=2 划分索引到逻辑分区的个数。 浏览http://localhost(本地主机):8983/solr/#/~cloud可以看到集群的状态。 通过目录树可以看到配置文件已经上传到了/configs/myconf/目录下，一个叫collection1的collection已经创建，在collection1下是shard的列表，这些shard组成了完整的collection。 接下来准备启动第二个服务器，因为没有明确的设置shard的id，该服务会自动分配到shard2。 启动第二个服务，并将其指向集群。 12cd example2java -Djetty.port=7574 -DzkHost=localhost:9983 -jar start.jar -Djetty.port=7574来指定Jetty的端口号。 -DzkHost=localhost:9983用来指定Zookeeper集群。在本例中，在第一个Solr服务中运行了一个单独的Zookeeper服务。默认情况下，Zookeeper的端口号为Solr服务的端口号加上1000，即9983。 通过访问http://localhost(本地主机):8983/solr/#/~cloud，在collection1中就可以看到shard1和shard2。 下面对一些文档建立索引。1234567cd exampledocsjava -Durl=http://localhost:8983/solr/collection1/update -jar post.jar ipod_video.xmljava -Durl=http://localhost:8983/solr/collection1/update -jar post.jar monitor.xmljava -Durl=http://localhost:8983/solr/collection1/update -jar post.jar mem.xml 无论是向集群中的任何一台服务器请求都会得到全部的collection：http://localhost:8983/solr/collection1/select?q=*:*。 假如想更改配置，可以在关闭所有服务之后删除solr/zoo_data目录下的所有内容。 实际测试插入速度要比单个服务慢。 例子B：简单的两个shard重复的shard集群本例子会通过复制shard1和shard2来创建上一个例子。额外的shard备份可以有高可用性和容错性，简单提升索引的查询能力。 首先，在运行先前的例子中我们已经有了两个shard和一些索引文档。然后简单的复制这两个服务：12cp -r example exampleBcp -r example2 example2B 然后，在不同的端口上启动两个新的服务：12cd exampleBjava -Djetty.port=8900 -DzkHost=localhost:9983 -jar start.jar 12cd example2Bjava -Djetty.port=7500 -DzkHost=localhost:9983 -jar start.jar 重新浏览网址http://localhost(本地主机):8983/solr/#/~cloud，检查四个solr节点是否已经都启动。因为我们已经告诉Solr我们需要两个逻辑上的shard，启动后的实例3和4会自动的成为原来shard的备份。 向集群中的任意一个服务发起查询：http://localhost:7500/solr/collection1/select?q=*:*。多次发起这个查询并查看solr服务的日志。可以观察到Solr通过备份对请求做了平衡，通过不同的服务来处理请求。 为了证明高可用性，在除了运行Zookeeper的服务上按下CTRL-C。（在例子C中将会讨论Zookeeper的冗余）当服务终止后，发送另外一个查询请求到其他服务，仍然能够看到所有的结果。 在没一个shard至少还有一个服务时，SolrCloud仍然可以提供服务。可以通过关闭每一个实例来查看结果。假如关闭了一个shard的所有的服务，到其他服务的请求就会收到503错误。为了能够返回其他可用的shard中的文档，可以在请求中增加参数：shards.tolerant=true。 SolrCloud用leaders和overseer来作为具体的实现。一些节点或备份将会扮演特殊的角色。不需要担心杀死了leader或overseer，假如杀死了其中的一个，集群会自动选择一个新的leader或overseer，并自动接管工作。任何的Solr实例都可以成为这种角色。 例子C：两个shard集群，shard带备份和zookeeper集群 在例子B中问题是虽然有足够的Solr服务器可以避免集群挂掉，但是仅有一个zookeeper服务来维持集群的状态。假如zookeeper服务挂掉了，分布式的查询还是可以工作的，因为solr服务记录了zookeeper最后一次报告的状态。问题是没有新的服务器或客户端能发现集群的状态，集群的状态也不会改变。 运行多个zookeeper服务可以保证zookeeper服务具有高可用性。每一个zookeeper服务需要知道集群中的其他服务，大部分服务需要提供服务。例如，一个含有三个zookeeper服务的集群允许其中一个失败剩余的两个仍然可以提供服务。五个zookeeper服务的集群可以允许一次失败两个。 从产品角度考虑，推荐使用单独的zookeeper服务而不是solr服务中集成的zookeeper服务。你可以从这里读取到更多的zookeeper集群。在这个简单的例子中，我们仅简单的使用了集成的zookeeper。 首先，停止四个服务，并清空zookeeper中的数据作为一个新的开始。1rm -r example*/solr/zoo_data 我们仍然将服务分别运行在8983,7574,8900,7500端口。默认是在端口号+1000的端口上启动一个zookeeper服务，第一次运行的时候在另外三台服务器上zookeeper的地址分别为：localhost:9983,localhost:8574,localhost:9900。 为了方便通过第一个服务上传solr的配置到zookeeper集群中。在第二个zookeeper服务启动之前程序会阻塞。这是因为zookeeper在工作的时候需要其他服务。123cd examplejava -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -DzkRun -DzkHost=localhost:9983,localhost:8574,localhost:9900 -DnumShards=2 -jar start.jar 12cd example2java -Djetty.port=7574 -DzkRun -DzkHost=localhost:9983,localhost:8574,localhost:9900 -jar start.jar 12cd exampleBjava -Djetty.port=8900 -DzkRun -DzkHost=localhost:9983,localhost:8574,localhost:9900 -jar start.jar 12cd example2Bjava -Djetty.port=7500 -DzkHost=localhost:9983,localhost:8574,localhost:9900 -jar start.jar 现在我们运行了三个内置的zookeeper服务，如果一个服务挂掉之后其他一切正常。为了证明，在exampleB上按下CTRL+C杀掉服务，然后浏览http://localhost:8983/solr/#/~cloud来核实zookeeper服务仍然可以工作。 需要注意的是，当运行在多个机器上，需要在每一台机器上设置-DzkRun=hostname:port属性。 ZooKeeper多个zookeeper服务同时运行来避免错误和高可用性叫做ensemble。从产品角度，推荐运行外部的zookeeper ensemble来代替solr集成的zookeeper。浏览zookeeper官方网站下载和运行一个zookeeper ensemble。可以参考Getting Started和ZooKeeper Admin。非常简单就可以运行。可以坚持使用solr来运行zookeeper集群，但是必须知道zookeeper集群不是非常容易动态改变的。除非solr增加对zookeeper更好的支持，重新开始是最好的改变方式。zookeeper和solr是两个不同的进程是最好的方式。 当solr运行内置的zookeeper服务时，默认会使用solr服务的端口号加上1000作为zookeeper的客户端端口号。另外，默认会增加一个zookeeper的客户端端口号和两个zookeeper的选举端口号。所以在第一个例子中，solr运行在8983端口，内置的zookeeper服务运行在9983端口作为客户端端口，9984和9985作为服务端端口。 当增加了更多zookeeper节点可以提高读性能，但是会稍微降低写性能。当集群状态稳定的时候，Solr用的Zookeeper非常少。下面有一些优化zookeeper的建议： 最好的情况是zookeeper有一个专用的机器。zookeeper是一个准时的服务，专用的机器可以确保及时响应。当然专用的机器不是必须的。 当把事务日志和snap-shots放到不同的磁盘上可以提高性能。 假如zookeeper和solr运行在同一台机器上，利用不同的磁盘可以提高性能。 参考文档https://wiki.apache.org/solr/SolrCloud]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在tomcat7.0.41上搭建solr4.3.1]]></title>
      <url>%2Fpost%2Fsolr4.3.1_setup%2F</url>
      <content type="text"><![CDATA[前几天写了篇《在Linux上搭建solr环境》的博文，是基于solr3.6.2的安装。本文仅记录在tomcat7.0.41上搭建solr4.3.1搭建过程中需要注意的地方，其他地方可以参考上一篇博文。 配置完成之后发现http://192.168.20.38:8090/solr无法访问，但是http://192.168.20.38:8090/却可以访问，通过查看tomcat的日志文件localhost.2013-07-03.log，发现里面有如下错误提示。12严重: Exception starting filter SolrRequestFilterorg.apache.solr.common.SolrException: Could not find necessary SLF4j logging jars. If using Jetty, the SLF4j logging jars need to go in the jetty lib/ext directory. For other containers, the corresponding directory should be used. For more information, see: http://wiki.apache.org/solr/SolrLogging 解决办法：将~/solr-4.3.1/example/lib/ext目录下的所有jar文件复制到~/apache-tomcat-7.0.41/lib目录下，然后重启tomcat即可。 相关下载用到的文件]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建分布式的solr环境]]></title>
      <url>%2Fpost%2Fsolr_setup_distribute%2F</url>
      <content type="text"><![CDATA[本文以《在Linux上搭建solr环境》为基础，假设已经在192.168.20.6和192.168.20.38上搭建了单机版solr环境。 主服务器配置找到solr的环境目录下的conf文件夹下的solrconfig.xml文件，我的是在/hadoop/solr/conf/solrconfig.xml目录下，打开后找到如下行1&lt;requestHandler name=&quot;/replication&quot; class=&quot;solr.ReplicationHandler&quot; &gt; 默认是被注释的，将其修改为1234567&lt;requestHandler name=&quot;/replication&quot; class=&quot;solr.ReplicationHandler&quot; &gt; &lt;lst name=&quot;master&quot;&gt; &lt;str name=&quot;replicateAfter&quot;&gt;commit&lt;/str&gt; &lt;str name=&quot;replicateAfter&quot;&gt;startup&lt;/str&gt; &lt;str name=&quot;confFiles&quot;&gt;schema.xml,stopwords.txt&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; replicateAfter表示solr会在什么情况下复制，可选项包括：commit、startup、optimize，这里保持默认。confFiles表示要分发的配置文件。 从服务器配置在从服务器上，将/hadoop/solr/conf/solrconfig.xml文件相应的修改为123456&lt;requestHandler name=&quot;/replication&quot; class=&quot;solr.ReplicationHandler&quot; &gt; &lt;lst name=&quot;slave&quot;&gt; &lt;str name=&quot;masterUrl&quot;&gt;http://192.168.20.6:8080/solr/replication&lt;/str&gt; &lt;str name=&quot;pollInterval&quot;&gt;00:00:60&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt; masterUrl为服务器的url地址。pollInterval为从服务器的同步时间间隔。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Linux上搭建solr环境]]></title>
      <url>%2Fpost%2Fsolr_setup%2F</url>
      <content type="text"><![CDATA[本文采用Linux操作系统在hadoop用户下安装，solr采用3.x中的最新版本3.6.2，tomcat采用6.0.37版本，安装包可以从本文下方链接下载。这里有两种安装方式，一种方式为利用solr自带的jetty来启动solr，默认端口为8983。另外一种方式为将solr集成到tomcat中。其中第一种方式较为简单，推荐新手采用。 独立启动 将sorl的安装包解压到用户的根目录下，解压后文件夹为apache-solr-3.6.2。 进入到example目录下，执行java -jar start.jar命令，solr服务启动，端口为8983。 通过http://IP地址:8983/solr/来访问solr的web页面，进入admin页面后可以通过输入字符串来查找索引。查找索引默认显示的格式为xml格式，可以通过在url的后面加上参数wt=json来显示json格式的结果。 利用tomcat安装tomcat1. 将apache-tomcat-6.0.37.tar.gz解压到hadoop的跟目录下。2. 修改hadoop用户的环境变量，执行vi ~/.bash_profile命令，添加如下：123export CATALINA_HOME=/home/hadoop/apache-tomcat-6.0.37export CLASSPATH=.:$JAVA_HOME/lib:$CATALINA_HOME/libexport PATH=$PATH:$CATALINA_HOME/bin 3. 执行source ~/.bash_profile使修改的环境变量生效。4. 执行tomcat的bin目录下的startup.bat脚本来启动tomcat。5. 通过netstat -anp | grep 8080命令查看tomcat是否启动。 安装solr1. 将solr的dist/apache-solr-3.6.2.war文件复制到tomcat的webapps目录下，并将文件命名为solr.war。执行cp ~/apache-solr-3.6.2/dist/apache-solr-3.6.2.war ~/apache-tomcat-6.0.37/webapps/solr.war命令。WAR是一个完整的web应用程序，包括了Solr的jar文件和所有运行Solr所依赖的Jar文件，Jsp和很多的配置文件与资源文件。 2. 修改~/apache-tomcat-6.0.37/conf/server.xml文件相应行的内容如下：1234&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; URIEncoding=&quot;UTF-8&quot; redirectPort=&quot;8443&quot; /&gt; 增加URIEncoding=&quot;UTF-8&quot;来支持中文。这是因为solr基于xml，json，javabin，php，python等多种格式传输请求和返回结果。 3.复制~/apache-solr-3.6.2/example/solr目录到/home/hadoop/solr位置。该位置为solr的应用环境目录。 4. 修改/home/hadoop/solr/conf/solrconfig.xml文件中的dataDir一行内容为：1&lt;dataDir&gt;$&#123;solr.data.dir:/home/hadoop/solr/data&#125;&lt;/dataDir&gt; 目的是为了指定存放索引数据的路径。 5. 在~/apache-tomcat-6.0.37/conf/Catalina/localhost目录下新建文件solr.xml。增加内容如下：123&lt;Context docBase=&quot;/home/hadoop/apache-tomcat-6.0.37/webapps/solr.war&quot; debug=&quot;0&quot; crossContext=&quot;true&quot; &gt; &lt;Environment name=&quot;solr/home&quot; type=&quot;java.lang.String&quot; value=&quot;/home/hadoop/solr&quot; override=&quot;true&quot; /&gt;&lt;/Context&gt; 其中docBase为tomcat的webapps下的solr.war完整路径。Environment的value属性的值为存放solr索引的文件夹，即第三步中复制的文件夹。需要注意的是：Catalina目录在首次启动tomcat时创建，因此在此步骤前需要启动过tomcat。 6. 在tomcat的bin目录下通过startup.sh启动tomcat。 7. 通过http://IP地址:8080/solr/来访问solr的web页面。 相关命令放入数据到solr中在apache-solr-3.6.2/example/exampledocs目录下，执行java -jar post.jar 要存放的文件名。这里自己新建一个文件test.xml放入到solr中，文件内容如下：123456&lt;add&gt; &lt;doc&gt; &lt;field name=&quot;id&quot;&gt;company&lt;/field&gt; &lt;field name=&quot;text&quot;&gt;kaitone&lt;/field&gt; &lt;/doc&gt;&lt;/add&gt; 执行java -jar post.jar test.xml将数据放入solr中。 删除数据新建文本文件test_delete.xml，内容如下123&lt;delete&gt; &lt;id&gt;company&lt;/id&gt;&lt;/delete&gt; 执行java -jar post.jar test_delete.xml将数据从solr中删除。另外还可以通过命令行的方式来删除，命令为java -Ddate=args -jar post.jar &#39;&lt;delete&gt;&lt;id&gt;company&lt;/id&gt;&lt;/delete&gt;&#39;。 在Eclipse中搭建环境操作Solr api1. 新建一个java工程2. 在工程中引入如下包： commons-httpclient-3.1.jar commons-codec-1.6.jar apache-solr-solrj-3.6.2.jar slf4j-api-1.6.1.jar slf4j-log4j12-1.6.1.jar commons-logging-1.1.3.jar log4j-1.2.12.jar httpclient-4.2.5.jar httpcore-4.2.4.jar httpmime-4.2.5.jar 其中commons-httpclient-3.1.jar、commons-codec-1.6.jar、apache-solr-solrj-3.6.2.jar、slf4j-api-1.6.1.jar可以从solr的目录apache-solr-3.6.2中的dist目录下找到。 slf4j-log4j12-1.6.1.jar可以从slf4j的压缩包中slf4j-1.6.1.tar.gz找到。 commons-logging-1.1.3.jar可以从slf4j的压缩包中commons-logging-1.1.3-bin.zip找到。 log4j-1.2.12.jar可以从log4j的压缩包中logging-log4j-1.2.12.tar.gz找到。 httpclient-4.2.5.jar、httpcore-4.2.4.jar、httpmime-4.2.5.jar在httpcomponents-client-4.2.5-bin.tar.gz文件中。 具体的API编程可以参考Solr开发文档。 在linux上编译并执行程序1. 将工程中用到的jar包复制到Linux机器上，这里复制到/home/hadoop/test_solr/lib目录下。 2. 将测试程序的源码放到Linux机器上，这里复制到/home/hadoop/test_solr目录下。其中源码包括三个文件：SolrTest.java、SolrClient.java、Index.java。该三个文件将会包含在下面相关下载中的Eclipse工程中。 3. 在/home/hadoop/test_solr目录下执行1javac -cp lib/apache-solr-solrj-3.6.2.jar:lib/commons-httpclient-3.1.jar:lib/log4j-1.2.12.jar:lib/commons-codec-1.6.jar:lib/commons-logging-1.1.3.jar:lib/slf4j-api-1.6.1.jar:lib/httpclient-4.2.5.jar:lib/httpcore-4.2.4.jar:lib/httpmime-4.2.5.jar:. SolrTest.java 其中-cp等同于-classpath参数，指定编译SolrTest.java文件需要的ClassPath路径，不要忘记路径后面的.表示当前路径，否则找不到当前目录下的其他java文件。命令执行后会在/home/hadoop/test_solr目录下生成Index.class、SolrClient.class、SolrTest.class三个class文件。 4. 在/home/hadoop/test_solr目录下执行1java -cp lib/apache-solr-solrj-3.6.2.jar:lib/commons-httpclient-3.1.jar:lib/log4j-1.2.12.jar:lib/commons-codec-1.6.jar:lib/commons-logging-1.1.3.jar:lib/slf4j-api-1.6.1.jar:lib/httpclient-4.2.5.jar:lib/httpcore-4.2.4.jar:lib/:httpmime-4.2.5.jar:. SolrTest 来运行程序。 在Linux上打包并执行1. 在上面步骤基础上，为了方便执行，可以将class文件打成jar包来执行，这样在使用java命令执行的时候就不用指定classpath路径了，只需要在jar包的MANIFEST.MF文件中指定classpath。 2. 在/home/hadoop/test_solr下新建一个文件，文件名可以随便，这里取名为MANIFEST.MF，与生成的jar包中的文件名一致，文件内容为1234567Manifest-Version: 1.0Created-By: 1.6.0_10 (Sun Microsystems Inc.)Main-Class: SolrTestClass-Path: /home/hadoop/test_solr/lib/apache-solr-solrj-3.6.2.jar /home/hadoop/test_solr/lib/commons-httpclient-3.1.jar /home/hadoop/test_solr/lib/log4j-1.2.12.jar /home/hadoop/test_solr/lib/commons-codec-1.6.jar /home/hadoop/test_solr/lib/commons-logging-1.1.3.jar /home/hadoop/test_solr/lib/slf4j-api-1.6.1.jar/home/hadoop/test_solr/lib/httpclient-4.2.5.jar/home/hadoop/test_solr/lib/httpcore-4.2.4.jar/home/hadoop/test_solr/lib/httpmime-4.2.5.jar 其中Main-Class指定main函数所在的类。Class-Path指定用到的jar所在的路径。其中Class-Path的各个jar文件之间通过空格分隔而不是通过:分隔。 3. 将class文件打包成jar文件。执行1jar -cfm solrtest.jar MANIFEST.MF Index.class SolrClient.class SolrTest.class 会在此目录下生成solrtest.jar文件。jar命令会根据指定的MANIFEST.MF文件来产生jar包中的META-INF/MANIFEST.MF文件。两个文件内容并不完全一致，jar命令会根据格式对内容进行调整。 4. 运行jar文件。通过java -jar solrtest.jar来执行。 相关下载本文中用到的安装包 参考文档简单的Solr安装配置官方安装教程Solr初体验系列讲的非常详细，适合初学者Solr开发文档]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[通过命令编译java程序]]></title>
      <url>%2Fpost%2Fcompile_java_code%2F</url>
      <content type="text"><![CDATA[通过Eclipse编写java程序久了，发现已经不会用命令来编译java程序了。今天在windows下搭建了一个solr环境，想放到linux下去跑一下，在windows上打成jar包后放在linux下不能运行，是时候回顾一下java的编译命令了。而且网上的资料比较零散，没有特别系统的资料。 本文在linux测试，同windows下的命令行工具差别不大。 编译并执行单个文件1. 在目录下~/test_java/com/kuring下新建HelloWorld.java的文件，文件内容为1234567package com.kuring;public class HellowWorld &#123; public static void main(String[] args) &#123; System.out.println("hello world"); &#125;&#125; 2. 在目录~/test_java下执行javac com/kuring/HelloWorld.java命令来编译文件。此时会在HelloWorld.java文件所在的目录下生成HelloWorld.class的二进制文件。 3. 在目录~/test_java下执行java com.kuring.HelloWorld来执行HelloWorld.class。屏幕会输出hello world，说明文件执行成功。也可以在任意路径下指定classpath路径来执行，命令为java -classpath ~/test_java com.kuring.HelloWorld，其中classpath指定了类的搜索路径。 编译并执行多个文件1. 在目录下~/test_java/com/kuring下新建HelloWorld2.java和Main.java的文件，HelloWorld2.java文件内容为1234567package com.kuring;public class HellowWorld2 &#123; public void print() &#123; System.out.println("hello world too"); &#125;&#125; Main.java的文件内容为12345678package com.kuring;public class Main &#123; public static void main(String[] args) &#123; HelloWorld2 hello = new HelloWorld2(); hello.print(); &#125;&#125; 2. 在目录~/test_java下执行javac com/kuring/Main.java命令来编译文件。此时会在Main.java文件所在的目录下生成Main.class和HelloWorld2.class两个文件，可以看出javac有自动推导编译的功能。 3. 在目录~/test_java下执行java com.kuring.Main。屏幕会输出hello world too，说明文件执行成功。 打包将上述例子中的程序打成jar包，可以在~/test_java目录下通过执行命令jar cvf my.jar com来生成jar文件。其中my.jar为要生成的jar文件的名字。通过java -classpath my.jar com.kuring.Main来执行jar文件。上述命令需要指定要执行的类名Main，如果想通过java -jar my.jar命令即可执行程序需要在jar包的META-INF/MANIFEST.MF文件中增加一行1Main-Class: SolrTest 来执行含有main函数的类。然后通过jar -cfm my.jar MANIFEST.MF路径 要打包的目录或文件来重新生成jar包。这样就可以通过java -jar my.jar来执行jar包了。 关于如何创建并执行引用了其他jar包的jar包，可以参考我的另外一篇博客《在Linux上搭建solr环境》，这里不再赘述。 常用jar命令 功能 命令 用一个单独的文件创建一个 JAR 文件 jar cf jar-file input-file… 用一个目录创建一个 JAR 文件 jar cf jar-file dir-name 创建一个未压缩的 JAR 文件 jar cf0 jar-file dir-name 更新一个 JAR 文件 jar uf jar-file input-file… 查看一个 JAR 文件的内容 jar tf jar-file 提取一个 JAR 文件的内容 jar xf jar-file 从一个 JAR 文件中提取特定的文件 jar xf jar-file archived-file… 运行一个打包为可执行 JAR 文件的应用程序 java -jar app.jar 参考文档JAR 文件揭密Java程序的编译、执行和打包]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HBase性能测试]]></title>
      <url>%2Fpost%2Fhbase_test%2F</url>
      <content type="text"><![CDATA[本文选择四台机器作为集群环境，hadoop采用0.20.2，HBase采用0.90.2，zookeeper采用独立安装的3.3.2稳定版。本文所采用的数据均为简单的测试数据，如果插入的数据量大可能会对结果产生影响。集群环境部署情况如下： 机器名 IP地址 用途 Hadoop模块 HBase模块 ZooKeeper模块 server206 192.168.20.6 Master NameNode、JobTracker、SecondaryNameNode HMaster QuorumPeerMain ap1 192.168.20.36 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain ap2 192.168.20.38 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain ap2 192.168.20.8 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain 单线程插入100万行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;public class InsertRowThreadTest &#123; private static Configuration conf = null; private static String tableName = &quot;blog&quot;; static &#123; Configuration conf1 = new Configuration(); conf1.set(&quot;hbase.zookeeper.quorum&quot;, &quot;server206,ap1,ap2&quot;); conf1.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); conf = HBaseConfiguration.create(conf1); &#125; /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception &#123; // 列族 String[] familys = &#123;&quot;article&quot;, &quot;author&quot;&#125;; // 创建表 try &#123; HBaseAdmin admin = new HBaseAdmin(conf); if (admin.tableExists(tableName)) &#123; System.out.println(&quot;表已经存在，首先删除表&quot;); admin.disableTable(tableName); admin.deleteTable(tableName); &#125; HTableDescriptor tableDesc = new HTableDescriptor(tableName); for(int i=0; i&lt;familys.length; i++)&#123; HColumnDescriptor columnDescriptor = new HColumnDescriptor(familys[i]); tableDesc.addFamily(columnDescriptor); &#125; admin.createTable(tableDesc); System.out.println(&quot;创建表成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 向表中插入数据 long time1 = System.currentTimeMillis(); System.out.println(&quot;开始向表中插入数据，当前时间为:&quot; + time1); for (int i=0; i&lt;1; i++) &#123; InsertThread thread = new InsertThread(i * 1000000, 1000000, &quot;thread&quot; + i, time1); thread.start(); &#125; &#125; public static class InsertThread extends Thread &#123; private int beginSite; private int insertCount; private String name; private long beginTime; public InsertThread(int beginSite, int insertCount, String name, long beginTime) &#123; this.beginSite = beginSite; this.insertCount = insertCount; this.name = name; this.beginTime = beginTime; &#125; @Override public void run() &#123; HTable table = null; try &#123; table = new HTable(conf, Bytes.toBytes(tableName)); table.setAutoFlush(false); table.setWriteBufferSize(1 * 1024 * 1024); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; System.out.println(&quot;线程&quot; + name + &quot;从&quot; + beginSite + &quot;开始插入&quot;); List&lt;Put&gt; putList = new ArrayList&lt;Put&gt;(); for (int i=beginSite; i&lt;beginSite + insertCount; i++) &#123; Put put = new Put(Bytes.toBytes(&quot;&quot; + i)); put.add(Bytes.toBytes(&quot;article&quot;), Bytes.toBytes(&quot;tag&quot;), Bytes.toBytes(&quot;hadoop&quot;)); putList.add(put); if (putList.size() &gt; 10000) &#123; try &#123; table.put(putList); table.flushCommits(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; putList.clear(); try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; try &#123; table.put(putList); table.flushCommits(); table.close(); &#125; catch (IOException e) &#123; System.out.println(&quot;线程&quot; + name + &quot;失败&quot;); e.printStackTrace(); &#125; long currentTime = System.currentTimeMillis(); System.out.println(&quot;线程&quot; + name + &quot;结束，用时&quot; + (currentTime - beginTime)); &#125; &#125;&#125; 测试5次的结果分布图如下：其中Y轴单位为毫秒。平均速度在1秒插入3万行记录。 10个线程每个线程插入10万行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;public class InsertRowThreadTest &#123; private static Configuration conf = null; private static String tableName = &quot;blog&quot;; static &#123; Configuration conf1 = new Configuration(); conf1.set(&quot;hbase.zookeeper.quorum&quot;, &quot;server206,ap1,ap2&quot;); conf1.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); conf = HBaseConfiguration.create(conf1); &#125; /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception &#123; // 列族 String[] familys = &#123;&quot;article&quot;, &quot;author&quot;&#125;; // 创建表 try &#123; HBaseAdmin admin = new HBaseAdmin(conf); if (admin.tableExists(tableName)) &#123; System.out.println(&quot;表已经存在，首先删除表&quot;); admin.disableTable(tableName); admin.deleteTable(tableName); &#125; HTableDescriptor tableDesc = new HTableDescriptor(tableName); for(int i=0; i&lt;familys.length; i++)&#123; HColumnDescriptor columnDescriptor = new HColumnDescriptor(familys[i]); tableDesc.addFamily(columnDescriptor); &#125; admin.createTable(tableDesc); System.out.println(&quot;创建表成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 向表中插入数据 long time1 = System.currentTimeMillis(); System.out.println(&quot;开始向表中插入数据，当前时间为:&quot; + time1); for (int i=0; i&lt;10; i++) &#123; InsertThread thread = new InsertThread(i * 100000, 100000, &quot;thread&quot; + i, time1); thread.start(); &#125; &#125; public static class InsertThread extends Thread &#123; private int beginSite; private int insertCount; private String name; private long beginTime; public InsertThread(int beginSite, int insertCount, String name, long beginTime) &#123; this.beginSite = beginSite; this.insertCount = insertCount; this.name = name; this.beginTime = beginTime; &#125; @Override public void run() &#123; HTable table = null; try &#123; table = new HTable(conf, Bytes.toBytes(tableName)); table.setAutoFlush(false); table.setWriteBufferSize(1 * 1024 * 1024); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; System.out.println(&quot;线程&quot; + name + &quot;从&quot; + beginSite + &quot;开始插入&quot;); List&lt;Put&gt; putList = new ArrayList&lt;Put&gt;(); for (int i=beginSite; i&lt;beginSite + insertCount; i++) &#123; Put put = new Put(Bytes.toBytes(&quot;&quot; + i)); put.add(Bytes.toBytes(&quot;article&quot;), Bytes.toBytes(&quot;tag&quot;), Bytes.toBytes(&quot;hadoop&quot;)); putList.add(put); if (putList.size() &gt; 10000) &#123; try &#123; table.put(putList); table.flushCommits(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; putList.clear(); try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; try &#123; table.put(putList); table.flushCommits(); table.close(); &#125; catch (IOException e) &#123; System.out.println(&quot;线程&quot; + name + &quot;失败&quot;); e.printStackTrace(); &#125; long currentTime = System.currentTimeMillis(); System.out.println(&quot;线程&quot; + name + &quot;结束，用时&quot; + (currentTime - beginTime)); &#125; &#125;&#125; 耗时分布图为：结果比单线程插入有提升。 20个线程每个线程插入5万行123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.util.Bytes;public class InsertRowThreadTest &#123; private static Configuration conf = null; private static String tableName = &quot;blog&quot;; static &#123; Configuration conf1 = new Configuration(); conf1.set(&quot;hbase.zookeeper.quorum&quot;, &quot;server206,ap1,ap2&quot;); conf1.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); conf = HBaseConfiguration.create(conf1); &#125; /** * @param args * @throws Exception */ public static void main(String[] args) throws Exception &#123; // 列族 String[] familys = &#123;&quot;article&quot;, &quot;author&quot;&#125;; // 创建表 try &#123; HBaseAdmin admin = new HBaseAdmin(conf); if (admin.tableExists(tableName)) &#123; System.out.println(&quot;表已经存在，首先删除表&quot;); admin.disableTable(tableName); admin.deleteTable(tableName); &#125; HTableDescriptor tableDesc = new HTableDescriptor(tableName); for(int i=0; i&lt;familys.length; i++)&#123; HColumnDescriptor columnDescriptor = new HColumnDescriptor(familys[i]); tableDesc.addFamily(columnDescriptor); &#125; admin.createTable(tableDesc); System.out.println(&quot;创建表成功&quot;); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 向表中插入数据 long time1 = System.currentTimeMillis(); System.out.println(&quot;开始向表中插入数据，当前时间为:&quot; + time1); for (int i=0; i&lt;20; i++) &#123; InsertThread thread = new InsertThread(i * 50000, 50000, &quot;thread&quot; + i, time1); thread.start(); &#125; &#125; public static class InsertThread extends Thread &#123; private int beginSite; private int insertCount; private String name; private long beginTime; public InsertThread(int beginSite, int insertCount, String name, long beginTime) &#123; this.beginSite = beginSite; this.insertCount = insertCount; this.name = name; this.beginTime = beginTime; &#125; @Override public void run() &#123; HTable table = null; try &#123; table = new HTable(conf, Bytes.toBytes(tableName)); table.setAutoFlush(false); table.setWriteBufferSize(1 * 1024 * 1024); &#125; catch (IOException e1) &#123; e1.printStackTrace(); &#125; System.out.println(&quot;线程&quot; + name + &quot;从&quot; + beginSite + &quot;开始插入&quot;); List&lt;Put&gt; putList = new ArrayList&lt;Put&gt;(); for (int i=beginSite; i&lt;beginSite + insertCount; i++) &#123; Put put = new Put(Bytes.toBytes(&quot;&quot; + i)); put.add(Bytes.toBytes(&quot;article&quot;), Bytes.toBytes(&quot;tag&quot;), Bytes.toBytes(&quot;hadoop&quot;)); putList.add(put); if (putList.size() &gt; 10000) &#123; try &#123; table.put(putList); table.flushCommits(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; putList.clear(); try &#123; Thread.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; try &#123; table.put(putList); table.flushCommits(); table.close(); &#125; catch (IOException e) &#123; System.out.println(&quot;线程&quot; + name + &quot;失败&quot;); e.printStackTrace(); &#125; long currentTime = System.currentTimeMillis(); System.out.println(&quot;线程&quot; + name + &quot;结束，用时&quot; + (currentTime - beginTime)); &#125; &#125;&#125; 结果如下：执行结果跟10个线程效果差不多。 10个线程每个线程插入100万行代码跟前面例子雷同，为节约篇幅未列出。执行结果如下： 20个线程每个线程插入50万行执行结果如下： 总结 多线程比单线程的插入效率有所提高，开10个线程与开20个线程的插入行效率差不多。 插入效率存在不稳定情况，通过折线图可以看出。 相关文章在Linux上搭建Hadoop集群环境在Linux上搭建HBase集群环境]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Linux上搭建Hadoop集群环境]]></title>
      <url>%2Fpost%2Fhadoop_setup%2F</url>
      <content type="text"><![CDATA[本文选择安装的hadoop版本为网上资料较多的0.20.2，对于不懂的新技术要持保守态度。遇到问题解决问题的痛苦远比体会用不着功能的新版本的快感来的更猛烈。 安装环境本文选择了三台机器来搭建hadoop集群，1个Master和2个Slave。本文中的master主机即namenode所在的机器，slave即datanode所在的机器。节点的机器名和IP地址如下 机器名 IP地址 用途 运行模块 server206 192.168.20.6 Master NameNode、JobTracker、SecondaryNameNode ap1 192.168.20.36 Slave DataNode、TaskTracker ap2 192.168.20.38 Slave DataNode、TaskTracker 安装Java 检查本机是否已安装Java在命令行中输入java -version判断是否已经安装。如果已经安装检查Java的版本，某些操作系统在安装的时候会安装Jdk，但可能版本会太低。如果版本过低，需要将旧的版本删除。在Redhat操作系统中可以通过rpm命令来删除系统自带的Jdk。 安装java本文选择jdk1.6安装，将解压出的文件夹jdk1.6.0_10复制到/usr/java目录下。 设置java的环境变量添加系统环境变量，修改/etc/profile文件，在文件末尾添加如下内容： export JAVA_HOME=/usr/java/jdk1.6.0_10export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jarexport PATH=$JAVA_HOME/bin:$PATHexport JRE_HOME=$JAVA_HOME/jre 修改完profile文件后要执行source /etc/profile命令才能使刚才的修改在该命令行环境下生效。 检查java是否安装成功在命令行中输入java -version、javac命令来查看是否安装成功及安装版本。 配置hosts文件本步骤必须操作，需要root用户来操作，修改完成之后立即生效。在三台机器的/etc/hosts文件末尾添加如下内容： 192.168.20.6 server206192.168.20.36 ap1192.168.20.38 ap2 修改完成之后可以通过ping 主机名的方式来测试hosts文件是否正确。 新建hadoop用户在三台机器上分别新建hadoop用户，该用户的目录为/home/hadoop。利用useradd命令来添加用户，利用passwd命令给用户添加密码。 配置SSH免登录该步骤非必须，推荐配置，否则在Master上执行start-all.sh命令来启动hadoop集群的时候需要手动输入ssh密码，非常麻烦。原理：用户将自己的公钥储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的私钥加密后，再发回来。远程主机用事先储存的公钥进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求输入密码。在本例中，需要实现的是192.168.20.6上的hadoop用户可以无密码登录自己、192.168.20.36和192.168.20.38的hadoop用户。需要将192.168.20.6上的ssh公钥复制到192.168.20.36和192.168.20.38机器上。 在192.168.20.6上执行ssh-keygen –t rsa命令来生成ssh密钥对。会在/home/hadoop/.ssh目录下生成id_rsa.pub和id_rsa两个文件，其中id_rsa.pub为公钥文件，id_rsa为私钥文件。 在192.168.20.6上执行cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys命令将公钥添加到授权的key里。 权限设置。在192.168.20.6上执行chmod 600 ~/.ssh/authorized_keys来修改authorized_keys文件的权限，执行chmod 700 ~/.ssh命令将.ssh文件夹的权限设置为700。如果权限不对无密码登录就配置不成功，而且没有错误提示，这一步特别注意。 在本机上测试是否设置无密码登录成功。在192.168.20.6上执行ssh -p 本机SSH服务端口号 localhost，如果不需要输入密码则登录成功。 利用scp命令将192.168.20.6上的公钥文件id_rsa.pub追加到192.168.20.36和192.168.20.38机器上的~/.ssh/authorized_keys文件中。scp命令的格式如下：1scp -P ssh端口号 ~/.ssh/id_rsa.pub hadoop@192.168.20.36:~/id_rsa.pub 在192.168.20.36和192.168.20.38机器上分别执行cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys命令将192.168.20.6机器上的公钥添加到authorized_keys文件的尾部。 配置无密码登录完成，在Master机器上执行ssh -P 本机SSH服务端口号 要连接的服务器IP地址命令进行测试。 搭建单机版hadoop在192.168.20.6上首先搭建单机版hadoop进行测试。 将hadoop-0.20.2.tar.gz文件解压到hadoop用户的目录下。 配置hadoop的环境变量。修改/etc/profile文件，在文件的下面加入如下： HADOOP_HOME=/home/hadoop/hadoop-0.20.2export HADOOP_HOMEexport HADOOP=$HADOOP_HOME/binexport PATH=$HADOOP:$PATH 修改完成之后执行source /etc/profile使修改的环境变量生效。 配置hadoop用到的java环境变量修改conf/hadoop-env.sh文件，添加export JAVA_HOME=/usr/java/jdk1.6.0_10。 修改conf/core-site.xml的内容如下： 12345678910111213141516&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop-0.20.2/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改/conf/mapred-site.xml的内容如下： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 至此单击版搭建完毕。可以通过hadoop自带的wordcount程序测试是否运行正常。下面为运行wordcount例子的步骤。 在hadoop目录下新建input文件夹。 将conf目录下的内容拷贝到input文件夹下，执行cp conf/* input。 通过start-all.sh脚本来启动单机版hadoop。 执行wordcount程序：hadoop jar hadoop-0.20.2-examples.jar wordcount input output。 通过stop-all.sh脚本来停止单机版hadoop。 搭建分布式hadoop在上述基础之上，在192.168.20.6上执行如下操作。1. 修改/home/hadoop/hadoop-0.20.2/conf目录下的core-site.xml文件。12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://192.168.20.6:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/hadoop-0.20.2/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop。而这个目录在每次重启后都会被干掉，必须重新执行format才行，否则会出错。2. 修改/home/hadoop/hadoop-0.20.2/conf目录下的hdfs-site.xml文件。12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.support.append&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. 修改/home/hadoop/hadoop-0.20.2/conf目录下的mapred-site.xml文件。123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;http://192.168.20.6:9001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4. 修改/home/hadoop/hadoop-0.20.2/conf目录下的masters文件。将Master机器的IP地址或主机名添加进文件，如192.168.20.6。5. 修改/home/hadoop/hadoop-0.20.2/conf目录下的slaves文件。Master主机特有在其中将slave节点的Ip地址或主机名添加进文件中，本例中加入12192.168.20.36192.168.20.38 6. hadoop主机的master主机已经配置完毕，利用scp命令将hadoop-0.20.2目录复制到两台slave机器的hadoop目录下。命令为：scp -r /home/hadoop hadoop@服务器IP:/home/hadoop/。注意slaves文件在master和slave机器上是不同的。 常用命令 hadoop dfsadmin -report 查看集群状态 http://192.168.20.6:50070/dfshealth.jsp 查看NameNode状态 http://192.168.20.6:50030/jobtracker.jsp Map/Reduce管理 hadoop fs -mkdir input 在HDFS上创建文件夹 hadoop fs -put ~/file/file*.txt input 将文件放入HDFS文件系统中 参考文档 细细品味Hadoop系列。超详细的hadoop教程，作者非常用心。 下载链接http://pan.baidu.com/share/link?shareid=1235031445&amp;uk=3506813023 提取码：v8ok]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux中信号处理举例]]></title>
      <url>%2Fpost%2Flinux_signal_deal_example%2F</url>
      <content type="text"><![CDATA[Linux处理ctrl+c信号的例子当按下ctrl+c时如果代码正在执行sleep则会停止睡眠，调用信号处理函数。中断位置可能位于for循环代码段的任意位置，中断位置不可控。123456789101112131415161718192021#include &lt;stdio.h&gt;#include &lt;signal.h&gt;void h(int s)&#123; printf("抽空处理int信号\n");&#125;main()&#123; int sum=0; int i; signal(SIGINT,h); sigset_t sigs; for(i=1;i&lt;=10;i++) &#123; sum+=i; sleep(1); &#125; printf("sum=%d\n",sum); printf("Over!\n");&#125; 信号屏蔽的例子1当按下ctrl+c时不会调用信号处理函数，当循环执行完毕后会调用信号处理函数，并且printf(“Over!\n”)会被执行。1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;#include &lt;signal.h&gt;void h(int s)&#123; printf("抽空处理int信号\n");&#125;main()&#123; int sum=0; int i; // 声明信号集合 sigset_t sigs; signal(SIGINT,h); // 清空集合 sigemptyset(&amp;sigs); // 加入屏蔽信号 sigaddset(&amp;sigs,SIGINT); // 屏蔽信号 sigprocmask(SIG_BLOCK,&amp;sigs,0); for(i=1;i&lt;=10;i++) &#123; sum+=i; sleep(1); &#125; printf("sum=%d\n",sum); // 消除屏蔽信号 sigprocmask(SIG_UNBLOCK,&amp;sigs,0); // 如果在上面按下ctrl+c，在此句不执行 printf("Over!\n");&#125; 当在循环中按下ctrl+c后，该函数输出结果为：123sum=55抽空处理int信号Over! 信号屏蔽的例子212345678910111213141516171819202122232425262728293031323334353637#include &lt;stdio.h&gt;#include &lt;signal.h&gt;// 信号处理函数void h(int s)&#123; printf("抽空处理int信号\n");&#125;main()&#123; int sum=0; int i; signal(SIGINT,h); sigset_t sigs,sigp,sigq; sigemptyset(&amp;sigs); sigemptyset(&amp;sigp); sigemptyset(&amp;sigq); sigaddset(&amp;sigs,SIGINT); sigprocmask(SIG_BLOCK,&amp;sigs,0); for(i=1;i&lt;=10;i++) &#123; sum+=i; sigpending(&amp;sigp); if(sigismember(&amp;sigp,SIGINT)) &#123; printf("SIGINT在排队!\n"); // 是信号SIGINT有效 sigsuspend(&amp;sigq); // 函数调用完毕后信号SIGINT无效 &#125; sleep(1); &#125; printf("sum=%d\n",sum); // 消除屏蔽信号 sigprocmask(SIG_UNBLOCK,&amp;sigs,0); printf("Over!\n");&#125; 该例子可以实现在指定的代码处处理信号。其中sigsuspend函数原先如下：1int sigsuspend(const sigset_t *mask); 函数解释：屏蔽新的信号，原来的屏蔽信号失效。是一个阻塞函数，该函数屏蔽mask信号；对非mask信号不屏蔽，信号处理函数调用完毕该函数返回；如果非mask信号没有信号处理函数，则此函数不返回。即返回条件：信号发生且信号为非屏蔽信号且信号必须要调用信号处理函数完毕。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[我幻想拥有一只听话的袋鼠作为宠物]]></title>
      <url>%2Fpost%2FI_want_to_have_one_kangaroos%2F</url>
      <content type="text"><![CDATA[我想拥有一只雌性袋鼠作为宠物，我会将其取名为”点点”。之所以是雌性是看中了袋鼠的温暖舒适的育儿袋。 如果可以我会给育儿袋上面缝上个拉链，这样我就不会担心放在袋袋里面的东西会掉出来了。 我可以领着我的点点去超市购物，将购买的东西放到袋袋里面，拉链一拉，然后蹦蹦跳跳的就回家了。 晚上吃完饭，我可以领着点点去大街上走走，我可以将我的钱包、手机放到点点的袋袋里，拉链一拉，完全不用担心手机会摔坏。 如果下起了雨或遇到了寒风，我可以钻到袋袋里，露个头在外面，一跳10米远，然后蹦蹦跳跳的就回家了。 当然，这是不现实的，我在做梦，做一个好笑的梦。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[参加完姐姐的婚礼之后]]></title>
      <url>%2Fpost%2Fmy_cousin_wedding%2F</url>
      <content type="text"><![CDATA[参加完姐姐的婚礼回到家后，静下心来之后内心莫名的感伤。也许是早晨三点醒四点起床导致身体在下午已经出现了疲惫。也许是生活本来就应该是平淡的，兴奋的多了，也自然疲惫的多，总之要保持一个符合每个人性格的平衡。也许是因为如女朋友同事的例子中一样，当丈夫娶妻过门的时候，丈夫的妹妹是哭的，因为妹妹有恋哥情节，难不成我也有恋姐情节的存在，还好我仅是有点伤感而已，不严重。也许是因为对时间流逝的无奈，总以为我们还没有长大，转眼间姐姐已经步入了婚姻的殿堂，成立了自己的新家庭，逝者如斯。趁着自己还有点文艺范的精神状态，写写这两天的一些感想，要不然明天一接触需要理性思考的计算机语言，这种感性点的状态又会归于平淡，又会在我鲜有的心路历程文章里少了一篇。 小的时候总是羡慕姐姐在城里的生活，每年在仅有的几次见面中总能感受到一个不一样的姐姐。那时的我对于城市的大小、城市的好坏还没有概念，仅仅知道城市要比农村好不少倍，仅仅知道姐姐是城里的，是城市小朋友的代言人，是引以为荣的小榜样。也许下面这张小时候的珍贵照片最能反应出城市孩子和农村孩子之间的区别。记得大约上八年级寒假的时候见过姐姐写的字，字迹铿锵有力且独特，不是常见的楷体字，由特定的字体加上自己的独到之处柔和而成。分明是经过训练方能造就，我从未在我认识的同学中见过能够写出如此华丽的字体，即使有书法天赋的同学由于没有经过特定的练习，字体往往都是课本上常见的楷体字。因为从上高中起，我已经开始渐渐地融入曾经向往的城市生活，直到如今我也算半个城市人。我和姐姐之间的差距在不断的缩短。当然会发现原来城市生活也有城市生活的弊端，如果让我选择一次我还会选择在农村度过我的童年，那种各种玩各种贴近大自然的生活是在高楼间无法体会到的。 姐姐终于找到了可以寄托的归宿，姐夫是很优秀的。姐姐属于离不开爱情的类型，但身体却不是很好，也许是姐夫喊姐姐玉妹的缘由。期望在外来姐夫能够好好照顾姐姐，同时姐姐应该多注意身体，多吃粗粮。 一个我想象不到的地方是随着现在人民文化水平的提高，婚礼的举办方式正在朝着复杂化的方向发展。现在的婚礼已经将旧有的当地风俗和现代的风俗结合在了一起，而且这两者是叠加的关系，意味着风俗越来越多，需要处理的事情越来越复杂。其实婚礼的作用无非就是热闹、喜庆， 我实在是搞不太明白复杂和热闹之间是一个什么样的关系。我期望我婚礼是简单的，简单的不能再简单，我甚至期望不需要亲朋好友的过多参与，我甚至不期望举办复杂的婚礼仪式，热闹的背后是家人的操劳，是长达数月的准备。我只希望能有一次难忘的旅行，两个人的旅行就足够幸福。 祝福的含义我是不理解的，祝福原本是一个人向另外一个人的未来的美好祝愿，就是说些客套话，说些不切实际的话。其实一个人很难对另外一个人的外来向好的方面发展做出贡献，就如同大臣们天天喊着万岁万岁万万岁，却未见过哪个皇帝超过百岁一般。很多人都会向姐姐的婚礼说出祝福，但有多少是过后还记得自己曾经说过的。我在这里同样祝福姐姐和姐夫新婚快乐，生活幸福，和睦相处，恩恩爱爱，当然我是发自内心的，而且我的话是有文字可考究的。同样在给姐姐的红包里我写下了“原寻寻觅觅，现卿卿我我，年年岁岁情爱深，岁岁年年无不同”的祝福，这里同样做一个备忘。 目前的婚庆已经完全市场化，只要是服务项没有不收费的可能性，而且往死里要。没有信仰的民族是个可怕的民族，如果马克思主义不能所谓一种信仰，那么中国人中的绝大多数是没有信仰的。即使大家上初中、高中、大学、研究生阶段都在马克思的理论，在当前的国情下估计也没有多少人能视马克思如珍宝。如果在国外我估计会出现在教堂举办婚礼非常廉价，甚至免费的可能性，因为能够为新郎新娘主持婚礼本身就是一件非常荣幸的事情，金钱不是最重要的，当然我不了解海外的真实情况。总感觉一件本来可以免费的事情只要跟金钱沾边总会变味。 中国父母的典型形象是碎碎念，对于子女总会絮絮叨叨那么一箩筐。在不经意间，子女已经长大成人，可在父母的眼中子女永远是未长大的孩子。中国的父母在子女身上花费了太多的心血，以至于子女比自己更重要。 一个活动的缺点是非常容易被人察觉的，而活动的优点往往不容易被人发觉。因此，要想顺利完成一个活动是不容易的。举办婚礼的时候我发现有很多事先未准备妥当的地方，而这些恰恰是最容易被发现的，而做得特别好的地方却是较难被人们发现。 年龄之间的代沟还是比较明显的，这一点在酒桌上特别能够体现，在酒桌上话语最多的永远是上一辈人，因为年轻人跟上一辈人在思想上还是有较大的差距，这是好事。如果两代人之间差距过小，说明这个社会变化太慢。年轻人应该吸取上一代人的长处，去其槽粕，取其精华。我想未来的酒桌文化会大变样。 我目前的理想生活状态是平淡充实的，我喜欢静下心来学习，我喜欢那种学习的充实感，我喜欢跟同龄人人心贴心的交谈，而不是在噪杂的环境中大家泛泛而谈。我的性格中腼腆的一面表现在我在与人沟通时内心的想法总会羞于表达，比如很难从我的嘴里向父母说出生日快乐之类的话语，向姐姐当面说出新婚快乐，白头偕老。一方面想着跟人沟通，另一方面却羞与吐露自己的内心想法，貌似有点矛盾的样子。 人类本该是感性的，接触的外部环境多了之后会渐渐趋于理性，当然人类的科技进步需要理性。理性多了，自然感性就会少一些。这几年跟计算机打交道的时间成了生活、工作的最重要部分，我大部分生活状态处于理性状态，心情以平淡居多。包括我现在写这篇文章，也是在用理性的思维方式来写感性的文章，换成在高中时期写的文章却是用感性的思维方式来写感性的文章。 我的性格未曾被工作改变，甚至是相貌在工作三年的时间里也未曾有大的变动。我现在仍然看上去像个学生，路人也经常会将我当成一名学生。女朋友说这是我不成熟的表现，我不知道成熟如何去界定，我认为我每天学到的东西都在助我向成熟迈进，直到我老去的那一刻我也不可能成熟，因为只要我还有进步我就不会成熟。我对于三年来的未改变我引以为豪，至少我没有被这个残缺的社会给同化掉，至少我还拥有自己的思想，同时也证明了一点我在大学期间思想就已经在保持了一种相对稳定的状态。我不认为工作后在官场上混的如鱼得水，在酒桌上的机智多变是一种成熟的表现，我对此持悲观态度。 行行出状元。今天看到我姑在玩QQ农场、QQ牧场的游戏，熟练程度相当高。由于我姑空闲时间较多，常常一个人在家，已经玩了多年的游戏。我姑对电脑的使用还处在初级阶段，平常电脑的作用还定位在玩农场的阶段。只要做的次数多了，行行都可以出状元。 本来想着把自己参加完姐姐婚礼之后的感想记录下来，结果写着写着开始有些怀旧，写自己的居多，困的不知所云，到此为止。不知等我老去的那天当我读到此文章时会是一种什么样的心情，难不成还会跟现在一样有点压抑的感觉？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux中创建新进程的方式]]></title>
      <url>%2Fpost%2Flinux_create_process_methods%2F</url>
      <content type="text"><![CDATA[system函数函数包含在C语言的标准库中，在头文件stdlib.h中声明如下：1int system(const char *command); 该函数会创建一个独立的进程，该进程拥有独立的进程空间，为阻塞函数，只有当新进程执行完毕该函数才返回。 返回值：可以通过返回值来获取新进程的main函数的返回值，返回值保存在int类型的第二个字节即8-15比特，可以通过向右移位或者利用宏WEXITSTATUS(status)来获取新进程的返回值。其中WEXITSTATUS(status)宏包含在头文件&lt;sys/wait.h&gt;中。 这里以调用ls命令为例来展示用法：123456789101112#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/wait.h&gt;int main()&#123; int r=system("ls"); // 用右移位的方式来获取新进程的返回值 //printf("%d\n",r&gt;&gt;8&amp;255); // 用WEXITSTATUS(status)宏的方式来获取新进程的返回值 printf("%d\n",WEXITSTATUS(r));&#125; popen函数函数包含在头文件stdio.h中，相关函数如下：12FILE *popen(const char *command, const char *type);int pclose(FILE *stream); popen函数在父子进程之间建立一个管道，其中type指定管道的类型，可以为”r”或”w”即只读或可写。在shell中的管道符”|”即采用此函数来实现。popen函数为阻塞函数，函数的具体用法如下：1234567891011121314151617181920#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/wait.h&gt;main()&#123; char buf[1024]; FILE *f=popen("ls","r"); // 根据管道获取文件描述符 int fd=fileno(f); int r; while((r=read(fd,buf,1024))&gt;0) &#123; buf[r]=0; printf("%s\n",buf); &#125; close(fd); // 关闭管道 pclose(f); &#125; exec系列函数该系列函数并不创建新的进程，而是将程序加载到当前进程的代码空间来执行并替换当前进程的代码空间，在exec*函数后面的代码将无法执行。12345int execl(const char *path, const char *arg, ...);int execlp(const char *file, const char *arg, ...);int execle(const char *path, const char *arg, ..., char * const envp[]);int execv(const char *path, char *const argv[]);int execvp(const char *file, char *const argv[]); fork函数该函数非常常用。函数原型如下：1pid_t fork(void); 调用该函数会产生一个子进程，该子进程不仅复制了父进程的代码空间、堆、栈，而且还复制了父进程的执行位置。之后父子进程同时执行，通常由于操作系统任务调度的原因，子进程会先执行。父进程和子进程之间的并不会共享堆、栈上的数据，可以通过文件或共享内存的方式来通讯。 返回值：该函数父进程返回子进程的id，子进程返回0。通常在代码中通过返回值来判断是子进程还是父进程，用来执行不同的代码。 如果父进程先结束，则子进程会成为孤儿进程，子进程仍然可以继续执行。进程数中的根进程init会成为该子进程的父进程。 如果子进程先结束，则子进程会成为僵尸进程。僵尸进程并不再占用内存和CPU资源，但是会在进程数中看到僵尸进程。因此代码中必须对僵尸进程的情况做处理，通常的处理办法为采用wait函数和信号机制。子进程在结束的时候会向父进程发送一个信号SIGCHLD，整数值为17。父进程扑捉到该信号后通过调用wait函数来回收子进程的资源。其中wait函数为阻塞函数，会一直等待子进程结束。wait函数返回子进程退出时的状态码。下面通过实例演示一下当子进程先结束时父进程怎样回收子进程。123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt;void processChildProcess()&#123; printf("receive the child process end\n"); wait();&#125;int main()&#123; int pid; pid = fork(); if (pid &gt; 0) &#123; // 父进程 signal(SIGCHLD, processChildProcess); while(1) &#123; sleep(1); &#125; &#125; else &#123; // 子进程 sleep(1); printf("child process end\n"); &#125; &#125; 关于fork的详细理解可以看这里：一个fork的面试题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从项目管理的角度分析一个失败的软件项目]]></title>
      <url>%2Fpost%2Fone_failed_project_in_pm%2F</url>
      <content type="text"><![CDATA[前段时间公司领导安排我完成一个android的小项目，功能为完成手机短信的分类功能。由于考虑到可能会和某运营商合作，项目需求多变，项目名到项目死掉的那一刻也没有想好，暂定叫短信管家。前段时间参加了今年软考中的高级项目管理师考试，对项目管理有一定的了解，由于在实际的工作中没有参与项目经理的角色，对项目管理的知识理论和实际结合还有一定差距。我一向是喜欢挑人毛病的，本文也不例外，将结合项目管理的知识领域来分析项目中存在的问题，并给出纠正的措施。本文并不想按照项目管理的44个过程组来写，不想受项目管理理论知识条条框框的约束。 项目简介在2013年元旦后上班第一天，经过领导简单介绍项目后，项目正式开始启动。项目参与人员包括我和一名美工。由于公司没有做android项目的经验，我是公司唯一懂点android项目的技术人员，我之前也仅仅是自学过一点。经过两个星期的学习之后开始了android项目的开发，春节前领导希望看到一个版本出来。离春节放假还有三天，美工搞出一个首界面来，我仿照首界面基本搞定。后来领导一看不符合需求，其实领导之前就看过首界面效果图，只是没有仔细看。当然领导也不知道要做出个什么样的东西，估计心想着技术人员弄出个什么样的东西来再修改。年后，对需求开会进行了重新整理，通过思维导图的方式跟领导确定需求，就需求中存在的疑问进行确认。不过后来事实证明经过领导确认的需求也是在不断变更。这次的需求要远比上次复杂，将原来的短信分类从一级短信分类更改为了二级短信分类。公司没有互联网行业产品的经验，美工给出的效果图也仅是从桌面端来考虑移动产品的设计。我最终决定自己开始重新设计软件的界面，参考了众多同类软件，花费的时间比较多。大约到2013年4月底产品已经基本可用，经过我自己的测试和领导的使用效果还算可以。后来发现不支持彩信，经过我的一番研究后发现现有的产品功能要对彩信支持需要耗费非常大的工作量，因为软件本身的功能打破了android系统短信的设计，也难怪了android的应用中没有功能相似的产品出现。这也成为了该项目失败的直接原因。 可行性研究技术可行性项目的起源来自领导的短信泛滥，领导想着能够搞一个拦截垃圾短信并对短信进行管理的软件将是酷的一件事情。领导接触过一些android，片面的认为这种方案在技术上可行。我也做了一个调研，发现现有的android软件项目中没有此功能的产品。 经济可行性支出分析：由于项目主要参与人员仅有我一人，支出的成本较小。收益分析：现在的互联网产品都是先圈用户再考虑盈利，我们这个软件产品也不例外。 运行环境可行性对于产品的运营存在两种方式：1、跟某地方运营商合作；2、自己单独发布。 总体而言，由于项目较小，对可行性研究基本忽略，没有科学、可观、公证的对项目进行可行性分析。 范围管理在项目立项的时候仅有一个产品的大致方向，做一款android系统中的短信分类管理类应用。从始至终领导对于产品的理解在不断变化，每次变化都需要修改很多代码，跟领导对于导致产品研发过程做了很多无用功。在项目进行到一个月后，我认识到了项目这样下去反复修改不可能有好的结果，因此跟领导召开了需求定义的会议，就产品的需求用思维导图的方式就项目的范围进行了定义，会后就需求中没有考虑到的问题跟领导进行了沟通确认。此次会议之后项目的需求基本敲定，虽然之后领导有需求变更的情况，但是相对较少。 进度管理由于公司初次开展android类项目，对项目的进度很难掌控。项目开始时负责市场部的领导给出了一个月之内完成的计划，我心想我自己加把劲应该能搞定吧，因为最初的项目需求还比较简单。我用了两个星期的时间来学习android知识，用两个星期的时间来开发，结果到最后我才刚根据美工提供的效果图完成了首页，我总是不能很好的估算自己的工作进度。这篇《为什么程序员总是不能准确预估工作量》的文章或许能替我解释些什么。在项目范围确定之后我对自己工作的进度管理也不够好，第一次尝试做android项目，未知因素太多，编码中总能遇到这样那样的问题。 成本管理项目的整个成本应该我的人力成本占了绝大部分，本来软件项目中的人力成本就是占的比重较多的，何况主要是我一个人参与的项目，自然我那微薄的人力占了项目成本的大部分。至于后期项目的盈利并没有过多考虑，现在的互联网行业本来就是先圈用户后盈利。 质量管理我在项目的编码过程中，一般一个小功能完成之后都会进行详尽的功能测试。由于功能不负责，产品的质量只要多测试一般问题就不大了。 人力资源管理项目的初期我就感觉项目不可能成功，但是领导坚持要做，而且我一个做C++的程序员来写android的代码，我当时居然仅有听从领导的意识，没有适当的表达自己的想法。当时怀着一己之心认为可能一个月项目就完成了，完成之后工作依旧，还可以熟练熟练android的开发，谁知一练手就是做了五个月，而到最后项目仍然没有成型。由于一直对项目持悲观态度，始终认为我自己开发出来的产品我都不想去用，用户怎么可能去用。长期自己独自做，兼任了产品经理、美工、码农的工作，做的过程中还要担心着需求的变更，在项目进行到中后期的时候， 缺少激励因素，对工作激情不够，这也影响了项目的进度。 沟通管理在项目的开发过程中基本能保持没完成一个关键功能跟领导汇报一次项目进展。在项目中沟通方面存在不少的问题，尤其以前期严重。前期由于跟领导的需求不一致，导致做了很多的重复性劳动。领导对项目的需求本来就不够明确，一旦有好的想法就加到项目中，这也导致了跟领导的沟通困难，因为这样的沟通效率太低。 风险管理项目可研阶段对技术可行性分析研究不够，导致了后期在对彩信进行分类处理的时候遇到了技术上的难题。当然前期的可研阶段做充分的技术可行性研究也是不太现实了，因为本来技术对公司而言就是未知的。最好的办法就是对自己不熟悉而且短期内不能实现盈利的项目不参与。 文档管理由于项目管理中要求每个阶段都有输出的文档，这么繁琐的文档我相信中国找不出几家软件公司能够办到，能办到的估计都已经死掉了。此项目中仅有范围定义的思维导图、我在编写某个具体的功能代码前绘制的流程图，应该说文档及其少。但我目前不认为这有什么不对，我还是坚信某些情况下看代码比看文档要来的更快，特别是看我这种android菜鸟写的代码。更何况此项目的需求在不断随着领导的想法而变化，文档都没法写，写文档的成本太高。 我的一些反思：在项目中我也存在不少问题。 项目开始阶段应该向领导表达清楚自己的想法，阐明自己对项目的看法。 对项目的进度没有很好的把握。 对技术较为熟悉之后可以适当的学习后期开发功能需要技术，比如一直没有研究彩信的技术实现，导致后期在开发彩信功能时遇到技术问题。如果能够提前学习相关技术就能够提前预支项目的风险。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在linux程序中获取和设置环境变量]]></title>
      <url>%2Fpost%2Flinux_get_and_set_env_variable%2F</url>
      <content type="text"><![CDATA[shell中的环境变量查看环境变量 通过env命令可以查看所有的环境变量 通过echo $环境变量名方式来查看单个环境变量设置环境变量export命令来设置环境变量 在程序中该如何获取和设置环境变量呢？ 通过main函数的第三个参数通常大家接触比较多的是两个参数的main函数，实际上还有一个包含三个参数的main函数，第三个参数为包含了系统的环境变量的二级指针。用法如下：1234567891011#include &lt;stdio.h&gt;int main ( int argc, char *argv[], char *arge[])&#123; while (*arge) &#123; printf("%s\n", *arge); arge++; &#125; return 0;&#125; 实例将会输出该用户的所有环境变量。 通过外部环境变量environ该变量定义在“unistd.h”头文件中，定义为：extern char **environ;用法如下：1234567891011121314#include &lt;stdio.h&gt;extern char **environ;int main ( int argc, char *argv[] )&#123; char **env = environ; while (*env) &#123; printf("%s\n", *env); env++; &#125; return 0;&#125; 实例将会输出该用户的所有环境变量。 通过系统函数在C语言的头文件“stdlib.h”中定义了三个和环境变量相关的函数：123char *getenv(const char *name);int setenv(const char *name, const char *value, int overwrite);int unsetenv(const char *name); 比较简单，不再举例。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mmap和munmap函数的用法]]></title>
      <url>%2Fpost%2Fmmap_and_munmap%2F</url>
      <content type="text"><![CDATA[在Linux的头文件sys/mman.h中提供了两个用来分配内存的函数：mmap和munmap，函数定义原型如下：12void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);int munmap(void *start, size_t length); mmap说明返回值：内存映射后返回虚拟内存的首地址。参数： start为指定的映射的首地址，该地址应该没有映射过，如果为0则有系统指定位置。 length为映射的空间大小，真正分配空间大小为(length/pagesize+1)。 prot为映射的权限，分为四种未指定（PROT_NONE）、读（PROT_READ）、写（PROT_WRITE）、执行（PROT_EXEC）。如果为PROT_WRITE，则直接可以PROT_READ。 flags：映射方式，分为内存映射和文件映射。内存映射：匿名映射。当值为文件映射是后面两个参数才有效。常用的值有：MAP_ANONYMOUS、MAP_SHARED、MAP_PRIVATE。 fd：映射的文件描述符。 offset为从文件的偏移位置开始映射。 munmap说明从start位置开始释放length个字节的内存。 应用举例1234567891011121314#include &lt;stdio.h&gt;#include &lt;sys/mman.h&gt;#include &lt;stdlib.h&gt;main()&#123; int *p = mmap(NULL, getpagesize(), PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_SHARED, 0, 0); munmap(p, getpagesize());&#125; 其中getpagesize()函数的作用为获取一个页的大小，系统默认为4K。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux中获取错误信息的方式]]></title>
      <url>%2Fpost%2Flinux_get_error_of_function%2F</url>
      <content type="text"><![CDATA[当linux中的函数内部出错时通常函数会返回-1，并且将错误码保存到全局变量errno中，用来表示错误代码。errno全局变量包含在头文件errno.h文件中。下面给出三种打印错误信息的方法。 perror函数应用举例如下：123456789101112#include &lt;stdio.h&gt;int main(void)&#123; FILE *fp ; fp = fopen( "/root/noexitfile", "r+" ); if ( NULL == fp ) &#123; perror("error : "); &#125; return 0;&#125; 输出如下：Permission denied strerror函数strerror函数原型为：char *strerror(int errnum);将参数errnum转换为对应的错误码。应用举例如下：123456789101112#include &lt;stdio.h&gt;int main(void)&#123; FILE *fp ; fp = fopen( "/root/noexitfile", "r+" ); if ( NULL == fp ) &#123; printf("%s\n", strerror(errno)); &#125; return 0;&#125; 输出如下：Permission denied printf中的%m打印应用举例如下：123456789101112#include &lt;stdio.h&gt;int main(void)&#123; FILE *fp ; fp = fopen( "/root/noexitfile", "r+" ); if ( NULL == fp ) &#123; printf("%m\n"); &#125; return 0;&#125; 输出如下：Permission denied]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[sbrk和brk函数]]></title>
      <url>%2Fpost%2Fsbrk_and_brk%2F</url>
      <content type="text"><![CDATA[Linux系统中提供了两个在堆中分配空间的底层函数，函数原型如下： void sbrk(intptr_t increment);int brk(void end_data_segment); 两个函数的作用均为从堆中分配空间，并且在内部维护一个指针，指针的值默认为NULL。如果内部指针为NULL，则得到一页的空闲地址，系统默认为4K字节。指针向后移动即为分配空间，指针向前移动为释放空间。当内部指针的位置移动到一个页的开始位置时，整个页会被操作系统回收。brk为绝对改变位置，sbrk为相对改变位置。 sbrk函数在sbrk函数中，参数increment为要增加的字节数，increment可以为负数。当increment为负数时表示释放空间。当increment==0时，内部指针位置不动。函数调用成功返回内部指针改变前的值，失败返回(void *)-1。函数使用举例如下：123456789101112131415161718192021222324#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main()&#123; int *p0 = sbrk(0); // 打印堆地址 printf("this site of p0 is : %d\n", p0); int *p1 = sbrk(1000); // 这里仍然打印的是第一次的堆地址 printf("this site of p1 is : %d\n", p1); int *p2 = sbrk(1); // 打印第一次堆地址+1000后的地址 printf("this site of p2 is : %d\n", p2); // 回到初始堆地址，释放空间 sbrk(-1001); int *p3 = sbrk(0); // 检查是否回到初始地址 printf("this site of p3 is : %d\n", p3);&#125; 输出如下内容：this site of p0 is : 264622080this site of p1 is : 264622080this site of p2 is : 264623080this site of p3 is : 264622080 brk函数在brk函数中，函数作用为将当前的内部指针移动到end_data_segment位置。成功返回0，失败返回-1。函数使用举例如下：1234567891011121314151617#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;int main()&#123; int *p0 = sbrk(0); printf("this site of p0 is : %d\n", p0); brk(p0 + 1000); printf("this site is : %d\n", sbrk(0)); brk(p0 + 1001); printf("this site is : %d\n", sbrk(0)); brk(p0); printf("this site is : %d\n", sbrk(0));&#125; 函数输出如下：this site of p0 is : 206979072this site is : 206983072this site is : 206983076this site is : 206979072 适用场景：内存的管理方式比malloc和free更加灵活，适合申请不确定的内存空间的情况，特别适合同类型的大块数据。如果用malloc则可能存在申请内存空间过多浪费的情况，过少时需要重新调用realloc来重新申请内存的情况。速度比malloc快。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[farbox与jekyll对比]]></title>
      <url>%2Fpost%2Ffarbox%E4%B8%8Ejekyll%E5%AF%B9%E6%AF%94%2F</url>
      <content type="text"><![CDATA[前端时间折腾了一段时间的Github博客，终于搞明白了jekyll，开始写了一篇博客发现问题比较多。比如中文编码问题，Github对makedown的支持问题，Github的文章同步问题，网速问题。总体而言，感觉用Github来写博客还不是非常满意。 今天下午偶然想起了前段时间看过faxbox的一篇文章，今天下载下来尝试，果然非常酷。优点如下： 博客放在了Dropbox不会丢失。当然没有Github酷，Github可以通过版本管理来查看历史版本。貌似Dropbox本身有版本管理的功能。 写博客的方式简捷。Github的jekyll还需要利用rake命令来创建文章，这样才能保证文章的头部含有YAML标签。而faxbox直接编辑文本文件即可。 文本编辑器给力。可以通过实时预览功能查看makedown的解析情况，当然也可以通过在线的makedown编辑器stackedit来编写makedown。 网速比Github快。测试一下感觉网速还比较快。 可以修改模板，fork模板的方式简捷更酷。 对中文的支持好。因为本来就是本土化的软件当然对中文全力支持。 代码语法高亮。测试了一下默认的模板支持语法高亮。 自带的模板更漂亮。自带的模版虽然不多，但是有说明是适合博客还是相册。 网站自带网站分析工具。可以通过简单的浏览网页就可以知道网站的访问情况，虽然farbox在访问量达到1W之后要收费，按照目前的价格，但是我个人觉得比较值，希望farbox能够一直坚持做下去。做的好我愿意付费。 缺点： 经过一下午的了解，暂时没有发现博客可以分类的功能，不过这个功能我暂时可以不需要。以后准备先写些博客在Github和Faxbox上同步更新，以便多比较比较这两种Geek的写博客方式。后来发现是有文章分类功能的，具体操作为将文章放入不同的文件夹中，文件夹的名字即为分类的名字，这种方式比jekyll的YAML语法方式要好用。 感觉评论功能有点鸡肋。博客还没写完有事工作，准备续写时发现有人留言指正错误，想回复谢谢，发现无法直接回复。用多说代替之，方法为直接将多说的js脚本存入新建的comment_js.md文件中即可，非常简洁。 Linux下的用户不太方便。由于farbox软件仅有windows版和mac版，dropbox在linux有相应版本，在linux下的用户不能享受到farbox编辑工具带来的方便。相反在linux下结合git和vim来编写博客却比较方便。 PS：Github博客地址：kuring.github.comFixbox博客地址：kuring.faxbox.com]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Linux上搭建HBase集群环境]]></title>
      <url>%2Fpost%2Fsetup_hbase%2F</url>
      <content type="text"><![CDATA[本文是在安装完成Hadoop的基础之上进行的，Hadoop的安装戳这里。本文采用的Hadoop版本为0.20.2，HBase版本为0.90.6，ZooKeeper的版本为3.3.2（stable版）。本文仍然采用了Hadoop安装的环境，机器如下： 机器名 IP地址 用途 Hadoop模块 HBase模块 ZooKeeper模块 server206 192.168.20.6 Master NameNode、JobTracker、SecondaryNameNode HMaster QuorumPeerMain ap1 192.168.20.36 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain ap2 192.168.20.38 Slave DataNode、TaskTracker HRegionServer QuorumPeerMain 安装ZooKeeper由于HBase默认集成了ZooKeeper，可以不用单独安装ZooKeeper。本文采用独立安装ZooKeeper的方式。1. 将zookeeper解压到/home/hadoop目录下。2. 将/home/hadoop/zookeeper-3.3.2/conf目录下的zoo_sample.cfg文件拷贝一份，命名为为“zoo.cfg”。3. 修改zoo.cfg文件，修改后内容如下：12345678910111213141516# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.dataDir=/home/hadoop/zookeeper-3.3.2/zookeeper_data# the port at which the clients will connectclientPort=2181dataLogDir=/home/hadoop/zookeeper-3.3.2/logsserver.1=192.168.20.6:2888:3888server.2=192.168.20.36:2888:3888server.3=192.168.20.38:2888:3888 其中，2888端口号是zookeeper服务之间通信的端口，而3888是zookeeper与其他应用程序通信的端口。这里修改了dataDir和dataLogDir的值。需要特别注意的是：如果要修改dataDir的值不能将原来的行在前面加个“#”注释掉后在后面再增加一行，这样是不起作用的。可以参考bin目录下的zkServer.sh文件中的72行，内容如下：1ZOOPIDFILE=$(grep dataDir &quot;$ZOOCFG&quot; | sed -e &apos;s/.*=//&apos;)/zookeeper_server.pid 这里通过grep和sed命令来获取dataDir的值，对于行前面添加“#”注释是不起作用的。4. 创建zoo.cfg文件中的dataDir和dataLogDir所指定的目录。5. 在dataDir目录下创建文件myid。6. 通过scp命令将zookeeper-3.3.2目录拷贝到其他节点机上。7. 修改myid文件，在对应的IP的机器上输入对应的编号，该编号要和zoo.cfg中的一致。本例中在192.168.20.6上文件内容为1；在192.168.20.36上文件内容为2；在192.168.20.38上文件内容为3。至此ＺooＫeeper的安装已经完成。 运行ZooKeeper1. 在节点机上依次执行/home/hadoop/zookeeper-3.3.2/bin/zkServer.sh start脚本。运行第一个ZooKeeper的时候会因等待其他节点而出现刷屏现象，等启动起第二个节点上的ZooKeeper后就正常了。运行完成之后该脚本会出现刷屏现象，我这里没有理会。2. 通过jps命令来查看各节点机上是否含有QuorumPeerMain进程。3. 通过/home/hadoop/zookeeper-3.3.2/bin/zkServer.sh status命令来查看状态。本例中有三个节点机，其中必有一个leader，两个follower存在。4. 在各节点机上依次执行/home/hadoop/zookeeper-3.3.2/bin/zkServer.sh stop来停止ZooKeeper服务。 配置时间同步ntp服务HBase在运行的时候各个节点之间时间不同步会存在莫名其妙的问题，这里选择以192.168.20.36机器作为时间同步服务器，其他机器从该机器同步时间。在192.168.20.36上通过service ntpd start命令来启动ntp服务。在其他机器上配置crontab命令，增加下面一行：10 */1 * * * /usr/sbin/ntpdate 192.168.20.36 &amp;&amp; /sbin/hwclock -w 这里采用一个小时同步一次时间的方式。 安装HBase1. 在HMaster机器上将HBase解压到/home/hadoop目录下。2. 修改配置文件hbase-env.sh，使export HBASE_MANAGES_ZK=false。如果想要HBase使用自带的ZooKeeper则使用设置为true。使export JAVA_HOME=/usr/java/jdk1.6.0_10来指定java的安装路径。3. 修改配置文件hbase-site.xml如下：12345678910111213141516171819202122232425262728293031323334&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://server206:9000/hbase&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;60000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;server206,ap1,ap2&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/hadoop/zookeeper-3.3.2/zookeeper_data&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.support.append&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.handler.count&lt;/name&gt; &lt;value&gt;100&lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; 4. 修改regionservers，添加HRegionServer模块所运行机器的主机名。在本例中内容如下：12ap1ap2 5. 为了确保HBase和Hadoop的兼容性，这里将/home/hadoop/hadoop-0.20.2/hadoop-0.20.2-core.jar文件复制到/home/hadoop/hbase-0.90.6/lib目录下，并将原先的Hadoop的jar文件删掉或重命名为其他后缀的文件。6. 将hbase-0.90.6文件夹通过scp命令复制到其他节点机上。 HBase的启动 在Hadoop的NameNode所在的机器上使用start-all.sh脚本来启动Hadoop集群。 在各个节点机上调用zkServer.sh脚本来启动ZooKeeper。 在HMaster所在的机器上使用start-hbase.sh脚本来启动HBase集群。 HBase启动后会在HDFS自动创建/hbase的文件夹，可以通过hadoop fs -ls /hbase命令来查看，该目录不需要自动创建。如果在安装HBase的过程中失败需要重新启动，最好将此目录从集群中删除，通过命令hadoop fs -rmr /hbase来删除。 需要特别注意的是在hadoop集群中hadoop fs -ls /hbase目录和hadoop fs -ls hbase目录并非一个目录，通过hadoop fs -ls hbase查看到的目录实际上为/user/hadoop/hbase目录。 HBase管理界面Master的界面：http://192.168.20.6:60010/master.jspRegionServer的界面：http://192.168.20.36:60030/regionserver.jsp和http://192.168.20.38:60030/regionserver.jsp 参考资料 http://blog.csdn.net/gudaoqianfu/article/details/7327191 http://thomas0988.iteye.com/blog/1309867 下载链接http://pan.baidu.com/share/link?shareid=1235031445&amp;uk=3506813023 提取码：v8ok]]></content>
    </entry>

    
  
  
</search>
